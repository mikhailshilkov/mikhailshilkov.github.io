<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <generator>Hugo -- gohugo.io</generator>
    <title>Mikhail Shilkov</title><link href="https://mikhail.io/tags/browsersync/feed" rel="self" />
    <link href="https://mikhail.io/tags/browsersync/"/>
    <updated>2021-11-30T17:06:37-08:00</updated>
    <author>
            <name>Mikhail Shilkov</name>
            </author>
    <id>https://mikhail.io/tags/browsersync/</id>
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[2021s]]></title>
            <link href="https://mikhail.io/2021/"/>
            <id>https://mikhail.io/2021/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Deploying new Azure Container Apps with familiar languages]]></title>
            <link href="https://mikhail.io/2021/11/azure-container-apps/"/>
            <id>https://mikhail.io/2021/11/azure-container-apps/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Use the Pulumi Azure Native Provider to deploy containerized apps to Microsoft&rsquo;s new Azure Container Apps platform for serverless apps.</blockquote><p>Today, Microsoft <a href="https://aka.ms/containerapps/ignite-blog">announced</a> a new general-purpose serverless container platform: <a href="https://aka.ms/containerapps/">Azure Container Apps</a>. Container Apps is a fully managed platform for microservice applications that runs on top of Kubernetes and open-source technologies like KEDA, Envoy, and Dapr.</p>
<p>Container Apps are designed to abstract infrastructure management with flexible serverless containers. Developers can run containers at scale without the burden of standing up and managing a Kubernetes cluster manually.</p>
<p>We are happy to announce same-day support for Azure Container Apps in the Pulumi <a href="https://pulumi.com/registry/packages/azure-native">Azure Native Provider</a>, which covers 100% of the Azure Resource Manager APIs and gives you highest fidelity integration with Azure&rsquo;s resources.</p>
<p>The new service supports a broad range of usage scenarios, including</p>
<ul>
<li>Microservices over HTTP or gRPC</li>
<li>HTTP APIs and websites</li>
<li>Event processing workers</li>
<li>Long-running background jobs</li>
</ul>
<h2 id="serverless-containers">Serverless containers</h2>
<p>Container Apps provide serverless containers for microservice developers.</p>
<ul>
<li><strong>Managed infrastructure</strong>. Microsoft operates the control plane that takes care of orchestrating containers and their configuration, allowing developers to focus on apps, not cloud infrastructure.</li>
<li><strong>Fully integrated auto-scaling with scale to zero</strong>. The platform relies on Kubernetes Event-driven Autoscaling (KEDA) to scale apps and microservices dynamically based on HTTP traffic or event workload.</li>
<li><strong>Consumption pricing</strong>. The billing model is based on the actual resource consumption with per-second granularity. Applications incur charges per request, compute time and memory used with no need to pre-provision a fixed capacity.</li>
<li><strong>Any language, any base image</strong>. Container Apps put no limitation on the container images. You may use an arbitrary base image and host any web server or a console application, ensuring flexibility and interoperability with other container orchestrators.</li>
</ul>
<h2 id="how-it-works">How it works</h2>
<p>Microsoft has built Container Apps as a managed service on the foundation of open-source technology in the Kubernetes ecosystem. This enables teams to build microservices without having to manage Kubernetes directly while providing application portability by leveraging open standards and APIs. Behind the scenes, every container app runs on the <a href="https://azure.microsoft.com/en-us/overview/kubernetes-on-azure/">Azure Kubernetes Service (AKS)</a>. Enabling open-source integrations include:</p>
<p><strong>Envoy</strong>, a built-in ingress controller that exposes user containers internally and externally via HTTP endpoints. It supports multiple container versions with dynamic load balancing and several traffic rollout strategies.</p>
<p><strong>KEDA</strong> enables dynamic auto-scaling for user applications based on the current workload, including scaling down to zero during idle periods.</p>
<p>Container Apps are also tightly integrated with <strong>Dapr</strong>—an open-source runtime system designed to support cloud-native microservice applications. It provides extra blocks for service discovery, state management, asynchronous message passing.</p>
<h2 id="how-its-different">How it&rsquo;s different</h2>
<p>A few other services provided by Microsoft Azure and other cloud providers operate in the space of running container-based workloads:</p>
<p><strong>Azure Kubernetes Service</strong> delivers the full power of Kubernetes but requires expertise in configuring and operating the cluster. When building on AKS, users handle most infrastructure management aspects themselves. Instead, Container Apps provide a platform built on top of AKS, focusing on developer productivity and switching to consumption-based pricing.</p>
<p><strong>Azure Container Instances</strong> (ACI) provides atomic infrastructure units with per-usage pricing. However, ACI comes without higher-level functionality like auto-scaling, load balancing, versioning, and managed rollouts.</p>
<p><strong>Azure App Service</strong> is a Platform-as-a-Service comparable to Container Apps in terms of being simple-to-operate and developer-friendly. App Service can also run arbitrary containers. However, it is best suited to run websites. The billing model is mostly capacity-based with some built-in autoscaling but without scaling to zero.</p>
<p><strong>Azure Functions</strong> is a developer-oriented truly-serverless offering. However, it comes with an opinionated programming model that is focused on achieving a high developer productivity as long as your application can use the Azure Functions SDK or container base images. Unlike Container APps, it does not support long-running jobs in consumption mode.</p>
<p>Comparing to other vendors: Azure Container Apps are in the same space as <strong>Google Cloud Run</strong> and <strong>AWS App Runner</strong>. In contrast to the competition, Azure Container Apps is built on Kubernetes and related open-source projects, which should benefit its users in terms of interoperability and transparency. Additionally, Container Apps is the only service that provides features like service discovery for microservices-style communication out of the box.</p>
<h2 id="example-run-an-http-api-with-azure-container-apps-and-pulumi">Example: Run an HTTP API with Azure Container Apps and Pulumi</h2>
<p>Let&rsquo;s walk through the steps to build an example application with Azure Container Apps using infrastructure as code in familiar languages. In this scenario, we create an HTTP application that is available via a public domain name. We&rsquo;ll use Pulumi to provision the necessary resources. In this example, we will use TypeScript however you could also use JavaScript, Python, Go, and C#.</p>
<p>You can check out the complete source code in the Pulumi Examples:</p>
<ul>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-ts-containerapps">TypeScript Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-cs-containerapps">C# Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-py-containerapps">Python Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-go-containerapps">Go Azure Container Apps Example</a></li>
</ul>
<h3 id="define-a-dockerfile-and-app">Define a Dockerfile and app</h3>
<p>Here are the key features of the container image for our application:</p>
<ul>
<li>Based on the <code>node:8.9.3-alpine</code> image</li>
<li>Installs <code>express</code> with NPM</li>
<li>Runs a node web app using <code>index.js</code> and <code>index.html</code> user files</li>
</ul>
<p><a href="https://github.com/pulumi/examples/tree/master/azure-ts-containerapps/node-app/Dockerfile">Full Dockerfile</a>.</p>
<h3 id="set-up-the-environment">Set up the environment</h3>
<p>The resource <code>KubeEnvironment</code> defines a cluster that can host multiple Container Apps. Behind the scenes, it creates an AKS cluster in a subscription managed internally by Microsoft and deploys the Apps control plane.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> web <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure-native/web/v20210301&#34;</span>;

<span style="color:#00f">const</span> env = <span style="color:#00f">new</span> web.KubeEnvironment(<span style="color:#a31515">&#34;env&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;Managed&#34;</span>,
});
</code></pre></div><h3 id="build-and-publish-a-container-image">Build and publish a container image</h3>
<p>We can build the Docker image and publish it to a new Azure Container Registry (ACR) repository. The code below assumes</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> docker <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/docker&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> containerregistry <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure-native/containerregistry&#34;</span>;

<span style="color:#00f">const</span> customImage = <span style="color:#a31515">&#34;node-app&#34;</span>;
<span style="color:#00f">const</span> registry = <span style="color:#00f">new</span> containerregistry.Registry(<span style="color:#a31515">&#34;registry&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   sku: {
       name: <span style="color:#a31515">&#34;Basic&#34;</span>,
   },
   adminUserEnabled: <span style="color:#2b91af">true</span>,
});

<span style="color:#00f">const</span> credentials = containerregistry.listRegistryCredentialsOutput({
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    registryName: <span style="color:#2b91af">registry.name</span>,
});
<span style="color:#00f">const</span> adminUsername = credentials.apply(c =&gt; c.username!);
<span style="color:#00f">const</span> adminPassword = credentials.apply(c =&gt; c.passwords![0].value!);

<span style="color:#00f">const</span> myImage = <span style="color:#00f">new</span> docker.Image(customImage, {
   imageName: <span style="color:#2b91af">pulumi.interpolate</span><span style="color:#a31515">`</span><span style="color:#a31515">${</span>registry.loginServer<span style="color:#a31515">}</span><span style="color:#a31515">/</span><span style="color:#a31515">${</span>customImage<span style="color:#a31515">}</span><span style="color:#a31515">:v1.0.0`</span>,
   build: { context: <span style="color:#a31515">`./</span><span style="color:#a31515">${</span>customImage<span style="color:#a31515">}</span><span style="color:#a31515">`</span> },
   registry: {
       server: <span style="color:#2b91af">registry.loginServer</span>,
       username: <span style="color:#2b91af">adminUsername</span>,
       password: <span style="color:#2b91af">adminPassword</span>,
   },
});
</code></pre></div><h2 id="deploy-the-container-app">Deploy the container app</h2>
<p>Finally, we can define the Container App itself. We point the App to the environment resource and instruct it to run our custom image. Image container credentials are specified in the <code>configuration</code> block, with the password marked as a secret. We&rsquo;ve also enabled external ingress to publish the app on the web.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> containerApp = <span style="color:#00f">new</span> web.ContainerApp(<span style="color:#a31515">&#34;app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   kubeEnvironmentId: <span style="color:#2b91af">env.id</span>,
   configuration: {
       ingress: {
           external: <span style="color:#2b91af">true</span>,
           targetPort: <span style="color:#2b91af">80</span>,
       },
       registries: [{
           server: <span style="color:#2b91af">registry.loginServer</span>,
           username: <span style="color:#2b91af">adminUsername</span>,
           passwordSecretRef: <span style="color:#a31515">&#34;pwd&#34;</span>,
       }],
       secrets: [{
           name: <span style="color:#a31515">&#34;pwd&#34;</span>,
           value: <span style="color:#2b91af">adminPassword</span>,
       }],
   },
   template: {
       containers: [{
           name: <span style="color:#a31515">&#34;myapp&#34;</span>,
           image: <span style="color:#2b91af">myImage.imageName</span>,
       }],
   }
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> url = pulumi.interpolate<span style="color:#a31515">`https://</span><span style="color:#a31515">${</span>containerApp.configuration.ingress.fqdn<span style="color:#a31515">}</span><span style="color:#a31515">`</span>;
</code></pre></div><h3 id="test-the-app">Test the app</h3>
<p>And that is it! We run <code>pulumi up</code> to get the application up and running. Once the deployment completes, we can send an HTTP request and check the response.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ curl <span style="color:#00f">$(</span>pulumi stack output url<span style="color:#00f">)</span>
&lt;html&gt;
&lt;body&gt;
&lt;h1&gt;Your custom docker image is running in Azure Container Apps!&lt;/h1&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>Azure Container Apps enable you to build applications in your favorite language with any dependencies and tools, package them as a container image, and deploy them in seconds. Container Apps abstract away infrastructure management by automatically scaling up and down to zero and only charging for the exact resources you use.</p>
<p>This post shows how to use Pulumi to build a container image and publish it as an Azure Container App. Pulumi makes it easy to create artifacts and provision and manage cloud infrastructure on any cloud using familiar programming languages, including C#, TypeScript, Python, and Go. Docker images, ACR registries, container environments, and Apps can be managed within the same infrastructure definition.</p>
<h4 id="complete-azure-container-apps-example">Complete Azure Container Apps example</h4>
<ul>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-ts-containerapps">TypeScript Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-cs-containerapps">C# Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-py-containerapps">Python Azure Container Apps Example</a></li>
<li><a href="https://github.com/pulumi/examples/tree/master/azure-go-containerapps">Go Azure Container Apps Example</a></li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                            
                        
                    
                
            
        </entry>
    
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[Performance]]></title>
            <link href="https://mikhail.io/tags/performance/"/>
            <id>https://mikhail.io/tags/performance/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Tags]]></title>
            <link href="https://mikhail.io/tags/"/>
            <id>https://mikhail.io/tags/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Temporal]]></title>
            <link href="https://mikhail.io/tags/temporal/"/>
            <id>https://mikhail.io/tags/temporal/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Workflows]]></title>
            <link href="https://mikhail.io/tags/workflows/"/>
            <id>https://mikhail.io/tags/workflows/</id>
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Choosing the Number of Shards in Temporal History Service]]></title>
            <link href="https://mikhail.io/2021/05/choose-the-number-of-shards-in-temporal-history-service/"/>
            <id>https://mikhail.io/2021/05/choose-the-number-of-shards-in-temporal-history-service/</id>
            
            <published>2021-05-25T00:00:00+00:00</published>
            <updated>2021-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Tuning the sharding configuration for the optimal cluster performance with the numHistoryShards config.</blockquote><p>Today, I&rsquo;m diving into the topic of tuning your Temporal clusters for optimal performance. More specifically, I will be discussing the single configuration option: the number of History service shards. Through a series of experiments, I’ll explain how a low number of shards can lead to contention, while a large number can cause excessive resource consumption.</p>
<p>All the experimental data is collected with <a href="https://github.com/temporalio/maru/">Maru</a>—the open-source Temporal load simulator and benchmarking tool.</p>
<p>Let&rsquo;s start with a hypothetical scenario that illustrates the struggle of a misconfigured Temporal cluster.</p>
<h2 id="symptom-contention-in-history-service">Symptom: Contention in History Service</h2>
<p>Imagine you are benchmarking your Temporal deployment. You are not entirely happy with the throughput yet, so you look into performance metrics. Storage utilization seems relatively low, and storage latency is excellent. CPU usage of Temporal services is low too. Where is the bottleneck?</p>
<p>You start looking at response times and notice that the History service has high latency. Why is that the case? If the storage latency is good, what is the History service waiting for?</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="latency-pattern.png"
            alt="Example of a problematic latency pattern"
             />
        
    
    <figcaption>
        <h4>Example of a problematic latency pattern</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>If the latencies in your system look somewhat like the picture above, your cluster is probably not configured appropriately for the current workload. You are likely in a situation of lock contention due to an insufficient number of History shards.</p>
<h2 id="theory-history-shards">Theory: History Shards</h2>
<p>Temporal provides high guarantees around Workflow consistency. To achieve this goal, Temporal allocates a single host of the History service to process all events of a given Workflow execution.</p>
<p>Each Workflow execution receives a unique identifier. Temporal calculates a hash for each identifier and allocates it to a specific <strong>shard</strong> based on the hash value. A shard is represented as a number from 1 to N. Executions with the same shard value are processed by the same host.</p>
<p>The History service has a configuration value <code>numHistoryShards</code> to define the total number of shards, set once and forever. Each instance of the History service obtains a lock on multiple shards and starts processing Workflow executions that belong to them.</p>
<p>Shards are logically independent and isolated from each other. To achieve the consistency requirements with Cassandra, Temporal serializes all updates belonging to the same shard.</p>
<p>Therefore, a shard is a unit of parallelism. Since each shard executes all updates under a single lock, all updates are sequential. The maximum theoretical throughput of a single shard is limited by the latency of a database operation. For example, if a single database update takes 10 ms, then a single shard can do a maximum of 100 updates per second. If you try to push more updates, you will see the History service latencies go up without database latency going up. Precisely the case illustrated in the chart above!</p>
<h2 id="experiment-observe-history-service-contention">Experiment: Observe History Service Contention</h2>
<p>Let&rsquo;s design an experiment to illustrate the impact of the number of History shards on cluster throughput and latency. I created a test Kubernetes cluster with three node pools: one for Cassandra, one for Elasticsearch, and one for Temporal services and workers. I deployed the datastores and Temporal cluster. For the sake of simplicity, every Temporal service has just one pod.</p>
<p>Then, using <a href="https://github.com/temporalio/maru/">Maru</a>, I run a benchmark of 25,000 workflow executions to collect metrics around processing speed, response latency, and resource utilization. I conduct the same experiment several times but tweak just one configuration parameter: the number of History service shards.</p>
<h3 id="single-shard">Single Shard</h3>
<p>Let&rsquo;s start with a base case of just a single shard. All Workflow executions are assigned to the same shard and have to be serialized. As expected, the system throughput is quite limited, and it takes 28 minutes to complete the scenario:</p>
<figure >
    
        <img src="1-workflow-rate.png"
            alt="Workflow processing rates for a single-shard configuration"
             />
        
    
    <figcaption>
        <h4>Workflow processing rates for a single-shard configuration</h4>
    </figcaption>
    
</figure>
<p>Due to the interference of read and write operations, the processing rate starts really low but goes up as the backlog decreases. This behavior makes sense for the high-contention scenario.</p>
<p>Here is the latency chart of the History service versus the underlying Cassandra database:</p>
<figure >
    
        <img src="1-latency.png"
            alt="Database and service latencies for a single-shard configuration"
             />
        
    
    <figcaption>
        <h4>Database and service latencies for a single-shard configuration</h4>
    </figcaption>
    
</figure>
<p>Cassandra&rsquo;s response time is very close to zero as compared to the History service response time. The vast majority of the latter is internal contention.</p>
<p>The CPU utilization reasonably fairly low too:</p>
<figure >
    
        <img src="1-cpu.png"
            alt="Database and service CPU usage for a single-shard configuration"
             />
        
    
    <figcaption>
        <h4>Database and service CPU usage for a single-shard configuration</h4>
    </figcaption>
    
</figure>
<p>Both the database and the service are underutilized, as the cluster spends the majority of time waiting.</p>
<h3 id="increasing-the-number-of-shards">Increasing the Number of Shards</h3>
<p>I re-ran the exact same experiment with 4, 8, 64, 512, and 4096 shards and correlated the shard number with the key metrics. Note that the horizontal axis is out of scale on all the charts below.</p>
<p>The processing time goes down as the number of shards grows:</p>
<figure >
    
        <img src="processing-time.png"
            alt="Total time to process the workflows as a function of the number of shards"
             />
        
    
    <figcaption>
        <h4>Total time to process the workflows as a function of the number of shards</h4>
    </figcaption>
    
</figure>
<p>The History service latency gets closer and closer to the persistence latency as the former decreases and the latter increases due to a higher load:</p>
<figure >
    
        <img src="latency.png"
            alt="Database and service latencies as functions of the number of shards"
             />
        
    
    <figcaption>
        <h4>Database and service latencies as functions of the number of shards</h4>
    </figcaption>
    
</figure>
<p>The CPU utilization is pretty good for all experiments with 8 or more shards.</p>
<figure >
    
        <img src="cpu.png"
            alt="Database and service CPU usage as functions of the number of shards"
             />
        
    
    <figcaption>
        <h4>Database and service CPU usage as functions of the number of shards</h4>
    </figcaption>
    
</figure>
<p>Adding more and more shards has diminishing returns: the difference between 1 and 4 or 4 and 8 is much more pronounced than 8 to 64 or 512 to 4096. However, it still looks like more shards are universally better than fewer shards. Would it be reasonable to set the shard number to an arbitrarily large number and be done with it?</p>
<h2 id="too-many-shards">Too Many Shards</h2>
<p>Can there be too many shards?</p>
<p>Yes. Each shard consumes resources of History service pods and performs some background processing on the database. So, the system pays an overhead fee for each additional shard. Also, an excessive number of shards slows down node recovery as each shard has to load its state from the database on redistribution after a service host goes down.</p>
<p>The next chart shows the distribution of memory consumption of the History service host across all experiments.</p>
<figure >
    
        <img src="memory.png"
            alt="Service memory consumption as functions of the number of shards"
             />
        
    
    <figcaption>
        <h4>Service memory consumption as functions of the number of shards</h4>
    </figcaption>
    
</figure>
<p>In fact, I tried running the experiment with 32.768 shards, but it utterly failed because the History service pod consumed too much memory, and the controller kept evicting it.</p>
<h2 id="conclusions">Conclusions</h2>
<p>The shard number of the History service is a critical configuration of a Temporal cluster. The configured value has a direct impact on throughput, latency, and resource utilization of the system.</p>
<p>It&rsquo;s essential to get this configuration right for your setup and your performance targets. Even more so because the value can&rsquo;t be changed after the initial cluster deployment.</p>
<p><a href="https://github.com/temporalio/maru/">Maru</a> is a performance testing tool that can help you determine the optimal configuration value empirically. You can use Maru to run multiple experiments with your cluster and application code, collect the results, and decide before going into production. The data in this article was collected with Maru.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Maru: Load Testing Tool for Temporal Workflows]]></title>
            <link href="https://mikhail.io/2021/03/maru-load-testing-tool-for-temporal-workflows/"/>
            <id>https://mikhail.io/2021/03/maru-load-testing-tool-for-temporal-workflows/</id>
            
            <published>2021-03-18T00:00:00+00:00</published>
            <updated>2021-03-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Benchmarking Temporal deployments with a simple load simulator tool</blockquote><p><a href="https://temporal.io">Temporal</a> enables developers to build highly reliable applications without having to worry about all the edge cases. If you are new to Temporal, check out my article <a href="https://mikhail.io/2020/10/temporal-open-source-workflows-as-code/">Open Source Workflows as Code</a>.</p>
<p>Temporal is an open-source project, a system <a href="https://mikhail.io/2020/10/practical-approach-to-temporal-architecture/">consisting of several components</a>. While a managed Temporal service is coming, all the existing Temporal end-users administer and operate deployments themselves. Typically, the Temporal services run in a Kubernetes cluster. I described a sample Temporal installation in <a href="https://mikhail.io/2020/11/how-to-deploy-temporal-to-azure-kubernetes-aks/">How To Deploy Temporal to Azure Kubernetes Service (AKS)</a>.</p>
<p>Many Temporal newcomers have a reasonable question: <em>&ldquo;What kind of infrastructure setup do I need to handle my target workload? What is the right capacity for a Kubernetes cluster, a persistent datastore, server components, and workers?”</em></p>
<p>So a key goal became helping users answer the above question. I decided to create a load simulating tool that could be a practical first step in the capacity planning process. Here is the primary idea:</p>
<ul>
<li>Choose a supported persistence solution and deploy it or pay someone to run it (ideal)</li>
<li>Build your Kubernetes cluster and deploy Temporal services to it.</li>
<li>Deploy a <strong>target</strong> workflow that is a simplified representation of your real-life workflows.</li>
<li>Also, deploy the <strong>bench</strong> workflow that I created. This specialized Temporal workflow orchestrates a load-test scenario, invokes the target workflow accordingly, and collects the execution statistics.</li>
</ul>
<p>The bench workflow can give you the first approximation of whether the current cluster size would be sufficient for the target workload. For more detailed insights, you can also use tools like Prometheus and Grafana to observe the metrics and find issues.</p>
<p>Based on the statistics, you find a bottleneck, tweak the cluster size, persistence configuration, number of service instances, workers, shards, or a combination of those. Then, you re-run the tests and obtain the new results. Repeat the process until you are happy with the outcome.</p>
<p>Meet <a href="https://github.com/temporalio/maru/">Maru</a>—an open-source load simulating tool for Temporal! You can give it a try today. The rest of this blog post describes the steps to get from nothing to the first benchmark results.</p>
<h2 id="build-a-temporal-cluster">Build a Temporal Cluster</h2>
<p>A working Temporal Cluster is the first thing you need to run the benchmark. If you already have one, skip to the next section. Otherwise, you can build a sample AKS cluster with a compatible deployment of Temporal using <a href="https://github.com/temporalio/maru/tree/master/pulumi">this Pulumi program</a>.</p>
<p>For this blog post, I&rsquo;m using a sample deployment that provisions the following resources:</p>
<ul>
<li><strong>Azure Kubernetes Service</strong> managed cluster with three nodes of size <code>Standard_DS2_v2</code> (2 vCore, 7 GB RAM).</li>
<li><strong>Temporal Helm chart</strong> with all defaults, except Kafka is disabled. The Temporal service uses 512 history shards.</li>
<li>In-cluster <strong>Cassandra</strong> as the datastore.</li>
<li><strong>Elasticsearch</strong> as the visibility store. Note: Elasticsearch is currently required for the benchmark.</li>
<li><strong>Prometheus</strong> and <strong>Grafana</strong>. I won&rsquo;t use those in this post, but they come in handy for performance deep-dives.</li>
</ul>
<p>This setup is pretty basic and does not resemble a realistic production-grade deployment. It’s simply good enough for illustration purposes.</p>
<p>The next step is to define the target scenario for the load-testing.</p>
<h2 id="target-workflow">Target Workflow</h2>
<p>The bench comes with a simple &ldquo;basic&rdquo; workflow that executes several sequential activities. The number of activities is configurable; I set it to three for the test run.</p>
<p>The basic workflow is just an example of a workflow that generates a very predictable workload. It makes sense to start with it, but you should adjust the target scenario to be closer to a real-life workload later on. There&rsquo;s no specific interface that such a workflow has to match: you can tweak it to your needs.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#008000">// Workflow implements a basic bench scenario to schedule activities in sequence.
</span><span style="color:#008000"></span><span style="color:#00f">func</span> Workflow(ctx workflow.Context, request workflowRequest) (<span style="color:#2b91af">string</span>, <span style="color:#2b91af">error</span>) {
   ao := workflow.ActivityOptions{ TaskQueue: common.TaskQueue }
   ctx = workflow.WithActivityOptions(ctx, ao)
   <span style="color:#00f">for</span> i := 0; i &lt; request.SequenceCount; i++ {
       <span style="color:#00f">var</span> result <span style="color:#2b91af">string</span>
       err := workflow.ExecuteActivity(ctx, <span style="color:#a31515">&#34;basic-activity&#34;</span>, request.ActivityDurationMilliseconds).Get(ctx, &amp;result)
       <span style="color:#00f">if</span> err != <span style="color:#00f">nil</span> {
           <span style="color:#00f">return</span> <span style="color:#a31515">&#34;&#34;</span>, err
       }
   }
   <span style="color:#00f">return</span> request.ResultPayload, <span style="color:#00f">nil</span>
}

<span style="color:#008000">// Activity sleeps and returns the result.
</span><span style="color:#008000"></span><span style="color:#00f">func</span> Activity(ctx context.Context, req basicActivityRequest) (<span style="color:#2b91af">string</span>, <span style="color:#2b91af">error</span>) {
   time.Sleep(time.Duration(req.ActivityDelayMilliseconds) * time.Millisecond)   
   <span style="color:#00f">return</span> req.ResultPayload, <span style="color:#00f">nil</span>
}
</code></pre></div><h2 id="bench-workflow">Bench Workflow</h2>
<p>A simple configuration file drives the bench workflow. The file specifies the target workflow and the load scenario: how many executions to start per second and the total number of executions.</p>
<p>The bench has four sequential phases:</p>
<ul>
<li><strong>Driver</strong> schedules target workflow executions according to the configured scenarios.</li>
<li><strong>Monitor</strong> queries the Temporal visibility database for open target workflows and waits until they all complete.</li>
<li><strong>Statistic</strong> queries all target workflow executions to collect the time they started and closed. It places each start and each close event to a bucket and builds a histogram.</li>
<li><strong>Query</strong> provides a query for users to retrieve the statistics.</li>
</ul>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#00f">func</span> (w *benchWorkflow) run() <span style="color:#2b91af">error</span> {
   startTime := workflow.Now(w.ctx)
   <span style="color:#00f">for</span> i, step := <span style="color:#00f">range</span> w.request.Steps {
       <span style="color:#00f">if</span> err := w.executeDriverActivities(i, step); err != <span style="color:#00f">nil</span> {
           <span style="color:#00f">return</span> err
       }
   }
   res, err := w.executeMonitorStatisicsActivity(startTime)
   <span style="color:#00f">if</span> err != <span style="color:#00f">nil</span> {
       <span style="color:#00f">return</span> err
   }
   <span style="color:#00f">if</span> err = w.setupQueries(res); err != <span style="color:#00f">nil</span> {
       <span style="color:#00f">return</span> err
   }
   <span style="color:#00f">return</span> <span style="color:#00f">nil</span>
}
</code></pre></div><h2 id="worker">Worker</h2>
<p>We deploy both target and bench as Temporal workflows to the same cluster. In my setup, they both run on the same worker (a Kubernetes deployment), but they could easily be split to separate workers. They do need to connect to the same Temporal service.</p>
<p>Now that I have my simple AKS cluster, a Temporal server, and workflows deployed to it. I&rsquo;m ready to run my first benchmark!</p>
<h2 id="run-1-10-executions-per-second">Run 1: 10 Executions per Second</h2>
<p>For my first run, I have a test that runs for five minutes and creates ten workflow executions per second (3000 total). Here is the scenario definition for the bench:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;steps&#34;: [{
        &#34;count&#34;: 3000,
        &#34;ratePerSecond&#34;: 10
    }],
    &#34;workflow&#34;: {
        &#34;name&#34;: <span style="color:#a31515">&#34;basic-workflow&#34;</span>,
        &#34;args&#34;: {
            &#34;sequenceCount&#34;: 3
        }
    },
    &#34;report&#34;: {
        &#34;intervalInSeconds&#34;: 10
    }
}
</code></pre></div><p>The <code>steps[0].count</code> is calculated as <code>10 wf/sec * 60 sec/min * 5 min = 3,000 wf</code>.</p>
<p>I start the benchmark by running this command (read <a href="https://docs.temporal.io/docs/tctl/">here</a> how to connect the CLI to your cluster):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tctl wf start --tq temporal-bench --wt bench-workflow --wtt 5 --et 1800 --if ./scenarios/run1.json --wid run1
</code></pre></div><p>Then, I wait for five minutes and query the results.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tctl wf query --qt histogram_csv --wid run1
</code></pre></div><p>The response of the query is a CSV file with the bench statistics: see the file here. I can import this file to Google Spreadsheets and visualize it in a chart like this one:</p>
<p><img src="./run1.png" alt="Run 1 Results"></p>
<p>You can see that the test ran for five minutes, and the bench started workflows at the constant rate of 10 per second. They were also closed at the same speed with only a handful of open executions at any moment.</p>
<p>This test clearly shows that my current deployment is sufficient to handle this type of workload.</p>
<h2 id="run-2-100-executions-per-second">Run 2: 100 Executions per Second</h2>
<p>Let&rsquo;s bump the rate to 100 per second (10x compared to Run 1). Here is the scenario file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;steps&#34;: [{
        &#34;count&#34;: 10000,
        &#34;ratePerSecond&#34;: 100
    }],
    &#34;workflow&#34;: {
        &#34;name&#34;: <span style="color:#a31515">&#34;basic-workflow&#34;</span>,
        &#34;args&#34;: {
            &#34;sequenceCount&#34;: 3
        }
    },
    &#34;report&#34;: {
        &#34;intervalInSeconds&#34;: 10
    }
}
</code></pre></div><p>Using the same command sequence as above, I get the following chart of started and closed workflows.</p>
<p><img src="./run2rates.png" alt="Run 2 Rates"></p>
<p>You can see that it&rsquo;s not smooth anymore. The bench failed to start 100 workflow executions per second consistently, and the processing rate oscillated around 35 executions per second. This delay caused the backlog to grow while new executions were being added continually. The backlog started to decrease only after the bench stopped scheduling executions.</p>
<p><img src="./run2backlog.png" alt="Run 2 Backlog"></p>
<p>Clearly, the system in its current configuration isn&rsquo;t capable of processing this workload. I would guess this may be caused by contention in the datastore (Cassandra), but that&rsquo;s a topic for another blog post.</p>
<p>Nonetheless, all the executions eventually succeeded, and no work has been lost. Temporal retained its high-reliability guarantees even with a severely underprovisioned cluster. Great job!</p>
<p>I want to emphasize that this test does not show any fundamental limitations of the Temporal server. I ran it in a small cluster with all the default configuration knobs, which means there was a single instance of each Temporal server component and a single worker, for example.</p>
<h2 id="how-you-can-get-started">How You Can Get Started</h2>
<p><a href="https://github.com/temporalio/maru">Maru, a Temporal Load Simulator tool</a> is now open-source. I encourage everyone to give it a try to benchmark their own setups. Please file an issue if you have ideas on how to make the tool better.</p>
<p>I&rsquo;m planning to describe how the benchmark works, tune underperforming clusters, and conduct other experiments in my follow-up blog posts. Stay tuned!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[AWS]]></title>
            <link href="https://mikhail.io/tags/aws/"/>
            <id>https://mikhail.io/tags/aws/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[AWS Lambda]]></title>
            <link href="https://mikhail.io/tags/aws-lambda/"/>
            <id>https://mikhail.io/tags/aws-lambda/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[Azure]]></title>
            <link href="https://mikhail.io/tags/azure/"/>
            <id>https://mikhail.io/tags/azure/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions]]></title>
            <link href="https://mikhail.io/tags/azure-functions/"/>
            <id>https://mikhail.io/tags/azure-functions/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[C#]]></title>
            <link href="https://mikhail.io/tags/csharp/"/>
            <id>https://mikhail.io/tags/csharp/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts]]></title>
            <link href="https://mikhail.io/tags/cold-starts/"/>
            <id>https://mikhail.io/tags/cold-starts/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts in AWS Lambda]]></title>
            <link href="https://mikhail.io/serverless/coldstarts/aws/"/>
            <id>https://mikhail.io/serverless/coldstarts/aws/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Selection of languages, instance sizes, dependencies, VPC, and more</blockquote><p>This article describes AWS Lambda—the dynamically scaled and billed-per-execution compute service. Instances of Lambdas are added and removed dynamically. When a new instance handles its first request, the response time increases, which is called a <strong>cold start</strong>.</p>
<p>Read more: <a href="/serverless/coldstarts/define/">Cold Starts in Serverless Functions</a>.</p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>The very first cold start happens when the first request comes in after deployment.</p>
<p>After that request is processed, the instance stays alive to be reused for subsequent requests. There is no predefined threshold after the instance gets recycled, the empiric data show some variance of the idle period.</p>
<p>The following chart estimates the probability of an instance to be recycled after the given period of inactivity:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'Time');
  data.addColumn('number', 'Probability');
  data.addRows([[0,0],[1,0],[2,0],[3,0],[4,0.0030303030303030303],[5,0.0030303030303030303],[6,0.40606060606060607],[7,1],[8,1],[9,1],[10,1],[11,1],[12,1],[13,1],[14,1],[15,1],[16,1],[17,1],[18,1],[19,1],[20,1],[21,1],[22,1],[23,1],[24,1],[25,1],[26,1],[27,1],[28,1],[29,1],[30,1]]);

  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability'
  };

  return new google.visualization.AreaChart(document.getElementById('chart_div_coldstart_aws_interval'));
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start happening before minute X</h4></figcaption>
</figure>
<p>Cold starts happen <strong>5 to 7 minutes</strong> after the previous request.</p>
<p>Read more: <a href="/serverless/coldstarts/aws/intervals/">When Does Cold Start Happen on AWS Lambda?</a></p>
<h2 id="how-slow-are-cold-starts">How Slow Are Cold Starts?</h2>
<p>The following chart shows the typical range of cold starts in AWS Lambda, broken down per language. The darker ranges are the most common 67% of durations, and lighter ranges include 95%.</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["JavaScript",0.26414167,"Median: 0.3s",0.19532940166666665,0.5952134600000012,0.22944015066666665,0.3203392613333333,"{color: #F1E05A; fill-color: #F1E05A}"],["Python",0.21543107666666667,"Median: 0.2s",0.14793076,0.5304724266666669,0.17918235999999998,0.2729796933333333,"{color: #3572A5; fill-color: #3572A5}"],["Go",0.306622582,"Median: 0.3s",0.19087315283333334,0.6611401278333335,0.24820689866666668,0.4129826986666667,"{color: #375EAB; fill-color: #375EAB}"],["Java",0.37206728666666666,"Median: 0.4s",0.2754296766666667,0.6896294450000003,0.32165270933333334,0.44366388000000007,"{color: #B07219; fill-color: #B07219}"],["Ruby",0.2769988973333333,"Median: 0.3s",0.20400577733333333,0.595965724,0.241346848,0.342534648,"{color: #701516; fill-color: #701516}"],["C# (2GB+)",0.51708294,"Median: 0.5s",0.44521963999999997,0.8273971066666665,0.48302365333333336,0.5955547400000001,"{color: #178600; fill-color: #178600}"],["Docker",0.79451566,"Median: 0.8s",0.6162031433333333,1.403937518333334,0.6690103266666666,1.0809897266666668,"{color: #333333; fill-color: #333333}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_bylanguage'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_bylanguage"></div>
  <figcaption class="imageCaption"><h4>Typical cold start durations per language</h4></figcaption>
</figure>
<p>JavaScript, Python, Go, Java, and Ruby are all comparable: most of the time they complete within <strong>400 milliseconds</strong> and almost always within <strong>700 milliseconds</strong>.</p>
<p>C# is a distinct underdog. The chart shows statistics for instances with 2+ GB of allocated RAM, which are the faster than smaller ones (see below). Cold starts of this instance size span between <strong>0.4 and 0.9 seconds</strong>.</p>
<p>Lambda functions packaged as Docker images are yet slower. A basic container based on the recommended Node.js base image starts up in <strong>0.6 and 1.4 seconds</strong>.</p>
<p>View detailed distributions: <a href="/serverless/coldstarts/aws/languages/">Cold Start Duration per Language</a>.</p>
<h2 id="does-instance-size-matter">Does Instance Size Matter?</h2>
<p>AWS Lambda has a setting to define the memory size that gets allocated to a single instance of a function. Are larger instances faster to load?</p>
<p>Most language runtimes have no visible difference in cold start duration of different instance sizes. Here is the chart for JavaScript:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["128 MB",0.25430470800000005,"Median: 0.3s",0.18102600133333333,0.5888562679999999,0.22522212666666666,0.3063981893333333,"{color: #5BA3F1; fill-color: #5BA3F1}"],["256 MB",0.2599622733333333,"Median: 0.3s",0.19354032666666665,0.8722825266666656,0.21614424533333332,0.3099200506666666,"{color: #428AD8; fill-color: #428AD8}"],["512 MB",0.26887212800000004,"Median: 0.3s",0.19150643216666666,0.6177974988333331,0.2343021346666667,0.35611481466666667,"{color: #2870BE; fill-color: #2870BE}"],["1024 MB",0.2702226013333333,"Median: 0.3s",0.205308533,0.7226502696666652,0.23894373200000002,0.3221015639999999,"{color: #0F57A5; fill-color: #0F57A5}"],["2048 MB",0.252074676,"Median: 0.3s",0.18917741600000001,0.49821349599999937,0.21213535200000003,0.30322812266666666,"{color: #003D8B; fill-color: #003D8B}"],["4096 MB",0.25068667533333333,"Median: 0.3s",0.18053354116666667,0.5477366261666666,0.22370415866666668,0.3042927546666666,""],["8192 MB",0.24134085866666669,"Median: 0.2s",0.16975453366666668,0.5788255044999987,0.19703044533333333,0.295449052,""]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_bymemory'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_bymemory"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per instance size, JavaScript functions</h4></figcaption>
</figure>
<p>However, .NET (C#/F#) functions are the exception. The bigger the instance, the faster startup time it has:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["128 MB",3.2838890960000002,"Median: 3.3s",3.0810558968333335,3.597002674333333,3.216616650666667,3.3723630959999995,"{color: #5BA3F1; fill-color: #5BA3F1}"],["256 MB",1.7529555993333334,"Median: 1.8s",1.6223465993333335,2.105583081833333,1.6895313200000002,1.8427999853333334,"{color: #428AD8; fill-color: #428AD8}"],["512 MB",0.999520892,"Median: 1.0s",0.921858192,1.278185192,0.947686952,1.0560347853333334,"{color: #2870BE; fill-color: #2870BE}"],["1024 MB",0.6537268680000001,"Median: 0.7s",0.5723044305,0.9414929396666657,0.6076733293333334,0.7155458039999999,"{color: #0F57A5; fill-color: #0F57A5}"],["2048 MB",0.5158667499999999,"Median: 0.5s",0.44378214416666667,0.8566797891666639,0.479521396,0.5943056106666665,"{color: #003D8B; fill-color: #003D8B}"],["4096 MB",0.5157839,"Median: 0.5s",0.43205403666666664,0.8039594333333333,0.4711577493333334,0.5718562506666667,""],["8192 MB",0.5172021913333333,"Median: 0.5s",0.4450570738333333,0.8340588855,0.48749449466666667,0.6027540999999998,""]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_bymemorycs'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_bymemorycs"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per instance size, C# functions</h4></figcaption>
</figure>
<p>Same comparison for larger functions: <a href="/serverless/coldstarts/aws/instances/">Cold Start Duration per Instance Size</a>.</p>
<h2 id="does-package-size-matter">Does Package Size Matter?</h2>
<p>The above charts show the statistics for tiny &ldquo;Hello World&rdquo;-style functions. Adding dependencies and thus increasing the deployed package size will further increase the cold start durations.</p>
<p>The following chart compares three JavaScript functions with the various number of referenced NPM packages:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [[" 1 KB",0.26414167,"Median: 0.3s",0.19532940166666665,0.5952134600000012,0.22944015066666665,0.3203392613333333,"{color: #5BA3F1; fill-color: #5BA3F1}"],[" 14 MB",1.6954576053333335,"Median: 1.7s",1.4502989370000001,2.002174150333334,1.5477744173333334,1.8110833266666668,"{color: #2870BE; fill-color: #2870BE}"],[" 35 MB",3.8748015533333335,"Median: 3.9s",3.1725208116666668,5.350264196666667,3.363002934666667,4.440574528000001,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_bydependencies'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_bydependencies"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per deployment size (zipped)</h4></figcaption>
</figure>
<p>Indeed, the functions with many dependencies can be 5-10 times slower to start.</p>
<p>The following chart compares three functions packaged as container images of different size. Every container is based on the official Node.js image but have different additional files baked in:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["Minimal",0.79451566,"Median: 0.8s",0.6162031433333333,1.403937518333334,0.6690103266666666,1.0809897266666668,"{color: #5BA3F1; fill-color: #5BA3F1}"],["+ 100 MB",0.8676688446666666,"Median: 0.9s",0.6448365905,1.360419578000001,0.7053716746666667,1.139656988,"{color: #2870BE; fill-color: #2870BE}"],["+ 5 GB",0.8179444786666666,"Median: 0.8s",0.6053505786666666,1.3177632095000016,0.6685665479999999,1.0854932080000002,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_bydocker'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_bydocker"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per Docker image extra size</h4></figcaption>
</figure>
<p>Container image size does not seem to influence the cold start duration.</p>
<h2 id="what-is-the-effect-of-vpc-access">What Is The Effect Of VPC Access?</h2>
<p>AWS Lambda might need to access resources inside Amazon Virtual Private Cloud (Amazon VPC). In the past, configuring VPC access slowed down the cold starts significantly.</p>
<p>This is not true anymore, as the effect of VPC is minimal:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No VPC",0.26414167,"Median: 0.3s",0.19532940166666665,0.5952134600000012,0.22944015066666665,0.3203392613333333,"{color: green; fill-color: green}"],["VPC",0.4966838333333333,"Median: 0.5s",0.41168818166666665,0.8238139625000012,0.451274864,0.569808848,"{color: red; fill-color: red}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_aws_byvpc'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_aws_byvpc"></div>
  <figcaption class="imageCaption"><h4>Cold start durations of the same Node.js Lambda with and without VPC access</h4></figcaption>
</figure>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts in Azure Functions]]></title>
            <link href="https://mikhail.io/serverless/coldstarts/azure/"/>
            <id>https://mikhail.io/serverless/coldstarts/azure/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Influence of dependecies, language, runtime selection on Consumption Plan</blockquote><p>This article describes Azure Functions running on Consumption Plan—the dynamically scaled and billed-per-execution compute service. Consumption Plan adds and removes instances dynamically. When a new instance handles its first request, the response time increases, which is called a <strong>cold start</strong>.</p>
<p>Learn more: <a href="/serverless/coldstarts/define/">Cold Starts in Serverless Functions</a>.</p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>The very first cold start happens when the first request comes in after deployment.</p>
<p>After that request is processed, the instance stays alive mostly <strong>from 20 to 30 minutes</strong> to be reused for subsequent requests:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'Time');
  data.addColumn('number', 'Probability');
  data.addRows([[0,0],[1,0],[2,0.1],[3,0.07546476429191169],[4,0.07546476429191169],[5,0.07546476429191169],[6,0.07546476429191169],[7,0.07546476429191169],[8,0.1282051282051282],[9,0.10320678827893033],[10,0.10320678827893033],[11,0.10320678827893033],[12,0.16216216216216217],[13,0.2046691016832265],[14,0.2046691016832265],[15,0.2046691016832265],[16,0.2046691016832265],[17,0.21297467850776675],[18,0.21297467850776675],[19,0.21297467850776675],[20,0.21297467850776675],[21,0.40816326530612246],[22,0.5],[23,0.7608695652173914],[24,0.7847516365036581],[25,0.7847516365036581],[26,0.8190724206349207],[27,0.8190724206349207],[28,0.9101640656262505],[29,0.9101640656262505],[30,0.9443044764443506],[31,0.9443044764443506],[32,0.9443044764443506],[33,0.9443044764443506],[34,0.9443044764443506],[35,0.9443044764443506],[36,0.9443044764443506],[37,0.9443044764443506],[38,0.9443044764443506],[39,0.9443044764443506],[40,0.9443044764443506],[41,0.9443044764443506],[42,0.9443044764443506],[43,0.9443044764443506],[44,0.9443044764443506],[45,0.9443044764443506],[46,0.9443044764443506],[47,0.9443044764443506],[48,0.9362866886474991],[49,0.9362866886474991],[50,0.9362866886474991],[51,0.9362866886474991],[52,0.9362866886474991],[53,0.9362866886474991],[54,0.9362866886474991],[55,0.9362866886474991],[56,0.9362866886474991],[57,0.9362866886474991],[58,0.9362866886474991],[59,0.975],[60,0.975]]);

  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability'
  };

  return new google.visualization.AreaChart(document.getElementById('chart_div_coldstart_azure_interval'));
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start happening before minute X</h4></figcaption>
</figure>
<p>As you can see, some cold starts happen earlier than 10 minutes, some instances last for more than 50 minutes, so the behavior seems less deterministic than it used to be in the past. It&rsquo;s likely related to the new <a href="https://mikhail.io/2020/06/eliminate-cold-starts-by-predicting-invocations-of-serverless-functions/">algorithm to predict the next invocation</a>.</p>
<p>Read more: <a href="/serverless/coldstarts/azure/intervals/">When Does Cold Start Happen on Azure Functions?</a></p>
<h2 id="how-slow-are-cold-starts">How Slow Are Cold Starts?</h2>
<p>The following chart shows the typical range of cold starts in Azure Functions V2, broken down per language. The darker ranges are the most common 67% of durations, and lighter ranges include 95%.</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["C#",1.1031019899999999,"Median: 1.1s",0.6679918483333334,10.607447039999998,0.8387957666666667,9.304306713333332,"{color: #178600; fill-color: #178600}"],["JavaScript",1.5911177106666667,"Median: 1.6s",0.9700218440000001,14.178741857333334,1.0575855840000001,9.806417288,"{color: #F1E05A; fill-color: #F1E05A}"],["Java",7.681953646666667,"Median: 7.7s",1.2073757166666668,19.568219678333328,1.3091131933333333,14.256910467999997,"{color: #B07219; fill-color: #B07219}"],["Python (Linux)",1.2805862146666667,"Median: 1.3s",0.04973198466666667,3.9015420188333336,0.9563436933333334,3.093084618666667,"{color: #3572A5; fill-color: #3572A5}"],["PowerShell",7.140260993333333,"Median: 7.1s",3.5566363099999996,27.05699023499999,4.031137833333332,14.651666346666666,"{color: #012456; fill-color: #012456}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_coldstart_azure_bylanguagega'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_bylanguagega"></div>
  <figcaption class="imageCaption"><h4>Typical cold start durations per language</h4></figcaption>
</figure>
<p>A typical cold start latency spans from 1 to 10 seconds. However, less lucky executions may take up to 30 seconds occasionally. PowerShell functions are especially slow to start with values from 4 to 27 seconds.</p>
<p>View detailed distributions: <a href="/serverless/coldstarts/azure/languages/">Cold Start Duration per Language</a>.</p>
<h2 id="windows-vs-linux">Windows vs. Linux</h2>
<p>Azure Functions can be deployed either on Windows or Linux environments, depending on the plan settings. Which operating system provides faster cold starts?</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["C# Windows",1.1031019899999999,"Median: 1.1s",0.6679918483333334,10.607447039999998,0.8387957666666667,9.304306713333332,"{color: #178600; fill-color: #178600}"],["JS Windows",1.5911177106666667,"Median: 1.6s",0.9700218440000001,14.178741857333334,1.0575855840000001,9.806417288,"{color: #F1E05A; fill-color: #F1E05A}"],["C# Linux",0.9323255893333332,"Median: 0.9s",0.07398760016666664,4.750454651833334,0.7170455479999999,2.803078432,"{color: #178600; fill-color: #178600}"],["JS Linux",1.5751024399999998,"Median: 1.6s",0.2678373358333333,4.33965740666667,0.9365863066666668,3.0940688333333335,"{color: #F1E05A; fill-color: #F1E05A}"],["Python Linux",1.2805862146666667,"Median: 1.3s",0.04973198466666667,3.9015420188333336,0.9563436933333334,3.093084618666667,"{color: #3572A5; fill-color: #3572A5}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_coldstart_azure_bylanguageos'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_bylanguageos"></div>
  <figcaption class="imageCaption"><h4>Comparison of cold start durations between two operating systems</h4></figcaption>
</figure>
<p>The median response time is comparable, but Linux has tighter distribution.</p>
<p>The Node.js runtime is consistently slower than the .NET Core one.</p>
<h2 id="does-package-size-matter">Does Package Size Matter?</h2>
<p>The above charts show the statistics for tiny &ldquo;Hello World&rdquo;-style functions. Adding dependencies and thus increasing the deployed package size will further increase the cold start durations.</p>
<p>The following chart compares three JavaScript functions with the various number of referenced NPM packages:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [[" 1 KB",1.5911177106666667,"Median: 1.6s",0.9700218440000001,14.178741857333334,1.0575855840000001,9.806417288,"{color: #5BA3F1; fill-color: #5BA3F1}"],[" 14 MB",7.320201888000001,"Median: 7.3s",5.013161164666667,19.522724471333337,5.497541298666667,15.409356537333332,"{color: #2870BE; fill-color: #2870BE}"],[" 35 MB",14.55078828,"Median: 14.6s",10.987529451666667,26.448669508333328,12.126174938666667,22.891839917333332,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_azure_bydependencies'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_bydependencies"></div>
  <figcaption class="imageCaption"><h4>Comparison of cold start durations per deployment size (zipped)</h4></figcaption>
</figure>
<p>Indeed, the functions with many dependencies can be several times slower to start.</p>
<h2 id="does-deployment-method-matter">Does Deployment Method Matter?</h2>
<p>There are multiple ways to deploy Azure Functions. The charts below compare three of them:</p>
<ul>
<li><strong>No Zip</strong>—traditional AppService-style deployment based on Kudu (<code>--nozip</code> option in <code>func</code> CLI)</li>
<li><strong>Local Zip</strong>—uploading a zip package to the local file system of the Function App and setting <code>RUN_FROM_PACKAGE=1</code> in application settings</li>
<li><strong>External Zip</strong>—uploading a zip package to blob storage and setting <code>RUN_FROM_PACKAGE=&lt;blob sas token&gt;</code> in application settings</li>
</ul>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No Zip",1.1030700199999999,"Median: 1.1s",0.75490886,10.835520906666664,0.9000855973333334,9.073883681333331,"{color: #008F95; fill-color: #008F95}"],["Local Zip",1.1031019899999999,"Median: 1.1s",0.6679918483333334,10.607447039999998,0.8387957666666667,9.304306713333332,"{color: #E9B000; fill-color: #E9B000}"],["External Zip",0.9933938666666666,"Median: 1.0s",0.6181695583333333,10.694083403333334,0.7683209093333333,9.300540545333334,"{color: #E24E42; fill-color: #E24E42}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_coldstart_azure_bydeploymentcs'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_bydeploymentcs"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per deployment method for C# functions</h4></figcaption>
</figure>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No Zip",2.4160054766666668,"Median: 2.4s",1.7489051766666666,11.987390494999998,1.8687155973333331,10.095676929333333,"{color: #008F95; fill-color: #008F95}"],["Local Zip",1.5911177106666667,"Median: 1.6s",0.9700218440000001,14.178741857333334,1.0575855840000001,9.806417288,"{color: #E9B000; fill-color: #E9B000}"],["External Zip",1.1893931706666667,"Median: 1.2s",0.8328060706666667,13.005890778999998,0.9714150173333332,10.564063504,"{color: #E24E42; fill-color: #E24E42}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_coldstart_azure_bydeploymentjs'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_azure_bydeploymentjs"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per deployment method for JavaScript functions</h4></figcaption>
</figure>
<p>The differences between deployment methods are hardly visible.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts in Google Cloud Functions]]></title>
            <link href="https://mikhail.io/serverless/coldstarts/gcp/"/>
            <id>https://mikhail.io/serverless/coldstarts/gcp/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Running GA and Beta languages on different instance sizes</blockquote><p>This article describes Google Cloud Functions—the dynamically scaled and billed-per-execution compute service. Instances of Cloud Functions are added and removed dynamically. When a new instance handles its first request, the response time suffers, which is called a <strong>cold start</strong>.</p>
<p>Learn more: <a href="/serverless/coldstarts/define">Cold Starts in Serverless Functions</a>.</p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>The very first cold start happens when the first request comes in after deployment.</p>
<p>After that request is processed, the instance stays alive to be reused for subsequent requests. There is no predefined threshold for instance recycling: the empiric data show a high variance of idle-but-alive periods.</p>
<p>The following chart estimates the probability of an instance to be recycled after the given period of inactivity:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'Time');
  data.addColumn('number', 'Probability');
  data.addRows([[0,0],[1,0],[2,0],[3,0],[5,0],[6,0],[7,0],[9,0],[10,0],[12,0],[13,0],[14,0],[15,0],[16,1],[17,1],[18,1],[19,1],[22,1],[24,1],[25,1],[29,1],[30,1],[31,1],[32,1],[33,1],[34,1],[35,1],[36,1],[37,1],[38,1],[39,1],[40,1],[41,1],[42,1],[43,1],[44,1],[45,1],[46,1],[47,1],[48,1],[49,1],[50,1],[51,1],[52,1],[54,1],[55,1],[56,1],[57,1],[58,1],[59,1]]);

  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability'
  };

  return new google.visualization.AreaChart(document.getElementById('chart_div_coldstart_gcp_interval'));
});
</script>
<figure>
  <div id="chart_div_coldstart_gcp_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start happening before minute X</h4></figcaption>
</figure>
<p>Instances are recycled after <strong>15 minutes</strong> of inactivity. Google stopped keeping idle instances for many hours, as they used to do.</p>
<p>Read more: <a href="/serverless/coldstarts/gcp/intervals">When Does Cold Start Happen on Google Cloud Functions?</a></p>
<h2 id="how-slow-are-cold-starts">How Slow Are Cold Starts?</h2>
<p>The following chart shows the typical range of cold starts in Google Cloud Functions, broken down per language. The darker ranges are the most common 67% of durations, and lighter ranges include 95%.</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["JavaScript",0.665357772,"Median: 0.7s",0.4622354753333333,1.49407009866668,0.52307868,0.863901676,"{color: #F1E05A; fill-color: #F1E05A}"],["Go",0.58938325,"Median: 0.6s",0.48661635,1.2900779874999975,0.5198140133333333,0.7103175133333334,"{color: #375EAB; fill-color: #375EAB}"],["Python",1.1055170166666668,"Median: 1.1s",0.9082244233333333,1.7618197566666658,0.9896485893333333,1.3598947626666666,"{color: #3572A5; fill-color: #3572A5}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_gcp_bylanguage'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_gcp_bylanguage"></div>
  <figcaption class="imageCaption"><h4>Typical cold start durations per language</h4></figcaption>
</figure>
<p>Python functions look a bit slower, however, not significantly.</p>
<p>View detailed distributions: <a href="/serverless/coldstarts/gcp/languages">Cold Start Duration per Language</a>.</p>
<h2 id="does-package-size-matter">Does Package Size Matter?</h2>
<p>The above charts show the statistics for tiny &ldquo;Hello World&rdquo;-style functions. Adding dependencies and thus increasing the deployed package size will further increase the cold start durations.</p>
<p>The following chart compares three JavaScript functions with the various number of referenced NPM packages:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [[" 1 KB",0.665357772,"Median: 0.7s",0.4622354753333333,1.49407009866668,0.52307868,0.863901676,"{color: #5BA3F1; fill-color: #5BA3F1}"],[" 14 MB",2.1428311853333333,"Median: 2.1s",1.9554957903333334,2.7917092720000007,2.0310164693333332,2.3888415213333336,"{color: #2870BE; fill-color: #2870BE}"],[" 35 MB",4.209702213333333,"Median: 4.2s",3.699496486666667,4.909959489999999,3.9415168613333336,4.576283189333333,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_gcp_bydependencies'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_gcp_bydependencies"></div>
  <figcaption class="imageCaption"><h4>Comparison of cold start durations per deployment size (zipped)</h4></figcaption>
</figure>
<p>Indeed, the functions with many dependencies can be 5 times slower to start.</p>
<h2 id="does-instance-size-matter">Does Instance Size Matter?</h2>
<p>Google Cloud Functions have a setting to define the memory size that gets allocated to a single instance of a function. Are larger instances faster to load?</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["128 MB",0.5085932499999999,"Median: 0.5s",0.3053842,1.8566299000000002,0.3901505053333333,0.957725579999998,"{color: #5BA3F1; fill-color: #5BA3F1}"],["256 MB",0.6784210500000001,"Median: 0.7s",0.47467231666666665,1.2741795083333327,0.5843337000000001,0.8323447746666666,"{color: #428AD8; fill-color: #428AD8}"],["512 MB",0.7718927,"Median: 0.8s",0.5047560833333333,3.2474134833333395,0.5715917333333334,0.9077337666666666,"{color: #2870BE; fill-color: #2870BE}"],["1024 MB",0.74610955,"Median: 0.7s",0.5081680291666667,2.7309098458333256,0.6368742333333333,0.9227848399999999,"{color: #0F57A5; fill-color: #0F57A5}"],["2048 MB",0.6800682499999999,"Median: 0.7s",0.5115378041666667,1.1002482291666662,0.5497723,0.8724159266666666,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_gcp_bymemory'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_gcp_bymemory"></div>
  <figcaption class="imageCaption"><h4>Comparison of cold start durations per instance size</h4></figcaption>
</figure>
<p>There seems to be no significant speed-up of the cold start as the instance size grows.</p>
<p>Same comparison for larger functions: <a href="/serverless/coldstarts/gcp/instances">Cold Start Duration per Instance Size</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts in Serverless Functions]]></title>
            <link href="https://mikhail.io/serverless/coldstarts/"/>
            <id>https://mikhail.io/serverless/coldstarts/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Exploring the phenomenon of increased latency while instances of cloud functions are dynamically allocated.</blockquote>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Comparison of Cold Starts in Serverless Functions across AWS, Azure, and GCP]]></title>
            <link href="https://mikhail.io/serverless/coldstarts/big3/"/>
            <id>https://mikhail.io/serverless/coldstarts/big3/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>AWS Lambda, Azure Functions, and Google Cloud Functions compared in terms of cold starts across all supported languages</blockquote><p>This article compares Function-as-a-Service offerings of Big-3 cloud providers in terms of cold starts. AWS Lambda, Azure Functions (Consumption Plan), and Google Cloud Functions are all dynamically scaled and billed-per-execution compute services. Instances of cloud functions are added and removed dynamically. When a new instance handles its first request, the response time increases, which is called a <strong>cold start</strong>.</p>
<p>Read more: <a href="/serverless/coldstarts/define/">Cold Starts in Serverless Functions</a>.</p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>The very first cold start happens when the first request comes in after deployment.</p>
<p>After that request is processed, the instance stays alive to be reused for subsequent requests.</p>
<p>The strategy for reuse differs very between the cloud vendors:</p>
<table>
<thead>
<tr>
<th>Service</th>
<th>Idle instance lifetime</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWS Lambda</td>
<td>5-7 minutes</td>
</tr>
<tr>
<td>Azure Functions</td>
<td>Mostly between 20 and 30 minutes</td>
</tr>
<tr>
<td>Google Cloud Functions</td>
<td>15 minutes</td>
</tr>
</tbody>
</table>
<p>AWS and Google Cloud have the policies to recycle an idle instance after a predefined period, 5 to 7 and 15 minutes respectively. Azure employ some other strategies to determine the threshold, potentially based on the history of the past invocations.</p>
<p>Learn more about lifetime: <a href="/serverless/coldstarts/aws/intervals/">AWS Lambda</a>, <a href="/serverless/coldstarts/azure/intervals/">Azure Functions</a>, <a href="/serverless/coldstarts/gcp/intervals/">Google Cloud Functions</a>.</p>
<h2 id="how-slow-are-cold-starts">How Slow Are Cold Starts?</h2>
<p>The following chart shows the comparison of typical cold start durations for common languages across three cloud providers. The darker ranges are the most common 67% of durations, and lighter ranges include 95%.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        






  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["AWS JavaScript",0.26414167,"Median: 0.3s",0.19532940166666665,0.5952134600000012,0.22944015066666665,0.3203392613333333,"{color: #F1E05A; fill-color: #F1E05A}"],["AWS Python",0.21543107666666667,"Median: 0.2s",0.14793076,0.5304724266666669,0.17918235999999998,0.2729796933333333,"{color: #3572A5; fill-color: #3572A5}"],["AWS Go",0.306622582,"Median: 0.3s",0.19087315283333334,0.6611401278333335,0.24820689866666668,0.4129826986666667,"{color: #375EAB; fill-color: #375EAB}"],["GCP JavaScript",0.665357772,"Median: 0.7s",0.4622354753333333,1.49407009866668,0.52307868,0.863901676,"{color: #F1E05A; fill-color: #F1E05A}"],["GCP Python",1.1055170166666668,"Median: 1.1s",0.9082244233333333,1.7618197566666658,0.9896485893333333,1.3598947626666666,"{color: #3572A5; fill-color: #3572A5}"],["GCP Go",0.58938325,"Median: 0.6s",0.48661635,1.2900779874999975,0.5198140133333333,0.7103175133333334,"{color: #375EAB; fill-color: #375EAB}"],["Azure JavaScript",1.5751024399999998,"Median: 1.6s",0.2678373358333333,4.33965740666667,0.9365863066666668,3.0940688333333335,"{color: #F1E05A; fill-color: #F1E05A}"],["Azure C#",0.9323255893333332,"Median: 0.9s",0.07398760016666664,4.750454651833334,0.7170455479999999,2.803078432,"{color: #178600; fill-color: #178600}"],["Azure Python",1.2805862146666667,"Median: 1.3s",0.04973198466666667,3.9015420188333336,0.9563436933333334,3.093084618666667,"{color: #3572A5; fill-color: #3572A5}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_all_bylanguage'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_all_bylanguage"></div>
  <figcaption class="imageCaption"><h4>Typical cold start durations per language</h4></figcaption>
</figure>


    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>AWS clearly leads with all languages being <strong>well below 1 second</strong>. GCP start-up usually takes <strong>between 0.5 and 2 seconds</strong>. Azure is a clear underdog with startup times often <strong>up to 5 seconds</strong>.</p>
<p>All functions above run on Linux.</p>
<p>Read the detailed statistics: <a href="/serverless/coldstarts/aws/languages/">AWS Lambda</a>, <a href="/serverless/coldstarts/azure/languages/">Azure Functions</a>, <a href="/serverless/coldstarts/gcp/languages/">Google Cloud Functions</a>.</p>
<h2 id="does-package-size-matter">Does Package Size Matter?</h2>
<p>The above charts show the statistics for tiny &ldquo;Hello World&rdquo;-style functions. Adding dependencies and thus increasing the deployment package size will further increase the cold start latency.</p>
<p>The following chart compares three JavaScript functions with the various number of referenced NPM packages:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        






  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["AWS 1 KB",0.26414167,"Median: 0.3s",0.19532940166666665,0.5952134600000012,0.22944015066666665,0.3203392613333333,"{color: #5BA3F1; fill-color: #5BA3F1}"],["AWS 14 MB",1.6954576053333335,"Median: 1.7s",1.4502989370000001,2.002174150333334,1.5477744173333334,1.8110833266666668,"{color: #2870BE; fill-color: #2870BE}"],["AWS 35 MB",3.8748015533333335,"Median: 3.9s",3.1725208116666668,5.350264196666667,3.363002934666667,4.440574528000001,"{color: #003D8B; fill-color: #003D8B}"],["GCP 1 KB",0.665357772,"Median: 0.7s",0.4622354753333333,1.49407009866668,0.52307868,0.863901676,"{color: #5BA3F1; fill-color: #5BA3F1}"],["GCP 14 MB",2.1428311853333333,"Median: 2.1s",1.9554957903333334,2.7917092720000007,2.0310164693333332,2.3888415213333336,"{color: #2870BE; fill-color: #2870BE}"],["GCP 35 MB",4.209702213333333,"Median: 4.2s",3.699496486666667,4.909959489999999,3.9415168613333336,4.576283189333333,"{color: #003D8B; fill-color: #003D8B}"],["Azure 1 KB",1.5911177106666667,"Median: 1.6s",0.9700218440000001,14.178741857333334,1.0575855840000001,9.806417288,"{color: #5BA3F1; fill-color: #5BA3F1}"],["Azure 14 MB",7.320201888000001,"Median: 7.3s",5.013161164666667,19.522724471333337,5.497541298666667,15.409356537333332,"{color: #2870BE; fill-color: #2870BE}"],["Azure 35 MB",14.55078828,"Median: 14.6s",10.987529451666667,26.448669508333328,12.126174938666667,22.891839917333332,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.LineChart(document.getElementById('chart_div_coldstart_all_bydependencies'));
  
});
</script>
<figure>
  <div id="chart_div_coldstart_all_bydependencies"></div>
  <figcaption class="imageCaption"><h4>Comparison of cold start durations per deployment size (zipped)</h4></figcaption>
</figure>


    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The trend is quite consistent: larger packages cause a significant slowdown of the cold start.</p>
<p>AWS and GCP are mostly comparable. Note that this test deployed Azure Functions to Windows, which explains the difference with the per-language chart.</p>
<p>Details per provider: <a href="/serverless/coldstarts/aws/">AWS Lambda</a>, <a href="/serverless/coldstarts/azure/">Azure Functions</a>, <a href="/serverless/coldstarts/gcp/">Google Cloud Functions</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[GCP]]></title>
            <link href="https://mikhail.io/tags/gcp/"/>
            <id>https://mikhail.io/tags/gcp/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Go]]></title>
            <link href="https://mikhail.io/tags/go/"/>
            <id>https://mikhail.io/tags/go/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Google Cloud Functions]]></title>
            <link href="https://mikhail.io/tags/google-cloud-functions/"/>
            <id>https://mikhail.io/tags/google-cloud-functions/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[Java]]></title>
            <link href="https://mikhail.io/tags/java/"/>
            <id>https://mikhail.io/tags/java/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Javascript]]></title>
            <link href="https://mikhail.io/tags/javascript/"/>
            <id>https://mikhail.io/tags/javascript/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Python]]></title>
            <link href="https://mikhail.io/tags/python/"/>
            <id>https://mikhail.io/tags/python/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Ruby]]></title>
            <link href="https://mikhail.io/tags/ruby/"/>
            <id>https://mikhail.io/tags/ruby/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverlesses]]></title>
            <link href="https://mikhail.io/serverless/"/>
            <id>https://mikhail.io/serverless/</id>
            
            <published>2021-01-05T00:00:00+00:00</published>
            <updated>2021-01-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[2020s]]></title>
            <link href="https://mikhail.io/2020/"/>
            <id>https://mikhail.io/2020/</id>
            
            <published>2020-12-16T00:00:00+00:00</published>
            <updated>2020-12-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[F#]]></title>
            <link href="https://mikhail.io/tags/fsharp/"/>
            <id>https://mikhail.io/tags/fsharp/</id>
            
            <published>2020-12-16T00:00:00+00:00</published>
            <updated>2020-12-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Farmer or Pulumi? Why not both!]]></title>
            <link href="https://mikhail.io/2020/12/farmer-or-pulumi-why-not-both/"/>
            <id>https://mikhail.io/2020/12/farmer-or-pulumi-why-not-both/</id>
            
            <published>2020-12-16T00:00:00+00:00</published>
            <updated>2020-12-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Azure Infrastucture as Code using F#: combining Pulumi and Farmer</blockquote><p><em>The post is a part of
<a href="https://sergeytihon.com/2020/10/22/f-advent-calendar-in-english-2020/">F# Advent Calendar 2020</a>.</em></p>
<p>You are a proud F# developer. You deploy your applications to Microsoft Azure. You know that you should never right-click-deploy to production. You don&rsquo;t create important Azure resources via the Azure portal. You are awesome.</p>
<p>You want to define your cloud resources with infrastructure as code tools. However, you do NOT enjoy writing large JSON files to deploy with the Azure Resource Manager (ARM) templates.</p>
<p>It turns out there are at least two tools that enable you to use F# to define Azure resources: <a href="https://compositionalit.github.io/farmer/">Farmer</a> and <a href="https://pulumi.com">Pulumi</a>.</p>
<p>In this article, I&rsquo;ll give a quick comparison of these two options. More excitingly, I&rsquo;ll show you how you could use both tools together in the same F# program!</p>
<h2 id="what-is-farmer">What is Farmer?</h2>
<p>Farmer is an F# library for rapidly authoring and deploying Azure architectures. It&rsquo;s a hand-crafted library that provides simple but powerful primitives to produce ARM Templates from F# code. Essentially, the result of a Farmer execution is a JSON ARM template that you can feed into Azure deployment tools.</p>
<p>Farmer relies heavily on F# computation expressions and feels like a DSL but with strong type safety and excellent IDE support. It&rsquo;s open-source, free to use, and it&rsquo;s just a NuGet library to install.</p>
<h2 id="what-is-pulumi">What is Pulumi?</h2>
<p>Pulumi is a more ambitious tool for all aspects of modern infrastructure as code. It&rsquo;s designed to create, deploy, and manage infrastructure on any cloud using several programming languages.</p>
<p>Pulumi comes with its own command-line interface (CLI) tool to orchestrate deployments, so it doesn&rsquo;t rely on ARM templates in any way. Pulumi supports Azure among other cloud providers, and the Azure SDK is automatically generated from formal specifications.</p>
<h2 id="how-are-they-different">How are they different?</h2>
<p>Farmer and Pulumi are different in many ways, but I want to focus on two aspects.</p>
<h3 id="azure-vs-any-cloud">Azure vs. Any Cloud</h3>
<p>Farmer is focused entirely on Azure, while Pulumi supports Azure, Azure Active Directory, AWS, Google Cloud, Kubernetes, Cloudflare, Digital Ocean, and several dozens of other deployment targets. A single Pulumi program can deploy resources to multiple environments, for instance, Azure, Azure AD, Kubernetes, and Cloudflare.</p>
<p>Farmer relies on Azure deployment tools, while Pulumi comes with its own. There are pros and cons to both approaches.</p>
<h3 id="hand-crafted-vs-auto-generated">Hand-crafted vs. Auto-Generated</h3>
<p>Farmer&rsquo;s F# code is designed to be concise and look great. It&rsquo;s opinionated and makes the most common scenarios short and straightforward. More obscure deployments may not be expressible directly with Farmer, but the library provides escape hatches to fall back to JSON.</p>
<p>Pulumi relies heavily on code generation. A resource definition is always a constructor call that accepts a bag of input properties and returns output properties. This means that you have access to the full API surface area, but you work on a low abstraction level.</p>
<h2 id="can-i-use-both">Can I use both?</h2>
<p>Essentially, Farmer is a DSL to build ARM Templates, while Pulumi is a deployment orchestration tool. They operate on a different level and aren&rsquo;t directly interchangeable.</p>
<p>This also means that you may want to use both to combine their strengths and powers. Below, I show exactly this: I deploy a simple Farmer template from a Pulumi program.</p>
<p><strong>Disclaimer</strong>: This is a proof-of-concept and can&rsquo;t be used for production deployments yet. See the &ldquo;How It Works&rdquo; section for details.</p>
<p>Let&rsquo;s write a sample Pulumi-Farmer program to deploy a Web App!</p>
<h3 id="resource-definitions">Resource Definitions</h3>
<p>I created a .NET Core console application and referenced NuGet packages <code>Pulumi.AzureNextGen</code>, <code>Pulumi.FSharp</code>, and <code>Farmer</code>. Now, I can define resources with Farmer builders:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// Create a storage account
</span><span style="color:#008000"></span><span style="color:#00f">let</span> myStorageAccount = storageAccount {
    name <span style="color:#a31515">&#34;farmerpulumisa&#34;</span>
}

<span style="color:#008000">// Create a web app with application insights that are connected to the storage account
</span><span style="color:#008000"></span><span style="color:#00f">let</span> myWebApp = webApp {
    name <span style="color:#a31515">&#34;farmerpulumiweb&#34;</span>
    setting <span style="color:#a31515">&#34;storageKey&#34;</span> myStorageAccount.Key
}
</code></pre></div><p>These definitions will result in four Azure resources: a Storage Account, an App Service Plan, an Application Insights component, and a Web App (App Service).</p>
<h3 id="deployment">Deployment</h3>
<p>The next step is to define a deployment with a target location and a list of resources to deploy:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// Create an ARM template
</span><span style="color:#008000"></span><span style="color:#00f">let</span> deployment = arm {
    location Location.NorthEurope
    add_resources [
        myStorageAccount
        myWebApp
    ]
}
</code></pre></div><h3 id="run-pulumi-deployment">Run Pulumi Deployment</h3>
<p>The last step is to declare the application entry point. The application calls a helper function of mine called <code>FarmerDeploy.run</code>, which accepts a resource group name and the deployment object:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// Deploy with Pulumi
</span><span style="color:#008000"></span>[&lt;EntryPoint&gt;]
<span style="color:#00f">let</span> main _ = FarmerDeploy.run <span style="color:#a31515">&#34;my-resource-group&#34;</span> deployment
</code></pre></div><p>Now, I navigate to the application folder in a command line and run <code>pulumi up</code> to deploy the program:</p>
<pre><code>$ pulumi up

Updating (dev)

     Type                                               Name                  Status      
 +   pulumi:pulumi:Stack                                azure-nextgen-fs-dev  created     
 +   ├─ azure-nextgen:storage/v20190401:StorageAccount  farmerpulumisa        created     
 +   ├─ azure-nextgen:web/v20180201:AppServicePlan      farmerpulumiwebFarm   created     
 +   ├─ azure-nextgen:insights/v20150501:Component      farmerpulumiwebAi     created     
 +   └─ azure-nextgen:web/v20160801:WebApp              farmerpulumiweb       created     
 
Resources:
    + 5 created

Duration: 53s
</code></pre><p>Tada! The Farmer-Pulumi application is deployed to Azure!</p>
<h2 id="how-it-works">How It Works</h2>
<p>Here is how this deployment worked:</p>
<ol>
<li>Pulumi orchestrates the deployment, so the project is a Pulumi F# project.</li>
<li><code>FarmerDeploy.run</code> accepts a Farmer deployment and converts it to a raw JSON string.</li>
<li>It sends the JSON to the Pulumi Azure NextGen provider (a plugin installed on my system).</li>
<li>The provider uses <a href="https://github.com/pulumi/arm2pulumi">arm2pulumi</a> to parse the JSON template to the Pulumi resource model.</li>
<li>The resource model is sent back to the F# program, which instantiates resources with proper arguments.</li>
<li>The resources are registered with the Pulumi engine.</li>
<li>The engine orchestrates the deployment and invokes Azure API to create the resources.</li>
</ol>
<p><strong>Note</strong>: The Pulumi Azure NextGen provider is currently in preview. In particular, steps 4 and 5 above and not production-ready yet. This means that the prototype may fall short for more sophisticated Farmer deployments (and, therefore, more sophisticated JSON templates).</p>
<p>You can find the full example <a href="https://github.com/mikhailshilkov/fsharp-advent-pulumi/tree/master/2020">here</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>More than anything, this article is a manifestation of the power of applying general-purpose programming languages to engineering cloud applications. Different tools and approaches may be combined in ways they were not initially designed for.</p>
<p>If you are interested in a production-grade Farmer-in-Pulumi integration, consider leaving a comment below with a scenario you have in mind, or just hit the Heart button on the top-left. Thank you!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Pulumi]]></title>
            <link href="https://mikhail.io/tags/pulumi/"/>
            <id>https://mikhail.io/tags/pulumi/</id>
            
            <published>2020-12-16T00:00:00+00:00</published>
            <updated>2020-12-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Get Up and Running with Azure Synapse and Pulumi]]></title>
            <link href="https://mikhail.io/2020/12/get-up-and-running-with-azure-synapse-and-pulumi/"/>
            <id>https://mikhail.io/2020/12/get-up-and-running-with-azure-synapse-and-pulumi/</id>
            
            <published>2020-12-04T00:00:00+00:00</published>
            <updated>2020-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Use infrastructure as code to automate deployment of an Azure Synapse workspace</blockquote><p>Azure Synapse is an integrated analytics service that combines enterprise data warehousing of Azure SQL Data Warehouse and Big Data analytics of Apache Spark. Azure Synapse is a managed service well integrated with other Azure services for data ingestion and business analytics.</p>
<p>You could use the Azure portal to get started with Azure Synapse, but it can be hard to define sophisticated infrastructure for your analytics pipeline using the portal alone, and many users need to apply version control to their cloud configurations.</p>
<p>The alternative is to use an infrastructure as code tool to automate building and deploying cloud resources. This article demonstrates how to provision an Azure Synapse workspace using Pulumi and general-purpose programming languages like Python and C#.</p>
<h2 id="azure-synapse-components">Azure Synapse Components</h2>
<p>Let&rsquo;s start by introducing the components required to provision a basic Azure Synapse workspace. To follow along with the <a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started">Synapse Getting Started Guide</a>, you need the following key Azure infrastructure components:</p>
<ul>
<li><strong>Resource Group</strong> to contain all other resources.</li>
<li><strong>Storage Account</strong> to store input data and analytics artifacts.</li>
<li><strong>Azure Synapse Workspace</strong>—a collaboration boundary for cloud-based analytics in Azure.</li>
<li><strong>SQL Pool</strong>—a dedicated Synapse SQL pool to run T-SQL based analytics.</li>
<li><strong>Spark Pool</strong> to use Apache Spark analytics.</li>
<li><strong>IP Filters</strong> and <strong>Role Assignments</strong> for secure access control.</li>
</ul>
<h2 id="infrastructure-as-code">Infrastructure as Code</h2>
<p>Let&rsquo;s walk through the steps to build a workspace with all the components mentioned above. We&rsquo;ll use Pulumi to provision the necessary resources. Feel free to pick the language of your choice that will apply to all code snippets.</p>
<p>You can check out the <a href="https://github.com/pulumi/examples/tree/master/aws-ts-lambda-thumbnailer">full source code</a> in the Pulumi Examples.</p>
<h3 id="resource-group">Resource Group</h3>
<p>Let&rsquo;s start by defining a resource group to contain all other resources. Be sure to adjust its name and region to your preferred values.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">resource_group = resources.ResourceGroup(<span style="color:#a31515">&#34;resourceGroup&#34;</span>,
    resource_group_name=<span style="color:#a31515">&#34;synapse-rg&#34;</span>,
    location=<span style="color:#a31515">&#34;westus2&#34;</span>)
</code></pre></div><h3 id="data-lake-storage-account">Data Lake Storage Account</h3>
<p>Synapse workspace will store data in a data lake storage account. We use a Standard Read-Access Geo-Redundant Storage account (SKU <code>Standard_RAGRS</code>) for this purpose. Make sure to change the <code>accountName</code> to your own globally unique name.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">storage_account = storage.StorageAccount(<span style="color:#a31515">&#34;storageAccount&#34;</span>,
    resource_group_name=resource_group.name,
    location=resource_group.location,
    account_name=<span style="color:#a31515">&#34;yoursynapsesa&#34;</span>,
    access_tier=<span style="color:#a31515">&#34;Hot&#34;</span>,
    enable_https_traffic_only=True,
    is_hns_enabled=True,
    kind=<span style="color:#a31515">&#34;StorageV2&#34;</span>,
    sku=storage.SkuArgs(
        name=<span style="color:#a31515">&#34;Standard_RAGRS&#34;</span>,
    ))
</code></pre></div><p>We&rsquo;ll use the <code>users</code> blob container as the analytics file system.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">users = storage.BlobContainer(<span style="color:#a31515">&#34;users&#34;</span>,
    resource_group_name=resource_group.name,
    account_name=storage_account.name,
    container_name=<span style="color:#a31515">&#34;users&#34;</span>,
    public_access=<span style="color:#a31515">&#34;None&#34;</span>)
</code></pre></div><h3 id="synapse-workspace">Synapse Workspace</h3>
<p>It&rsquo;s time to use all of the above to provision an Azure Synapse workspace! Adjust the name and the SQL credentials in the definition below.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">workspace = synapse.Workspace(<span style="color:#a31515">&#34;workspace&#34;</span>,
    resource_group_name=resource_group.name,
    location=resource_group.location,
    workspace_name=<span style="color:#a31515">&#34;mysynapse&#34;</span>,
    default_data_lake_storage=synapse.DataLakeStorageAccountDetailsArgs(
        account_url=data_lake_storage_account_url,
        filesystem=<span style="color:#a31515">&#34;users&#34;</span>,
    ),
    identity=synapse.ManagedIdentityArgs(
        type=<span style="color:#a31515">&#34;SystemAssigned&#34;</span>,
    ),
    sql_administrator_login=<span style="color:#a31515">&#34;sqladminuser&#34;</span>,
    sql_administrator_login_password=random.RandomPassword(<span style="color:#a31515">&#34;workspacePwd&#34;</span>, length=12).result)
</code></pre></div><blockquote>
<p>Note that we also defined a system-assigned managed identity for the workspace.</p>
</blockquote>
<h3 id="security-setup">Security Setup</h3>
<p>You need to allow access to the workspace with a firewall rule. The following is a blank access rule but feel free to restrict it to your target IP range.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">allow_all = synapse.IpFirewallRule(<span style="color:#a31515">&#34;allowAll&#34;</span>,
    resource_group_name=resource_group.name,
    workspace_name=workspace.name,
    rule_name=<span style="color:#a31515">&#34;allowAll&#34;</span>,
    end_ip_address=<span style="color:#a31515">&#34;255.255.255.255&#34;</span>,
    start_ip_address=<span style="color:#a31515">&#34;0.0.0.0&#34;</span>)
</code></pre></div><p>The following snippet assigns the <strong>Storage Blob Data Contributor</strong> role to the workspace managed identity and your target user. If you use the Azure CLI, run <code>az ad signed-in-user show --query=objectId</code> to look up your user ID.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">subscription_id = resource_group.id.apply(<span style="color:#00f">lambda</span> id: id.split(<span style="color:#a31515">&#39;/&#39;</span>)[2])
role_definition_id = subscription_id.apply(<span style="color:#00f">lambda</span> id: f<span style="color:#a31515">&#34;/subscriptions/{id}/providers/Microsoft.Authorization/roleDefinitions/ba92f5b4-2d11-453d-a403-e96b0029c9fe&#34;</span>)

authorization.RoleAssignment(<span style="color:#a31515">&#34;storageAccess&#34;</span>,
    role_assignment_name=random.RandomUuid(<span style="color:#a31515">&#34;roleName&#34;</span>).result,
    scope=storage_account.id,
    principal_id=workspace.identity.principal_id.apply(<span style="color:#00f">lambda</span> v: v <span style="color:#00f">or</span> <span style="color:#a31515">&#34;&lt;preview&gt;&#34;</span>),
    principal_type=<span style="color:#a31515">&#34;ServicePrincipal&#34;</span>,
    role_definition_id=role_definition_id)

authorization.RoleAssignment(<span style="color:#a31515">&#34;userAccess&#34;</span>,
    role_assignment_name=random.RandomUuid(<span style="color:#a31515">&#34;userRoleName&#34;</span>).result,
    scope=storage_account.id,
    principal_id=config.get(<span style="color:#a31515">&#34;userObjectId&#34;</span>),
    principal_type=<span style="color:#a31515">&#34;User&#34;</span>,
    role_definition_id=role_definition_id)
</code></pre></div><h3 id="sql-and-spark-pools">SQL and Spark Pools</h3>
<p>Finally, let&rsquo;s add two worker pools to the Synapse workspace. A SQL pool for T-SQL analytic queries&hellip;</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sql_pool = synapse.SqlPool(<span style="color:#a31515">&#34;sqlPool&#34;</span>,
    resource_group_name=resource_group.name,
    location=resource_group.location,
    workspace_name=workspace.name,
    sql_pool_name=<span style="color:#a31515">&#34;SQLPOOL1&#34;</span>,
    collation=<span style="color:#a31515">&#34;SQL_Latin1_General_CP1_CI_AS&#34;</span>,
    create_mode=<span style="color:#a31515">&#34;Default&#34;</span>,
    sku=synapse.SkuArgs(
        name=<span style="color:#a31515">&#34;DW100c&#34;</span>,
    ))
</code></pre></div><p>&hellip; and a Spark pool for Big Data analytics.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">spark_pool = synapse.BigDataPool(<span style="color:#a31515">&#34;sparkPool&#34;</span>,
    resource_group_name=resource_group.name,
    location=resource_group.location,
    workspace_name=workspace.name,
    big_data_pool_name=<span style="color:#a31515">&#34;Spark1&#34;</span>,
    auto_pause=synapse.AutoPausePropertiesArgs(
        delay_in_minutes=15,
        enabled=True,
    ),
    auto_scale=synapse.AutoScalePropertiesArgs(
        enabled=True,
        max_node_count=3,
        min_node_count=3,
    ),
    node_count=3,
    node_size=<span style="color:#a31515">&#34;Small&#34;</span>,
    node_size_family=<span style="color:#a31515">&#34;MemoryOptimized&#34;</span>,
    spark_version=<span style="color:#a31515">&#34;2.4&#34;</span>)
</code></pre></div><h2 id="ready-to-dive-into-analytics">Ready to Dive into Analytics</h2>
<p>Our resource definitions are ready. Run <code>pulumi up</code> to provision your Azure Synapse infrastructure.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ pulumi up
...
Do you want to perform this update? yes
Updating (dev)

     Type                                                            Name                  Plan
 +   pulumi:pulumi:Stack                                             azure-py-synapse-dev  created
 +   ├─ azure-nextgen:resources/latest:ResourceGroup                 resourceGroup         created
 +   ├─ azure-nextgen:storage/latest:StorageAccount                  storageAccount        created
 +   ├─ azure-nextgen:storage/latest:BlobContainer                   users                 created
 +   ├─ azure-nextgen:synapse/v20190601preview:Workspace             workspace             created
 +   ├─ random:index:RandomUuid                                      roleName              created
 +   └─ azure-nextgen:authorization/v20200401preview:RoleAssignment  storageAccess         created
 +   ├─ random:index:RandomUuid                                      userRoleName          created
 +   ├─ azure-nextgen:authorization/v20200401preview:RoleAssignment  userAccess            created
 +   ├─ azure-nextgen:synapse/v20190601preview:IpFirewallRule        allowAll              created
 +   ├─ azure-nextgen:synapse/v20190601preview:SqlPool               sqlPool               created
 +   ├─ azure-nextgen:synapse/v20190601preview:BigDataPool           sparkPool             created

 Resources:
    + 12 created

Duration: 10m51s
</code></pre></div><p>You can now navigate to the <a href="https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-pool">Azure Synapse Quickstart, Step 2</a>, and follow along with the data analysis tutorial.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Azure Synapse is a managed analytics service that accelerates time to insight across data warehouses and big data workloads. A Synapse workspace is a critical component of your cloud infrastructure that you should provision with infrastructure as code and other management best practices.</p>
<p>Pulumi and Azure NextGen provider open up full access to all types of Azure resources using your favorite programming languages, including Python, C#, and TypeScript. Navigate to the complete Azure Synapse example in <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-py-synapse">Python</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-cs-synapse">C#</a>, or <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-ts-synapse">TypeScript</a> and get started today.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Containers]]></title>
            <link href="https://mikhail.io/tags/containers/"/>
            <id>https://mikhail.io/tags/containers/</id>
            
            <published>2020-12-01T00:00:00+00:00</published>
            <updated>2020-12-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Running Container Images in AWS Lambda]]></title>
            <link href="https://mikhail.io/2020/12/aws-lambda-container-support/"/>
            <id>https://mikhail.io/2020/12/aws-lambda-container-support/</id>
            
            <published>2020-12-01T00:00:00+00:00</published>
            <updated>2020-12-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>AWS Lambda launches support for packaging and deploying functions as container images</blockquote><p>When AWS Lambda launched in 2014, it pioneered the concept of Function-as-a-Service. Developers could write a function in one of the supported programming languages, upload it to AWS, and Lambda executes the function on every invocation.</p>
<p>Ever since then, a zip archive of application code or binaries has been the only supported deployment option. Even AWS Lambda Layers—reusable components automatically merged into the application code—used the zip packaging format.</p>
<p>Today, AWS announced that AWS Lambda now supports packaging serverless functions as container images. This means that you can deploy a custom Docker or OCI image as an AWS Lambda function.</p>
<h2 id="why-use-container-images-for-aws-lambda">Why Use Container Images for AWS Lambda?</h2>
<p>The first production-ready version of Docker was released in October 2014, just one month before the announcement of AWS Lambda. Container images are now the de-facto standard of application packaging. Containers run in local development loops, in Kubernetes clusters, including Amazon EKS, as well as in Amazon ECS and AWS Fargate. Docker is embraced across the cloud industry, for instance, Google Cloud Run is a serverless offering centered around container images.</p>
<p>With today&rsquo;s launch, AWS Lambda can run functions packaged as container images, enabling customers to build serverless applications using familiar tools, preferred languages, and required dependencies.</p>
<p>Here are essential scenarios enabled and improved by container image deployments:</p>
<ul>
<li><strong>Package binaries and libraries</strong> that may not be available as a NPM (or another package manager) module. Our video processing example below demonstrates this capability.</li>
<li><strong>Code in an arbitrary programming language</strong>. While already possible with Lambda Layers and Runtime API, container images will streamline the experience of developing functions in languages not native to AWS Lambda.</li>
<li><strong>Use custom OS and custom runtime versions</strong> to match requirements and standard practices in a given company.</li>
<li><strong>Deploy large files and packages</strong>. Container images up to 10 GB are supported, as opposed to the previous hard limit of 250 MB of unzipped files.</li>
<li><strong>Reuse existing base images</strong> to bring reliable and battle-tested implementations from the broad community or domain-specific scenarios.</li>
<li><strong>Apply centralized container image building and packaging governance and security requirements</strong> to AWS Lambda deployments. This provides Enterprise customers with a higher level of control.</li>
</ul>
<p>AWS Lambda functions packaged as container images will continue to benefit from the event-driven execution model, consumption-based billing, automatic scaling, high availability, fast start-up, and native integrations with numerous AWS services.</p>
<h2 id="how-it-works">How It Works</h2>
<p>You can get started with deploying containers to AWS Lambda in three steps:</p>
<ol>
<li><strong>Prepare a container definition</strong> that implements the Lambda Runtime Interface as explained below.</li>
<li><strong>Build</strong> the container image and <strong>publish</strong> it to Amazon Elastic Container Registry (ECR).</li>
<li><strong>Deploy an AWS Lambda</strong>, grant it access to the ECR, and point it to the container image.</li>
</ol>
<p><img src="lambdacontainers.png" alt="Lambda Function packaged as a Container"></p>
<p>Your container image has to implement AWS Lambda runtime API. Runtime API is a simple HTTP-based protocol with operations to retrieve invocation data, submit responses, and report errors.</p>
<p>Therefore, not every container image may be deployed to AWS Lambda. You have two main options:</p>
<ul>
<li><strong>Choose a base image provided by AWS</strong>, which already includes the Runtime Interface Client (RIC). AWS provides base images for Node.js, Python, Java, Go, .NET Core, and Ruby.</li>
<li><strong>Use an arbitrary base image</strong> and implement the API with an AWS Lambda runtime client SDK. Using a custom base image, you can leverage open-source AWS Lambda Runtime Interface Client to make the image compatible with Lambda’s runtime API.</li>
</ul>
<h2 id="container-images-in-aws-lambda-vs-aws-fargate">Container Images in AWS Lambda vs. AWS Fargate</h2>
<p>Lambda Container image support further blurs the lines between Lambda and Fargate. It’s important to understand the remaining differences to decide which service to use in a given scenario:</p>
<ul>
<li><strong>Cost structure</strong>. Lambda is charged per execution, while Fargate has a fixed cost per vCPU per hour regardless of the actual workload. Depending on requests per second served by a single CPU, one model can be much more frugal than the other.</li>
<li><strong>Scale to zero</strong>. Consequently, an idle Lambda function costs nothing while Fargate still incurs fixed minimal running costs. Therefore, breaking an application down to many small specialized Lambda functions is much more practical and common than micro Fargate instances.</li>
<li><strong>Burst workloads</strong>. Lambda scales on a per-request basis and can go from zero to a thousand instances in seconds. It’s a great fit for applications with bursty workloads that need to switch from idle to full capacity and back.</li>
<li><strong>Performance</strong>. Fargate runs on more dedicated resources, so overall performance will likely be better than on Lambda. For time-sensitive and critical APIs, Fargate may offer a fast and consistent experience superior to Lambda, especially on high percentiles.</li>
<li><strong>Integration with AWS services</strong>. Lambda comes with native integration with 100+ AWS services. It’s straightforward to trigger a function for incoming SQS messages or new files in an S3 bucket, which requires more wiring for Fargate tasks.</li>
<li><strong>Resource limits</strong>. Lambda executions are limited to 15 minutes and may only consume up to 10 GB of RAM. Fargate may be the only option for long-running resource-demanding jobs.</li>
</ul>
<p>Overall, Lambda shines for unpredictable or inconsistent workloads and applications easily expressed as isolated functions triggered by events in other AWS services.</p>
<h2 id="example-serverless-video-thumbnailer-with-aws-lambda-and-pulumi">Example: Serverless Video Thumbnailer with AWS Lambda and Pulumi</h2>
<p>Let&rsquo;s walk through the steps to build an example application with container-based AWS Lambda. In this scenario, a function runs every time a new video is uploaded to an Amazon S3 bucket. It relies on FFmpeg tools to produce a thumbnail of the uploaded video and uploads the thumbnail back to the same S3 bucket.</p>
<p>We&rsquo;ll use Pulumi to provision the necessary resources. You can check out the <a href="https://github.com/pulumi/examples/tree/master/aws-ts-lambda-thumbnailer">full source code</a> in the Pulumi Examples.</p>
<h3 id="define-a-dockerfile">Define a Dockerfile</h3>
<p>Here are the key features of the container image for our Thumbnailer:</p>
<ul>
<li>Based on the <code>amazon/aws-lambda-nodejs:12</code> image</li>
<li>Installs AWS CLI to copy objects to and from S3</li>
<li>Installs FFmpeg to process video files</li>
<li>Copies the <code>index.js</code> with function implementation</li>
<li>Points the <code>index.handler</code> command</li>
</ul>
<p>You can find the full Dockerfile <a href="https://github.com/pulumi/examples/blob/master/aws-ts-lambda-thumbnailer/docker-ffmpeg-thumb/Dockerfile">here</a>.</p>
<h3 id="create-an-s3-bucket">Create an S3 Bucket</h3>
<p>Now, let&rsquo;s start composing our Pulumi program in TypeScript. The first step is to define an S3 bucket.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> aws <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/aws&#34;</span>;

<span style="color:#008000">// A bucket to store videos and thumbnails.
</span><span style="color:#008000"></span><span style="color:#00f">const</span> bucket = <span style="color:#00f">new</span> aws.s3.Bucket(<span style="color:#a31515">&#34;bucket&#34;</span>);
</code></pre></div><h3 id="build-the-container-image-and-publish-it-to-ecr">Build the container image and publish it to ECR</h3>
<p>We can use <a href="https://www.pulumi.com/docs/guides/crosswalk/aws/">Pulumi Crosswalk for AWS</a> to build the Docker image and publish it to a new ECR repository with just three lines of code.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> awsx <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/awsx&#34;</span>;

<span style="color:#00f">const</span> image = awsx.ecr.buildAndPushImage(<span style="color:#a31515">&#34;image&#34;</span>, {
   context: <span style="color:#a31515">&#34;./docker-ffmpeg-thumb&#34;</span>,
});
</code></pre></div><p>The local <code>docker-ffmpeg-thumb</code> folder contains the application files (<code>Dockerfile</code> and <code>index.js</code>).</p>
<h3 id="setup-a-role">Setup a role</h3>
<p>Next, we define an IAM role and a policy attachment to grant AWS Lambda access to ECR, CloudWatch, and other supporting services.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> role = <span style="color:#00f">new</span> aws.iam.Role(<span style="color:#a31515">&#34;thumbnailerRole&#34;</span>, {
   assumeRolePolicy: <span style="color:#2b91af">aws.iam.assumeRolePolicyForPrincipal</span>({ Service: <span style="color:#a31515">&#34;lambda.amazonaws.com&#34;</span> }),
});
<span style="color:#00f">new</span> aws.iam.RolePolicyAttachment(<span style="color:#a31515">&#34;lambdaFullAccess&#34;</span>, {
   role: <span style="color:#2b91af">role.name</span>,
   policyArn: <span style="color:#2b91af">aws.iam.ManagedPolicies.AWSLambdaFullAccess</span>,
});
</code></pre></div><h3 id="configure-your-aws-lambda-function">Configure your AWS Lambda function</h3>
<p>It&rsquo;s time to define the AWS Lambda function itself! It&rsquo;s as simple as giving it a name and pointing to the image URI returned from the ECR. Also, we assign the role and increase the timeout to 15 minutes, as video processing may take a while.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> thumbnailer = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;thumbnailer&#34;</span>, {
   packageType: <span style="color:#a31515">&#34;Image&#34;</span>,
   imageUri: <span style="color:#2b91af">image.imageValue</span>,
   role: <span style="color:#2b91af">role.arn</span>,
   timeout: <span style="color:#2b91af">900</span>,
});
</code></pre></div><h3 id="trigger-on-new-videos">Trigger on new videos</h3>
<p>Finally, we can assign a trigger to the function using the <code>bucket.onObjectCreated</code> helper method. We want to limit the function to only process <code>mp4</code> files.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// When a new video is uploaded, run the FFMPEG task on the video file.
</span><span style="color:#008000"></span>bucket.onObjectCreated(<span style="color:#a31515">&#34;onNewVideo&#34;</span>, thumbnailer, { filterSuffix: <span style="color:#a31515">&#34;.mp4&#34;</span> });
</code></pre></div><p>And that is it! We run <code>pulumi up</code> to get the thumbnailed service up and running.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Support for container images in AWS Lambda brings the power and usability of industry-standard packaging to serverless functions. It becomes easy to reuse application components packaged with the ubiquitous deployment format.</p>
<p>In this post, we’ve shown how to use Pulumi to build a container image and configure an AWS Lambda to run it. Pulumi makes it easy to create artifacts and provision and manage cloud infrastructure on any cloud using familiar programming languages, including TypeScript, Python, Go and .NET. Docker images, ECR registries, and Lambda functions can be managed within the same infrastructure definition.</p>
<p>Further steps:</p>
<ul>
<li>Check out the full <a href="https://github.com/pulumi/examples/tree/master/aws-ts-lambda-thumbnailer">Lambda + Docker example</a> in the Pulumi Examples.</li>
<li><a href="https://www.pulumi.com/docs/get-started/aws/">Get Started</a> with Pulumi for AWS today.</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/containers" term="containers" label="Containers" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless]]></title>
            <link href="https://mikhail.io/tags/serverless/"/>
            <id>https://mikhail.io/tags/serverless/</id>
            
            <published>2020-12-01T00:00:00+00:00</published>
            <updated>2020-12-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Guest Post]]></title>
            <link href="https://mikhail.io/tags/guest-post/"/>
            <id>https://mikhail.io/tags/guest-post/</id>
            
            <published>2020-11-18T00:00:00+00:00</published>
            <updated>2020-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How to Monitor Your Business with BAM Tools?]]></title>
            <link href="https://mikhail.io/2020/11/how-to-monitor-your-business-with-bam-tools/"/>
            <id>https://mikhail.io/2020/11/how-to-monitor-your-business-with-bam-tools/</id>
            
                    <author>
                        <name>Nadeem Ahamed Riswanbasha</name>
                    </author>
            <published>2020-11-18T00:00:00+00:00</published>
            <updated>2020-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Guest Post by Nadeem Ahamed Riswanbasha</blockquote><h2 id="introduction">Introduction</h2>
<p>Business Activity Monitoring (BAM) plays a vital role in identifying and resolving issues in the flow of a business process. Also, the concept of BAM is not very new to the world of technology because there are already a handful of organisations leveraging BAM to achieve end-to-end business tracking.</p>
<p>This blog will feed you with everything that you must know about <a href="https://www.serverless360.com/business-activity-monitoring">Business Activity Monitoring</a> starting right from scratch.</p>
<p>Moving forward, we will see terms like business processes, transactions, and stages very often. It would be better to figure out these terms to understand the concept of BAM.</p>
<ul>
<li><strong>Business Process</strong> - A collection of one or more related business transactions</li>
<li><strong>Transaction</strong> - An operation of the business which represents a set of business activities</li>
<li><strong>Stage</strong> - An activity within the transaction. A transaction is made up of one or more stages.</li>
</ul>
<p><img src="./image-1.png" alt="BAM"></p>
<h2 id="what-is-bam">What is BAM?</h2>
<ul>
<li>As mentioned above, BAM is a technology that is used to identify issues and risks in a business process where this increases the efficiency of any application.</li>
<li>It is a process of tracking events and transactions and then presenting the information in the form of dashboards and reports which eventually helps in performing analysis on the collected set of data.</li>
</ul>
<blockquote>
<p><strong>Business Activity Monitoring is used for both on-premise and cloud applications to gain complete visibility.</strong></p>
</blockquote>
<p>Any BAM tool should cover the following key areas irrespective of the type of application (cloud, on-premise, or hybrid).</p>
<ul>
<li><strong>Aggregation</strong> - This is the process of capturing &amp; collecting data from your business processes.</li>
<li><strong>Analysis</strong> - After that, the data should be analysed by performing various data operations.</li>
<li><strong>Presentation</strong> - The analysed data is represented in the form of dashboards for quick understanding.</li>
</ul>
<h2 id="role-of-bam-in-monitoring-serverless-applications">Role of BAM in Monitoring Serverless Applications</h2>
<p>Imagine a business scenario where mostly more than one Azure resource will be used to define a business process. As the needs of the user keep increasing, the business process keeps growing, which turns out to be complex and makes monitoring business processes hard and challenging.</p>
<p>Here is a real-time example to make you clearly understand what business process is and how Business Activity Monitoring can be used in these situations.</p>
<p>Below is a car booking application consisting of various Azure services like Web Apps, Service Bus, Logic Apps, and Azure Functions where all these are integrated to provide a particular solution.</p>
<ul>
<li>Consider a Business user, who needs message tracking through every stage in the above business activity.</li>
<li>There would also be needs for the user to be informed of any exception in the business transaction along with the reason behind the failure.</li>
<li>A Business development manager would need to have analytic information on the booking trends at various locations.</li>
</ul>
<p><img src="./image-2.png" alt="BAM"></p>
<p>Serverless360 is one tool that can satisfy all the above needs. It has an in-built feature called Business Activity Monitoring to provide complete visibility and end to end tracking on your business process.</p>
<p>The picture below is a sample flow diagram for the car booking application where each stage &amp; message can be tracked very easily, and that would reduce the support overhead.</p>
<p><img src="./image-3.png" alt="BAM"></p>
<h2 id="serverless360-bam">Serverless360 BAM</h2>
<p>This provides business process monitoring solution for cloud architectures, and there are numerous operations supported by BAM to perform on these business processes.</p>
<p>So, now lets look at what Serverless360 BAM is capable of when it comes to providing end to end visibility.</p>
<h3 id="tracking">Tracking</h3>
<p>A graphical designer will facilitate modelling your business process transactions in the form of an activity diagram. You will define the stages of your business transactions here.</p>
<p>In the above-mentioned car booking example, there would be a need to track the Driver Id, User Id &amp; Location, which can be achieved at ease by configuring the required properties in the stages of the business process.</p>
<p><strong>Also, each stage is represented with different colours to make it easy for the support team to identify if they are in success, failure, in progress, or not executed state.</strong></p>
<p><img src="./image-4.png" alt="BAM"></p>
<h3 id="filtering">Filtering</h3>
<p>BAM in Serverless360 provides <strong>advanced filtering</strong> where you can comfortably filter the specified business transactions by using simple <strong>search queries</strong>. In Addition to query filtering, it also supports filtering based on the time interval.</p>
<p>In the car booking scenario, this feature allows you to manage and search for your business transactions using properties like customer id, driver id, location, and even by the status of the transaction.</p>
<p><img src="./image-5.png" alt="BAM"></p>
<h3 id="reprocessing">Reprocessing</h3>
<p>By tracking the business processes, it would be offhand to identify the failed transactions but just identifying will not help the user in any way and that is why BAM in Serverless360 is enriched with a feature called Reprocessing.</p>
<p>It also states the reason for failure, and that makes it easy for the support person to change the required data and reprocess the message to the stage by just clicking the reprocess option just above your business flow diagram.</p>
<p>It supports two types of reprocessing</p>
<ul>
<li><strong>Dynamic reprocessing:</strong> This can be useful when the user wants to reprocess along with the tracked property values as the header.</li>
<li><strong>Bulk reprocessing:</strong> For this, you need to map a reprocess setting of a particular stage to that respective transaction.</li>
</ul>
<p><img src="./image-6.png" alt="BAM"></p>
<h2 id="monitoring">Monitoring</h2>
<p>Serverless360 has several monitors to monitor Azure Serverless applications, and in the same way, it also has a powerful monitor that is the Business Process Monitoring for BAM, which includes Exception Monitoring and Query Monitoring.</p>
<h3 id="business-process-monitoring">Business Process Monitoring</h3>
<ul>
<li>Serverless360 comes with the out of box monitoring solution for <strong>monitoring business processes based on queries and exceptions</strong> called Business Process Monitor.</li>
<li>These monitors support alerting through notification channels when the count of failed transactions goes beyond the certain limit at a location or alert whenever there is an Exception.</li>
<li>It is also possible to view the historical record of alert reports in the calendar view.</li>
</ul>
<p><img src="./image-7.png" alt="BAM"></p>
<h3 id="exception-monitoring">Exception Monitoring</h3>
<ul>
<li>This allows users to log exceptions along with exception code in the stages of a business transaction.</li>
<li>The logged exceptions will be displayed to the users on selecting the transaction instance.</li>
<li>Users will also need alerts immediately whenever an exception gets logged in any of the stages of the transactions.</li>
</ul>
<p><img src="./image-8.png" alt="BAM"></p>
<h3 id="query-monitoring">Query Monitoring</h3>
<ul>
<li>This type of monitoring will alert the configured user when a given query violates the configured threshold values on the business process.</li>
<li>You can manually set the error and warning threshold conditions while creating the query monitor.</li>
</ul>
<p><img src="./image-9.png" alt="BAM"></p>
<h2 id="analytics">Analytics</h2>
<ul>
<li><strong>Dashboards</strong> play a major role in analysing the data as it supports creating widgets and custom dashboards based on the data in your business processes.</li>
<li>It helps the support team to visualise the data in the form of easily understandable charts based on the configured queries.</li>
</ul>
<p><img src="./image-10.png" alt="BAM"></p>
<blockquote>
<p><strong>BAM in Serverless360 supports tracking not only Azure solutions but also other hybrid applications (a combination of cloud and on-premise components).</strong></p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>As discussed above, BAM is highly essential for both cloud and on-premise applications as it will reduce the pressure on the support team by identifying the issues in your business transactions. Also, I hope this blog would help you to get a basic idea of how Serverless360 works when it comes to Business Activity Monitoring.</p>
<p>Also read,</p>
<ul>
<li><a href="https://www.serverless360.com/blog/serverless360-bam-quick-walkthrough">Serverless360 BAM – A Quick Walkthrough</a></li>
<li><a href="https://www.serverless360.com/blog/bam-message-tracking-in-azure-serverless-application">BAM Message tracking in Azure Applications</a></li>
<li><a href="https://www.serverless360.com/blog/microsoft-flow-monitoring-using-serverless360-bam">Microsoft Flow Monitoring using Serverless360 BAM</a></li>
<li><a href="https://www.serverless360.com/blog/hybrid-application-tracking-using-serverless360-bam">Hybrid Application Tracking Using Serverless360 BAM</a></li>
<li><a href="https://www.serverless360.com/business-activity-monitoring">Know more features of BAM in Serverless360</a></li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/guest-post" term="guest-post" label="Guest Post" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How To Deploy Temporal to Azure Kubernetes Service (AKS)]]></title>
            <link href="https://mikhail.io/2020/11/how-to-deploy-temporal-to-azure-kubernetes-aks/"/>
            <id>https://mikhail.io/2020/11/how-to-deploy-temporal-to-azure-kubernetes-aks/</id>
            
            <published>2020-11-11T00:00:00+00:00</published>
            <updated>2020-11-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Get up and running with Temporal workflows in Azure and Kubernetes in several CLI commands</blockquote><p>In my article <a href="https://mikhail.io/2020/10/practical-approach-to-temporal-architecture/">A Practical Approach to Temporal Architecture</a>, I outlined the various <a href="https://temporal.io/">Temporal</a> components and how they interact. Today’s blog builds on this knowledge and demonstrates an example of deploying Temporal to Kubernetes and, more specifically, to Azure Kubernetes Service (AKS).</p>
<p>My example is self-contained: it provisions a full environment with all the required Azure resources, Temporal service, and application deployment artifacts. Here is a diagram of the cloud infrastructure:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="temporal-aks.png"
            alt="Deployment architecture"
             />
        
    
    <figcaption>
        <h4>Deployment architecture</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>This sample deployment is implemented as a <a href="https://pulumi.com/">Pulumi</a> program in TypeScript. You can find the full code in <a href="https://github.com/mikhailshilkov/temporal-samples/tree/main/azure-aks">my GitHub</a>.</p>
<h2 id="application-code">Application Code</h2>
<p>The <code>workflow</code> folder contains all of the application code. The application is written with Go and consists of two source files:</p>
<ol>
<li><code>helloworld.go</code> - defines a workflow &amp; activity</li>
<li><code>main.go</code> - application entry point.</li>
</ol>
<p>The example deploys a “Hello World” Temporal application copied from <a href="https://github.com/temporalio/samples-go/blob/master/helloworld/helloworld.go">this Go sample</a>. Once you get it up and running, you can certainly customize the code with your own workflow and activities.</p>
<p>The <code>main.go</code> file does two things.</p>
<p><strong>First</strong>, it spins up a worker:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">w := worker.New(c, <span style="color:#a31515">&#34;hello-world&#34;</span>, worker.Options{})
w.RegisterWorkflow(helloworld.Workflow)
w.RegisterActivity(helloworld.Activity)
</code></pre></div><p><strong>Second</strong>, it launches an HTTP server in the same process. The server exposes endpoints to start workflows. The <code>/async?name=&lt;yourname&gt;</code> endpoint starts a new workflow and immediately returns, while the <code>/sync?name=&lt;yourname&gt;</code> blocks and waits for the result of the execution and returns the response.</p>
<p>You can find the implementation in the <a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/workflow/main.go#L22"><code>start</code></a> function. Note that this simplistic starter is specifc to the &ldquo;Hello World&rdquo; workflow as it expects one argument and one result, both strings.</p>
<h2 id="deployment-structure">Deployment Structure</h2>
<p>My program combines three component resources: a MySQL Database, an AKS cluster, and a Temporal deployment. As a result, the main file deploy all of these resources to a single Azure Resource Group:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> resources.ResourceGroup(<span style="color:#a31515">&#34;resourceGroup&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroupName</span>,
    location: <span style="color:#a31515">&#34;WestEurope&#34;</span>,
});

<span style="color:#00f">const</span> database = <span style="color:#00f">new</span> MySql(<span style="color:#a31515">&#34;mysql&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    location: <span style="color:#2b91af">resourceGroup.location</span>,
    administratorLogin: <span style="color:#a31515">&#34;mikhail&#34;</span>,
    administratorPassword: <span style="color:#2b91af">mysqlPassword</span>,
});

<span style="color:#00f">const</span> cluster = <span style="color:#00f">new</span> AksCluster(<span style="color:#a31515">&#34;aks&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    location: <span style="color:#2b91af">resourceGroup.location</span>,
    kubernetesVersion: <span style="color:#a31515">&#34;1.16.13&#34;</span>,
    vmSize: <span style="color:#a31515">&#34;Standard_DS2_v2&#34;</span>,
    vmCount: <span style="color:#2b91af">3</span>,
});

<span style="color:#00f">const</span> temporal = <span style="color:#00f">new</span> Temporal(<span style="color:#a31515">&#34;temporal&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    location: <span style="color:#2b91af">resourceGroup.location</span>,
    version: <span style="color:#a31515">&#34;1.1.1&#34;</span>,
    storage: {
        <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;mysql&#34;</span>,
        hostName: <span style="color:#2b91af">database.hostName</span>,
        login: <span style="color:#2b91af">database.administratorLogin</span>,
        password: <span style="color:#2b91af">database.administratorPassword</span>,
    },
    cluster: <span style="color:#2b91af">cluster</span>,
    app: {
        <span style="color:#00f">namespace</span>: <span style="color:#a31515">&#34;temporal&#34;</span>,
        folder: <span style="color:#a31515">&#34;./workflow&#34;</span>,
        port: <span style="color:#2b91af">8080</span>,
    },
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> webEndpoint = temporal.webEndpoint;
<span style="color:#00f">export</span> <span style="color:#00f">const</span> starterEndpoint = temporal.starterEndpoint;
</code></pre></div><p>The rest of the article gives an overview of the building blocks of these three components.</p>
<h2 id="docker-image">Docker Image</h2>
<p>Since the application is deployed to Kubernetes, we need to produce a custom Docker image. The <a href="https://github.com/mikhailshilkov/temporal-samples/blob/main/azure-aks/workflow/Dockerfile"><code>Dockerfile</code></a> builds the Go application and exposes port <code>8080</code> to the outside world so we can access the starter HTTP endpoints.</p>
<p>Pulumi deploys this <code>Dockerfile</code> to Azure in three steps:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L42-L50">Deploy</a> an Azure Container Registry.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L52-L58">Retrieve</a> the registry’s admin credentials generated by Azure.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L61-L69">Publish</a> the application image to the registry.</li>
</ul>
<h2 id="mysql-database">MySQL Database</h2>
<p>There are several persistence options supported by Temporal. A straightforward option for Azure users is to deploy an instance of Azure Database for MySQL. It’s a fully managed database service where Azure is responsible for uptime and maintenance, and users pay a flat fee per hour.</p>
<p>My example <a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/mysql.ts#L24-L50">provisions</a> an instance of MySQL 5.7 at the Basic tier. The database size is limited to 5 GB.</p>
<p>A final tweak is to <a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/mysql.ts#L52-L58">add</a> a firewall rule for the IP address <code>0.0.0.0</code>, which enables network access to MySQL from any Azure service. Note that this option isn’t secure for production workloads: read more in <a href="https://docs.microsoft.com/en-us/azure/mysql/concepts-firewall-rules#connecting-from-azure">Connecting from Azure</a>.</p>
<h2 id="azure-kubernetes-cluster">Azure Kubernetes Cluster</h2>
<p>The example creates a new AKS cluster and deploys the Temporal service and applications components to that cluster.</p>
<p>The <code>AksCluster</code> component:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/cluster.ts#L23-L39">Sets up</a> an Azure Active Directory Application and a Service Principal.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/cluster.ts#L42-L45">Creates</a> an SSH key for the cluster’s admin user profile.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/cluster.ts#L48-L86">Provisions</a> a managed Kubernetes cluster base on VM Scale Sets node pool. Feel free to adjust the VM size, count, and the Kubernetes version.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/cluster.ts#L88-L96">Builds</a> the Kubeconfig YAML to connect to the cluster and deploy application components.</li>
</ul>
<h2 id="temporal-service-and-web-console">Temporal Service and Web Console</h2>
<p>Next, we deploy the Temporal Service and Temporal Web Console as two Kubernetes services.</p>
<p>We start with sound groundwork:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L71-L78">Declare</a> a custom Pulumi Kubernetes provider and point it to the Kubeconfig string that we retrieved from the managed cluster.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L71-L78">Grant</a> permission for the managed cluster’s service principal to access images from the Azure Container Registry.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L85-L89">Define</a> a new Kubernetes namespace that contains all Temporal deployments and services.</li>
</ul>
<p>Then, we can deploy the Temporal Service:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L91-L103">Stores</a> the MySQL password as a Kubernetes secret.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L136">Refers</a> to the <code>temporalio/auto-setup</code> Docker image provided by Temporal. The image automatically populates the database schema during the first run.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L143-L163">Sets up</a> environment variables to connect to MySQL.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L186-L213">Deploys</a> a <code>ClusterIP</code> service using port <code>7233</code>.</li>
</ul>
<p>The Web Console follows:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L246">Refers</a> to the <code>temporalio/web</code> Docker image provided by Temporal.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L249-L252">Connects</a> to the gRPC endpoint of the Temporal Service.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L275-L301">Deploys</a> a <code>ClusterIP</code> service using port <code>8088</code>.</li>
</ul>
<h2 id="temporal-worker">Temporal Worker</h2>
<p>The final component is a Temporal worker that runs application workflows and activities. In my setup, the worker is a Kubernetes deployment that pulls the custom Docker image from the container registry.</p>
<p>The application component:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L337">Refers</a> to the custom Docker image created above.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L337">Connects</a> to the gRPC endpoint of the Temporal Service.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/7ce8c36eb64628d899ce618339f3691486f37e81/azure-aks/temporal.ts#L337">Deploys</a> a <code>ClusterIP</code> service using port <code>8080</code>.</li>
</ul>
<h2 id="get-started">Get Started</h2>
<p>The Pulumi Command-Line Interface (CLI) runs the deployment. <a href="https://www.pulumi.com/docs/get-started/install/">Install Pulumi</a>, navigate to the folder where you have the example cloned, and run the following commands:</p>
<ol>
<li>Create a new stack (a Pulumi deployment environment):</li>
</ol>
<pre><code>pulumi stack init dev
</code></pre><ol>
<li>Login to <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">Azure CLI</a>:</li>
</ol>
<pre><code>az login
</code></pre><ol>
<li>Install NPM dependencies:</li>
</ol>
<pre><code>npm install
</code></pre><ol>
<li>Run <code>pulumi up</code> and confirm when asked if you want to deploy. Azure resources are provisioned:</li>
</ol>
<pre><code>$ pulumi up...
Performing changes:...

Outputs:
    starterEndpoint: &quot;http://21.55.177.186:8080/async?name=&quot;
    webEndpoint : &quot;http://52.136.6.198:8088&quot;

Resources:+ 27 created
Duration: 6m46s
</code></pre><ol>
<li>The output above prints the endpoints to interact with the application. Run the following command to start a “Hello World” workflow:</li>
</ol>
<pre><code>curl $(pulumi stack output starterEndpoint)WorldStarted workflow ID=World, RunID=b4f6db00-bb2f-498b-b620-caad81c91a81%
</code></pre><p>Now, open the <code>webEndpoint</code> URL in your browser and find the workflow (it’s probably already in the Completed state).</p>
<h2 id="cost-security-and-further-steps">Cost, Security, and Further Steps</h2>
<p>The deployment above provisions real Azure resources, so be mindful of the related costs. Here is an estimated calculation for the “West US 2” region:</p>
<ul>
<li>Azure Database for MySQL Gen5 Basic with 1 vCore and 5 GB of storage = $25.32/month</li>
<li>Azure Kubernetes Cluster of 3 VMs type Standard_DS2_v2: 3 x $83.22/month = $249.66/month (but feel free to adjust to your needs)</li>
<li>Azure Container Registry Basic = $5.00/month</li>
</ul>
<p>The total cost for this example is approximately $280 per month.</p>
<p>Whenever you are done experimenting, run <code>pulumi destroy</code> to delete the resources. Note that all the data will be lost after destruction.</p>
<p>You can find the full code in <a href="https://github.com/mikhailshilkov/temporal-samples/tree/main/azure-aks">my GitHub</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/kubernetes" term="kubernetes" label="Kubernetes" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes]]></title>
            <link href="https://mikhail.io/tags/kubernetes/"/>
            <id>https://mikhail.io/tags/kubernetes/</id>
            
            <published>2020-11-11T00:00:00+00:00</published>
            <updated>2020-11-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How To Deploy Temporal to Azure Container Instances]]></title>
            <link href="https://mikhail.io/2020/10/how-to-deploy-temporal-to-azure-container-instances/"/>
            <id>https://mikhail.io/2020/10/how-to-deploy-temporal-to-azure-container-instances/</id>
            
            <published>2020-10-28T00:00:00+00:00</published>
            <updated>2020-10-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Get up and running with Temporal workflows in Azure in several CLI commands</blockquote><p>In <a href="https://mikhail.io/2020/10/practical-approach-to-temporal-architecture/">my previous article</a>, I outlined the various components of <a href="https://temporal.io">Temporal</a> and how they interact. Today&rsquo;s blog builds on this knowledge and demonstrates an example Temporal deployment.</p>
<p>It&rsquo;s a minimalistic deployment on Azure which combines a managed MySQL database with Azure Container Instances, suitable for simple experimentation and development. Here is a diagram of the cloud infrastructure:</p>
<p><img src="./azure.png" alt="Azure Diagram"></p>
<p>This sample deployment is implemented as a <a href="https://pulumi.com">Pulumi</a> program in TypeScript. You can find the full code in <a href="https://github.com/mikhailshilkov/temporal-samples/tree/main/azure-aci">my GitHub</a>.</p>
<h2 id="application-code">Application Code</h2>
<p>The <code>workflow</code> folder contains all of the application code. The application is written with Go and consists of two source files:</p>
<ol>
<li><code>helloworld.go</code> - defines a workflow and an activity</li>
<li><code>main.go</code> - application entry point.</li>
</ol>
<p>The example deploys a &ldquo;Hello World&rdquo; Temporal application copied from <a href="https://github.com/temporalio/samples-go/blob/master/helloworld/helloworld.go">this Go sample</a>. Once you get it up and running, you can certainly customize the code with your own workflow and activities.</p>
<p>The <code>main.go</code> file does two things. First, it spins up a worker:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">w := worker.New(c, <span style="color:#a31515">&#34;hello-world&#34;</span>, worker.Options{})
w.RegisterWorkflow(helloworld.Workflow)
w.RegisterActivity(helloworld.Activity)
</code></pre></div><p>Second, it launches an HTTP server in the same process. The server exposes endpoints to start workflows. The <code>/async?name=&lt;yourname&gt;</code> endpoint starts a new workflow and immediately returns, while the <code>/sync?name=&lt;yourname&gt;</code> blocks and waits for the result of the execution and returns the response. You can find the implementation in the <a href="https://github.com/mikhailshilkov/temporal-samples/blob/33024f614d4a99a7700eacf2142c8ef2b7cea0fc/azure-aci/workflow/main.go#L22"><code>start</code></a> function.</p>
<h2 id="docker-image">Docker Image</h2>
<p>Since the application is deployed to Azure Container Instances, we need to produce a custom Docker image. <a href="https://github.com/mikhailshilkov/temporal-samples/blob/33024f614d4a99a7700eacf2142c8ef2b7cea0fc/azure-aci/workflow/Dockerfile">The <code>Dockerfile</code></a> builds the Go application and exposes port <code>8080</code> to the outside world so we can access the starter HTTP endpoints.</p>
<p>Pulumi deploys this <code>Dockerfile</code> to Azure in three steps:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/33024f614d4a99a7700eacf2142c8ef2b7cea0fc/azure-aci/temporal.ts#L99-L107">Deploy</a> an Azure Container Registry.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/33024f614d4a99a7700eacf2142c8ef2b7cea0fc/azure-aci/temporal.ts#L109-L115">Retrieve</a> the registry&rsquo;s admin credentials generated by Azure.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/33024f614d4a99a7700eacf2142c8ef2b7cea0fc/azure-aci/temporal.ts#L117-L125">Publish</a> the application image to the registry.</li>
</ul>
<h2 id="mysql-database">MySQL Database</h2>
<p>There are several persistence options supported by Temporal. A straightforward option in Azure is to deploy an instance of Azure Database for MySQL. It&rsquo;s a fully managed database service where Azure is responsible for uptime and maintenance, and users pay a flat fee per hour.</p>
<p>My example <a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/mysql.ts#L24-L50">provisions</a> an instance of MySQL 5.7 at the Basic tier. The database size is limited to 5 GB.</p>
<p>A final tweak is to <a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/mysql.ts#L52-L58">add</a> a firewall rule for the IP address <code>0.0.0.0</code>, which enables network access to MySQL from any Azure service. Note that this option isn&rsquo;t secure for production workloads: read more in <a href="https://docs.microsoft.com/en-us/azure/mysql/concepts-firewall-rules#connecting-from-azure">Connecting from Azure</a>.</p>
<h2 id="temporal-service-and-web-console">Temporal Service and Web Console</h2>
<p>Next, we deploy the Temporal Service and Temporal Web Console as two Azure Container Instances.</p>
<p>The Service container:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L56">Refers</a> to the <code>temporalio/server</code> Docker image provided by Temporal.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L34-L43">Sets up</a> environment variables to connect to MySQL.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L51-L52">Exposes</a> port <code>7233</code> to the outside world. Note that this is not secure for a production environment!</li>
</ul>
<p>The Web Console container:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L82">Refers</a> to the <code>temporalio/web</code> Docker image provided by Temporal.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L91">Connects</a> to the gRPC endpoint gathered from the Service container.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L77-L78">Exposes</a> port <code>8088</code> to the outside world. Note that this is not secure for a production environment!</li>
</ul>
<h2 id="temporal-worker">Temporal Worker</h2>
<p>The final component is a Temporal worker that runs application workflows and activities. In my setup, the worker is another Azure Container Instance that pulls the custom Docker image from the container registry. The worker container:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L143">Refers</a> to the custom Docker image created above.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L152">Connects</a> to the gRPC endpoint gathered from the Service container.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L136-L140">Configures</a> registry credentials to access the private Azure Container Registry.</li>
<li><a href="https://github.com/mikhailshilkov/temporal-samples/blob/f17738aff73ae88e1b5f503790e9247f40f88b38/azure-aci/temporal.ts#L77-L78">Exposes</a> the starter endpoints at the port <code>8080</code>.</li>
</ul>
<h2 id="get-started">Get Started</h2>
<p>The Pulumi Command-Line Interface (CLI) runs the deployment. <a href="https://www.pulumi.com/docs/get-started/install/">Install Pulumi</a>, navigate to the folder where you have the example cloned, and run the following commands:</p>
<ol>
<li>
<p>Create a new stack (a Pulumi deployment environment):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pulumi stack init dev
</code></pre></div></li>
<li>
<p>Login to <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">Azure CLI</a>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">az login
</code></pre></div></li>
<li>
<p>Install NPM dependencies:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">npm install
</code></pre></div></li>
<li>
<p>Run <code>pulumi up</code> and confirm when asked if you want to deploy. Azure resources are provisioned:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ pulumi up
...
Performing changes:
  Type                                                         Name                    Status
  pulumi:pulumi:Stack                                          temporal-azure-aci-dev  created     
  ├─ my:example:MySql                                          mysql                   created  
  │  ├─ azure-nextgen:dbformysql/latest:Server                 mysql                   created
  │  └─ azure-nextgen:dbformysql/latest:FirewallRule           mysql-allow-all         created
  ├─ my:example:Temporal                                       temporal                created  
  │  ├─ docker:image:Image                                     temporal-worker         created
  │  ├─ azure-nextgen:containerregistry/latest:Registry        registry                created
  │  ├─ azure-nextgen:containerinstance/latest:ContainerGroup  temporal-server         created
  │  ├─ azure-nextgen:containerinstance/latest:ContainerGroup  temporal-web            created
  │  └─ azure-nextgen:containerinstance/latest:ContainerGroup  temporal-worker         created
  ├─ random:index:RandomString                                 resourcegroup-name      created  
  ├─ random:index:RandomPassword                               mysql-password          created  
  └─ azure-nextgen:resources/latest:ResourceGroup              rg                      created  
   
Outputs:
    serverEndpoint : <span style="color:#a31515">&#34;21.55.179.245:7233&#34;</span>
    starterEndpoint: <span style="color:#a31515">&#34;http://21.55.177.186:8080/async?name=&#34;</span>
    webEndpoint    : <span style="color:#a31515">&#34;http://52.136.6.198:8088&#34;</span>

Resources:
    + 13 created

Duration: 7m48s
</code></pre></div></li>
<li>
<p>The output above prints the endpoints to interact with the application. Run the following command to start a &ldquo;Hello World&rdquo; workflow:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl <span style="color:#00f">$(</span>pulumi stack output starterEndpoint<span style="color:#00f">)</span>World
Started workflow ID=World, RunID=b4f6db00-bb2f-498b-b620-caad81c91a81%
</code></pre></div></li>
</ol>
<p>Now, open the <code>webEndpoint</code> URL in your browser and find the workflow (it&rsquo;s probably already in the Completed state).</p>
<h2 id="cost-security-and-further-steps">Cost, Security, and Further Steps</h2>
<p>The deployment above provisions real Azure resources, so be mindful of the related costs. Here is an estimated calculation for the &ldquo;West US 2&rdquo; region:</p>
<ul>
<li>Azure Database for MySQL Gen5 Basic with 1 vCore and 5 GB of storage = $25.32/month</li>
<li>Azure Container Instance with 1 vCPU and 1 GB of RAM: 3 x $32.36/month = $97.08/month</li>
<li>Azure Container Registry Basic = $5.00/month</li>
</ul>
<p>The total cost for this example is approximately $127.40 per month.</p>
<p>Whenever you are done experimenting, run <code>pulumi destroy</code> to delete the resources. Note that all the data will be lost after destruction.</p>
<p>As noted in the sections above, the security setup is minimal and is not suitable for any environment that processes real data. In addition to a secure networking setup, a production environment would need to handle scalability, resilience, backups, observability, and so on.</p>
<p>I plan to address those topics in future blog posts. Stay tuned!</p>
<p>You can find the full code in <a href="https://github.com/mikhailshilkov/temporal-samples/tree/main/azure-aci">my GitHub</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[A Practical Approach to Temporal Architecture]]></title>
            <link href="https://mikhail.io/2020/10/practical-approach-to-temporal-architecture/"/>
            <id>https://mikhail.io/2020/10/practical-approach-to-temporal-architecture/</id>
            
            <published>2020-10-22T00:00:00+00:00</published>
            <updated>2020-10-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>What it takes to get Temporal workflows up and running</blockquote><p><a href="https://temporal.io">Temporal</a> enables developers to build highly reliable applications without having to worry about all the edge cases. If you are new to Temporal, check out my article <a href="https://mikhail.io/2020/10/temporal-open-source-workflows-as-code/">Open Source Workflows as Code</a>. Now that you&rsquo;re excited, I&rsquo;ll cover how you can get up and running with Temporal.</p>
<p>Temporal consists of several components. In this post, I want to outline the primary building blocks and their interactions. By the end, you&rsquo;ll have a broad picture of Temporal and the considerations of deploying to development, staging, and production environments.</p>
<h2 id="workers">Workers</h2>
<p>Workers are compute nodes that run your Temporal application code. You compile your workflows and activities into a worker executable. Next you can run the worker executable in the background and it will listen for new tasks to process.</p>
<p>Your first development environment will probably only need one worker that runs both workflows and activities in a single process. The worker can be hosted anywhere you wish: as a local process on your laptop, as an AWS Fargate task, as a pod in Kubernetes, and so on.</p>
<figure >
    
        <img src="worker.png"
            alt="A Temporal worker executes workflows and activities"
             />
        
    
    <figcaption>
        <h4>A Temporal worker executes workflows and activities</h4>
    </figcaption>
    
</figure>
<p>Production deployments typically run numerous workflows with significant load, so you will likely need to run multiple workers in parallel. Ensuring that nodes are spread out over a compute cluster enables resiliency and scalability.</p>
<figure >
    
        <img src="workers.png"
            alt="Workload scaled out to several Temporal workers"
             />
        
    
    <figcaption>
        <h4>Workload scaled out to several Temporal workers</h4>
    </figcaption>
    
</figure>
<p>Workers operate independently, each crunching through its share of the workload. Workers are logically &ldquo;stateless&rdquo;: they don&rsquo;t keep track of the past and future. Workers don&rsquo;t talk to each other directly, but they coordinate via a central Temporal service—the brain of the system.</p>
<h2 id="temporal-service">Temporal Service</h2>
<p>Temporal Service is a component provided by Temporal. Its purpose is to keep track of workflows, activities, and tasks and coordinate workers' execution.</p>
<p>Your early environments may have a single Service instance running in the background. A worker knows the Service&rsquo;s domain name (IP) and port and connects to the Service via gRPC.</p>
<figure >
    
        <img src="service.png"
            alt="Worker interacting with Temporal Service"
             />
        
    
    <figcaption>
        <h4>Worker interacting with Temporal Service</h4>
    </figcaption>
    
</figure>
<p>Temporal Service itself consists of multiple components: a front-end, matching, history, and others. However, it&rsquo;s okay to treat it as a black-box &ldquo;Service&rdquo; for now.</p>
<p>Once you start growing your workloads, you will need to scale the Service components out to multiple instances for high resilience and throughput. So, several workers are now talking to several service instances.</p>
<figure >
    
        <img src="services.png"
            alt="Temporal Cluster running multiple components"
             />
        
    
    <figcaption>
        <h4>Temporal Cluster running multiple components</h4>
    </figcaption>
    
</figure>
<p>Temporal Service can tolerate node failures because it stores its state in an external storage.</p>
<h2 id="data-store">Data Store</h2>
<p>All the workflow data—task queues, execution state, activity logs—are stored in a persistent Data Store. Persistence technology is pluggable with two options currently officially supported: Cassandra and MySQL. More alternatives, including PostgreSQL, are on the way.</p>
<p>A MySQL database completes the simplest Temporal deployment diagram.</p>
<figure >
    
        <img src="mysql.png"
            alt="MySQL as a Data Store for Temporal"
             />
        
    
    <figcaption>
        <h4>MySQL as a Data Store for Temporal</h4>
    </figcaption>
    
</figure>
<p>For heavily distributed applications that experience high load, it may be better to use Cassandra instead. This database could be a cluster managed by yourself or a managed cloud service (like Azure Cosmos DB):</p>
<figure >
    
        <img src="cassandra.png"
            alt="Cassandra as a Data Store for Temporal"
             />
        
    
    <figcaption>
        <h4>Cassandra as a Data Store for Temporal</h4>
    </figcaption>
    
</figure>
<p>Your code never interacts with the Data Store directly. Basically, it&rsquo;s a (rather important) implementation detail of the Temporal Service.</p>
<h2 id="temporal-web-console">Temporal Web Console</h2>
<p>Temporal provides a handy web console to view namespaces, workflows, and activity history. Technically, it&rsquo;s not a required component—but you probably want to have it under your belt for manual inspection and troubleshooting.</p>
<p>You&rsquo;ll likely have other client components, for instance, to start workflow executions.</p>
<h2 id="external-client">External Client</h2>
<p>You need a client to start your very first workflow execution. It can be the Temporal command-line interface <code>tctl</code> or any custom code that you wrote with a call to initiate a workflow using an SDK or raw gRPC.</p>
<p>Each client must connect to a Temporal Service in order to start or terminate workflows, wait for completion, list the results from the history, and so on. For instance, a web application could start a workflow execution every time it handles a particular HTTP endpoint.</p>
<h2 id="put-it-all-together">Put It All Together</h2>
<p>The following picture puts all the pieces covered above together.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="components.png"
            alt="High-level Temporal architecture"
             />
        
    
    <figcaption>
        <h4>High-level Temporal architecture</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>As you can tell, there are quite a few components that go into making Temporal tick! Temporal is designed to handle millions of workflows reliably with high-performance guarantees while being open-source and portable across different infrastructure options. This dictates the usage of a multi-tier approach.</p>
<p>You can deploy everything to your development machine with the <a href="https://docs.temporal.io/docs/go-run-your-first-app/">getting started guide</a>. After that, you can start moving the components to your favorite cloud environment. Over the next weeks, I&rsquo;ll be working through automation scenarios and accompanying blog posts to help you manage typical deployment needs.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Temporal: Open Source Workflows as Code]]></title>
            <link href="https://mikhail.io/2020/10/temporal-open-source-workflows-as-code/"/>
            <id>https://mikhail.io/2020/10/temporal-open-source-workflows-as-code/</id>
            
            <published>2020-10-15T00:00:00+00:00</published>
            <updated>2020-10-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Temporal reimagines state-dependent service-orchestrated application development</blockquote><p>Regular readers of my blog may recognize me as a big fan of <a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Azure Durable Functions</a>. Durable Functions are an extension of Azure Functions that lets you write <strong>stateful</strong> functions and <strong>workflows</strong>. The SDK does a lot behind the scenes allowing you to focus exclusively on business logic:</p>
<ul>
<li>State management</li>
<li>Automatic checkpointing</li>
<li>Handles restarts on your behalf</li>
</ul>
<p>However, many people have a hard time understanding what Durable Functions are and, especially, when they are useful. I tried to help spread the ideas: I wrote an extensive article <a href="/2018/12/making-sense-of-azure-durable-functions/">Making Sense of Azure Durable Functions</a>, published <a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">DurableFunctions.FSharp</a> library, presented a few conference talks.</p>
<h2 id="beyond-the-scope-of-durable-functions">Beyond The Scope of Durable Functions</h2>
<p>While Durable Functions are great and pretty simple to get started with, their reliance on Azure services implies limitations on applicability.</p>
<p>Durable Functions are designed to run inside the Azure cloud. While you don&rsquo;t have to run functions in a serverless compute model, the durability features rely heavily on <strong>Azure Storage</strong>. There was an initiative to bring Redis as an alternative backend, but the attempt has seemingly stagnated.</p>
<p>Ideally, a durable workflow library would not tie into any specific hosting model (Function Apps), programming model (Azure Functions SDK), or storage backend (Azure Storage).</p>
<p>Besides, only a limited set of programming languages is supported. <strong>C#</strong> developers are the primary audience, but Node.js and Python SDKs are also available.</p>
<p>Recently, I&rsquo;ve been hopping between different languages and cloud providers. Are there other solutions that could help address similar challenges of stateful data processing?</p>
<p>In turns out, yes! My friend and tech blogger <a href="https://twitter.com/taillogs">Ryland</a> introduced me to <a href="https://www.temporal.io/">Temporal</a>—an open-source product to build workflows in code and operate resilient applications using developer-friendly primitives.</p>
<p>Before discussing Temporal, let&rsquo;s define what a workflow is.</p>
<h2 id="you-are-already-running-workflows">You Are Already Running Workflows</h2>
<p>The word &ldquo;workflow&rdquo; has this unfortunate connotation coming from business process automation, BizTalk, and BPMN tools. This scope is too narrow and boring compared to what I&rsquo;m thinking of here.</p>
<p>For me, workflows can be anything that matches three criteria:</p>
<ul>
<li><strong>Multi-step</strong>. Several related actions need to run towards a common goal.</li>
<li><strong>Distributed</strong>. Actions run across multiple servers and services, bringing all the hard problems of distributed systems.</li>
<li><strong>Arbitrary long</strong>. While workflows may complete in less than a second, they may also span days or even years. They are not a part of a request-response flow.</li>
</ul>
<p>This definition is very open. Running in the cloud? You probably implement workflows. Doing microservices? I bet you have tons of workflows in there. Event-driven? Aggregating data? Running cron jobs? Anything to do with the money? Queues? Event streams? Automation? Customer relations? Workflows, workflows, workflows all the way.</p>
<p>However, when it comes to implementation, most of these workflows are defined implicitly. Processes are running in the system, but they aren&rsquo;t defined in a single place. Instead, multiple pieces are spread over distributed components that have to do precisely the right thing for the workflow to complete.</p>
<p>The challenges of reliability, consistency, correctness are therefore addressed in an ad-hoc best-effort, poorly structured manner. These responsibilities are everyone&rsquo;s and no one&rsquo;s job at the same time.</p>
<h2 id="temporal-workflow-as-code">Temporal: Workflow as Code</h2>
<p>Temporal is an open-source tool focused on first-class support of workflows in a sense described above. The workflows are functions in a general-purpose programming language. Temporal comes with developer SDKs and a backend service that takes care of state management and event handling.</p>
<p>Each workflow is a cohesive function defining the whole scenario out of provided building blocks. While some learning and onboarding are required, the &ldquo;Workflow as Code&rdquo; concept feels natural to developers.</p>
<p>Let&rsquo;s take a look at an example.</p>
<h2 id="example-subscription-management">Example: Subscription Management</h2>
<p>We all buy subscriptions to online services these days, and many of us had to <strong>implement</strong> subscription management as part of the business applications we develop. An onboarding flow for a SaaS product (Spotify, Netflix, Dropbox, etc.) could look like this:</p>
<pre><code>function subscribe(customerIdentifier) {
    onboardToFreeTrial(customerIdentifier);

    events.onCancellation(
        processSubscriptionCancellation(customerIdentifier)
        stopWorkflow;
    );

    wait(60 * days);
    upgradeFromTrialToPaid(customerIdentifier);
    forever {
        wait(30 * days);
        chargeMonthlyFee(customerIdentifier);
    }
}
</code></pre><p>I wrote this snippet in pseudocode, and it reflects how a developer might <strong>think</strong> about the workflow. The code relies on several building blocks:</p>
<ul>
<li>Domain-specific actions: <code>onboardToFreeTrial</code>, <code>chargeMonthlyFee</code>, and others;</li>
<li>Time scheduling with <code>wait</code> and <code>forever</code>;</li>
<li>External events to handle user&rsquo;s action with <code>onCancellation</code> and <code>stopWorkflow</code> to stop any further processing.</li>
</ul>
<p>The snippet is concise and easy to read. However, it&rsquo;s seemingly impossible to convert it to real code as-is. Once invoked, this function may need to run for months or years. We can&rsquo;t run a function on the same server for years: it would constantly consume resources and crash on any reboot or hardware failure.</p>
<h3 id="ad-hoc-implementation">Ad-hoc implementation</h3>
<p>Instead, state of the art is to design a distributed asynchronous event-driven bespoke solution. There are multiple options, but here is one of them:</p>
<ul>
<li>Store the status and renewal schedule of each subscription in a <strong>database</strong>.</li>
<li>Have a <strong>cron job</strong> that runs periodically, queries the database for subscriptions due, and sends commands to charge a fee.</li>
<li>The commands are processed by background <strong>workers</strong> triggered off a persistent queue.</li>
<li>A separate <strong>queue</strong> handles cancellation requests and updates subscriptions in the database.</li>
<li>A frontend service provides external <strong>APIs</strong> to interact with the components above.</li>
</ul>
<p><img src="./adhoc-workflow.png" alt="Picture of the ad-hoc solution"></p>
<p>This approach may work, but it&rsquo;s a radical departure from the original snippet in terms of complexity. We have to manage a few durable stores, and several components have to play nicely together. Failure scenarios are numerous, and, anecdotally, not many applications get this 100% right.</p>
<p>More importantly, all the machinery consumes expensive developer&rsquo;s time and yields no business value beyond what would be delivered by the single function perpetually running on a hypothetical error-free and limitless server.</p>
<h3 id="temporal-workflow">Temporal workflow</h3>
<p>Temporal provides a solution where your actual production code looks similar to the original pseudocode. Instead of synchronously running it forever, Temporal SDK would pause the execution when needed, create a checkpoint to store the status, and resume the execution automatically when required.</p>
<p>Temporal currently has two language SDKs: <strong>Go</strong> and <strong>Java</strong>, while more languages are on the way. Meanwhile, here is our workflow in Go:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#00f">func</span> SubscriptionWorkflow(ctx workflow.Context, customerId <span style="color:#2b91af">string</span>) <span style="color:#2b91af">error</span> {
  <span style="color:#00f">var</span> err <span style="color:#2b91af">error</span>
  <span style="color:#00f">if</span> err = workflow.ExecuteActivity(ctx, OnboardToFreeTrial, customerId).Get(ctx, <span style="color:#00f">nil</span>); err != <span style="color:#00f">nil</span> { <span style="color:#00f">return</span> err }
  <span style="color:#00f">defer</span> <span style="color:#00f">func</span>() {
    <span style="color:#00f">if</span> err != <span style="color:#00f">nil</span> &amp;&amp; temporal.IsCanceledError(err) {
      workflow.ExecuteActivity(ctx, ProcessSubscriptionCancellation, customerId).Get(ctx, <span style="color:#00f">nil</span>)
    }
  }()
  <span style="color:#00f">if</span> err = workflow.Sleep(ctx, 60*24*time.Hour); err != <span style="color:#00f">nil</span> { <span style="color:#00f">return</span> err }
  <span style="color:#00f">if</span> err = workflow.ExecuteActivity(ctx, UpgradeFromTrialToPaid, customerId).Get(ctx, <span style="color:#00f">nil</span>); err != <span style="color:#00f">nil</span> { <span style="color:#00f">return</span> err }
  <span style="color:#00f">for</span> {
    <span style="color:#00f">if</span> err = workflow.Sleep(ctx, 30*24*time.Hour); err != <span style="color:#00f">nil</span> { <span style="color:#00f">return</span> err }
    <span style="color:#00f">if</span> err = workflow.ExecuteActivity(ctx, ChargeMonthlyFee, customerId).Get(ctx, <span style="color:#00f">nil</span>); err != <span style="color:#00f">nil</span> { <span style="color:#00f">return</span> err }
  }
}
</code></pre></div><p>And here is the Java version:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">SubscriptionWorkflowImpl</span> <span style="color:#00f">implements</span> SubscriptionWorkflow {
  <span style="color:#00f">public</span> <span style="color:#2b91af">void</span> execute(String customerIdentifier) {
    activities.onboardToFreeTrial(customerIdentifier);
    <span style="color:#00f">try</span> {
      Workflow.sleep(Duration.ofDays(60));
      activities.upgradeFromTrialToPaid(customerIdentifier);
      <span style="color:#00f">while</span> (<span style="color:#00f">true</span>) {
        Workflow.sleep(Duration.ofDays(30));
        activities.chargeMonthlyFee(customerIdentifier);
      }
    } <span style="color:#00f">catch</span> (CancellationException e) {
      activities.processSubscriptionCancellation(customerIdentifier);
    }
  }
}
</code></pre></div><p>It&rsquo;s not the same code as the prototype sketch, but the structure is strikingly similar. Both languages have their ways to handle errors producing some nuance and visual noise, but those ways are native and intimately familiar to developers in respective ecosystems.</p>
<p>The benefits of the Temporal&rsquo;s approach are quite clear:</p>
<ul>
<li>The flow is explicitly defined in a single place and can be formally reasoned through.</li>
<li>There is no bespoke infrastructure to manage beyond a worker running the code and the backend service (more on those below).</li>
<li>Temporal takes care of state management, queueing, resilience, deduplication, and other safety properties.</li>
</ul>
<p>Temporal workflows use the same pause-resume-replay approach as Azure Durable Functions. For example, a call to <code>Workflow.sleep</code> doesn&rsquo;t pause the current thread for 60 days. Instead, the actual Java method call completes, and Temporal schedules another execution in 60 days. The new execution has the history of previous runs, so it replays to the point where the last execution stopped and takes the next step.</p>
<h2 id="how-temporal-works">How Temporal Works</h2>
<p>There&rsquo;s no voodoo here, so a backend service and a data store are required to support the almost-magical workflow execution. Any Temporal environment includes <strong>workers</strong>, a <strong>backend service</strong>, and a <strong>data store</strong>.</p>
<p><img src="components.png" alt="Picture of workers, service, database"></p>
<p>Workers execute the end user&rsquo;s code as the one shown above. The user is responsible for running enough workers, which can be any long-lived compute nodes: VMs, containers, Kubernetes pods, etc.</p>
<p>Each worker connects to the service via a gRPC protocol. The service provides scheduling, queueing, and state manipulation primitives to distribute the workload between workers and ensure liveness and safety guarantees of the system.</p>
<p>The service saves its data in persistent external storage, which currently can be <strong>Cassandra</strong> or <strong>MySQL</strong>.</p>
<p>The service is designed for multi-tenant usage, so your organization only needs a single instance up and running, and multiple applications can benefit. I suspect there will be a managed SaaS option somewhere in the future.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Microservices, serverless functions, cloud-native applications—distributed event-driven business applications are hot, and we will deal with them for years. I worry that the application development industry underestimates the complexity of such systems. Ad-hoc solutions to common problems may rapidly increase the technical debt and slow down the ability to innovate.</p>
<p>I&rsquo;m excited to see tools like Temporal enter the space of open-source workflows, or rather the space of asynchronous data processing. My firm belief is that every developer can benefit from a higher-level framework that helps them focus on business logic. And the business logic is still written in code with languages that they know and love.</p>
<p>In future installments, I plan to explore Temporal in a deeper way, including different usage scenarios, getting up and running in the cloud, SDK features, query APIs, and more. Stay tuned!</p>
<p>If you want to give Temporal a try, head to the <a href="https://docs.temporal.io/docs/overview/">docs site</a>, explore the code at <a href="https://github.com/temporalio/temporal">GitHub</a> or ask questions in <a href="https://community.temporal.io/">Discourse</a> and <a href="https://join.slack.com/t/temporalio/shared_invite/zt-c1e99p8g-beF7~ZZW2HP6gGStXD8Nuw">Slack</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/temporal" term="temporal" label="Temporal" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Announcing Next Generation Pulumi Azure Provider]]></title>
            <link href="https://mikhail.io/2020/09/announcing-nextgen-azure-provider/"/>
            <id>https://mikhail.io/2020/09/announcing-nextgen-azure-provider/</id>
            
            <published>2020-09-21T00:00:00+00:00</published>
            <updated>2020-09-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Next Generation Pulumi Azure Provider with 100% API Coverage and Same-Day Feature Support is now available in beta</blockquote><p>I am excited to announce the beta release of a next generation Microsoft Azure provider for Pulumi. Azure has been a rapidly growing cloud platform among Pulumi users over the last year, and with the next generation Azure provider, we are doubling down on providing the best support possible for the Azure platform in Pulumi. We designed the new provider to expose the entire API surface of Azure to developers and operators, now and forever.</p>
<p>The new Azure provider for Pulumi (<code>azure-nextgen</code>) works directly with the Azure Resource Manager (ARM) platform instead of depending on a handwritten layer as with the previous provider. This approach ensures higher quality and higher fidelity with the Azure platform.</p>
<h2 id="full-api-coverage">Full API Coverage</h2>
<p>The next generation Pulumi Azure provider covers 100% of the resources available in Azure Resource Manager. The new provider supports 890 resource types at launch, nearly double the number supported by the previous Pulumi Azure Provider. Every property of each resource is always represented in the SDKs.</p>
<p><img src="resources-properties.png" alt="Resource and Properties comparison"></p>
<p>The provider also contains functions to retrieve keys, secrets, and connection strings from all resources that expose them.</p>
<p>The new SDKs include full coverage for Azure services, including Azure Static Web Apps, Azure Synapse Analytics, Azure Logic Apps, Azure Service Fabric, Azure Blockchain Service, Azure API Management, and dozens of other services.</p>
<p>If you can deploy a resource with ARM Templates, you can deploy it with the next Generation Pulumi Azure provider!</p>
<h2 id="always-up-to-date">Always Up-to-Date</h2>
<p>Unlike the original Azure provider, which requires manual work to keep updated, the new provider is designed to be always up-to-date with additions and changes to Azure APIs.</p>
<p>We generate Pulumi SDKs for <code>azure-nextgen</code> automatically from Azure API specifications published by Microsoft. An automated pipeline releases updated resources within hours after any current API specifications are merged. We publish daily updates via automated builds and cut minor SDK versions every two weeks.</p>
<p>By generating the SDKs directly from the Azure Resource Manager resource model specifications maintained by Azure service teams, the Azure NextGen provider is robust and reliable, with fewer moving parts involved and fewer sources of potential bugs and incompatibilities.</p>
<p>Excited about a new service announced by Microsoft? Chances are it’s already in the latest Azure NextGen package!</p>
<h2 id="api-versions">API Versions</h2>
<p>The Azure Resource Manager API is structured around Resource Providers—high-level groups like &ldquo;storage&rdquo;, &ldquo;compute&rdquo;, or &ldquo;web&rdquo;. We map Resource Providers to top-level modules or namespaces in Pulumi SDKs.</p>
<p>Each resource provider defines one or more API versions, for example, &ldquo;2015-05-01&rdquo;, &ldquo;2020-09-01&rdquo;, or &ldquo;2020-08-01-preview&rdquo;. Every version of every ARM API is available in Pulumi SDKs, and each version has its own module or namespace.</p>
<p><img src="vscode-versions.png" alt="API Versions in VS Code"></p>
<p>In addition, a <code>latest</code> version  in the Pulumi SDK maps to the latest stable API version as determined by the Azure SDK teams. Using a specific version ensures full compatibility, while using the <code>latest</code> version is a quick way to start, but does not guarantee compatibility between minor versions of the provider.</p>
<p>Now, you always have access to the latest and greatest, but at the same time, we don&rsquo;t force you to upgrade your production resources unless you are ready.</p>
<h2 id="every-language">Every Language</h2>
<p>The next generation Azure provider is available in preview today for all Pulumi languages. The new <code>azure-nextgen</code> SDKs are open source on GitHub and available in NPM, NuGet, PyPI, and Go Modules.</p>
<p>Here is an example in TypeScript:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> resources <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure-nextgen/resources/latest&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> storage <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure-nextgen/storage/latest&#34;</span>;

<span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> resources.ResourceGroup(<span style="color:#a31515">&#34;resourceGroup&#34;</span>, {
    resourceGroupName: <span style="color:#a31515">&#34;my-rg&#34;</span>,
    location: <span style="color:#a31515">&#34;WestUS&#34;</span>,
});

<span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> storage.StorageAccount(<span style="color:#a31515">&#34;sa&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    accountName: <span style="color:#a31515">&#34;mystorageaccount&#34;</span>,
    location: <span style="color:#2b91af">resourceGroup.location</span>,
    sku: {
        name: <span style="color:#a31515">&#34;Standard_LRS&#34;</span>,
        tier: <span style="color:#a31515">&#34;Standard&#34;</span>,
    },
    kind: <span style="color:#a31515">&#34;StorageV2&#34;</span>,
});
</code></pre></div><p>API documentation is available at <a href="https://pulumi.com/docs/reference/pkg/azure-nextgen">Azure NextGen API Reference</a> and includes more than 1,000 resource examples.</p>
<h2 id="integrated-with-azure-ecosystem">Integrated with Azure Ecosystem</h2>
<p>Relying on the shape of the Azure API, we can integrate the NextGen provider into the broader Azure ecosystem. In the coming weeks, we will release several capabilities to simplify the adoption of the new provider:</p>
<ul>
<li>Command-line and web-based tools to convert Azure Resource Manager Templates to Pulumi programs in the language of your choice</li>
<li>A flow to import an existing Azure Resource Group and all its resources to your Pulumi project</li>
<li>A multi-language component resource to embed ARM Templates in Pulumi programs, including per-resource previews</li>
<li>Integrations with ARM Template generation tools like Project Bicep and Farmer</li>
</ul>
<p>Bring your existing resources and assets and start managing them with the NextGen Azure provider!</p>
<h2 id="provider-coexistence">Provider Coexistence</h2>
<p>We will continue to invest in the existing <code>azure</code> provider, and Pulumi users can use either provider or both, side-by-side, in their applications.</p>
<p>A single Pulumi project can use both providers. Each provider must be configured independently, but they accept mostly the same configuration options. The outputs of a resource from one provider can flow to inputs of a resource from the other provider. In both cases, they are the values available from Azure itself, such as the name or id of an Azure resource.</p>
<p>Pulumi has a separate <code>azuread</code> provider, which will continue to manage Azure Active Directory resources.</p>
<p>If you have existing projects using the current Pulumi Azure provider, you can continue to use that provider, and it will be updated indefinitely. You can add new cloud resources using the existing provider or start creating new infrastructure with the new provider.</p>
<p>New Pulumi Azure projects will default to use the <code>azure-nextgen</code> provider once the provider reaches general availability.</p>
<h2 id="getting-started">Getting Started</h2>
<p>New Pulumi templates are available for the new <code>azure-nextgen</code> provider:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">$ pulumi new azure-nextgen-typescript
</code></pre></div><p>Several larger examples are available in the Pulumi Examples repo:</p>
<ul>
<li>Web Applications with Azure App Service and Docker: <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-ts-appservice-docker">TypeScript</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-cs-appservice-docker">C#</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-py-appservice-docker">Python</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-go-appservice-docker">Go</a></li>
<li>Azure AKS cluster: <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-ts-aks">TypeScript</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-cs-aks">C#</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-py-aks">Python</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-go-aks">Go</a></li>
<li>Web Application with Azure Container Instances: <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-ts-aci">TypeScript</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-cs-aci">C#</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-py-aci">Python</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-go-aci">Go</a></li>
<li>Web Server Using Azure Virtual Machine: <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-ts-webserver">TypeScript</a>, <a href="https://github.com/pulumi/examples/tree/master/azure-nextgen-py-webserver">Python</a></li>
</ul>
<p>You can browse <a href="https://www.pulumi.com/docs/reference/pkg/azure-nextgen/">API reference docs</a> with inline examples or explore the <a href="https://github.com/pulumi/pulumi-azure-nextgen">Pulumi Azure NextGen SDKs</a> repository.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[.NET]]></title>
            <link href="https://mikhail.io/tags/.net/"/>
            <id>https://mikhail.io/tags/.net/</id>
            
            <published>2020-09-03T00:00:00+00:00</published>
            <updated>2020-09-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How to Drain a List of .NET Tasks to Completion]]></title>
            <link href="https://mikhail.io/2020/09/how-to-drain-dotnet-tasks-to-completion/"/>
            <id>https://mikhail.io/2020/09/how-to-drain-dotnet-tasks-to-completion/</id>
            
            <published>2020-09-03T00:00:00+00:00</published>
            <updated>2020-09-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Custom await logic for a dynamic list of .NET tasks, fast and on-time</blockquote><p>The Pulumi .NET SDK operates with an unusual asynchronicity model. Resource declarations are synchronous calls and complete instantaneously.</p>
<p>Yet, they kick off the actual operations of resource creation as background tasks. An end-user does not see these tasks and does not await them.</p>
<p>Nonetheless, Pulumi must not stop the program execution until all the tasks are completed. The SDK should collect all open tasks and reliably await them.</p>
<p>The following model illustrates the goal:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">async</span> Task Main()
{
    <span style="color:#00f">for</span> (<span style="color:#2b91af">int</span> i = 0; i &lt; N; i++)
        DoWork(<span style="color:#a31515">$&#34;Job {i}&#34;</span>); <span style="color:#008000">// creates a Task and returns immediately
</span><span style="color:#008000"></span>
    <span style="color:#00f">await</span> WaitForAllOpenTasksToComplete();
}

<span style="color:#00f">static</span> <span style="color:#00f">void</span> DoWork(<span style="color:#2b91af">string</span> message)
{
    RegisterTask(message, RunAsync());
    <span style="color:#00f">async</span> Task RunAsync()
    {
        <span style="color:#2b91af">var</span> delay = <span style="color:#00f">new</span> Random().Next(1000);
        <span style="color:#00f">await</span> Task.Delay(delay).ConfigureAwait(<span style="color:#00f">false</span>);
        Log(<span style="color:#a31515">$&#34;{message} finishing&#34;</span>);
    }
}
</code></pre></div><p>The <code>DoWork</code> method creates a <code>Task</code> that pauses for a random delay below one second. It passes the task to a <code>RegisterTask</code> method (to be defined) and returns without awaiting.</p>
<p>In our real use case, tasks may also produce new tasks, which can produce even more tasks, and so on. I emulate this by extending the <code>DoWork</code> method with an extra parameter asking to schedule more work:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">void</span> DoWork(<span style="color:#2b91af">string</span> message, <span style="color:#2b91af">bool</span> moreWork = <span style="color:#00f">true</span>)
{
    RegisterTask(message, RunAsync());
    <span style="color:#00f">async</span> Task RunAsync()
    {
        <span style="color:#2b91af">var</span> delay = <span style="color:#00f">new</span> Random().Next(1000);
        <span style="color:#00f">await</span> Task.Delay(delay).ConfigureAwait(<span style="color:#00f">false</span>);
        Log(<span style="color:#a31515">$&#34;{message} finishing&#34;</span>);
        
        <span style="color:#00f">if</span> (moreWork)
        {
            <span style="color:#00f">for</span> (<span style="color:#2b91af">int</span> i = 0; i &lt; N; i++)
                DoWork(<span style="color:#a31515">$&#34;{message}.{i}&#34;</span>, <span style="color:#00f">false</span>);
        }
    }
}
</code></pre></div><p>The <code>Main</code> method creates initial work items and needs to await the completion of all of the tasks, immediate or descendant ones, with <code>WaitForAllOpenTasksToComplete</code>.</p>
<p>Let&rsquo;s look at several options on how to implement such processing.</p>
<h2 id="registering-tasks">Registering Tasks</h2>
<p>First thing, I need to store the in-flight tasks somewhere, so I&rsquo;ll allocate a static collection for that. I want to keep a description with each task, so my collection is a dictionary:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">readonly</span> Dictionary&lt;Task, <span style="color:#2b91af">string</span>&gt; _inFlightTasks = <span style="color:#00f">new</span> Dictionary&lt;Task, <span style="color:#2b91af">string</span>&gt;();
</code></pre></div><p>Now I can implement a method to register a new task:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">void</span> RegisterTask(<span style="color:#2b91af">string</span> description, Task task)
{
    <span style="color:#00f">lock</span> (_inFlightTasks)
    {
        <span style="color:#008000">// Duplicates may happen if we try registering things like Task.CompletedTask.
</span><span style="color:#008000"></span>        <span style="color:#008000">// We&#39;ll ignore duplicates for now.
</span><span style="color:#008000"></span>        <span style="color:#00f">if</span> (!_inFlightTasks.ContainsKey(task))
        {
            _inFlightTasks.Add(task, description);
        }
    }
}
</code></pre></div><p>Now, how do we implement the awaiting of these tasks?</p>
<h2 id="awaiting-the-tasks">Awaiting the Tasks</h2>
<p>We can&rsquo;t wait for the tasks just once, because new tasks may be coming over time. Therefore, the <code>WaitForAllOpenTasksToComplete</code> will have a loop to wait until all the in-flight tasks are drained. Here is a draft without the actual awaiting yet:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task WaitForAllOpenTasksToComplete_Draft()
{
    <span style="color:#008000">// Keep looping as long as there are outstanding tasks that are still running.
</span><span style="color:#008000"></span>    <span style="color:#00f">while</span> (<span style="color:#00f">true</span>)
    {
        <span style="color:#2b91af">var</span> tasks = <span style="color:#00f">new</span> List&lt;Task&gt;();
        <span style="color:#00f">lock</span> (_inFlightTasks)
        {
            <span style="color:#00f">if</span> (_inFlightTasks.Count == 0)
            {
                <span style="color:#008000">// No more tasks in flight: exit the loop.
</span><span style="color:#008000"></span>                <span style="color:#00f">return</span>;
            }

            <span style="color:#008000">// Grab all the tasks we currently have running.
</span><span style="color:#008000"></span>            tasks.AddRange(_inFlightTasks.Keys);
        }

        <span style="color:#008000">// TODO: await tasks, then proceed to the next iteration
</span><span style="color:#008000"></span>    }
}
</code></pre></div><p>How do we await these tasks?</p>
<h2 id="use-whenall">Use WhenAll</h2>
<p>The obvious option is to use <code>WhenAll</code> and then remove all the tasks from the in-flight collections:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#008000">// ... we are inside the loop
</span><span style="color:#008000"></span>
<span style="color:#00f">await</span> Task.WhenAll(tasks).ConfigureAwait(<span style="color:#00f">false</span>);
        
<span style="color:#00f">lock</span> (_inFlightTasks)
{
    <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> task <span style="color:#00f">in</span> tasks)
    {
        Log(<span style="color:#a31515">$&#34;{_inFlightTasks[task]} handled&#34;</span>);
        _inFlightTasks.Remove(task);
    }
}

<span style="color:#008000">// ... the loop continues
</span></code></pre></div><p>Let&rsquo;s run the program for <code>N</code> = 2 (two parent tasks, each creating two child tasks) and log the messages and timings:</p>
<pre><code>0.048: Job 0 finishing
0.464: Job 0.0 finishing
0.539: Job 0.1 finishing
0.660: Job 1 finishing
0.661: Job 0 handled
0.661: Job 1 handled
0.910: Job 1.1 finishing
1.372: Job 1.0 finishing
1.372: Job 0.0 handled
1.372: Job 0.1 handled
1.372: Job 1.0 handled
1.372: Job 1.1 handled
1.373: Done!
</code></pre><p>The awaiting loop works, and the program quits correctly.</p>
<p>There is a downside, though. After a task completes, we may not respond to its completion until all the other tasks of the same batch complete.</p>
<p>This is not desirable for Pulumi&rsquo;s scenario when cloud operations may run for minutes. If two resources are created in parallel, we want to report success as soon as it happens. Even more importantly, we want to handle the errors as they occur.</p>
<p>How can we respond to tasks one-by-one?</p>
<h2 id="use-whenany">Use WhenAny</h2>
<p>The next option is to use <code>Task.WhenAny</code> to handle the completion of tasks one-by-one. <code>WhenAny</code> accepts a collection of tasks and returns the first one that completes. After the <code>await</code> operator returns the first completed task, we can log it and exclude it from the in-flight tasks list. Then, we call <code>WhenAny</code> again with the list of all remaining tasks.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> task = <span style="color:#00f">await</span> Task.WhenAny(tasks).ConfigureAwait(<span style="color:#00f">false</span>);
                
<span style="color:#00f">lock</span> (_inFlightTasks)
{
    Log(<span style="color:#a31515">$&#34;{_inFlightTasks[task]} handled&#34;</span>);
    _inFlightTasks.Remove(task);
}

<span style="color:#008000">// Now actually await the returned task and realize any exceptions it may have thrown.
</span><span style="color:#008000"></span><span style="color:#00f">await</span> task.ConfigureAwait(<span style="color:#00f">false</span>);
</code></pre></div><p>Let&rsquo;s run the modified program and log messages again:</p>
<pre><code>0308: Job 0 finishing
0312: Job 0 handled
0440: Job 0.1 finishing
0440: Job 0.1 handled
0899: Job 1 finishing
0900: Job 1 handled
0948: Job 1.0 finishing
0948: Job 1.0 handled
0957: Job 0.0 finishing
0957: Job 0.0 handled
0979: Job 1.1 finishing
0979: Job 1.1 handled
0979: Done!
</code></pre><p>This looks great! The code responds to each completion immediately, which meets our goal.</p>
<p>The code works perfectly for the small number of tasks, but does it scale?</p>
<h2 id="stress-test">Stress Test</h2>
<p>We ran the test with <code>N=2</code> to create 6 tasks in total. No matter which <code>N</code> we choose, there are only two sequential tasks (a parent task and its child task). As each task completes within 1 second, the full test should finish in less than 2 seconds in theory.</p>
<p>However, this does not hold for the <code>WhenAny</code>-based program. The plot below shows the test completion time, depending on the total number of tasks.</p>
<figure >
    
        <img src="timing.png"
            alt="Time to completion grows rapidly as the total number of tasks increases"
             />
        
    
    <figcaption>
        <h4>Time to completion grows rapidly as the total number of tasks increases</h4>
    </figcaption>
    
</figure>
<p>The 2-second rule holds until ~8.000 tasks, but then the completion time is clearly quadratic from the number of tasks.</p>
<p>The logs confirm that the last completed task is always at the 2 seconds mark, while the handler loop becomes excessively busy with awaiting.</p>
<p>This makes sense. <code>WhenAny</code> doesn&rsquo;t have a constant complexity but at least <code>O(N)</code> complexity. Executing it in a loop gives us <code>O(N^2)</code>, demonstrated by the chart above.</p>
<p>This means that large Pulumi programs managing thousands of resources would tend to complete notably slower than desired.</p>
<p>Predictably, the implementation based on <code>WhenAll</code> doesn&rsquo;t have this problem: all the thousands of tasks complete in 2 seconds.</p>
<p>How do we combine the benefits of both approaches?</p>
<h2 id="custom-await-logic">Custom Await Logic</h2>
<p>It looks like the standard library doesn&rsquo;t have a method that satisfies the requirements. The custom implementation consists of multiple parts.</p>
<p>First, there is a <code>TaskCompletionSource</code> that tracks the overall task completion:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">private</span> <span style="color:#00f">static</span> TaskCompletionSource&lt;<span style="color:#2b91af">int</span>&gt; Tcs = <span style="color:#00f">new</span> TaskCompletionSource&lt;<span style="color:#2b91af">int</span>&gt;(TaskCreationOptions.RunContinuationsAsynchronously);
</code></pre></div><p>Now, all that the new <code>WaitForAllOpenTasksToComplete</code> does is waiting for this completion source to return:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task WaitForAllOpenTasksToComplete()
{
    <span style="color:#00f">await</span> Tcs.Task.ConfigureAwait(<span style="color:#00f">false</span>);
}
</code></pre></div><p>It becomes the responsibility of the <code>RegisterTask</code> method to initiate the completion of the task:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">void</span> RegisterTask(<span style="color:#2b91af">string</span> description, Task task)
{
    <span style="color:#00f">lock</span> (_inFlightTasks)
    {
        <span style="color:#008000">// Duplicates may happen if we try registering things like Task.CompletedTask.
</span><span style="color:#008000"></span>        <span style="color:#008000">// We&#39;ll ignore duplicates for now.
</span><span style="color:#008000"></span>        <span style="color:#00f">if</span> (!_inFlightTasks.ContainsKey(task))
        {
            _inFlightTasks.Add(task, description);
        }
    }
    HandleCompletion(task);
}
</code></pre></div><p>All the actual logic resides in <code>HandleCompletion</code> method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">static</span> <span style="color:#00f">async</span> <span style="color:#00f">void</span> HandleCompletion(Task task)
{
    <span style="color:#00f">try</span>
    {
        <span style="color:#008000">// Wait for the task completion.
</span><span style="color:#008000"></span>        <span style="color:#00f">await</span> task.ConfigureAwait(<span style="color:#00f">false</span>);

        Log(<span style="">$</span><span style="color:#a31515">&#34;{_inFlightTasks[task]} handled);
</span><span style="color:#a31515"></span>    }
    <span style="color:#00f">catch</span> (OperationCanceledException)
    {
        Tcs.TrySetCanceled();
    }
    <span style="color:#00f">catch</span> (Exception ex)
    {
        Tcs.TrySetException(ex);
    }
    <span style="color:#00f">finally</span>
    {
        <span style="color:#008000">// Once finished, remove the task from the set of tasks that are running.
</span><span style="color:#008000"></span>        <span style="color:#00f">lock</span> (_inFlightTasks)
        {
            _inFlightTasks.Remove(task);

            <span style="color:#008000">// Check if all the tasks are completed and signal the completion source if so.
</span><span style="color:#008000"></span>            <span style="color:#00f">if</span> (_inFlightTasks.Count == 0)
            {
                Tcs.TrySetResult(0);
            }
        }
    }
}
</code></pre></div><h2 id="testing-it-out">Testing It Out</h2>
<p>Let&rsquo;s make sure that the custom await logic works as desired. Here is the output for the N=2 case:</p>
<pre><code>0549: Job 1 finishing
0555: Job 1 handled
0805: Job 0 finishing
0805: Job 0 handled
0851: Job 1.0 finishing
0851: Job 1.0 handled
1473: Job 1.1 finishing
1474: Job 1.1 handled
1582: Job 0.0 finishing
1582: Job 0.0 handled
1627: Job 0.1 finishing
1627: Job 0.1 handled
1629: Done!
</code></pre><p>As desired, the handling happens immediately after finishing a job. What about a large number of tasks?</p>
<p>Even 100.000 tasks complete in 2 seconds! This looks great! Mission accomplished!</p>
<h2 id="conclusion">Conclusion</h2>
<p>.NET standard library comes with great primitives to handle common patterns around tasks. However, sometimes one has to understand their limitations and create a new pattern implementation.</p>
<p>Do you see a problem with the approach above? Do you know a more straightforward way to achieve both goals? Please respond below!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/.net" term=".net" label=".NET" />
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cloud]]></title>
            <link href="https://mikhail.io/tags/cloud/"/>
            <id>https://mikhail.io/tags/cloud/</id>
            
            <published>2020-07-14T00:00:00+00:00</published>
            <updated>2020-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Edge]]></title>
            <link href="https://mikhail.io/tags/edge/"/>
            <id>https://mikhail.io/tags/edge/</id>
            
            <published>2020-07-14T00:00:00+00:00</published>
            <updated>2020-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Paper review]]></title>
            <link href="https://mikhail.io/tags/paper-review/"/>
            <id>https://mikhail.io/tags/paper-review/</id>
            
            <published>2020-07-14T00:00:00+00:00</published>
            <updated>2020-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[The Emerging Landscape of Edge-Computing]]></title>
            <link href="https://mikhail.io/2020/07/emerging-landscape-of-edge-computing/"/>
            <id>https://mikhail.io/2020/07/emerging-landscape-of-edge-computing/</id>
            
            <published>2020-07-14T00:00:00+00:00</published>
            <updated>2020-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>What is edge computing, and what are the primary use cases in the world today? (a paper review)</blockquote><p>While I have a good understanding of what cloud computing is, &ldquo;edge computing&rdquo; has remained vague. What <em>is</em> edge computing, and what are the primary use cases in the world today?</p>
<p>As with other buzzwords, it&rsquo;s hard to give a single definite answer. Anyway, I found a paper that presents a consistent view of the topic. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/GetMobile__Edge_BW.pdf">The Emerging Landscape of Edge-Computing</a> summarizes the advantages, use cases, and challenges of edge computing in 2020.</p>
<p>It turns out, I participated in edge computing projects in the past!</p>
<h2 id="consumer-edge-the-vision-that-never-happened">Consumer Edge: The Vision That Never Happened</h2>
<p>The term <strong>Edge Computing</strong> was introduced more than a decade ago. The cloud was still in infancy: a handful of providers were at the start of the global IT crusade.</p>
<p>At the same time, mobile devices were all the rage. Smartphones have changed the world, and wearables were around the corner. Still, mobile devices had minimal computing power, so they were incapable of advanced tasks like fast image processing or machine learning inference.</p>
<p>Cloud has been rapidly increasing its capacity and introducing specialized hardware like tensor-processing units (TPUs). However, the latency between a mobile device and a cloud data center would be too high (100s of milliseconds) for most interactive applications.</p>
<p>The edge computing was born to fix this problem and provide nearby powerful machines accessible from lightweight devices over a low-latency, high-bandwidth network. A stark example would be a wearable device like Google Glass that overlays real-time guidance on a heads-up display by streaming video to a TPU in a nearby edge location.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="cyber-foraging.png"
            alt="Wearable devices migrating between edge locations for real-life experience augmentation"
             />
        
    
    <figcaption>
        <h4>Wearable devices migrating between edge locations for real-life experience augmentation</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>This vision has been described as <strong>cyber foraging</strong>. Edge computing research focused on enabling interactive applications on mobile devices. Future technology had to acquire several crucial capabilities:</p>
<ul>
<li><strong>Multi-tenant compute</strong> to serve legions of consumers on the same hardware.</li>
<li><strong>Millisecond-level latency</strong> required for interactive applications.</li>
<li><strong>Migration</strong> from one edge location to the other as the devices move through a physical environment.</li>
</ul>
<p>These were the goals five to ten years ago. Meanwhile, mobile devices grew powerful: A13 Bionic was unthinkable during the era of the first smartphones. Besides, cloud regions spread all over the planet and integrated into the global network. Today, sub-100ms consumer-to-cloud latencies are very common.</p>
<p>The authors of &ldquo;The Emerging Landscape&rdquo; argue that the cyber foraging vision hasn&rsquo;t been realized en-mass. Instead, the industry picked up edge computing for a different purpose. None of the three goals above are relevant for the actual edge applications, while new challenges arose.</p>
<h2 id="industrial-edge-reality-of-today">Industrial Edge: Reality of Today</h2>
<p>It turns out that edge computing&rsquo;s ideas found relevance in business and industrial applications, for example:</p>
<ol>
<li>
<p><strong>Industrial plants</strong> deploy numerous sensors that continuously monitor mechanical equipment, worker safety, and production workflows to ensure issues are spotted and mitigated promptly. They use edge computing because the internet connectivity at remote industrial sites may be unreliable and low-bandwidth.</p>
</li>
<li>
<p><strong>Railway industry</strong> uses high-definition cameras along the track to detect cracks in train wheels.  Cracks can cause the wheel to break and derail the entire train. The bandwidth and compute demand for this case is very dynamic: when a train passes a camera, the system generates GBs of data over several seconds. The analysis must be completed reliably within minutes to avoid severe casualties.</p>
</li>
<li>
<p><strong>Cities</strong> have deployed millions of cameras and sensors: at intersections, in parking lots, and in construction zones. The same cameras can be used to control the traffic flow across the city and alert drivers to avoid fatal accidents. Since many scenarios are centered around safety, continuous operation is critical and must not depend on the global network.</p>
</li>
<li>
<p><strong>Restaurants</strong> run prediction platforms to forecast when more food needs to be cooked. They use sensors to predict the number of customers entering the store.</p>
</li>
</ol>
<p>Many of today’s edge deployments are best described as <strong>edge-sites</strong> for long-running applications, such as industrial sensing and video analytics. These sites are single-tenant, and they rarely (if ever) host transient jobs for mobile devices.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="edge-site.png"
            alt="Typical edge-site deployment in enterprise environments"
             />
        
    
    <figcaption>
        <h4>Typical edge-site deployment in enterprise environments</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Somewhat surprisingly, these data-processing applications are not bound by the strict latency requirements of cyber-foraging applications such as cognitive assistance. The cloud is close enough, but it has other issues.</p>
<h2 id="why-not-cloud">Why Not Cloud?</h2>
<p>With such relatively high latency tolerance, and the high availability, scalability, and low-cost  computation offered by the cloud, why do these applications run on the edge rather than offloading to the cloud?</p>
<p>It turns out there are two main reasons:</p>
<ol>
<li>
<p><strong>Bandwidth to data centers is insufficient</strong>. The volume of generated data is immense, but the existing uplink internet channels are orders of magnitude slower than required. Furthermore, bandwidth needs may be intermittent and dynamic, causing huge spikes at the peaks.</p>
</li>
<li>
<p><strong>Connectivity is unreliable</strong>. When coupled with the mission-critical nature of the applications, even brief connectivity outages have a detrimental impact on safety and financial viability, so any downtime is unacceptable.</p>
</li>
</ol>
<p>Therefore, the dominant reasons for adopting edge computing are the need to tolerate cloud outages and the scarcity of network bandwidth.</p>
<h2 id="is-this-just-plain-old-on-prem">Is This Just Plain Old On-Prem?</h2>
<p>Why do we call these deployments an edge? Isn&rsquo;t this plain old on-premises hosting?</p>
<p>They are not. The edge-sites are still connected to the cloud for processing outside the critical path.</p>
<p>The  cloud  is a large pool of well-maintained resources with lower management overhead imposed on the user. It provides better resource efficiency by multiplexing across many users, high scalability, high availability, and low cost. Thus, it is preferable to utilize the cloud whenever possible.</p>
<p>Edge sites are a burden. The users happily offload the workloads to the cloud whenever feasible. But many scenarios are blocked because of connectivity issues and the criticality of applications.</p>
<h2 id="edge-cloud-collaboration">Edge-Cloud Collaboration</h2>
<p>As the edge and cloud are bound to coexist, they need to interact smoothly. A framework should enable developers to utilize both edge compute and cloud in multiple dimensions:</p>
<ol>
<li>
<p><strong>Graceful adaptation</strong> would allow applications to function optimally in the presence of disconnections, drops in bandwidth, or workload spikes. Adaptive applications can utilize the cloud when network conditions  permit but remain operational even in the face of network issues.</p>
</li>
<li>
<p><strong>Collaborative and application-aware orchestration</strong>. It is common for multiple applications of a single enterprise to share the edge cluster, e.g., run both fridge monitoring and customer tracking applications in a retail store. Not all apps have equal priority and criticality, but they share some degree of trust.</p>
</li>
<li>
<p><strong>Test and verification</strong>.  Debugging and testing an edge-site application is extremely difficult. As edge-site conditions can be challenging to recreate before deployment, incorrect behavior can arise from unanticipated interactions among adaptation strategies.</p>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/GetMobile__Edge_BW.pdf">The Emerging Landscape of Edge-Computing</a> presents several use cases for today&rsquo;s edge deployments. These applications are not end-user interactive mobile applications opportunistically using the edge as originally envisioned.</p>
<p>Instead, they are geographically constrained, mission-critical, industrial or enterprise applications. They rely primarily on edge computing and opportunistically use the cloud. Across these scenarios, network bandwidth and reliability drive the use of edge computing, especially given the high volume of generated data, limited and intermittent connectivity to the cloud.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/edge" term="edge" label="Edge" />
                             
                                <category scheme="https://mikhail.io/tags/cloud" term="cloud" label="Cloud" />
                             
                                <category scheme="https://mikhail.io/tags/paper-review" term="paper-review" label="Paper Review" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Jobs]]></title>
            <link href="https://mikhail.io/tags/jobs/"/>
            <id>https://mikhail.io/tags/jobs/</id>
            
            <published>2020-07-01T00:00:00+00:00</published>
            <updated>2020-07-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[The Best Interview is No Interview: How I Get Jobs Without Applying]]></title>
            <link href="https://mikhail.io/2020/07/best-interview-is-no-interview-get-jobs-without-applying/"/>
            <id>https://mikhail.io/2020/07/best-interview-is-no-interview-get-jobs-without-applying/</id>
            
            <published>2020-07-01T00:00:00+00:00</published>
            <updated>2020-07-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>My humble story of getting (or not) a job at Amazon, Qualcomm, Jet.com, Pulumi, and more</blockquote><h2 id="interviews">Interviews</h2>
<p>I think I&rsquo;m reasonably good at passing job interviews.</p>
<p>When I was open for a job change a couple of years ago, I interviewed at eight local companies here in the Netherlands. I got job offers from seven of them. Of course, I have enough experience, and the market was scorching and candidate-friendly.</p>
<p>Besides, Dutch interviews are not as extensive compared to major tech companies in the U.S. For me, it took from 40 minutes to 4 hours of conversations to get to offer negotiation. There would typically be no whiteboard coding or take-home assignments.</p>
<p>I had two interviews at the U.S. companies too. I got offers from both of them.</p>
<p>Amazon flew me to Stockholm for a hiring event. Before jumping on a plane, I spent a day or two studying interviews at Amazon and an algorithms book. Amazon hired a hotel room and hosted four 1-hour sessions with me. There was a whiteboard standing, and every new interviewer would give me a coding or design challenge to solve within that hour. I wasn&rsquo;t well-calibrated for such interviews, and couldn&rsquo;t quite tell if I&rsquo;m performing well. Apparently, I was okay, because I got an open offer for Canada, U.S., Luxembourg, and Germany to work on some Amazon projects.</p>
<p>I also had an interview with Jet.com—Amazon&rsquo;s competitor in retail—for a position in Dublin. Two online live-coding interviews, four onsite interviews with whiteboards: a tad stressful but also fun. Coincidentally, the onsite in Dublin was two weeks after the Amazon interview, and the offers came literally on the same day.</p>
<h2 id="no-interviews">No-Interviews</h2>
<p>There&rsquo;s one big problem with this narrative. I declined all of those nine offers.</p>
<p>I didn&rsquo;t feel like these were enough of a step forward for me. Offers from Amazon and Jet were cool, but not revolutionary enough to relocate away from a well-arranged life.</p>
<p>There were more declined offers in the past. In fact, in my whole 18-years (oh gosh) career as a software developer, I only accepted a single job offer after an interview process. I left that company nine months later, despite being already promoted in my role.</p>
<p>How did I get other jobs? Honestly, it felt like they happened by chance.</p>
<p>I got my first paid gig from my father when I was an 18-year-old student. He showed me screenshots of the applications from work and asked me if I can create that kind of apps. He knew nothing about software development but needed a bunch of utilities to calibrate hardware that they were working on.</p>
<p>Sure, I knew Delphi and C++ Builder, so I could make form-based apps. I prototyped one and got my first $100 paycheck. I started working for the company and spent the next three years learning the tech and taking over existing code. By the end of that engagement, I was writing all software in their stack from C code on microcontrollers with directly addressable memory to 60-screen Windows apps.</p>
<p>There wasn&rsquo;t much to learn there anymore. Still being a student, I hopped on a three-week bootcamp organized by a software consultancy company. They were teaching free classes to promote the best students to FTEs. I scored a second place on the overall leaderboard and got hired at the double my previous salary.</p>
<p>I spent five years there, learning from many amazing people. I tried myself in different roles from an individual coder to a technology &ldquo;expert&rdquo; to a team lead to a project manager. The company has been doing contracting projects for European customers (&ldquo;outsourcing&rdquo;).</p>
<p>In the end, I quit the company to try contracting on my own. A friend of mine connected me to somebody in the Netherlands who needed help with his ongoing projects. The guy just gave me the first task, I completed it, he paid. The second task, the first lengthy project, the second project, and I got rolling.</p>
<p>I&rsquo;ve been working in my bedroom for the next four years. My contact in the Netherlands got hired by Qualcomm and switched me to their projects. Eventually, Qualcomm decided to hire me full-time, and my family moved from Russia to the Netherlands.</p>
<p>Most certainly, Qualcomm had an extensive process for hiring, so I did fly over for an onsite &ldquo;interview&rdquo;. The interview lasted 5 minutes: I said hi to everyone and then worked on the project for the rest of the day.</p>
<h2 id="follow-the-passion">Follow The Passion</h2>
<p>The other day, I interviewed somebody for a developer role at Pulumi, where I currently work. They had a screening interview first, and now going through five rounds of tech interviews. Live-coding, design questions, debugging problems, tell-me-about-the-time-when situations. Very common for anyone applying for a software engineering job in the U.S.</p>
<p>Except, I never had a single interview at Pulumi. I jumped on their product as a user as soon as the first beta went public. I enjoyed the product, published blog posts, used it for my pet projects, gave one of the first Pulumi talks in Europe.</p>
<p>When I was in Seattle for the Microsoft MVP summit, I pinged Pulumi Slack and grabbed a beer with Luke, our CTO. Soon after, I started doing paid contracting for Pulumi: writing blogs, docs, examples, then fixing tiny issues here and there. Eventually, I went to Seattle for the Pulumi 1.0 launch and a week onsite. Finally, after 12 months as a user and 6 months as a contractor, I got the contract signed for full-time employment.</p>
<p>I had no interviews. When I first started, I had zero experience with Go and very limited TypeScript, our main programming languages. I&rsquo;m not a compiler guru, not ingrained into DevOps ecosystem, never worked on developer tooling or open-source SDKs.</p>
<p>However, I thoroughly enjoy my job. It means a lot to me. I love learning from exceptional people, and I have a lot of freedom.</p>
<h2 id="off-the-beaten-track">Off The Beaten Track</h2>
<p>There were two categories of companies that I interacted with. The first group invited me for an interview, and the second group allowed me to collaborate without a formal hiring process. I consistently chose to work in the latter group and enjoyed my time there. Why?</p>
<p>Every such job meant going out of a local maximum, out of comfort zone. Working on calibration software without any experience with metrology. Jumping between consulting projects, teams, and roles. Relocating to another country to join a multi-national company. Becoming one of the two non-U.S. employees at a DevOps tooling startup without ever seeing bash before.</p>
<p>None of these were &ldquo;the next logical step&rdquo; in my career. Maybe I could pass interviews at those companies, perhaps I wouldn&rsquo;t. That&rsquo;s mostly irrelevant because there wouldn&rsquo;t be a sequence of events to land me at a business-as-usual interview at one of those companies.</p>
<h2 id="expand-your-circle">Expand Your Circle</h2>
<p>There&rsquo;s a lot of arguments about the interview process being broken. A candidate spends several hours solving computer science puzzles on a whiteboard to get a job of fixing bugs in a React app. Companies may be too strict at not accepting great candidates and hiring unfit candidates at the same time. Perhaps that&rsquo;s all true.</p>
<p>I&rsquo;m actually making a different point. A successful job search doesn&rsquo;t have to be centered around an interview. If my experience generalizes to others at all, here are some activities that may help you get the next great job:</p>
<ol>
<li>
<p><strong>Build a strong professional network</strong>. And not just a bunch of hiring managers. Diverse connections might perform even better since you&rsquo;d get access to a broad set of opportunities. You don&rsquo;t know what you are looking for anyway.</p>
</li>
<li>
<p><strong>Keep learning and be curious</strong>, even a topic seems to bring no immediate value. The goal is expanding your surface area of interest, curiosity, and skills.</p>
</li>
<li>
<p><strong>Take part-time gigs</strong>, contracting, one-off apps. They probably won&rsquo;t make you rich on their own. Still, successful low-risk engagement with somebody may lead to longer happy and fulfilling relationships.</p>
</li>
<li>
<p><strong>Build a public profile</strong>. Publish your code, write blogs, share your interests, design, run, and publish experiments. More than once was I surprised about the way this helped me with jobs, networking, learning, or getting a consulting gig.</p>
</li>
</ol>
<p>And if you are wearing a hiring hat, try to give your potential future workers a chance to engage without full-blown employment commitment. Asking me to code a simple but useful real-life application. Running a free class for software engineering students. Hiring temporary contractors from distant countries. Those companies get me as their employee.</p>
<p>Enough about myself! I&rsquo;m keen to learn about your experience. Whether you can relate to my story or disagree with my take, please leave a comment below.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/jobs" term="jobs" label="Jobs" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Eliminate Cold Starts by Predicting Invocations of Serverless Functions]]></title>
            <link href="https://mikhail.io/2020/06/eliminate-cold-starts-by-predicting-invocations-of-serverless-functions/"/>
            <id>https://mikhail.io/2020/06/eliminate-cold-starts-by-predicting-invocations-of-serverless-functions/</id>
            
            <published>2020-06-18T00:00:00+00:00</published>
            <updated>2020-06-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Azure Functions introduce a data-driven strategy to pre-warm serverless applications right before the next request comes in</blockquote><p>Developers and decision-makers often mention <a href="/serverless/coldstarts/">cold starts</a> as a significant drawback of serverless functions. Cloud providers continually invest in reducing the latency of a cold start, but they haven&rsquo;t done much to prevent them altogether. The most common technique is to keep a worker alive for 10-20 minutes after each request, hoping that another request comes in and benefits from the warm instance.</p>
<p>This simple strategy works to some extent, but it&rsquo;s both wasteful in terms of resource utilization and not particularly helpful for low-usage applications. Is there an alternative strategy that could adapt to the workload, reduce the frequency of cold starts, <em>and</em> be more efficient?</p>
<p>In <a href="/2020/05/serverless-in-the-wild-azure-functions-usage-stats">Azure Functions Production Usage Statistics</a>, I reviewed the first part of the <a href="https://arxiv.org/pdf/2003.03423.pdf">Serverless in the Wild</a> paper, which outlines statistics of Azure Functions running in production. Actually, the ultimate goal of that paper is to suggest an improvement to the cold start mitigation policy and validate the proposed strategy based on the data.</p>
<p>This article is my second installment of the paper review. I focus on the idea of predicting future invocations and pre-warming of serverless workers. I describe the current state of cold starts, then explain the suggested improvements from the paper. Finally, I present my own take on those ideas.</p>
<p>The new policy is definitely worth studying because it will be applied to your Azure Functions soon!</p>
<h2 id="challenges">Challenges</h2>
<p>As the statistics show, many Azure Function Apps are called very infrequently. Let&rsquo;s consider a concrete example: an HTTP-triggered function that runs approximately once per hour and returns current data for a report.</p>
<p>Currently, every invocation of such a function would hit a cold start. Specifically, Azure uses a fixed “keep-alive” policy that retains the resources in memory for 20 minutes after execution. This isn&rsquo;t helpful in our scenario since requests come every 60 minutes.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="scenario.png"
            alt="Warm instances are recycled after 20 minutes, so an hourly request hits a cold start"
             />
        
    
    <figcaption>
        <h4>Warm instances are recycled after 20 minutes, so an hourly request hits a cold start</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>In this specific case, the fixed policy is problematic for everyone:</p>
<ul>
<li>Users hit a cold start every time, which significantly increases the response time.</li>
<li>Azure wastes resources on keeping a warm instance in memory for 20 minutes every hour without any benefits.</li>
</ul>
<p>Indeed, cloud providers seek to achieve high function performance at the lowest possible resource cost. They can&rsquo;t keep all functions in memory all the time: as the stats show, most functions run very seldom, so keeping all of them warm would be a massive waste of resources.</p>
<p>What if the cloud provider could observe each application and adapt the policy for each application workload according to its actual invocation frequency and pattern?</p>
<h2 id="optimal-strategy-predicts-the-future">Optimal Strategy Predicts The Future</h2>
<p>Let&rsquo;s start with a hypothetical ideal solution for our specific one-call-per-hour example. Instead of keeping a warm instance for a fixed period after each invocation, an efficient policy would shut down the instance immediately after each execution. Then, it would boot a new instance right before the next invocation is about to come in.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="prewarming.png"
            alt="A new instance is pre-warmed right before the request comes in and recycled immediately after the execution is complete"
             />
        
    
    <figcaption>
        <h4>A new instance is pre-warmed right before the request comes in and recycled immediately after the execution is complete</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>This solves both problems above. Now, all requests are served quickly because Azure creates a fresh instance <em>before</em> a request comes in. Also, Azure saves resources because it can pre-warm the instance shortly before each request and shut it down immediately after the invocation is completed.</p>
<p>This ideal case only works in the world of perfect information. Of course, Azure can&rsquo;t perfectly predict the future and spin up a new instance right before the next HTTP request. Instead, it would need to learn the workload and try to predict the next invocation probabilistically. That&rsquo;s the essence of the policy that the authors of &ldquo;Serverless in the Wild&rdquo; suggest.</p>
<p>Predicting arbitrary workloads can be pretty hard. Every application is unique, and invocations are caused by external events like human behavior or actions in business processes.</p>
<p>Also, there are limits to resources that a prediction policy may consume. The policy calculation should be efficient and not have a significant impact on the system&rsquo;s overall performance. A practical policy would have low overhead both in terms of the size of data structures and the CPU usage overhead.</p>
<h2 id="proposed-policy">Proposed Policy</h2>
<p>The paper suggests a practical policy that tries to predict future invocations without being too expensive.</p>
<p>Let&rsquo;s start at the point when a new application has just been deployed. Azure knows nothing about its invocation patterns yet. The new policy would default to the traditional fixed &ldquo;keep-alive&rdquo; interval but would keep an instance running for a generous 4 hours. Simultaneously, it starts learning the workload.</p>
<p>Every time a new request comes in, the policy calculates how many minutes passed since the end of the previous invocation and records this value in a histogram. The histogram&rsquo;s bins have one-minute granularity. So, in my example, if an invocation came 60 minutes after the previous one, the value for the bin <code>60</code> will increase by one.</p>
<p>At some point, the policy would decide that it knows enough to start predicting future invocations. The histogram for my application may look like this:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="histogram.png"
            alt="Every column shows how many requests landed on minute X after a previous execution"
             />
        
    
    <figcaption>
        <h4>Every column shows how many requests landed on minute X after a previous execution</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The new adaptive policy kicks in. Now, it shuts down the active instance after each request, because it knows that the next invocation is unlikely to come any time soon. Then, it uses two cut-off points to plan the warming strategy:</p>
<ul>
<li>The <strong>head cut-off</strong> point is when a new warm instance should be ready. Calculated as 5th percentile minus 10% margin. Approximately 53 minutes in my example.</li>
<li>The <strong>tail cut-off</strong> point is when to kill the warm instance if no request comes in. Calculated as 95th percentile plus 10% margin. Approximately 67 minutes in my example.</li>
</ul>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="cutoff.png"
            alt="An adaptive policy pre-warms an instance at the head cut-off and keeps it until a request comes or until the tail cut-off"
             />
        
    
    <figcaption>
        <h4>An adaptive policy pre-warms an instance at the head cut-off and keeps it until a request comes or until the tail cut-off</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Because my specific workload is highly predictable, it would enjoy the absence of cold starts. Would the policy work for other scenarios?</p>
<h2 id="scenarios">Scenarios</h2>
<p>The suggested policy makes sense for the example we considered so far. However, real-life workloads are very diverse. Let&rsquo;s try to generalize and consider several possible scenarios and how the policy handles them.</p>
<h4 id="regular-cadence">Regular cadence</h4>
<p>Consistent intervals between invocations are the ideal match for the suggested policy. If a histogram is well-shaped and relatively narrow, both head and tail cut-offs are easy to identify. These distributions produce the ideal situation: long shutdown periods and short keep-alive windows.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="regular.png"
            alt="Well-shaped narrow distribution produces clear cut-off points"
             />
        
    
    <figcaption>
        <h4>Well-shaped narrow distribution produces clear cut-off points</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>My example above falls into this category.</p>
<h4 id="frequent-invocations">Frequent invocations</h4>
<p>Many applications would invoke functions frequently, so, many measured intervals would be close to zero.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="frequent.png"
            alt="In this case, instances are not unloaded after a request is executed and wait for another one, or the tail cut-off moment"
             />
        
    
    <figcaption>
        <h4>In this case, instances are not unloaded after a request is executed and wait for another one, or the tail cut-off moment</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>In these cases, the head cut-off is rounded down to zero. The policy does not shut down the application after a function execution but keeps it alive until the next execution, or until the tail cut-off.</p>
<h4 id="inconsistent-invocations-or-not-enough-data">Inconsistent invocations or not enough data</h4>
<p>The policy needs a certain amount of quality data to start being useful in predicting the next invocation.
The application may be recently deployed and may not have enough points yet:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="sparse.png"
            alt="Too few data to make reliable predictions: wait and learn while using the default policy"
             />
        
    
    <figcaption>
        <h4>Too few data to make reliable predictions: wait and learn while using the default policy</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Alternatively, data points might not come in a well-shaped cluster:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="random.png"
            alt="No definite shape of the histogram: fall back to the default policy"
             />
        
    
    <figcaption>
        <h4>No definite shape of the histogram: fall back to the default policy</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>In both of these cases, the policy reverts to a default approach: no shutdown and a long keep-alive window. This puts an extra burden on the cloud provider, but the idea is that these scenarios would be rare enough to allow the policy to stay practical.</p>
<h4 id="invocations-beyond-4-hours">Invocations beyond 4 hours</h4>
<p>The policy defines a maximum value for histogram data to limit the storage capacity for the histogram. The paper suggests a maximum of 4 hours. All intervals beyond that threshold are recorded in a special overflow bin.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="outofbounds.png"
            alt="Values beyond the 4-hour limit end up in a special &#34;everything else&#34; overflow bin"
             />
        
    
    <figcaption>
        <h4>Values beyond the 4-hour limit end up in a special &#34;everything else&#34; overflow bin</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>If a lot of values start to fall into that range, the bin-based policy can&rsquo;t perform well anymore. For this category of applications, the paper suggests switching to time-series analysis to predict the next interval duration. They mention the <a href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average">autoregressive integrated moving average</a> model without providing many details.</p>
<p>As invocations of this type are infrequent, the overall overhead of such a model would stay relatively low.</p>
<h2 id="evaluation">Evaluation</h2>
<p>The authors evaluated the policy based on recorded <a href="/2020/05/serverless-in-the-wild-azure-functions-usage-stats">production usage statistics</a>. The evaluation consists of two parts:</p>
<ul>
<li>A simulation that iterates through the data, calculates the policy models, and evaluates whether each invocation would yield a cold starts with the new policy.</li>
<li>A replay of the recorded invocation trace on a modified version of Apache OpenWhisk, an open-source FaaS platform. The controller was modified to use the new policy while managing workers. The trace was scaled down by randomly selecting applications with mid-range popularity.</li>
</ul>
<p>Both parts showed similarly promising results of reducing the cold start frequency and resource waste. The new policy reduced the average and 99-percentile function execution time 33% and 82%, respectively, while also reducing the average memory consumption of workers by 16%.</p>
<p>The overhead of the policy looks manageable too. The controller&rsquo;s policy implementation adds less than 1 ms to the end-to-end latency and only a 5% increase in controller&rsquo;s CPU utilization.</p>
<p>By tuning the parameters of the policy (default window, cut-off points, granularity), its implementation can achieve the same number of cold starts at much lower resource cost, or keep the same resource cost but reduce the frequency of cold starts significantly.</p>
<h2 id="production-implementation">Production Implementation</h2>
<p>Encouraged by the position evaluation, the team implemented the policy in Azure Functions for HTTP-triggered applications, it will be rolling out to production in stages starting this month (June 2020).</p>
<p>Here are some implementation details:</p>
<ul>
<li>Each histogram contains data for 4 hours, filling up 960 bytes per application.</li>
<li>The histogram is stored in memory and backed up to a database once per hour.</li>
<li>A new histogram is started every day with a history of previous values stored for two weeks.</li>
<li>Worker warming is scheduled for the calculated head cut-off point minus 90 seconds.</li>
<li>Pre-warming loads function dependencies and performs JIT where possible.</li>
<li>All policy decisions are asynchronous, off the critical path to minimize the latency impact on the invocation.</li>
</ul>
<p>I look forward to testing this new policy once it&rsquo;s rolled out. Expect a follow-up blog post!</p>
<h2 id="open-questions">Open Questions</h2>
<p>Everything above is my summary of the &ldquo;Serverless in the Wild&rdquo; paper&rsquo;s ideas and findings. I want to close the blog post with some questions that I still have, personally.</p>
<p>Whether true in practice or not, the public opinion strongly perceives cold starts as a notable obstacle to serverless adoption. It&rsquo;s great to see some concrete suggestions driven by data that may improve cold starts in serverless functions.</p>
<p>The sweet spot of the suggested policy is applications with relatively regular intervals between invocations below several hours. I think the practical effect of the strategy might still be limited. Here are some concerns that I see.</p>
<p><em>The authors reviewed my questions and provided their answers, which I&rsquo;m including in the text below.</em></p>
<h4 id="who-benefits">Who benefits?</h4>
<p>Not every serverless function is sensitive to cold starts.</p>
<p>The policy favors functions with predictable intervals. Timer-based schedules (whether functions with timer trigger or external timers sending requests to the app or IoT devices communicating periodically) may produce perfect histograms. And yet, those functions are not damaged by a cold start and may entirely live with it.</p>
<p>On the other side, human-initiated requests that care about cold starts are likely to be less predictable. Does it mean they don&rsquo;t enjoy much improvement with the new policy?</p>
<p>The Azure Functions team is rolling the policy out for HTTP functions, so they do expect it to be useful. We will know soon!</p>
<h5 id="response-from-the-authors">Response from the authors:</h5>
<blockquote>
<p>There is a large fraction of the applications that will likely benefit. Because for many applications the keep-alive interval can drastically reduce, the provider can afford to increase the keep-alive for other applications. About 50% of the applications have average inter-invocation times of 30 minutes of more. These incur many cold starts in the fixed policy, and will likely see a reduction in cold starts because their keep-alive intervals will be able to grow. Applications with idle times longer than 4 hours can also benefit because of the ARIMA time-series prediction.</p>
</blockquote>
<h4 id="is-it-flexible-enough">Is it flexible enough?</h4>
<p>The policy assumes that the interval distribution is stable over time. How will this hold in practice over more extended periods?</p>
<p>Imagine an application that is only used during business hours. Or, an application that is mostly idle but is sporadically applied for a specific business process. Or, a demo app that you want to show to your colleagues during a planning meeting. Likely, all of them would still hit cold starts at the beginning of a session.</p>
<p>Maybe that&rsquo;s why the production implementation begins with a clean histogram every day. They will watch and learn, and may use data for the last two weeks to improve later on.</p>
<h5 id="response-from-the-authors-1">Response from the authors:</h5>
<blockquote>
<p>The policy does not have to assume a stationary distribution of arrival patterns. In the traces analyzed in the paper there was not a significant enough variation in the distributions to enable a deep investigation. With the production deployment, we will investigate different policies to decay information from previous histograms.</p>
</blockquote>
<h4 id="what-about-function-warming">What about function warming?</h4>
<p>There&rsquo;s the elephant in the room of cold starts: function &ldquo;warming&rdquo;. Warming is a trick when a developer adds an extra timer-based function to their Function App so that the timer triggers every few minutes. This way, the runtime would never unload it, and an instance would always be ready.</p>
<p>I suspect that this simple trick still outperforms the suggested histogram-based policy in terms of cold-start prevention. Obviously, it doesn&rsquo;t help the cloud provider to save costs.</p>
<p>Is there a way to combine two approaches and get the benefits of both?</p>
<h5 id="response-from-the-authors-2">Response from the authors:</h5>
<blockquote>
<p>The results in the paper take into account user-generated warm-up functions that are present in the data. If the hybrid policy is successful in preventing cold starts, there will be no need for users to create periodic warm-up functions, which represent extra effort and higher costs.</p>
</blockquote>
<h4 id="what-about-the-scale-out-cold-start">What about the scale-out cold start?</h4>
<p>The paper focuses on applications that are invoked rarely. However, even applications with higher utilization may still hit cold starts when scaled out on multiple instances. Every new instance would need to boot, and the requests would still be waiting.</p>
<p>The suggested policy does not address cold starts beyond the first instance, even though they do occur and potentially have a more significant impact on latency percentiles of real-world human-facing applications.</p>
<h5 id="response-from-the-authors-3">Response from the authors:</h5>
<blockquote>
<p>We didn’t address scale-out cold starts in the paper. We can pre-warm the scale-out instances by forcing the scale out to happen slightly before it normally would. For example, when the scale out is triggered by a threshold number of concurrent invocations, we can lower the threshold slightly. We will be experimenting with such techniques in our implementation in Azure Functions.</p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>Despite several open questions above, I&rsquo;m delighted that the paper was published. I welcome any structured effort that focuses on cold start optimization. The paper highlights usage statistics, the challenges of the cold start problem, and suggests several improvements.</p>
<p>It&rsquo;s even more exciting to see the finding being applied in Azure in production!</p>
<p>If you want to learn more, you can read the full paper here: <a href="https://arxiv.org/pdf/2003.03423.pdf">Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/paper-review" term="paper-review" label="Paper review" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[TDD]]></title>
            <link href="https://mikhail.io/tags/tdd/"/>
            <id>https://mikhail.io/tags/tdd/</id>
            
            <published>2020-05-21T00:00:00+00:00</published>
            <updated>2020-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Unit Testing]]></title>
            <link href="https://mikhail.io/tags/unit-testing/"/>
            <id>https://mikhail.io/tags/unit-testing/</id>
            
            <published>2020-05-21T00:00:00+00:00</published>
            <updated>2020-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Unit Testing Cloud Deployments with Pulumi in C#]]></title>
            <link href="https://mikhail.io/2020/05/unit-testing-cloud-deployments-with-pulumi-in-csharp/"/>
            <id>https://mikhail.io/2020/05/unit-testing-cloud-deployments-with-pulumi-in-csharp/</id>
            
            <published>2020-05-21T00:00:00+00:00</published>
            <updated>2020-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Developing infrastructure programs in C# with unit tests, TDD, and mocks</blockquote><p>Because Pulumi uses general-purpose programming languages to provision cloud resources, you can take advantage of native tools and perform automated tests of your infrastructure. The full power of each language is available, including access to libraries and frameworks for testing.</p>
<p>This blog post takes a deeper dive into mock-based unit testing of Pulumi programs written in C#. You can find an F# version of this blog post <a href="/2020/05/unit-testing-cloud-deployments-with-pulumi-in-fsharp/">here</a>.</p>
<h2 id="how-unit-testing-works-in-pulumi">How Unit Testing Works in Pulumi</h2>
<p>Let&rsquo;s start with a picture showing how Pulumi components interact during a typical deployment process.</p>
<p><img src="./engine.png" alt="Pulumi Components Interaction"></p>
<p>Whenever a new resource is instantiated in code, the program makes a remote call to the engine. The engine receives the resource&rsquo;s input values, validates them, and translates the request to an invocation of a cloud API. The API returns some data, which are translated to resource outputs and sent back to the program.</p>
<p>The remote calls may be slow, unreliable, and non-deterministic, which makes testing of these interactions hard.</p>
<p><a href="https://www.pulumi.com/docs/guides/testing/">Pulumi testing guide</a> outlines several testing methods, but today I want to focus on unit testing.</p>
<p>The Pulumi SDK provides a hook to replace all the remote calls with mocks. Mocks run in the same process and can respond immediately with hard-coded or calculated on-the-fly data.</p>
<p><img src="./mocks.png" alt="Pulumi with Mocks"></p>
<p>We can write fully deterministic and blazingly fast automated tests as all remote calls and uncertainty are eliminated. There is no cloud to respond to resource creation, so it&rsquo;s a developer&rsquo;s responsibility to mimic the cloud behavior with mocks adequately.</p>
<p>This blog post walks you through an example of unit testing and mocking with the .NET SDK.</p>
<h2 id="define-the-base-structure">Define the Base Structure</h2>
<p>In this article, I build a program that deploys a static website to Azure. I use TDD to add new tests and deployment components bit-by-bit.</p>
<p>Let&rsquo;s start with an empty project—go ahead and create a .NET Core Console Application with the .NET CLI or your favorite IDE.</p>
<h3 id="install-nuget-packages">Install NuGet packages</h3>
<p>You are free to choose your unit testing frameworks, mocking and assertions libraries. I&rsquo;m using NUnit with FluentAssertions, and my program tests Azure resources, so this is my project file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">&lt;Project Sdk=<span style="color:#a31515">&#34;Microsoft.NET.Sdk&#34;</span>&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
    &lt;Nullable&gt;enable&lt;/Nullable&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;PackageReference Include=<span style="color:#a31515">&#34;FluentAssertions&#34;</span> Version=<span style="color:#a31515">&#34;5.10.2&#34;</span> /&gt;
    &lt;PackageReference Include=<span style="color:#a31515">&#34;Microsoft.NET.Test.Sdk&#34;</span> Version=<span style="color:#a31515">&#34;16.5.0&#34;</span> /&gt;
    &lt;PackageReference Include=<span style="color:#a31515">&#34;NUnit&#34;</span> Version=<span style="color:#a31515">&#34;3.12.0&#34;</span> /&gt;
    &lt;PackageReference Include=<span style="color:#a31515">&#34;NUnit3TestAdapter&#34;</span> Version=<span style="color:#a31515">&#34;3.16.1&#34;</span> /&gt;
    &lt;PackageReference Include=<span style="color:#a31515">&#34;Pulumi.Azure&#34;</span> Version=<span style="color:#a31515">&#34;3.*&#34;</span> /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;
</code></pre></div><h3 id="stack">Stack</h3>
<p>A Pulumi stack is the &ldquo;unit&rdquo; of our testing. Every test instantiates a stack, retrieves the resources that the stack defines, and makes assertions about them.</p>
<p>Here is my starting point in <code>WebsiteStack.cs</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">using</span> System.IO;
<span style="color:#00f">using</span> Pulumi;
<span style="color:#00f">using</span> Pulumi.Azure.Core;
<span style="color:#00f">using</span> Storage = Pulumi.Azure.Storage;

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WebsiteStack</span> : Stack
{
    <span style="color:#00f">public</span> WebsiteStack()
    {
        <span style="color:#008000">// &lt;-- Cloud resources go here
</span><span style="color:#008000"></span>    }
}
</code></pre></div><p>This class is precisely what Pulumi expects to start deploying your resources to the cloud. Once the test suite is ready and green, you can deploy the stack with <code>pulumi up</code>.</p>
<h3 id="mocks">Mocks</h3>
<p>As I explained above, unit tests replace the real Pulumi engine with mocks. Mocks get all calls and respond with predefined values.</p>
<p>A mock class has to implement two methods of <code>Pulumi.Testing.IMocks</code> interface. <code>NewResourceAsync</code> is called whenever a new resource is defined, while <code>CallAsync</code> is invoked when our program retrieves information about existing cloud resources. We&rsquo;ll focus on the former in this post, but we still need to define both.</p>
<p>While you are free to use your favorite mocking library, I&rsquo;ll keep it simple and define the mocks as a plain class.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">using</span> System.Collections.Immutable;
<span style="color:#00f">using</span> System.Threading.Tasks;
<span style="color:#00f">using</span> Pulumi;
<span style="color:#00f">using</span> Pulumi.Testing;

<span style="color:#00f">namespace</span> UnitTesting
{
    <span style="color:#00f">class</span> <span style="color:#2b91af">Mocks</span> : IMocks
    {
        <span style="color:#00f">public</span> Task&lt;(<span style="color:#2b91af">string</span> id, <span style="color:#2b91af">object</span> state)&gt; NewResourceAsync(
            <span style="color:#2b91af">string</span> type, <span style="color:#2b91af">string</span> name, ImmutableDictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">object</span>&gt; inputs, <span style="color:#2b91af">string?</span> provider, <span style="color:#2b91af">string?</span> id)
        {
            <span style="color:#2b91af">var</span> outputs = ImmutableDictionary.CreateBuilder&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">object</span>&gt;();

            <span style="color:#008000">// Forward all input parameters as resource outputs, so that we could test them.
</span><span style="color:#008000"></span>            outputs.AddRange(inputs);

            <span style="color:#008000">// &lt;-- We&#39;ll customize the mocks here
</span><span style="color:#008000"></span>
            <span style="color:#008000">// Default the resource ID to `{name}_id`.
</span><span style="color:#008000"></span>            id ??= <span style="color:#a31515">$&#34;{name}_id&#34;</span>;
            <span style="color:#00f">return</span> Task.FromResult((id, (<span style="color:#2b91af">object</span>)outputs));
        }

        <span style="color:#00f">public</span> Task&lt;<span style="color:#2b91af">object</span>&gt; CallAsync(<span style="color:#2b91af">string</span> token, ImmutableDictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">object</span>&gt; inputs, <span style="color:#2b91af">string?</span> provider)
        {
            <span style="color:#008000">// We don&#39;t use this method in this particular test suite.
</span><span style="color:#008000"></span>            <span style="color:#008000">// Default to returning whatever we got as input.
</span><span style="color:#008000"></span>            <span style="color:#00f">return</span> Task.FromResult((<span style="color:#2b91af">object</span>)inputs);
        }
    }
}
</code></pre></div><p>Both methods receive several parameters and need to return a result. Notably, they get the list of <code>inputs</code>—the arguments that our program defined for the given resource. The goal of the mocks is to return the list of outputs, similarly to what a cloud provider would do.</p>
<p>My default implementation returns the outputs that include all the inputs and nothing else. That&rsquo;s a sensible default: usually, real outputs are a superset of inputs. We&rsquo;ll extend this behavior as we need later on.</p>
<h3 id="test-fixture">Test Fixture</h3>
<p>The next class to add is the container for my future unit tests. I named the file <code>WebsiteStackTests.cs</code> and it defines a test container, called a &ldquo;test fixture&rdquo; in NUnit:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">using</span> System.Linq;
<span style="color:#00f">using</span> System.Threading.Tasks;
<span style="color:#00f">using</span> FluentAssertions;
<span style="color:#00f">using</span> NUnit.Framework;
<span style="color:#00f">using</span> Pulumi;
<span style="color:#00f">using</span> Pulumi.Azure.Core;
<span style="color:#00f">using</span> Storage = Pulumi.Azure.Storage;

<span style="color:#00f">namespace</span> UnitTesting
{
	[TestFixture]
	<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WebserverStackTests</span>
	{
        <span style="color:#008000">// &lt;-- Tests go here
</span><span style="color:#008000"></span>
		<span style="color:#00f">private</span> <span style="color:#00f">static</span> Task&lt;ImmutableArray&lt;Resource&gt;&gt; TestAsync()
		{
			<span style="color:#00f">return</span> Deployment.TestAsync&lt;WebsiteStack&gt;(<span style="color:#00f">new</span> Mocks(), <span style="color:#00f">new</span> TestOptions {IsPreview = <span style="color:#00f">false</span>});
		}
    }
}
</code></pre></div><p>I defined a helper method <code>TestAsync</code> that points Pulumi&rsquo;s <code>Deployment.TestAsync</code> to our stack and mock classes.</p>
<p>As we progress through the article, I will add tests to this class one-by-one.</p>
<h3 id="retrieve-output-values">Retrieve output values</h3>
<p>Finally, I need a small extension method to extract values from outputs. Every Pulumi resource returns values wrapped inside an <code>Output&lt;T&gt;</code> container. While the real engine runs, those values may be known or unknown, but my mock-based tests always return known values. It&rsquo;s safe to get those values and convert them to a <code>Task&lt;T&gt;</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TestingExtensions</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Task&lt;T&gt; GetValueAsync&lt;T&gt;(<span style="color:#00f">this</span> Output&lt;T&gt; output)
    {
        <span style="color:#2b91af">var</span> tcs = <span style="color:#00f">new</span> TaskCompletionSource&lt;T&gt;();
        output.Apply(v =&gt; { tcs.SetResult(v); <span style="color:#00f">return</span> v; });
        <span style="color:#00f">return</span> tcs.Task;
    }
}
</code></pre></div><p>To learn more about outputs, read <a href="https://www.pulumi.com/docs/intro/concepts/programming-model/#stack-outputs%22">this article</a>.</p>
<h2 id="first-test">First Test</h2>
<p>The setup is done, time to write my first unit test! True to the TDD spirit, I write the tests before I define any resources.</p>
<p>Every Azure resource has to live inside a resource group, so my first test checks that a new resource group is defined.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task SingleResourceGroupExists()
{
    <span style="color:#2b91af">var</span> resources = <span style="color:#00f">await</span> TestAsync();

    <span style="color:#2b91af">var</span> resourceGroups = resources.OfType&lt;ResourceGroup&gt;().ToList();
    resourceGroups.Count.Should().Be(1, <span style="color:#a31515">&#34;a single resource group is expected&#34;</span>);
}
</code></pre></div><p>This code is great to illustrate the overall structure of each test:</p>
<ol>
<li>Call the <code>TestAsync</code> method to evaluate the stack.</li>
<li>Find the target resource in the collection of created resources.</li>
<li>Assert a property of the target resource.</li>
</ol>
<p>In this case, the test validates that there is one and only one resource group defined.</p>
<p>I can run <code>dotnet test</code> and see the expected test failure:</p>
<pre><code>$ dotnet test
...

X SingleResourceGroupExists [238ms]
  Error Message:
   Expected resourceGroups.Count to be 1 because a single resource group is expected, but found 0.
...
Total tests: 1
     Failed: 1
 Total time: 0.9270 Seconds
</code></pre><p>I go ahead and add the following definition to the <code>WebsiteStack</code> constructor.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> resourceGroup = <span style="color:#00f">new</span> ResourceGroup(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>);
</code></pre></div><p>This change is enough to make the tests green!</p>
<pre><code>$ dotnet test
...
Test Run Successful.
Total tests: 1
     Passed: 1
 Total time: 0.8462 Seconds
</code></pre><h2 id="tags">Tags</h2>
<p>For billing and lifecycle management, I want all my resource groups to be tagged. Therefore, my second test validates that the resource group has an <code>Environment</code> tag on it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task ResourceGroupHasEnvironmentTag()
{
    <span style="color:#2b91af">var</span> resources = <span style="color:#00f">await</span> TestAsync();
    <span style="color:#2b91af">var</span> resourceGroup = resources.OfType&lt;ResourceGroup&gt;().First();

    <span style="color:#2b91af">var</span> tags = <span style="color:#00f">await</span> resourceGroup.Tags.GetValueAsync();
    tags.Should().NotBeNull(<span style="color:#a31515">&#34;Tags must be defined&#34;</span>);
    tags.Should().ContainKey(<span style="color:#a31515">&#34;Environment&#34;</span>);
}
</code></pre></div><p>The test finds the resource group and then checks that the <code>Tags</code> dictionary is not null and contains a tag with the name <code>Environment</code>. Predictably, the test fails:</p>
<pre><code>$ dotnet test
...
  X ResourceGroupHasEnvironmentTag [240ms]
  Error Message:
   Expected tags not to be &lt;null&gt; because Tags must be defined.
...
Test Run Failed.
</code></pre><p>Now, I edit the stack class and change the resource group definition.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> resourceGroup = <span style="color:#00f">new</span> ResourceGroup(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>, <span style="color:#00f">new</span> ResourceGroupArgs
{
    Tags = { { <span style="color:#a31515">&#34;Environment&#34;</span>, <span style="color:#a31515">&#34;production&#34;</span> } }
});
</code></pre></div><p>This change makes the test suite green again, and we are good to proceed.</p>
<h2 id="storage-account">Storage Account</h2>
<p>It&rsquo;s time to add a test for a second resource in the stack: a Storage Account. To make things more interesting, I want to check that the storage account belongs to our resource group.</p>
<p>There&rsquo;s no direct link between a storage account object and a resource group object. Instead, we need to check the <code>ResourceGroupName</code> property of the account. The test below expects the account to belong to a resource group called <code>www-prod-rg</code>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task StorageAccountBelongsToResourceGroup()
{
    <span style="color:#2b91af">var</span> resources = <span style="color:#00f">await</span> TestAsync();
    <span style="color:#2b91af">var</span> storageAccount = resources.OfType&lt;Storage.Account&gt;().SingleOrDefault();
    storageAccount.Should().NotBeNull(<span style="color:#a31515">&#34;Storage account not found&#34;</span>);

    <span style="color:#2b91af">var</span> resourceGroupName = <span style="color:#00f">await</span> storageAccount.ResourceGroupName.GetValueAsync();
    resourceGroupName.Should().Be(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>);
}
</code></pre></div><p>Of course, the test fails. I try to fix it with what I think is the minimal change to make the test pass by adding a new resource to the stack and pointing it to the resource group.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> storageAccount = <span style="color:#00f">new</span> Storage.Account(<span style="color:#a31515">&#34;wwwprodsa&#34;</span>, <span style="color:#00f">new</span> Storage.AccountArgs
{
    ResourceGroupName = resourceGroup.Name,
});
</code></pre></div><p>However, when I rerun the test suite, all three tests fail. They complain about the missing required properties <code>AccountReplicationType</code> and <code>AccountTier</code> on the storage account, so I have to add those.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> storageAccount = <span style="color:#00f">new</span> Storage.Account(<span style="color:#a31515">&#34;wwwprodsa&#34;</span>, <span style="color:#00f">new</span> Storage.AccountArgs
{
    ResourceGroupName = resourceGroup.Name,
    AccountTier = <span style="color:#a31515">&#34;Standard&#34;</span>,
    AccountReplicationType = <span style="color:#a31515">&#34;LRS&#34;</span>,
    StaticWebsite = <span style="color:#00f">new</span> Storage.Inputs.AccountStaticWebsiteArgs
    {
        IndexDocument = <span style="color:#a31515">&#34;index.html&#34;</span>
    }
});
</code></pre></div><p>I defined the <code>StaticWebsite</code> property as well: I&rsquo;ll leave testing these values as an exercise for the reader.</p>
<p>Two resource group tests are back to green again, but the storage account test fails with a new error message.</p>
<pre><code>X StorageAccountBelongsToResourceGroup [84ms]
  Error Message:
   Expected resourceGroupName to be &quot;www-prod-rg&quot;, but found &lt;null&gt;.
</code></pre><p>What&rsquo;s going on, and why is it <code>null</code>?</p>
<p>I defined the account name like this: <code>ResourceGroupName = resourceGroup.Name</code>. However, if we look closely, the <code>resourceGroup</code> resource doesn&rsquo;t have an input property <code>Name</code> defined. <code>www-prod-rg</code> is a logical name for Pulumi deployment, not the physical name of the resource.</p>
<p>Under normal circumstances, the Pulumi engine would use the logical name to produce the physical name of the resource group automatically (see <a href="https://www.pulumi.com/docs/intro/concepts/programming-model/#names">resource names</a> for details). However, my mocks don&rsquo;t do that.</p>
<p>That&rsquo;s a good reason to change the <code>Mocks</code> implementation. I add the following lines to the <code>NewResourceAsync</code> method.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Set the name to resource name if it&#39;s not set explicitly in inputs.
</span><span style="color:#008000"></span><span style="color:#00f">if</span> (!inputs.ContainsKey(<span style="color:#a31515">&#34;name&#34;</span>))
    outputs.Add(<span style="color:#a31515">&#34;name&#34;</span>, name);
</code></pre></div><p>Note that mocks operate on weakly-typed dictionaries, so I need to get the property name right. Pulumi SDKs are open source, so I looked <a href="https://github.com/pulumi/pulumi-azure/blob/1fdcab88065175ced768d900e7dcedf3b1d1b0a7/sdk/dotnet/Core/ResourceGroup.cs#L90">here</a> to double-check the exact value.</p>
<p>After this change in <code>Mocks</code>, my tests go green again.</p>
<pre><code>Test Run Successful.
Total tests: 3
     Passed: 3
</code></pre><h2 id="website-files">Website Files</h2>
<p>The next step is to upload some files to the static website. Well, instead, to write an automated test that validates the upload with mocks.</p>
<p>I create a <code>wwwroot</code> folder with two HTML files in it and copy the folder to the build&rsquo;s output. Here is my change in the project file.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">&lt;ItemGroup&gt;
    &lt;None Update=<span style="color:#a31515">&#34;wwwroot\**\*.*&#34;</span>&gt;
        &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
&lt;/ItemGroup&gt;
</code></pre></div><p>Now, the test is straightforward: it expects two <code>Blob</code> resources in the stack.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task UploadsTwoFiles()
{
    <span style="color:#2b91af">var</span> resources = <span style="color:#00f">await</span> TestAsync();
    <span style="color:#2b91af">var</span> files = resources.OfType&lt;Storage.Blob&gt;().ToList();
    files.Count.Should().Be(2, <span style="color:#a31515">&#34;Should have uploaded files from `wwwroot`&#34;</span>);
}
</code></pre></div><p>As intended, the test fails immediately.</p>
<pre><code>X UploadsTwoFiles [95ms]
  Error Message:
   Expected files.Count to be 2 because Should have uploaded files from `wwwroot`, but found 0.
</code></pre><p>I extend my stack with the following loop that navigates through the files and creates a storage blob for each of them.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> files = Directory.GetFiles(<span style="color:#a31515">&#34;wwwroot&#34;</span>);
<span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> file <span style="color:#00f">in</span> files)
{
    <span style="color:#2b91af">var</span> blob = <span style="color:#00f">new</span> Storage.Blob(file, <span style="color:#00f">new</span> Storage.BlobArgs
    {
        ContentType = <span style="color:#a31515">&#34;application/html&#34;</span>,
        Source = <span style="color:#00f">new</span> FileAsset(file),
        StorageAccountName = storageAccount.Name,
        StorageContainerName = <span style="color:#a31515">&#34;$web&#34;</span>,
        Type = <span style="color:#a31515">&#34;Block&#34;</span>
    });
}
</code></pre></div><p>However, this change breaks the stack and all tests in the suite:</p>
<pre><code>Error Message:
   Pulumi.RunException : Running program failed with an unhandled exception:
System.InvalidOperationException: Unsupported value when converting to protobuf: Pulumi.FileAsset
   at Pulumi.Serialization.Serializer.CreateValue(Object value)
...
</code></pre><p>Once again, our mocks fail to represent the behavior of the engine accurately. The Pulumi engine knows about the <code>FileAsset</code> class pointing to a file on the disk and how to convert it to an uploaded blob. But, the engine doesn&rsquo;t copy this property to outputs. I need to adjust the mocks again.</p>
<p>I&rsquo;m not particularly interested in testing the binary contents of the files now, so I&rsquo;ll change the <code>Mocks</code> class to ignore the <code>source</code> property and not to include it into the output dictionary.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">if</span> (type == <span style="color:#a31515">&#34;azure:storage/blob:Blob&#34;</span>)
{
    <span style="color:#008000">// Assets can&#39;t directly go through the engine.
</span><span style="color:#008000"></span>    <span style="color:#008000">// We don&#39;t need them in the test, so blank out the property for now.
</span><span style="color:#008000"></span>    outputs.Remove(<span style="color:#a31515">&#34;source&#34;</span>);
}
</code></pre></div><p>This change makes my test suite happy again.</p>
<pre><code>Test Run Successful.
Total tests: 4
     Passed: 4
</code></pre><h2 id="validate-website-url">Validate Website URL</h2>
<p>So far, I&rsquo;ve been validating the input parameters of resources. What if I want to test something that is usually done by the cloud provider?</p>
<p>For instance, when Azure creates a static website, it automatically assigns a public endpoint. The endpoint is then available in the <code>PrimaryWebEndpoint</code> property after the Pulumi program ran and resources are created. I may want to export this value from stack outputs and validate it in a unit test.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task StackExportsWebsiteUrl()
{
    <span style="color:#2b91af">var</span> resources = <span style="color:#00f">await</span> TestAsync();
    <span style="color:#2b91af">var</span> stack = resources.OfType&lt;WebsiteStack&gt;().First();

    <span style="color:#2b91af">var</span> endpoint = <span style="color:#00f">await</span> stack.Endpoint.GetValueAsync();
    endpoint.Should().Be(<span style="color:#a31515">&#34;https://wwwprodsa.web.core.windows.net&#34;</span>);
}
</code></pre></div><p>This test doesn&rsquo;t even compile yet, so I have to define the <code>Endpoint</code> property and assign a value to it. Here is the complete implementation of the stack with the output property defined at the end.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WebsiteStack</span> : Stack
{
    <span style="color:#00f">public</span> WebsiteStack()
    {
        <span style="color:#2b91af">var</span> resourceGroup = <span style="color:#00f">new</span> ResourceGroup(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>, <span style="color:#00f">new</span> ResourceGroupArgs
        {
            Tags = { { <span style="color:#a31515">&#34;Environment&#34;</span>, <span style="color:#a31515">&#34;production&#34;</span> } }
        });

        <span style="color:#2b91af">var</span> storageAccount = <span style="color:#00f">new</span> Storage.Account(<span style="color:#a31515">&#34;wwwprodsa&#34;</span>, <span style="color:#00f">new</span> Storage.AccountArgs
        {
            ResourceGroupName = resourceGroup.Name,
            AccountTier = <span style="color:#a31515">&#34;Standard&#34;</span>,
            AccountReplicationType = <span style="color:#a31515">&#34;LRS&#34;</span>,
            StaticWebsite = <span style="color:#00f">new</span> Storage.Inputs.AccountStaticWebsiteArgs
            {
                IndexDocument = <span style="color:#a31515">&#34;index.html&#34;</span>
            }
        });

        <span style="color:#2b91af">var</span> files = Directory.GetFiles(<span style="color:#a31515">&#34;wwwroot&#34;</span>);
        <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> file <span style="color:#00f">in</span> files)
        {
            <span style="color:#2b91af">var</span> blob = <span style="color:#00f">new</span> Storage.Blob(file, <span style="color:#00f">new</span> Storage.BlobArgs
            {
                ContentType = <span style="color:#a31515">&#34;application/html&#34;</span>,
                Source = <span style="color:#00f">new</span> FileAsset(file),
                StorageAccountName = storageAccount.Name,
                StorageContainerName = <span style="color:#a31515">&#34;$web&#34;</span>,
                Type = <span style="color:#a31515">&#34;Block&#34;</span>
            });
        }

        <span style="color:#00f">this</span>.Endpoint = storageAccount.PrimaryWebEndpoint;
    }

    [Output] <span style="color:#00f">public</span> Output&lt;<span style="color:#2b91af">string</span>&gt; Endpoint { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>Now, the test compiles but fails when executed:</p>
<pre><code>X StackExportsWebsiteUrl [85ms]
  Error Message:
   Expected endpoint to be &quot;https://wwwprodsa.web.core.windows.net&quot;, but found &lt;null&gt;.
</code></pre><p>That&rsquo;s because there is no call to Azure to populate the endpoint. It&rsquo;s time to make the last change to my <code>Mocks</code> class to emulate that assignment.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// For a Storage Account...
</span><span style="color:#008000"></span><span style="color:#00f">if</span> (type == <span style="color:#a31515">&#34;azure:storage/account:Account&#34;</span>)
{
    <span style="color:#008000">// ... set its web endpoint property.
</span><span style="color:#008000"></span>    <span style="color:#008000">// Normally this would be calculated by Azure, so we have to mock it.
</span><span style="color:#008000"></span>    outputs.Add(<span style="color:#a31515">&#34;primaryWebEndpoint&#34;</span>, <span style="color:#a31515">$&#34;https://{name}.web.core.windows.net&#34;</span>);
}
</code></pre></div><p>And that&rsquo;s it! My static website is ready and tested!</p>
<pre><code>Test Run Successful.
Total tests: 5
     Passed: 5
 Total time: 0.9338 Seconds
</code></pre><p>Note that it still takes less than a second to run my test suite so that I can iterate very quickly.</p>
<h2 id="get-started">Get Started</h2>
<p>The tests above cover the basics of unit testing with Pulumi .NET SDK. You can take it from here and apply the techniques and practices that you use while testing the application code. You may also try more advanced practices like property-based testing or behavior-driven development—we believe that the mocks enable many testing styles.</p>
<p>Here are several useful pointers to get started with testing in Pulumi:</p>
<ul>
<li><a href="https://www.pulumi.com/docs/guides/testing/">Testing Guide</a>.</li>
<li><a href="https://github.com/pulumi/examples/tree/72c9480f4c1240f795f6020f50801733fbef37f2/testing-unit-cs-mocks">Full code for this blog post</a>.</li>
<li><a href="https://github.com/pulumi/examples/tree/de060e659e1bb4af15d895fe4de7a3f10218b669/testing-unit-cs">Another example of unit testing in C#</a></li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/unit-testing" term="unit-testing" label="Unit Testing" />
                             
                                <category scheme="https://mikhail.io/tags/tdd" term="tdd" label="TDD" />
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Unit Testing Cloud Deployments with Pulumi in F#]]></title>
            <link href="https://mikhail.io/2020/05/unit-testing-cloud-deployments-with-pulumi-in-fsharp/"/>
            <id>https://mikhail.io/2020/05/unit-testing-cloud-deployments-with-pulumi-in-fsharp/</id>
            
            <published>2020-05-21T00:00:00+00:00</published>
            <updated>2020-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Developing infrastructure programs in F# with unit tests, TDD, and mocks</blockquote><p>Because Pulumi uses general-purpose programming languages to provision cloud resources, you can take advantage of native tools and perform automated tests of your infrastructure. The full power of each language is available, including access to libraries and frameworks for testing.</p>
<p>This blog post takes a deeper dive into mock-based unit testing of Pulumi programs written in F#. You can find a C# version of this blog post <a href="/2020/05/unit-testing-cloud-deployments-with-pulumi-in-csharp/">here</a>.</p>
<h2 id="how-unit-testing-works-in-pulumi">How Unit Testing Works in Pulumi</h2>
<p>Let&rsquo;s start with a picture showing how Pulumi components interact during a typical deployment process.</p>
<p><img src="./engine.png" alt="Pulumi Components Interaction"></p>
<p>Whenever a new resource is instantiated in code, the program makes a remote call to the engine. The engine receives the resource&rsquo;s input values, validates them, and translates the request to an invocation of a cloud API. The API returns some data, which are translated to resource outputs and sent back to the program.</p>
<p>The remote calls may be slow, unreliable, and non-deterministic, which makes testing of these interactions hard.</p>
<p>Our <a href="https://www.pulumi.com/docs/guides/testing/">testing guide</a> outlines several testing methods, but today I want to focus on unit testing.</p>
<p>The Pulumi SDK provides a hook to replace all the remote calls with mocks. Mocks run in the same process and can respond immediately with hard-coded or calculated on-the-fly data.</p>
<p><img src="./mocks.png" alt="Pulumi with Mocks"></p>
<p>We can write fully deterministic and blazingly fast automated tests as all remote calls and uncertainty are eliminated. There is no cloud to respond to resource creation, so it&rsquo;s a developer&rsquo;s responsibility to mimic the cloud behavior with mocks adequately.</p>
<p>This blog post walks you through an example of unit testing and mocking with the .NET SDK.</p>
<h2 id="define-the-base-structure">Define the Base Structure</h2>
<p>In this article, I build a program that deploys a static website to Azure. I use TDD to add new tests and deployment components bit-by-bit.</p>
<p>Let&rsquo;s start with an empty project—go ahead and create a .NET Core Console Application with the .NET CLI or your favorite IDE.</p>
<h3 id="install-nuget-packages">Install NuGet packages</h3>
<p>You are free to choose your unit testing frameworks, mocking and assertions libraries. I&rsquo;m using NUnit with FluentAssertions, and my program tests Azure resources, so this is my project file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">&lt;Project Sdk=<span style="color:#a31515">&#34;Microsoft.NET.Sdk&#34;</span>&gt;

    &lt;PropertyGroup&gt;
        &lt;OutputType&gt;Exe&lt;/OutputType&gt;
        &lt;TargetFramework&gt;netcoreapp3.1&lt;/TargetFramework&gt;
        &lt;Nullable&gt;enable&lt;/Nullable&gt;
    &lt;/PropertyGroup&gt;

    &lt;ItemGroup&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;FluentAssertions&#34;</span> Version=<span style="color:#a31515">&#34;5.10.2&#34;</span> /&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;Microsoft.NET.Test.Sdk&#34;</span> Version=<span style="color:#a31515">&#34;16.5.0&#34;</span> /&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;NUnit&#34;</span> Version=<span style="color:#a31515">&#34;3.12.0&#34;</span> /&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;NUnit3TestAdapter&#34;</span> Version=<span style="color:#a31515">&#34;3.16.1&#34;</span> /&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;Pulumi.Azure&#34;</span> Version=<span style="color:#a31515">&#34;3.*&#34;</span> /&gt;
        &lt;PackageReference Include=<span style="color:#a31515">&#34;Pulumi.FSharp&#34;</span> Version=<span style="color:#a31515">&#34;2.*&#34;</span> /&gt;
    &lt;/ItemGroup&gt;

    &lt;ItemGroup&gt;
        &lt;Compile Include=<span style="color:#a31515">&#34;Testing.fs&#34;</span> /&gt;
        &lt;Compile Include=<span style="color:#a31515">&#34;WebsiteStack.fs&#34;</span> /&gt;
        &lt;Compile Include=<span style="color:#a31515">&#34;WebsiteStackTests.fs&#34;</span> /&gt;
    &lt;/ItemGroup&gt;

&lt;/Project&gt;
</code></pre></div><h3 id="stack">Stack</h3>
<p>A Pulumi stack is the &ldquo;unit&rdquo; of our testing. Every test instantiates a stack, retrieves the resources that the stack defines, and makes assertions about them.</p>
<p>Here is my starting point in <code>WebsiteStack.fs</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">namespace</span> UnitTesting

<span style="color:#00f">open</span> Pulumi
<span style="color:#00f">open</span> Pulumi.FSharp
<span style="color:#00f">open</span> Pulumi.Azure.Core

<span style="color:#00f">type</span> <span style="color:#2b91af">WebsiteStack</span>() =
    <span style="color:#00f">inherit</span> Stack()

    <span style="color:#008000">// &lt;-- Cloud resources go here
</span></code></pre></div><p>This class is precisely what Pulumi expects to start deploying your resources to the cloud. Once the test suite is ready and green, you can deploy the stack with <code>pulumi up</code>.</p>
<h3 id="mocks">Mocks</h3>
<p>As I explained above, unit tests replace the real Pulumi engine with mocks. Mocks get all calls and respond with predefined values.</p>
<p>A mock class has to implement two methods of <code>Pulumi.Testing.IMocks</code> interface. <code>NewResourceAsync</code> is called whenever a new resource is defined, while <code>CallAsync</code> is invoked when our program retrieves information about existing cloud resources. We&rsquo;ll focus on the former in this post, but we still need to define both.</p>
<p>While you are free to use your favorite mocking library, I&rsquo;ll keep it simple and define the mocks as a plain class.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">namespace</span> UnitTesting

<span style="color:#00f">open</span> System
<span style="color:#00f">open</span> System.Collections.Immutable
<span style="color:#00f">open</span> System.Threading.Tasks
<span style="color:#00f">open</span> Pulumi.Testing

<span style="color:#00f">type</span> <span style="color:#2b91af">Mocks</span>() =
    <span style="color:#00f">interface</span> IMocks <span style="color:#00f">with</span>
        <span style="color:#00f">member</span> this.NewResourceAsync(typeName: <span style="color:#2b91af">string</span>, name: <span style="color:#2b91af">string</span>, inputs: ImmutableDictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">obj</span>&gt;, provider: <span style="color:#2b91af">string</span>, id: <span style="color:#2b91af">string</span>): Task&lt;ValueTuple&lt;<span style="color:#2b91af">string</span>,<span style="color:#2b91af">obj</span>&gt;&gt; =
            <span style="color:#008000">// Forward all input parameters as resource outputs, so that we could test them.
</span><span style="color:#008000"></span>            <span style="color:#00f">let</span> dict = inputs.ToImmutableDictionary() :&gt; <span style="color:#2b91af">obj</span>

            <span style="color:#008000">// &lt;-- We&#39;ll customize the mocks here
</span><span style="color:#008000"></span>
            <span style="color:#008000">// Default the resource ID to `{name}_id`.
</span><span style="color:#008000"></span>            <span style="color:#00f">let</span> id = <span style="color:#00f">if</span> id = <span style="color:#00f">null</span> <span style="color:#00f">then</span> sprintf <span style="color:#a31515">&#34;%s_id&#34;</span> name <span style="color:#00f">else</span> id
            Task.FromResult(<span style="color:#00f">struct</span> (id, outputs :&gt; <span style="color:#2b91af">obj</span>))

        <span style="color:#00f">member</span> this.CallAsync(token: <span style="color:#2b91af">string</span>, inputs: ImmutableDictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">obj</span>&gt;, provider: <span style="color:#2b91af">string</span>): Task&lt;<span style="color:#2b91af">obj</span>&gt; =
            <span style="color:#008000">// We don&#39;t use this method in this particular test suite.
</span><span style="color:#008000"></span>            <span style="color:#008000">// Default to returning whatever we got as input.
</span><span style="color:#008000"></span>            Task.FromResult(<span style="color:#00f">null</span> :&gt; <span style="color:#2b91af">obj</span>)
</code></pre></div><p>Both methods receive several parameters and need to return a result. Notably, they get the list of <code>inputs</code>—the arguments that our program defined for the given resource. The goal of the mocks is to return the list of outputs, similarly to what a cloud provider would do.</p>
<p>My default implementation returns the outputs that include all the inputs and nothing else. That&rsquo;s a sensible default: usually, real outputs are a superset of inputs. We&rsquo;ll extend this behavior as we need later on.</p>
<h3 id="test-fixture">Test Fixture</h3>
<p>The next class to add is the container for my future unit tests. I named the file <code>WebsiteStackTests.fs</code> and it defines a test container, called a &ldquo;test fixture&rdquo; in NUnit:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">namespace</span> UnitTesting

<span style="color:#00f">open</span> System
<span style="color:#00f">open</span> System.Collections.Immutable
<span style="color:#00f">open</span> System.Threading.Tasks
<span style="color:#00f">open</span> NUnit.Framework
<span style="color:#00f">open</span> FluentAssertions
<span style="color:#00f">open</span> Pulumi
<span style="color:#00f">open</span> Pulumi.Azure.Core
<span style="color:#00f">open</span> Pulumi.Azure.Storage
<span style="color:#00f">open</span> Pulumi.Testing

[&lt;TestFixture&gt;]
<span style="color:#00f">type</span> <span style="color:#2b91af">WebserverStackTests</span>() =
    <span style="color:#00f">let</span> runTest(): ImmutableArray&lt;Resource&gt; =
        <span style="color:#00f">let</span> options = <span style="color:#00f">new</span> TestOptions(IsPreview = Nullable&lt;<span style="color:#2b91af">bool</span>&gt; <span style="color:#00f">false</span>)
        Deployment.TestAsync&lt;WebsiteStack&gt;(<span style="color:#00f">new</span> Mocks(), options)
        |&gt; Async.AwaitTask
        |&gt; Async.RunSynchronously

    <span style="color:#008000">// &lt;-- Tests go here
</span></code></pre></div><p>I defined a helper method <code>TestAsync</code> that points Pulumi&rsquo;s <code>Deployment.TestAsync</code> to our stack and mock classes.</p>
<p>As we progress through the article, I will add tests to this class one-by-one.</p>
<h3 id="retrieve-output-values">Retrieve output values</h3>
<p>Finally, I need a small extension method to extract values from outputs. Every Pulumi resource returns values wrapped inside an <code>Output&lt;T&gt;</code> container. While the real engine runs, those values may be known or unknown, but my mock-based tests always return known values. It&rsquo;s safe to get those values and convert them to a <code>Task&lt;T&gt;</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getValue(output: Output&lt;<span style="color:#00f">&#39;</span>a&gt;): <span style="color:#00f">&#39;</span>a =
    <span style="color:#00f">let</span> tcs = <span style="color:#00f">new</span> TaskCompletionSource&lt;<span style="color:#00f">&#39;</span>a&gt;()
    output.Apply(<span style="color:#00f">fun</span> v -&gt; tcs.SetResult(v); v) |&gt; ignore
    tcs.Task.Result
</code></pre></div><p>To learn more about outputs, read <a href="https://www.pulumi.com/docs/intro/concepts/programming-model/#stack-outputs">this article</a>.</p>
<h2 id="first-test">First Test</h2>
<p>The setup is done, time to write my first unit test! True to the TDD spirit, I write the tests before I define any resources.</p>
<p>Every Azure resource has to live inside a resource group, so my first test checks that a new resource group is defined.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;Test&gt;]
<span style="color:#00f">member</span> this.SingleResourceGroupExists() =
    <span style="color:#00f">let</span> resources = runTest()

    <span style="color:#00f">let</span> resourceGroupCount = resources.OfType&lt;ResourceGroup&gt;() |&gt; Seq.length
    resourceGroupCount.Should().Be(1, <span style="color:#a31515">&#34;a single resource group is expected&#34;</span>) |&gt; ignore
</code></pre></div><p>This code is great to illustrate the overall structure of each test:</p>
<ol>
<li>Call the <code>TestAsync</code> method to evaluate the stack.</li>
<li>Find the target resource in the collection of created resources.</li>
<li>Assert a property of the target resource.</li>
</ol>
<p>In this case, the test validates that there is one and only one resource group defined.</p>
<p>I can run <code>dotnet test</code> and see the expected test failure:</p>
<pre><code>$ dotnet test
...

X SingleResourceGroupExists [238ms]
  Error Message:
   Expected resourceGroups.Count to be 1 because a single resource group is expected, but found 0.
...
Total tests: 1
     Failed: 1
 Total time: 0.9270 Seconds
</code></pre><p>I go ahead and add the following definition to the <code>WebsiteStack</code> constructor.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> resourceGroup = <span style="color:#00f">new</span> ResourceGroup(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>)
</code></pre></div><p>This change is enough to make the tests green!</p>
<pre><code>$ dotnet test
...
Test Run Successful.
Total tests: 1
     Passed: 1
 Total time: 0.8462 Seconds
</code></pre><h2 id="tags">Tags</h2>
<p>For billing and lifecycle management, I want all my resource groups to be tagged. Therefore, my second test validates that the resource group has an <code>Environment</code> tag on it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;Test&gt;]
<span style="color:#00f">member</span> this.ResourceGroupHasEnvironmentTag() =
    <span style="color:#00f">let</span> resources = runTest()
    <span style="color:#00f">let</span> resourceGroup = resources.OfType&lt;ResourceGroup&gt;() |&gt; Seq.head

    <span style="color:#00f">let</span> tags = getValue resourceGroup.Tags
    tags.Should().NotBeNull(<span style="color:#a31515">&#34;Tags must be defined&#34;</span>) |&gt; ignore
    tags.Should().ContainKey(<span style="color:#a31515">&#34;Environment&#34;</span>, <span style="color:#00f">null</span>) |&gt; ignore
</code></pre></div><p>The test finds the resource group and then checks that the <code>Tags</code> dictionary is not null and contains a tag with the name <code>Environment</code>. Predictably, the test fails:</p>
<pre><code>$ dotnet test
...
  X ResourceGroupHasEnvironmentTag [240ms]
  Error Message:
   Expected tags not to be &lt;null&gt; because Tags must be defined.
...
Test Run Failed.
</code></pre><p>Now, I edit the stack class and change the resource group definition.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> resourceGroup =
    <span style="color:#00f">new</span> ResourceGroup(
        <span style="color:#a31515">&#34;www-prod-rg&#34;</span>,
        <span style="color:#00f">new</span> ResourceGroupArgs(Tags = inputMap([<span style="color:#a31515">&#34;Environment&#34;</span>, input <span style="color:#a31515">&#34;production&#34;</span>])))
</code></pre></div><p>This change makes the test suite green again, and we are good to proceed.</p>
<h2 id="storage-account">Storage Account</h2>
<p>It&rsquo;s time to add a test for a second resource in the stack: a Storage Account. To make things more interesting, I want to check that the storage account belongs to our resource group.</p>
<p>There&rsquo;s no direct link between a storage account object and a resource group object. Instead, we need to check the <code>ResourceGroupName</code> property of the account. The test below expects the account to belong to a resource group called <code>www-prod-rg</code>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;Test&gt;]
<span style="color:#00f">member</span> this.StorageAccountBelongsToResourceGroup() =
    <span style="color:#00f">let</span> resources = runTest()
    <span style="color:#00f">let</span> storageAccount = resources.OfType&lt;Account&gt;() |&gt; Seq.tryHead |&gt; Option.toObj
    storageAccount.Should().NotBeNull(<span style="color:#a31515">&#34;Storage account not found&#34;</span>) |&gt; ignore

    <span style="color:#00f">let</span> resourceGroupName = getValue storageAccount.ResourceGroupName
    resourceGroupName.Should().Be(<span style="color:#a31515">&#34;www-prod-rg&#34;</span>, <span style="color:#00f">null</span>) |&gt; ignore
</code></pre></div><p>Of course, the test fails. I try to fix it with what I think is the minimal change to make the test pass by adding a new resource to the stack and pointing it to the resource group.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> storageAccount =
    <span style="color:#00f">new</span> Account(
        <span style="color:#a31515">&#34;wwwprodsa&#34;</span>,
        <span style="color:#00f">new</span> AccountArgs(ResourceGroupName = io resourceGroup.Name))
</code></pre></div><p>However, when I rerun the test suite, all three tests fail. They complain about the missing required properties <code>AccountReplicationType</code> and <code>AccountTier</code> on the storage account, so I have to add those.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> storageAccount =
    <span style="color:#00f">new</span> Account(
        <span style="color:#a31515">&#34;wwwprodsa&#34;</span>,
        <span style="color:#00f">new</span> AccountArgs(
            ResourceGroupName = io resourceGroup.Name,
            AccountTier = input <span style="color:#a31515">&#34;Standard&#34;</span>,
            AccountReplicationType = input <span style="color:#a31515">&#34;LRS&#34;</span>,
            StaticWebsite = input (<span style="color:#00f">new</span> AccountStaticWebsiteArgs(IndexDocument = input <span style="color:#a31515">&#34;index.html&#34;</span>))))
</code></pre></div><p>I defined the <code>StaticWebsite</code> property as well: I&rsquo;ll leave testing these values as an exercise for the reader.</p>
<p>Two resource group tests are back to green again, but the storage account test fails with a new error message.</p>
<pre><code>X StorageAccountBelongsToResourceGroup [84ms]
  Error Message:
   Expected resourceGroupName to be &quot;www-prod-rg&quot;, but found &lt;null&gt;.
</code></pre><p>What&rsquo;s going on, and why is it <code>null</code>?</p>
<p>I defined the account name like this: <code>ResourceGroupName = resourceGroup.Name</code>. However, if we look closely, the <code>resourceGroup</code> resource doesn&rsquo;t have an input property <code>Name</code> defined. <code>www-prod-rg</code> is a logical name for Pulumi deployment, not the physical name of the resource.</p>
<p>Under normal circumstances, the Pulumi engine would use the logical name to produce the physical name of the resource group automatically (see <a href="https://www.pulumi.com/docs/intro/concepts/programming-model/#names">resource names</a> for details). However, my mocks don&rsquo;t do that.</p>
<p>That&rsquo;s a good reason to change the <code>Mocks</code> implementation. I add the following lines to the <code>NewResourceAsync</code> method.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> inputKVs = inputs |&gt; Seq.map(<span style="color:#00f">fun</span> kv -&gt; kv.Key, kv.Value) |&gt; Seq.toList
<span style="color:#00f">let</span> nameKVs = <span style="color:#00f">if</span> inputs.ContainsKey(<span style="color:#a31515">&#34;name&#34;</span>) <span style="color:#00f">then</span> [] <span style="color:#00f">else</span> [(<span style="color:#a31515">&#34;name&#34;</span>, name :&gt; <span style="color:#2b91af">obj</span>)]

<span style="color:#00f">let</span> outputs = [inputKVs; nameKVs] |&gt; Seq.concat |&gt; Seq.map KeyValuePair
<span style="color:#00f">let</span> dict = outputs.ToImmutableDictionary() :&gt; <span style="color:#2b91af">obj</span>
</code></pre></div><p>Note that mocks operate on weakly-typed dictionaries, so I need to get the property name right. Pulumi SDKs are open source, so I looked <a href="https://github.com/pulumi/pulumi-azure/blob/1fdcab88065175ced768d900e7dcedf3b1d1b0a7/sdk/dotnet/Core/ResourceGroup.cs#L90">here</a> to double-check the exact value.</p>
<p>After this change in <code>Mocks</code>, my tests go green again.</p>
<pre><code>Test Run Successful.
Total tests: 3
     Passed: 3
</code></pre><h2 id="website-files">Website Files</h2>
<p>The next step is to upload some files to the static website. Well, instead, to write an automated test that validates the upload with mocks.</p>
<p>I create a <code>wwwroot</code> folder with two HTML files in it and copy the folder to the build&rsquo;s output. Here is my change in the project file.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">&lt;ItemGroup&gt;
    &lt;None Update=<span style="color:#a31515">&#34;wwwroot\**\*.*&#34;</span>&gt;
        &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/None&gt;
&lt;/ItemGroup&gt;
</code></pre></div><p>Now, the test is straightforward: it expects two <code>Blob</code> resources in the stack.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;Test&gt;]
<span style="color:#00f">member</span> this.UploadsTwoFiles() =
    <span style="color:#00f">let</span> resources = runTest()
    <span style="color:#00f">let</span> filesCount = resources.OfType&lt;Blob&gt;() |&gt; Seq.length
    filesCount.Should().Be(2, <span style="color:#a31515">&#34;Should have uploaded files from `wwwroot`&#34;</span>) |&gt; ignore
</code></pre></div><p>As intended, the test fails immediately.</p>
<pre><code>X UploadsTwoFiles [95ms]
  Error Message:
   Expected files.Count to be 2 because Should have uploaded files from `wwwroot`, but found 0.
</code></pre><p>I extend my stack with the following loop that navigates through the files and creates a storage blob for each of them.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> files =
    <span style="color:#00f">let</span> fileNames = Directory.GetFiles(<span style="color:#a31515">&#34;wwwroot&#34;</span>)
    fileNames
    |&gt; Seq.map(<span style="color:#00f">fun</span> file -&gt;
        <span style="color:#00f">new</span> Blob(
            file,
            <span style="color:#00f">new</span> BlobArgs(
                ContentType = input <span style="color:#a31515">&#34;application/html&#34;</span>,
                Source = input (<span style="color:#00f">new</span> FileAsset(file) :&gt; AssetOrArchive),
                StorageAccountName = io storageAccount.Name,
                StorageContainerName = input <span style="color:#a31515">&#34;$web&#34;</span>,
                Type = input <span style="color:#a31515">&#34;Block&#34;</span>)))
    |&gt; List.ofSeq
</code></pre></div><p>However, this change breaks the stack and all tests in the suite:</p>
<pre><code>Error Message:
   Pulumi.RunException : Running program failed with an unhandled exception:
System.InvalidOperationException: Unsupported value when converting to protobuf: Pulumi.FileAsset
   at Pulumi.Serialization.Serializer.CreateValue(Object value)
...
</code></pre><p>Once again, our mocks fail to represent the behavior of the engine accurately. The Pulumi engine knows about the <code>FileAsset</code> class pointing to a file on the disk and how to convert it to an uploaded blob. But, the engine doesn&rsquo;t copy this property to outputs. I need to adjust the mocks again.</p>
<p>I&rsquo;m not particularly interested in testing the binary contents of the files now, so I&rsquo;ll change the <code>Mocks</code> class to ignore the <code>source</code> property and not to include it into the output dictionary.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> outputs =
    [inputKVs; nameKVs]
    |&gt; Seq.concat
    |&gt; Seq.filter (<span style="color:#00f">fun</span> (k, _) -&gt; typeName &lt;&gt; <span style="color:#a31515">&#34;azure:storage/blob:Blob&#34;</span> || k &lt;&gt; <span style="color:#a31515">&#34;source&#34;</span>)
    |&gt; Seq.map KeyValuePair
</code></pre></div><p>This change makes my test suite happy again.</p>
<pre><code>Test Run Successful.
Total tests: 4
     Passed: 4
</code></pre><h2 id="validate-website-url">Validate Website URL</h2>
<p>So far, I&rsquo;ve been validating the input parameters of resources. What if I want to test something that is usually done by the cloud provider?</p>
<p>For instance, when Azure creates a static website, it automatically assigns a public endpoint. The endpoint is then available in the <code>PrimaryWebEndpoint</code> property after the Pulumi program ran and resources are created. I may want to export this value from stack outputs and validate it in a unit test.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;Test&gt;]
<span style="color:#00f">member</span> this.StackExportsWebsiteUrl() =
    <span style="color:#00f">let</span> resources = runTest()
    <span style="color:#00f">let</span> stack = resources.OfType&lt;WebsiteStack&gt;() |&gt; Seq.head

    <span style="color:#00f">let</span> endpoint = getValue stack.Endpoint
    endpoint.Should().Be(<span style="color:#a31515">&#34;https://wwwprodsa.web.core.windows.net&#34;</span>, <span style="color:#00f">null</span>) |&gt; ignore
</code></pre></div><p>This test doesn&rsquo;t even compile yet, so I have to define the <code>Endpoint</code> property and assign a value to it. Here is the complete implementation of the stack with the output property defined at the end.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">WebsiteStack</span>() =
    <span style="color:#00f">inherit</span> Stack()

    <span style="color:#00f">let</span> resourceGroup =
        <span style="color:#00f">new</span> ResourceGroup(
            <span style="color:#a31515">&#34;www-prod-rg&#34;</span>,
            <span style="color:#00f">new</span> ResourceGroupArgs(Tags = inputMap([<span style="color:#a31515">&#34;Environment&#34;</span>, input <span style="color:#a31515">&#34;production&#34;</span>])))

    <span style="color:#00f">let</span> storageAccount =
        <span style="color:#00f">new</span> Account(
            <span style="color:#a31515">&#34;wwwprodsa&#34;</span>,
            <span style="color:#00f">new</span> AccountArgs(
                ResourceGroupName = io resourceGroup.Name,
                AccountTier = input <span style="color:#a31515">&#34;Standard&#34;</span>,
                AccountReplicationType = input <span style="color:#a31515">&#34;LRS&#34;</span>,
                StaticWebsite = input (<span style="color:#00f">new</span> AccountStaticWebsiteArgs(IndexDocument = input <span style="color:#a31515">&#34;index.html&#34;</span>))))

    <span style="color:#00f">let</span> files =
        <span style="color:#00f">let</span> fileNames = Directory.GetFiles(<span style="color:#a31515">&#34;wwwroot&#34;</span>)
        fileNames
        |&gt; Seq.map(<span style="color:#00f">fun</span> file -&gt;
            <span style="color:#00f">new</span> Blob(
                file,
                <span style="color:#00f">new</span> BlobArgs(
                    ContentType = input <span style="color:#a31515">&#34;application/html&#34;</span>,
                    Source = input (<span style="color:#00f">new</span> FileAsset(file) :&gt; AssetOrArchive),
                    StorageAccountName = io storageAccount.Name,
                    StorageContainerName = input <span style="color:#a31515">&#34;$web&#34;</span>,
                    Type = input <span style="color:#a31515">&#34;Block&#34;</span>)))
        |&gt; List.ofSeq

    [&lt;Output&gt;]
    <span style="color:#00f">member</span> <span style="color:#00f">val</span> Endpoint = storageAccount.PrimaryWebEndpoint <span style="color:#00f">with</span> get, set
</code></pre></div><p>Now, the test compiles but fails when executed:</p>
<pre><code>X StackExportsWebsiteUrl [85ms]
  Error Message:
   Expected endpoint to be &quot;https://wwwprodsa.web.core.windows.net&quot;, but found &lt;null&gt;.
</code></pre><p>That&rsquo;s because there is no call to Azure to populate the endpoint. It&rsquo;s time to make the last change to my <code>Mocks</code> class to emulate that assignment.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> endpointKVs =
    <span style="color:#00f">if</span> typeName = <span style="color:#a31515">&#34;azure:storage/account:Account&#34;</span>
    <span style="color:#00f">then</span> [<span style="color:#a31515">&#34;primaryWebEndpoint&#34;</span>, sprintf <span style="color:#a31515">&#34;https://%s.web.core.windows.net&#34;</span> name :&gt; <span style="color:#2b91af">obj</span>]
    <span style="color:#00f">else</span> []

<span style="color:#00f">let</span> outputs =
    [inputKVs; nameKVs; endpointKVs]
<span style="color:#008000">// ...
</span></code></pre></div><p>And that&rsquo;s it! My static website is ready and tested!</p>
<pre><code>Test Run Successful.
Total tests: 5
     Passed: 5
 Total time: 0.9338 Seconds
</code></pre><p>Note that it still takes less than a second to run my test suite so that I can iterate very quickly.</p>
<h2 id="get-started">Get Started</h2>
<p>The tests above cover the basics of unit testing with Pulumi .NET SDK. You can take it from here and apply the techniques and practices that you use while testing the application code. You may also try more advanced practices like property-based testing or behavior-driven development—we believe that the mocks enable many testing styles.</p>
<p>Here are several useful pointers to get started with testing in Pulumi:</p>
<ul>
<li><a href="https://www.pulumi.com/docs/guides/testing/">Testing Guide</a>.</li>
<li><a href="https://github.com/pulumi/examples/tree/72c9480f4c1240f795f6020f50801733fbef37f2/testing-unit-fs-mocks">Full code for this blog post</a>.</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/unit-testing" term="unit-testing" label="Unit Testing" />
                             
                                <category scheme="https://mikhail.io/tags/tdd" term="tdd" label="TDD" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless in the Wild: Azure Functions Production Usage Statistics]]></title>
            <link href="https://mikhail.io/2020/05/serverless-in-the-wild-azure-functions-usage-stats/"/>
            <id>https://mikhail.io/2020/05/serverless-in-the-wild-azure-functions-usage-stats/</id>
            
            <published>2020-05-05T00:00:00+00:00</published>
            <updated>2020-05-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Insightful statistics about the actual production usage of Azure Functions, based on the data from Microsoft&rsquo;s paper</blockquote><p>Microsoft Azure and Microsoft Research <a href="https://arxiv.org/pdf/2003.03423.pdf">released</a> a paper called &quot;Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider&quot;. In part 1 of the paper, they revealed some insightful statistics about the actual production usage of Azure Functions for two weeks in summer 2019.</p>

<p>This blog post is my summary and highlights of the most intriguing data points. I'll cover part 2 of the paper, focusing on cold starts, in a separate installment.</p>

<h2 id="trigger-types-and-invocations">Trigger Types and Invocations</h2>

<p>Every Azure Function has a trigger: it's linked to a specific event source, and every event in that source causes the Function to execute. Event sources include HTTP endpoints, timers, message queues, topics, event logs, and others. Which triggers are used more often?</p>

<p>The paper doesn't have a precise breakdown by specific sources like Service Bus or Event Grid, but it gives relative numbers per event category. It also shows the percentage of Function invocations that each type is responsible for.</p>

<p>Here is the comparison table:</p>

<table class="pure-table table table-striped">
<thead>
<tr>
<th>Trigger type</th>
<th>% of Functions</th>
<th>% of Invocations</th>
</tr>
</thead>

<tbody>
<tr>
<td>HTTP</td>
<td>55%</td>
<td>36%</td>
</tr>

<tr>
<td>Queue</td>
<td>15%</td>
<td>34%</td>
</tr>

<tr>
<td>Event</td>
<td>2%</td>
<td>25%</td>
</tr>

<tr>
<td>Timer</td>
<td>16%</td>
<td>2%</td>
</tr>

<tr>
<td>Orchestration</td>
<td>7%</td>
<td>2%</td>
</tr>

<tr>
<td>Others</td>
<td>5%</td>
<td>1%</td>
</tr>
</tbody>
</table>

<p>Notable facts:</p>

<ul>
<li>More than half of Functions are triggered by HTTP.</li>
<li>Asynchronous queue and, especially, event functions have much more invocations on average than any other Function type.</li>
<li>The opposite is accurate for timer Functions: a relatively high number of Functions translates to a fraction of invocations.</li>
<li>Durable functions are quite popular if that's what the authors mean by &quot;orchestration&quot; triggers.</li>
</ul>

<h2 id="functions-and-applications">Functions and Applications</h2>

<p>Function App is the deployment unit of Azure Functions. Each Function App may contain one or more Functions packaged together. How often do people use multiple Functions, and what for?</p>

<table class="pure-table table table-striped">
<thead>
<tr>
<th># of Functions</th>
<th>Percentage</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>54%</td>
</tr>

<tr>
<td>2</td>
<td>16%</td>
</tr>

<tr>
<td>3</td>
<td>10%</td>
</tr>

<tr>
<td>4 to 5</td>
<td>8%</td>
</tr>

<tr>
<td>6 to 10</td>
<td>7%</td>
</tr>

<tr>
<td>11+</td>
<td>5%</td>
</tr>

<tr>
<td>(100+)</td>
<td>0.04%</td>
</tr>

<tr>
<td>(2000+)</td>
<td>(a couple)</td>
</tr>
</tbody>
</table>

<p>Apparently, multi-function applications are widespread! The number of functions-per-app is all over the spectrum, with some wild usage scenarios of hundreds or even thousands of Functions in the same Function App.</p>

<p>Combining this with trigger types, it seems hard to derive any common patterns of trigger combinations in the same app. The paper lists several common ones, but none of them has more than 5% of usage. For instance, only 4.5% of applications are exactly one HTTP trigger and one timer trigger&mdash;which I could potentially attribute to the pattern of warming HTTP Functions against <a href="/serverless/coldstarts">cold starts</a>.</p>

<h2 id="invocation-patterns">Invocation Patterns</h2>

<p>How often are functions invoked? What are typical numbers for executions per day, minute, or second?</p>

<p>The variation in invocation frequency is tremendous. While many Functions may stay idle for days, others run many times per second. There's nothing unexpected here: it's free to create a Function App, so many apps exist for test purposes, one-off experiments, or to handle infrequent timers or automation tasks. At the same time, some production Functions would serve intensive workloads via HTTP, queues, or event buses.</p>

<p>The paper shows actual numbers for frequency distribution:</p>

<ul>
<li><strong>45%</strong> of Function Apps are invoked at most <strong>once per hour</strong>.</li>
<li>Another <strong>36%</strong> are invoked at most <strong>once per minute</strong>.</li>
<li>The remaining <strong>19%</strong> of more frequently invoked Functions represent more than 99.5% of all invocations.</li>
<li>Only about <strong>3%</strong> of applications are invoked more often than <strong>once a second</strong>.</li>
</ul>

<p>I'm surprised that the vast majority of Functions run, on average, very infrequently. It looks like more than 97% of Function Apps don't require any scalability beyond scaling from zero to a single instance and back.</p>

<p>The following chart shows the cumulative number of invocations that Azure Functions handle across all applications. Unfortunately, there are no absolute numbers&mdash;the chart is normalized to the peak.</p>

<figure >
    
        <img src="invocations.png"
            alt="Total invocations per hour for all Azure Functions, relative to the peak"
             />
        
    
    <figcaption>
        <h4>Total invocations per hour for all Azure Functions, relative to the peak</h4>
    </figcaption>
    
</figure>

<p>There are apparent repeatable patterns and a constant baseline of roughly 50% of the invocations. The platform needs to handle about 2x scalability through a given day.</p>

<p>Overall, the macro-scalability of the platform seems to be less of a challenge compared to optimizing the lifecycle of each individual Function App. Across applications, the number of invocations per day varies by 8 orders of magnitude, making the resources the provider has to dedicate to each application also highly variable.</p>

<p>Since the majority of applications are mostly idle, any inefficiency in handling them, being significant relative to their total execution (billable) time, can be prohibitively expensive. And that's a hard problem to solve!</p>

<h2 id="function-execution-times">Function Execution Times</h2>

<p>Once a Function is triggered, for how long will it typically run until completed?</p>

<p>Predictably, executions in Function-as-a-Service are very short compared to other cloud workloads. However, again, there are several orders of magnitude difference across different Functions:</p>

<ul>
<li>About <strong>20%</strong> of Functions would complete, on average, in less than <strong>100 ms</strong>.</li>
<li>Approximately <strong>50%</strong> of the Functions execute for less than <strong>1 second</strong> on average.</li>
<li>Also, <strong>50%</strong> of the Functions have <strong>maximum</strong> execution time shorter than <strong>3 seconds</strong>.</li>
<li>Yet, the slowest <strong>10%</strong> of the Functions have maximum execution time more than <strong>1 minute</strong>, and <strong>4% take more than a minute on average</strong>.</li>
</ul>

<p>The most common durations seem to be between 100 ms and several seconds, which is not surprising.</p>

<h2 id="conclusion">Conclusion</h2>

<p>It's incredible how much of a variation exists in real-world workloads running in Azure Functions. Across all the metrics above, there never seem to be one or two dominating scenarios. It's impressive that a single service can decently handle all the variability.</p>

<p>Kudos to folks at Microsoft Azure for releasing the data and analysis for their actual production workloads, albeit just for two weeks and without absolute numbers.</p>

<p>The second part of the paper uses this data to analyze potential improvements in cold starts of Azure Functions. And that's an excellent topic for a follow-up blog post, so stay tuned!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/paper-review" term="paper-review" label="Paper review" />
                            
                        
                    
                
            
        </entry>
    
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[VPC]]></title>
            <link href="https://mikhail.io/tags/vpc/"/>
            <id>https://mikhail.io/tags/vpc/</id>
            
            <published>2020-04-26T00:00:00+00:00</published>
            <updated>2020-04-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[InfiniCache: Distributed Cache on Top of AWS Lambda (paper review)]]></title>
            <link href="https://mikhail.io/2020/03/infinicache-distributed-cache-on-aws-lambda/"/>
            <id>https://mikhail.io/2020/03/infinicache-distributed-cache-on-aws-lambda/</id>
            
            <published>2020-03-10T00:00:00+00:00</published>
            <updated>2020-03-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>My review of the paper &ldquo;InfiniCache: Exploiting Ephemeral Serverless Functions to Build a Cost-Effective Memory Cache&rdquo;</blockquote><p>&ldquo;InfiniCache: Exploiting Ephemeral Serverless Functions to Build a Cost-Effective Memory Cache&rdquo; by Ao Wang, et al. (<a href="https://www.usenix.org/conference/fast20/presentation/wang-ao">link</a>) is a recently published paper which describes a prototype of a serverless distributed caching system sitting atop AWS Lambda.</p>
<p>Most distributed caching solutions run on a cluster of VMs. The cost of such a cluster is mostly fixed and depends on required memory allocation, no matter how many requests it serves. InfiniCache aims to bring a serverless pay-per-request dynamic pricing model to the world of in-memory data caching.</p>
<p>While reading through the paper, I got fascinated by several techniques that they employed while implementing the system. Therefore, I decided to review the paper and highlight its most intriguing parts. I target my review at software and IT engineers working on cloud applications. I assume the reader has a basic understanding of serverless cloud functions (Function-as-a-Service, FaaS) and their traditional use cases.</p>
<p>Let&rsquo;s start with a problem that InfiniCache aims to solve.</p>
<h2 id="i-state-in-stateless-functions">I. State in Stateless Functions</h2>
<p>Serverless functions excel at running stateless workloads. FaaS services use ephemeral compute instances to grow and shrink the capacity to adjust to a variable request rate. New virtual machines or containers are assigned and recycled by the cloud provider at arbitrary moments.</p>
<p>Functions are incredibly compelling for low-load usage scenarios, among others. Executions are charged per actual duration, at 100 ms granularity. If your application only needs to run several times per day in a test environment, it costs you very close to zero.</p>
<p>Cloud providers try to reuse compute instances for multiple subsequent requests. The price of bootstrapping new instances is too high to pay for every request. A warm instance would keep pre-loaded files in memory between requests to minimize the overhead and <a href="/serverless/coldstarts">cold start</a> duration. The exact behavior is a black box and varies between region, AZ, and in time.</p>
<p>There seems to be a fundamental opportunity to exploit the keep-warm behavior further. Each function instance has up to several gigabytes of memory, while our executable cache might only consume several megabytes. Can we use this memory to persist useful state?</p>
<h3 id="in-memory-cache">In-memory Cache</h3>
<p>The most straightforward way to use this capacity is to store reusable data between requests. If we need a piece of data from external storage, and that same piece is needed again and again, we could store it after the first load in a global in-memory hashmap. Think of a simple read-through cache backed by an external persistent data store.</p>
<figure >
    
        <img src="in-memory.png"
            alt="In-memory cache in AWS Lambda in front of a persistent database"
             />
        
    
    <figcaption>
        <h4>In-memory cache in AWS Lambda in front of a persistent database</h4>
    </figcaption>
    
</figure>
<p>This approach is still quite limited in several ways:</p>
<ul>
<li>The total size of the cache is up to 3GB (max size of AWS Lambda), which might not be enough for a large number of objects or large objects.</li>
<li>Any parallel request hits a different instance of the same AWS Lambda. The second instance then has to load the data from the data store once again.</li>
</ul>
<figure >
    
        <img src="two-in-memory.png"
            alt="A parallel request hits the second instance with no cache available"
             />
        
    
    <figcaption>
        <h4>A parallel request hits the second instance with no cache available</h4>
    </figcaption>
    
</figure>
<p>Each instance has to store its copy of cached data, which reduces the overall efficiency. Also, cache invalidation might get tricky in this case, particularly because each instance has its own, highly unpredictable lifetime.</p>
<h3 id="distributed-cache">Distributed Cache</h3>
<p>The previous scenario described a local cache co-located with data processing logic. Can we move the cache out of the currently invoked instance, distribute it among many other instances, and have multiple lambdas caching different portions of the data set? This move would help both with the cache size and duplication issues.</p>
<figure >
    
        <img src="distributed-cache.png"
            alt="Application loads data from a distributed cache of multiple AWS Lambdas"
             />
        
    
    <figcaption>
        <h4>Application loads data from a distributed cache of multiple AWS Lambdas</h4>
    </figcaption>
    
</figure>
<p>The idea is to create a Distributed Cache as a Service, with design goals inspired by the serverless mindset:</p>
<ul>
<li>Low management overhead</li>
<li>Scalability to hundreds or thousands of nodes</li>
<li>Pay per request</li>
</ul>
<p>The most significant potential upside is related to the cost structure. The cache capacity is billed only when a request needs an object. Such a pay-per-request model significantly differentiates against conventional cluster-based cache services, which typically charge for memory capacity on an hourly basis whether the cached objects are accessed or not.</p>
<h3 id="challenges">Challenges</h3>
<p>However, once again, serverless functions were not designed for stateful use cases. Utilizing the memory of cloud functions for object caching introduces non-trivial challenges due to the limitations and constraints of serverless computing platforms. The following list is specific to AWS Lambda, but other providers have very similar setups.</p>
<ul>
<li>Limited resource capacity is available: 1 or 2 CPU, up to several GB of memory, and limited network bandwidth.</li>
<li>Providers may reclaim a function and its memory at any time, creating a risk of loss of the cached data.</li>
<li>Each Lambda execution can run at most 900 seconds (15 minutes).</li>
<li>Strict network communication constraints are in place: Lambda only allows outbound network connections, meaning a Lambda function cannot be used to implement a server.</li>
<li>The lack of quality-of-service control. As a result, functions suffer from straggler issues, when response duration distributions have long tails.</li>
<li>Lambda functions run at EC2 Virtual Machines (VMs), where a single VM can host one or more functions. AWS provisions functions on the smallest possible number of VMs, which could cause severe network bandwidth contention for multiple network-intensive Lambda functions on the same host VM.</li>
<li>Executions are billed per 100 ms, even if a request only takes 5 or 10 ms.</li>
<li>No concurrent invocations are allowed, so the same instance can&rsquo;t handle multiple requests at the same time.</li>
<li>There is a non-trivial invocation latency on top of any response processing time.</li>
</ul>
<p>Honestly, that&rsquo;s an extensive list of pitfalls. Nonetheless, InfiniCache presents a careful combination of choosing the best use case, working around some issues, and hacking straight through the others.</p>
<h3 id="large-object-caching">Large Object Caching</h3>
<p>Traditional distributed cache products aren&rsquo;t great when it comes to caching large objects (megabytes):</p>
<ul>
<li>Large objects are accessed less often while consuming vast space in RAM.</li>
<li>They occupy memory and cause evictions of many small objects that might be reused shortly, thus hurting performance.</li>
<li>Requests to large objects consume significant network bandwidth, which may inevitably affect the latencies of small objects.</li>
</ul>
<p>Because of these reasons, it&rsquo;s common <strong>not</strong> to cache large objects and retrieve them from the backing storage like S3 directly.</p>
<p>What is missing is a truly elastic cloud storage service model that charges tenants in a request-driven mode instead of per capacity usage, which the emerging serverless computing naturally enables.</p>
<h2 id="ii-infinicache">II. InfiniCache</h2>
<p>The paper <a href="https://arxiv.org/pdf/2001.10483.pdf">InfiniCache: Exploiting Ephemeral Serverless Functions to Build a Cost-Effective Memory Cache</a> presents a prototype of a distributed cache service running on top of AWS Lambda. The service is aimed specifically at large object caching. It deploys a large fleet of Lambda functions, where each Lambda stores a slice of the total data pool.</p>
<blockquote>
<p>InfiniCache offers a virtually infinite (yet cheap) short-term capacity, which is advantageous for large object caching, since the tenants can invoke many cloud functions but have the provider pay the cost of function caching.</p>
</blockquote>
<p>The authors employ several smart techniques to work around the limitations that I described above and make the system performant and cost-efficient.</p>
<h3 id="multiple-lambda-functions">Multiple Lambda functions</h3>
<p>InfiniCache creates multiple AWS Lambdas, as opposed to multiple instances of the same Lambda. Each Lambda effectively runs on a single instance (container) that holds a slice of data. A second instance may be used for backups as explained below.</p>
<p>Therefore, there is no dynamic scaling: the cache cluster is pre-provisioned to the level of required capacity. This does not incur a high fixed cost, though: each Lambda is still charged per invocation, not for the number of Lambda services or deployments.</p>
<figure >
    
        <img src="lambda-cache.png"
            alt="AWS Lambdas as cache storage units"
             />
        
    
    <figcaption>
        <h4>AWS Lambdas as cache storage units</h4>
    </figcaption>
    
</figure>
<p>The described approach may sound very limiting. A single instance? Well, each instance can only handle a single request at a time, do the clients need to coordinate and wait for each other? How do the clients even know which Lambda to invoke to retrieve a piece of data?</p>
<p>That&rsquo;s why InfiniCache has an extra component called <strong>a proxy</strong>.</p>
<h3 id="proxy">Proxy</h3>
<p>A proxy is a static always-up well-known component deployed on a large EC2 virtual machine. A proxy has a stable endpoint and becomes the entry point for all end clients. It then talks to Lambdas via an internal protocol, as I explain below.</p>
<figure >
    
        <img src="proxy.png"
            alt="A client makes requests to the proxy which connects to a Lambda"
             />
        
    
    <figcaption>
        <h4>A client makes requests to the proxy which connects to a Lambda</h4>
    </figcaption>
    
</figure>
<p>It&rsquo;s now the responsibility of the proxy to distribute blobs between Lambdas and persist their addresses between the requests.</p>
<p>There may be multiple proxies for large deployments. InfiniCache&rsquo;s client library determines the destination proxy (and therefore its backing Lambda pool) by using a consistent hashing-based load balancing approach.</p>
<p>On the other side, for smaller deployments, the proxy could be placed directly on the client machine to save cost and an extra network hop.</p>
<h3 id="backward-connections">Backward Connections</h3>
<p>Firing a new Lambda invocation for each cache request proves to be inefficient: You pay for the minimum of 100 ms and block the entire instance for the single execution. Therefore, InfiniCache authors use a sophisticated hand-crafted model with two channels for proxy-to-lambda communication.</p>
<p>Traditional Lambda invocations are used only for activation and billed duration control. The same invocation is potentially reused to serve multiple cache requests via a separate TCP channel.</p>
<p>Since AWS Lambda does  not  allow  inbound  TCP  or  UDP  connections,  each Lambda runtime establishes a TCP connection with its designated proxy server the first time it is invoked. A Lambda node gets its proxy’s connection information via the invocation parameters.  The Lambda runtime then keeps the TCP connection established until reclaimed by the provider.</p>
<figure >
    
        <img src="channels.png"
            alt="During an active invocation, Lambda establishes a TCP connection for multiplexed data transfer"
             />
        
    
    <figcaption>
        <h4>During an active invocation, Lambda establishes a TCP connection for multiplexed data transfer</h4>
    </figcaption>
    
</figure>
<p>Here is a somewhat simplified explanation of the protocol:</p>
<ol>
<li>At first, there is no active invocation of a given Lambda.</li>
<li>Whenever the proxy needs to retrieve a blob from a Lambda, it invokes the Lambda with a &ldquo;Ping&rdquo; message, passing the IP of the proxy in the request body.</li>
<li>The Lambda invocation is now activated, but it does NOT return yet. Instead, it establishes a TCP connection to the proxy IP and sends a &ldquo;Pong&rdquo; message over that second channel.</li>
<li>The proxy accepts the TCP connection. Now it can send requests for blob data over that connection. It sends the first request immediately.</li>
<li>The Lambda instance now serves multiple requests on the same TCP connection.</li>
<li>Every billing cycle, the Lambda instance decides whether to keep the invocation alive and keep serving requests over TCP, or to terminate the invocation to stop the billing.</li>
</ol>
<blockquote>
<p>To maximize the use of each billing cycle and to avoid the overhead of restarting Lambdas, InfiniCache&rsquo;s Lambda runtime uses a timeout scheme to control how long a Lambda function runs. When a Lambda node is invoked by a chunk request, a timer is triggered to limit the function’s execution time. The timeout is initially set to expire within the first billing cycle.</p>
</blockquote>
<blockquote>
<p>If no further chunk request arrives within the first billing cycle, the timer expires and returns 2–10 ms (a short time buffer) before the 100 ms window ends.
If more than one request can be served within the current billing cycle, the heuristic extends the timeout by one more billing cycle, anticipating more incoming requests.</p>
</blockquote>
<p>This clever workaround overcomes two limitations simultaneously: adjusting to the round-up pricing model, and ensuring that parallel requests can be served by the single Lambda instance, both reusing the same memory cache.</p>
<h3 id="instance-sizes">Instance sizes</h3>
<p>AWS Lambda functions can be provisioned at a memory limit between 128 MB and 3 GB. The allocated CPU cycles are proportional to the provisioned RAM size.</p>
<p>InfiniCache authors find larger instances beneficial. Even though they end up paying more for each invocation, they greatly benefit from improved performance, reduced latency variation, and lack of network contention.</p>
<blockquote>
<p>We find that using relatively bigger Lambda functions largely eliminates Lambda co-location. Lambda’s VM hosts have approximately 3 GB memory.  As such, if we use Lambda functions with ≥ 1.5GB memory, every VM host is occupied exclusively by a single Lambda function.</p>
</blockquote>
<h3 id="chunking-and-erasure-coding">Chunking and erasure coding</h3>
<p>To serve large files with minimal latency, InfiniCache breaks each file down into several chunks. The proxy stores each chunk on a separate AWS Lambda and requests them in parallel on retrieval. This protocol improves performance by utilizing the aggregated network bandwidth of multiple cloud functions in parallel.</p>
<p>On top of that, InfiniCache uses an approach called <strong>erasure coding</strong>. Redundant information is introduced into each chunk so that the full file could be recovered from a smaller subset of the pieces. For example, &ldquo;10+2&rdquo; encoding breaks each file into 12 chunks, but any 10 chunks are enough to restore the complete file. This redundancy helps fight long-tail latencies: the slowest responses can be ignored, so the stragglers have less effect on the overall latency of the system.</p>
<p>The paper compares several erasure coding schemes (&ldquo;10+0&rdquo;, &ldquo;10+1&rdquo;, &ldquo;10+2&rdquo;, etc.) and finds that 10+1—requiring 10 any chunks out of 11—provides the best trade-off between response latency and computational overhead of encoding.</p>
<p>Predictably, the non-redundant &ldquo;10+0&rdquo; encoding suffers from Lambda straggler issues, which outweighs the performance gained by the elimination of the decoding overhead.</p>
<p>Interestingly, breaking down a file into chunks becomes the responsibility of the client library. The encoding is too computation-heavy to be executed in the proxy.</p>
<h3 id="warm-ups-and-backup">Warm-ups and backup</h3>
<p>Amazon can reclaim any inactive instance of an AWS Lambda at any time. It&rsquo;s not possible to avoid this altogether, but InfiniCache does three things to mitigate the issue and keep the data around for longer:</p>
<ul>
<li>
<p>They issue periodic <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">warm-up</a> requests to each Lambda to prolong the instance lifespan;</p>
</li>
<li>
<p>The redundancy of erasure coding helps recover lost chunks of a single object;</p>
</li>
<li>
<p>Each cloud function periodically performs data synchronization with a clone of itself to minimize the chances that a reclaimed function causes a data loss.</p>
</li>
</ul>
<p>The last point is referred to as “backup” and is uniquely elegant, in my opinion. How does one backup an instance of AWS Lambda? They make the Lambda invoke itself! Because such invocation can’t land on the same instance (the instance is busy calling), the second instance of the same Lambda is going to be created. The two instances can now sync the data, which happens over TCP through a relay co-located with the proxy.</p>
<figure >
    
        <img src="backup.png"
            alt="AWS Lambda creates the second execution of itself and syncs data between the two executions"
             />
        
    
    <figcaption>
        <h4>AWS Lambda creates the second execution of itself and syncs data between the two executions</h4>
    </figcaption>
    
</figure>
<p>If AWS decides to reclaim the primary instance, the next invocation naturally lands on the secondary instance, which already has the data in memory.</p>
<figure >
    
        <img src="failover.png"
            alt="AWS reclaims the primary instance which automatically promotes the secondary instance with hot data"
             />
        
    
    <figcaption>
        <h4>AWS reclaims the primary instance which automatically promotes the secondary instance with hot data</h4>
    </figcaption>
    
</figure>
<p>The promoted instance can now start another replication to create a backup of its own.</p>
<p>Both warm-ups and backups largely contribute to the total cost of the solution, but the authors find this cost justified in terms of improved cache hit rates.</p>
<h2 id="iii-practical-evaluation">III. Practical Evaluation</h2>
<p>The authors implemented a prototype of InfiniCache and benchmarked it against several synthetic tests and a real-life workload of a production Docker registry. Below are some key numbers from their real-life workload comparison against an ElastiCache cluster (a Redis service managed by AWS). Mind that the workload was selected to be a good match for InfiniCache properties (large objects with infrequent access).</p>
<h3 id="cost">Cost</h3>
<p>InfiniCache is one to two orders of magnitude cheaper than ElastiCache (managed Redis cluster) on some workloads:</p>
<blockquote>
<p>By the end of hour 50, ElastiCache costs $518.4, while InfiniCache with all objects costs $20.52. Caching only large objects bigger than 10 MB leads to a cost of $16.51 for InfiniCache. InfiniCache pay-per-use serverless substrate effectively brings down the total cost by 96.8% with a cost effectiveness improvement of 31x. By disabling the backup option, InfiniCache further lowers down the cost to $5.41, which is 96x cheaper than ElastiCache.</p>
</blockquote>
<p>However, they find that serverless computing platforms are not cost-effective for small-object caching with frequent access.</p>
<blockquote>
<p>The hourly cost increases monotonically with the access rate, and eventually overshoots ElastiCache when the access rate exceeds 312 K requests per hour (86 requests per second).</p>
</blockquote>
<p>Another gotcha is that the cost above does not include the price of a proxy: the proxy was co-located with the single client VM. A dedicated proxy running on an EC2 instance would increase the total price considerably.</p>
<h3 id="performance">Performance</h3>
<p>Here are the key conclusions based on a production-like test of InfiniCache:</p>
<ul>
<li>
<p>Compared to S3, InfiniCache achieves superior performance improvement for large objects: it&rsquo;s at least 100x for about 60% of all large requests. This trend demonstrates the efficacy of the idea of using a distributed in-memory cache in front of a cloud object store.</p>
</li>
<li>
<p>InfiniCache is particularly good at optimizing latencies for large objects. It is approximately on-par with ElastiCache for objects sizing from 1–100 MB, but InfiniCache achieves consistently lower latencies for objects larger than 100 MB, due to I/O parallelism. However, I suppose chunking could improve the results of ElastiCache too.</p>
</li>
<li>
<p>InfiniCache incurs significant overhead for objects smaller than 1 MB, since fetching an object often requires to invoke Lambda functions, which takes on average 13 ms and is much slower than  directly fetching a small object from ElastiCache.</p>
</li>
</ul>
<h3 id="availability">Availability</h3>
<p>Hit/miss ratio is an essential metric of any cache. A cache miss is usually a result of an object loss when all the replicas of several chunks are gone.</p>
<p>For the large object workload, a production-like test resulted in the availability of 95.4%. InfiniCache without backups sees a substantially lower availability of just 81.4%.</p>
<h2 id="iv-future-directions">IV. Future Directions</h2>
<p>Can a system like InfiniCache be productized and used for production workloads?</p>
<h3 id="service-providers-policy-changes">Service provider&rsquo;s policy changes</h3>
<p>InfiniCache uses several clever tricks to exploit the properties of the existing AWS Lambda service as it works today. However, the behavior of Lambda instances is broadly a black box and changes over time. Even small behavior changes may potentially have a massive impact on the properties of InfiniCache-like systems.</p>
<p>Even worse, service  providers may change their internal implementations and policies <em>in response</em> to systems like InfiniCache. Ideally, any production system would rely on explicit product features of serverless functions rather than implicitly observed characteristics.</p>
<h3 id="explicit-provisioning">Explicit provisioning</h3>
<p>AWS Lambda has recently launched a new feature called <a href="https://mikhail.io/2019/12/aws-lambda-provisioned-concurrency-no-cold-starts/">provisioned concurrency</a>, that allows pinning warm Lambda functions in memory for a fixed hourly fee. Provisioned Lambdas may still get reclaimed and re-initialized periodically, but the reclamation frequency is low compared to non-provisioned Lambdas.</p>
<p>The pay-per-hour pricing component is quite significant, which brings the cost closer to EC2 VMs' pricing model. Nonetheless, it opens up research opportunities for hybrid serverless-oriented cloud economics with higher predictability.</p>
<h3 id="internal-implementation-by-a-cloud-provider">Internal implementation by a cloud provider</h3>
<p>Finally, instead of fighting with third-party implementations of stateful systems on top of dynamic function pools, cloud providers could potentially leverage such techniques themselves. A new cloud service could provide short-term caching for data-intensive applications such as big data analytics.</p>
<p>Having the complete picture, datacenter operators would operate a white-box solution and could use the knowledge to optimize data availability and locality. I look forward to new storage products that can more efficiently utilize ephemeral datacenter resources.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/paper-review" term="paper-review" label="Paper review" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cloud Run]]></title>
            <link href="https://mikhail.io/tags/cloud-run/"/>
            <id>https://mikhail.io/tags/cloud-run/</id>
            
            <published>2020-02-14T00:00:00+00:00</published>
            <updated>2020-02-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Hosting Azure Functions in Google Cloud Run]]></title>
            <link href="https://mikhail.io/2020/02/azure-functions-in-google-cloud-run/"/>
            <id>https://mikhail.io/2020/02/azure-functions-in-google-cloud-run/</id>
            
            <published>2020-02-14T00:00:00+00:00</published>
            <updated>2020-02-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Running Azure Functions Docker container inside Google Cloud Run managed service</blockquote><p>Suppose you are a .NET developer, you love the Function-as-a-Service (FaaS) model, but you want to run your serverless functions in Google Cloud. You want to keep using C# or F#, and still leverage all serverless benefits—ease of use, scalability, elasticity, pay-per-value cost model—running in GCP.</p>
<p>You look at Google Cloud Functions, the native FaaS service of Google Cloud, but it only supports JavaScript, Python, and Go. No C# or F#. Time to give up on the plans?</p>
<p>Not so quickly!</p>
<h2 id="google-azure-functions-wat">Google Azure Functions?! Wat?</h2>
<p>You are probably already familiar with Azure Functions—the .NET-based FaaS runtime in Azure. Azure Functions has two faces: it&rsquo;s a managed service in the Azure cloud, and also it is a self-contained runtime that can run anywhere: on your local machine, in a VM, or in a container. As I&rsquo;m going to demonstrate, it can run in Google Cloud too.</p>
<p><img src="teaser.png" alt="Azure Functions Google Cloud Run"></p>
<p>Now, if I simply run an Azure Functions host on a VM in Google Cloud, I don&rsquo;t get all the serverless properties like scalability and pay-for-value. This is where Google Cloud Run comes into the mix.</p>
<p>Cloud Run is a fully managed cloud service that takes a container image and deploys it as an elastic HTTP application with scaling from zero to hero and applying per-request pricing. Cloud Run can host pretty much any container that listens to HTTP requests at a given port.</p>
<p>At the same time, Microsoft provides an official image of Azure Functions host. The host is a web application listening for HTTP requests&hellip; It sounds like we can stick it into Cloud Run!</p>
<h2 id="deploying-azure-functions-to-cloud-run">Deploying Azure Functions to Cloud Run</h2>
<p>My plan has three steps:</p>
<ol>
<li>Develop the code for an Azure Function App</li>
<li>Pack it as a Docker image that would fit the requirements of Google Cloud Run</li>
<li>Deploy the image as a Cloud Run service</li>
</ol>
<h3 id="net-azure-function">.NET Azure Function</h3>
<p>Use your favorite tool to create a new Function App. I employed <code>func</code> CLI to create a new Function App project in C# and define an HTTP Function &ldquo;HttpExample&rdquo;.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;HttpExample&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;IActionResult&gt; Run(
    [HttpTrigger(AuthorizationLevel.Anonymous, &#34;get&#34;)] HttpRequest req)
{
    <span style="color:#2b91af">string</span> name = (<span style="color:#2b91af">string</span>)req.Query[<span style="color:#a31515">&#34;name&#34;</span>] ?? <span style="color:#a31515">&#34;World&#34;</span>;
    <span style="color:#2b91af">var</span> service = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;K_SERVICE&#34;</span>) ?? <span style="color:#a31515">&#34;&lt;unknown&gt;&#34;</span>;
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> OkObjectResult(<span style="color:#a31515">$&#34;Hello from Azure Function in {service}, {name}&#34;</span>);
}
</code></pre></div><p>It&rsquo;s a standard hello-world Function, except it also retrieves the value of the <code>K_SERVICE</code> environment variable and appends it to the response. This variable should be present when hosted in Cloud Run.</p>
<h3 id="container-image">Container image</h3>
<p>Now, we can wrap the Function App into a Docker image. Here is my <code>Dockerfile</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#00f">FROM</span><span style="color:#a31515"> mcr.microsoft.com/dotnet/core/sdk:3.0 AS installer-env</span><span style="">
</span><span style="">
</span><span style=""></span><span style="color:#00f">COPY</span> . /src/dotnet-function-app<span style="">
</span><span style=""></span><span style="color:#00f">RUN</span> cd /src/dotnet-function-app &amp;&amp; <span style="color:#a31515">\
</span><span style="color:#a31515"></span>    mkdir -p /home/site/wwwroot &amp;&amp; <span style="color:#a31515">\
</span><span style="color:#a31515"></span>    dotnet publish *.csproj --output /home/site/wwwroot<span style="">
</span><span style="">
</span><span style=""></span><span style="color:#00f">FROM</span><span style="color:#a31515"> mcr.microsoft.com/azure-functions/dotnet:3.0</span><span style="">
</span><span style=""></span><span style="color:#00f">ENV</span> AzureWebJobsScriptRoot=/home/site/wwwroot <span style="color:#a31515">\
</span><span style="color:#a31515"></span>    AzureFunctionsJobHost__Logging__Console__IsEnabled=true <span style="color:#a31515">\
</span><span style="color:#a31515"></span>    ASPNETCORE_URLS=http://+:8080<span style="">
</span><span style="">
</span><span style=""></span><span style="color:#00f">COPY</span> --from=installer-env [<span style="color:#a31515">&#34;/home/site/wwwroot&#34;</span>, <span style="color:#a31515">&#34;/home/site/wwwroot&#34;</span>]<span style="">
</span></code></pre></div><p>It uses the .NET Core SDK to build the application and publish the binaries to the <code>wwwroot</code> folder. Then I use the official Azure Functions host image to run the application from <code>wwwroot</code>.</p>
<p>The only statement that I have to customize for Cloud Run is <code>ASPNETCORE_URLS=http://+:8080</code>. It instructs my application to listen on the port 8080: the one defined by the Cloud Run&rsquo;s <a href="https://cloud.google.com/run/docs/reference/container-contract">container contract</a>.</p>
<h3 id="deploy-to-cloud-run">Deploy to Cloud Run</h3>
<p>Finally, I can deploy the container definition to Google Cloud Run service. I use Pulumi to deploy all the infrastructure, see <a href="https://mikhail.io/2020/02/serverless-containers-with-google-cloud-run/">this post</a> for a detailed walkthrough.</p>
<p>You can find the complete code of Azure Function deployment to Cloud Run <a href="https://github.com/mikhailshilkov/mikhailio-hugo/tree/master/content/2020/02/azure-functions-in-google-cloud-run/code">here</a>. After running <code>pulumi up</code>, I get a URL back</p>
<pre><code>endpoint: &quot;https://cloudrun-functions-4f40772-q5zdxwsb2a-ew.a.run.app/api/HttpExample?name=&quot;
</code></pre><p>I can append my name, query the endpoint, and get the response back:</p>
<pre><code>curl $(pulumi stack output endpoint)Mikhail
Hello from Azure Function in cloudrun-functions-4f40772, Mikhail
</code></pre><p>It works! My Function confirms it&rsquo;s running in the <code>cloudrun-functions-4f40772</code> Cloud Run service.</p>
<h2 id="pros-and-cons">Pros and Cons</h2>
<p>I showed that it&rsquo;s possible to run an Azure Function App inside the managed Google Cloud Run service. Let&rsquo;s spend a moment to discuss the benefits and limitations of this approach.</p>
<h3 id="the-good-parts">The good parts</h3>
<p>You take full advantage of a serverless application model:</p>
<ul>
<li>Google operates Cloud Run and requires next to none management from your side.</li>
<li>The service automatically scales from zero to an arbitrary number of instances based on the actual workload.</li>
<li>You pay per request in chunks of 100 ms. An application that needs to handle few requests may stay below the free allowance.</li>
</ul>
<p>You write your application in a familiar language. I used .NET, it can be C#, F#, or VB, but the same approach should also work for other runtimes supported by Azure Functions, for example, JVM or PowerShell.</p>
<p>You can take advantage of many (but not all, see below) features of Azure Functions: HTTP routes, including parameters, authorization modes, logging, input and output bindings.</p>
<h3 id="not-so-great-parts">Not so great parts</h3>
<p>There is one substantial limitation to my approach, however. Cloud Run can only run HTTP-based workloads, so the set of Azure Function triggers available to you is basically limited to HTTP, EventGrid, and custom triggers based on HTTP. You can&rsquo;t deploy Functions listening to events like Service Bus, or Storage Queues. However, since your application runs in Google Cloud anyway, do you really need those? Google Pub/Sub has HTTP endpoint integration out of the box.</p>
<p>Azure Functions container is an ASP.NET Core application, and we ran it in Google Cloud Run. You may want to forgo the Azure Functions host and deploy your own custom ASP.NET Core application to Cloud Run. Both are possible, and the choice is yours.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Open source and open standards open (dfjokasj) new horizons of possibilities. The team behind Azure Functions provides a way to host Function App inside containers, and Google Cloud Run service enables running arbitrary containers in a serverless manner. Therefore, we can combine the two products to come up with the usage that nobody anticipated in advance.</p>
<p>Isn&rsquo;t that cool? Happy hacking, my cloud friends!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/containers" term="containers" label="Containers" />
                             
                                <category scheme="https://mikhail.io/tags/cloud-run" term="cloud-run" label="Cloud Run" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless Containers with Google Cloud Run]]></title>
            <link href="https://mikhail.io/2020/02/serverless-containers-with-google-cloud-run/"/>
            <id>https://mikhail.io/2020/02/serverless-containers-with-google-cloud-run/</id>
            
            <published>2020-02-04T00:00:00+00:00</published>
            <updated>2020-02-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Google Cloud Run is the latest addition to the serverless compute family. While it may look similar to existing services of public cloud, the feature set makes Cloud Run unique.</blockquote><p>Google <a href="https://cloud.google.com/run/">Cloud Run</a> is the latest addition to the serverless compute family. While it may look similar to existing services of public cloud, the feature set makes Cloud Run unique:</p>
<ul>
<li>Docker as a deployment package enables using any language, runtime, framework, or library that can respond to an HTTP request.</li>
<li>Automatic scaling, including scale to zero, means you pay for what you consume with no fixed cost and no management overhead.</li>
<li>HTTP load-balancing out of the box simplifies the usage.</li>
</ul>
<p>Cloud Run is targeted very specifically at stateless web applications. It uses ephemeral containers, and each execution is limited to 15 minutes.</p>
<p>Today, we will deploy our first Cloud Run services with Pulumi. Then, we&rsquo;ll discuss pricing and compare Cloud Run to the competition.</p>
<h2 id="hello-cloud-run">Hello Cloud Run</h2>
<p>We&rsquo;ll start by deploying a pre-built container image provided by Google to a Cloud Run service.</p>
<p>To follow along, create a new Pulumi project with the <a href="https://www.pulumi.com/docs/get-started/gcp/">Get Started with Google Cloud</a> guide.</p>
<h3 id="enable-cloud-run-for-the-project">Enable Cloud Run for the project</h3>
<p>As with any other service of Google Cloud, you need to enable it for the target project before the first deployment. You can do so with the <code>gcp.projects.Service</code> resource:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> enableCloudRun = <span style="color:#00f">new</span> gcp.projects.Service(<span style="color:#a31515">&#34;EnableCloudRun&#34;</span>, {
    service: <span style="color:#a31515">&#34;run.googleapis.com&#34;</span>,
});
</code></pre></div><p>Note: If you destroy the Pulumi stack and delete the <code>projects.Service</code> resource, the Cloud Run service will be disabled again. If you use Cloud Run in multiple Pulumi stacks, you should move the service management to a central location, i.e., a shared stack.</p>
<h3 id="choose-a-location">Choose a location</h3>
<p>Cloud Run is a regional service: all container instances run in a single location of your choice.</p>
<p>Set the region setting to one of the currently supported locations: <code>us-central1</code> (Iowa), <code>us-east1</code> (South Carolina), <code>europe-west1</code>( Belgium), or <code>asia-northeast1</code> (Tokyo).</p>
<pre><code>pulumi config set gcp:region &lt;region&gt;
</code></pre><p>Then, the program can read the value and reuse it for all resources:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Location to deploy Cloud Run services
</span><span style="color:#008000"></span><span style="color:#00f">const</span> location = gcp.config.region || <span style="color:#a31515">&#34;us-central1&#34;</span>;
</code></pre></div><h3 id="deploy-a-cloud-run-service">Deploy a Cloud Run service</h3>
<p>Google provides a pre-deployed &ldquo;Hello Cloud Run&rdquo; image at <code>gcr.io/cloudrun/hello</code>. The following resource deploys that image to your GCP project with the default settings.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> helloService = <span style="color:#00f">new</span> gcp.cloudrun.Service(<span style="color:#a31515">&#34;hello&#34;</span>, {
    location,
    template: {
        spec: {
            containers: [
                { image: <span style="color:#a31515">&#34;gcr.io/cloudrun/hello&#34;</span> },
            ],
        },
    },
}, { dependsOn: <span style="color:#2b91af">enableCloudRun</span> });
</code></pre></div><h3 id="expose-unrestricted-http-access">Expose unrestricted HTTP access</h3>
<p>By default, Google does not expose HTTP endpoints of a Cloud Run service to the Internet. To make it publicly available, you should grant the <code>roles/run.invoker</code> role to <code>allUsers</code>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> iamHello = <span style="color:#00f">new</span> gcp.cloudrun.IamMember(<span style="color:#a31515">&#34;hello-everyone&#34;</span>, {
    service: <span style="color:#2b91af">helloService.name</span>,
    location,
    role: <span style="color:#a31515">&#34;roles/run.invoker&#34;</span>,
    member: <span style="color:#a31515">&#34;allUsers&#34;</span>,
});
</code></pre></div><p>Note that the Contributor role is required for the user who executes Pulumi deployment to grant the access.</p>
<h3 id="try-it-out">Try it out</h3>
<p>Each Cloud Run service automatically gets a run.app subdomain. You can export the exact URL as a Pulumi output.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Export the URL
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">const</span> helloUrl = helloService.status.url;
</code></pre></div><p>Run <code>pulumi up</code> to deploy all the resources.</p>
<pre><code>$ pulumi up
...
Outputs:
  ~ helloUrl: &quot;https://hello-585ad15-q4wszdxb2a-ew.a.run.app&quot;
</code></pre><p>Navigate to the URL to see the welcome screen:</p>
<p><img src="./cloud-run-hello-running.png" alt="Cloud Run Hello World container running"></p>
<p>Congratulations, your first Cloud Run service is up and running. Now, it&rsquo;s time to deploy some custom code.</p>
<h2 id="deploy-a-custom-application">Deploy a Custom Application</h2>
<p>Cloud Run can run any Docker container so that you can write the application code in a language of your choice. I picked Ruby as an example of a language that is not supported by Cloud Functions.</p>
<h3 id="create-a-ruby-web-app">Create a Ruby web app</h3>
<p>Let&rsquo;s create an <code>app</code> subfolder and place all application files there. The first file app.rb is a Hello World web application in Ruby.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rb" data-lang="rb">require <span style="color:#a31515">&#39;sinatra&#39;</span>

set <span style="color:#a31515">:bind</span>, <span style="color:#a31515">&#39;0.0.0.0&#39;</span>

get <span style="color:#a31515">&#39;/&#39;</span> <span style="color:#00f">do</span>
  target = ENV[<span style="color:#a31515">&#39;TARGET&#39;</span>] || <span style="color:#a31515">&#39;Pulumi&#39;</span>
  <span style="color:#a31515">&#34;Hello </span><span style="color:#a31515">#{</span>target<span style="color:#a31515">}</span><span style="color:#a31515">!</span><span style="color:#a31515">\n</span><span style="color:#a31515">&#34;</span>
<span style="color:#00f">end</span>
</code></pre></div><p><a href="https://github.com/pulumi/examples/blob/master/gcp-ts-cloudrun/app/Gemfile">Gemfile</a> and <a href="https://github.com/pulumi/examples/blob/master/gcp-ts-cloudrun/app/Gemfile.lock">Gemfile.lock</a> files configure the required gems (packages).</p>
<h3 id="define-a-docker-image">Define a Docker image</h3>
<p><code>Dockerfile</code> defines the container image to deploy. It is based on a generic Ruby image, copies the application files, and runs the web application.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#008000"># Use the official lightweight Ruby image.</span><span style="">
</span><span style=""></span><span style="color:#00f">FROM</span><span style="color:#a31515"> ruby:2.7-slim</span><span style="">
</span><span style="">
</span><span style=""></span><span style="color:#008000"># Install production dependencies.</span><span style="">
</span><span style=""></span><span style="color:#00f">WORKDIR</span><span style="color:#a31515"> /usr/src/app</span><span style="">
</span><span style=""></span><span style="color:#00f">COPY</span> Gemfile Gemfile.lock ./<span style="">
</span><span style=""></span><span style="color:#00f">ENV</span> BUNDLE_FROZEN=true
<span style="color:#00f">RUN</span> bundle install<span style="">
</span><span style="">
</span><span style=""></span><span style="color:#008000"># Copy local code to the container image.</span><span style="">
</span><span style=""></span><span style="color:#00f">COPY</span> . ./<span style="">
</span><span style="">
</span><span style=""></span><span style="color:#008000"># Run the web service on container startup.</span><span style="">
</span><span style=""></span><span style="color:#00f">CMD</span> [<span style="color:#a31515">&#34;ruby&#34;</span>, <span style="color:#a31515">&#34;./app.rb&#34;</span>]<span style="">
</span></code></pre></div><p>Note that nothing in this web application is specific to Cloud Run.</p>
<h3 id="build-and-publish-the-docker-image">Build and publish the Docker image</h3>
<p>Cloud Run can only deploy images from Google Cloud Registry (GCR). Therefore, our Pulumi program needs to build the Docker image with the sample Ruby application and push it to GCR.</p>
<p>Run <code>gcloud auth configure-docker</code> in your command line to configure your local Docker installation to use GCR endpoints.</p>
<p>Run <code>npm i @pulumi/docker</code> to install the Pulumi Docker SDK. Then, add the <code>docker.Image</code> resource to the Pulumi program.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> myImage = <span style="color:#00f">new</span> docker.Image(<span style="color:#a31515">&#34;ruby-image&#34;</span>, {
    imageName: <span style="color:#2b91af">pulumi.interpolate</span><span style="color:#a31515">`gcr.io/</span><span style="color:#a31515">${</span>gcp.config.project<span style="color:#a31515">}</span><span style="color:#a31515">/my-ruby-app:v1.0.0`</span>,
    build: {
        context: <span style="color:#a31515">&#34;./app&#34;</span>,
    },
});
</code></pre></div><p>Pulumi takes care of building the image in the <code>app</code> folder and uploading it to GCR.</p>
<h3 id="deploy-to-cloud-run">Deploy to Cloud Run</h3>
<p>Now, you can deploy another Cloud Run service and point it to the custom image.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> rubyService = <span style="color:#00f">new</span> gcp.cloudrun.Service(<span style="color:#a31515">&#34;ruby&#34;</span>, {
    location,
    template: {
        spec: {
            containers: [{
                image: <span style="color:#2b91af">myImage.imageName</span>,
                resources: {
                    limits: {
                        memory: <span style="color:#a31515">&#34;1Gi&#34;</span>,
                    },
                },
            }],
            containerConcurrency: <span style="color:#2b91af">50</span>,
        },
    },
}, { dependsOn: <span style="color:#2b91af">enableCloudRun</span> });
</code></pre></div><p>This snippet illustrates two additional configuration values. The memory limit defines the amount of RAM available to each container. Container concurrency defines how many requests each container instance may process in parallel at any given time. Both parameters influence the price of the deployment, as explained in the Pricing section below.</p>
<p>Currently, it’s not possible to use partial vCPU or multiple vCPUs per instance: your instance has always one vCPU assigned. You can adjust the memory from 128Mb to 2Gb.</p>
<p>Finally, don&rsquo;t forget to enable unrestricted access and export the public URL.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> iamRuby = <span style="color:#00f">new</span> gcp.cloudrun.IamMember(<span style="color:#a31515">&#34;ruby-everyone&#34;</span>, {
    service: <span style="color:#2b91af">rubyService.name</span>,
    location,
    role: <span style="color:#a31515">&#34;roles/run.invoker&#34;</span>,
    member: <span style="color:#a31515">&#34;allUsers&#34;</span>,
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> rubyUrl = rubyService.status.url;
</code></pre></div><p>After deploying the program, you should be able to access the application with an HTTP call.</p>
<pre><code>$ curl &quot;$(pulumi stack output rubyUrl)&quot;
Hello Pulumi!
</code></pre><p>You can find the complete example in the Pulumi examples <a href="https://github.com/pulumi/examples/blob/master/gcp-ts-cloudrun">Github repository</a>.</p>
<h2 id="pricing">Pricing</h2>
<p>Cloud Run pricing is entirely consumption-based. After you exhaust the free tier, you start paying for four components, combined:</p>
<ul>
<li>Per CPU time ($24.00 / million seconds)</li>
<li>Per memory consumption ($2.50 / million GB-seconds)</li>
<li>Per request ($0.40 / million requests)</li>
<li>Egress traffic, same as any other Google Cloud usage</li>
</ul>
<p>Both time-based measurements are rounded up to the nearest 100 ms. All the provisioned memory is charged, the actual Mb/Gb consumption doesn&rsquo;t matter.</p>
<p>It&rsquo;s important to understand that the CPU and memory metrics are calculated per active instance (host, provisioned container), not per request. Each Cloud Run instance can handle multiple requests concurrently. Overlapping executions aren&rsquo;t double charged: billable time begins with the start of the first request and ends at the end of the last request.</p>
<p>The following picture illustrates the pricing for three executions running on the same container host.</p>
<p><img src="./executions.png" alt="Parallel executions at Cloud Run"></p>
<p>Multiple requests can share the allocated CPU and memory, so it makes sense to set the concurrency setting as high as possible for a given application (but not higher). This model is a big difference to Cloud Functions, which are charged for each request independently.</p>
<h2 id="comparing-cloud-run-to-other-services">Comparing Cloud Run to Other Services</h2>
<p>Despite the existence of multiple services that look somewhat similar, Cloud Run is unique in its capabilities.</p>
<h3 id="google-cloud-functions">Google Cloud Functions</h3>
<p><a href="https://cloud.google.com/functions/">Google Cloud Functions</a> (GCF) service deploys snippets of code as functions, while Cloud Run deploys a web application packaged as a Docker image. Currently, GCF only supports three runtimes (Node.js, Python, and Go), while Cloud Run can run practically any language and any runtime.</p>
<p>GCF has a notion of events and triggers: it can natively integrate with Pub/Sub, Cloud Storage, Cloud Firestore. Cloud Run is all about handling HTTP requests: Any connection to another service has to go via HTTP.</p>
<h3 id="aws-fargate">AWS Fargate</h3>
<p><a href="https://aws.amazon.com/fargate/">AWS Fargate</a> deploys container images. It requires an ECS cluster to run on and imposes more configuration burden on the user, including networking, load balancing, auto-scaling, and service discovery. Pulumi Crosswalk for AWS <a href="https://www.pulumi.com/docs/guides/crosswalk/aws/ecs/">can help</a> with these tasks.</p>
<p>Fargate is capable of hosting long-running workloads. Therefore, Fargate&rsquo;s scaling model is not tied to individual requests, and there is no scale-to-zero out of the box.</p>
<p>With Cloud Run, there&rsquo;s no notion of a cluster to manage, the scaling and billing models are based on individual requests. However, the workloads are HTTP-only, with a maximum duration of 15 minutes per call.</p>
<h3 id="azure-container-instances">Azure Container Instances</h3>
<p><a href="https://azure.microsoft.com/en-us/services/container-instances/">Azure Container Instances</a> (ACI) can also run arbitrary containers and has built-in HTTP endpoints. However, there&rsquo;s no auto-scaling: you get a single host for each instance. Also, there&rsquo;s no load balancing capability across multiple instances.</p>
<p>Cloud Run is elastically scalable, which positions it much better to host applications with variable workloads.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Cloud Run is a service with a unique combination of serverless ease of use and pricing with the full power of container deployments. It is easier to manage than Fargate, has automatic scaling missing from ACI, and greater flexibility compared to GCF. With concurrent requests model, Cloud Run is well-positioned to be significantly cheaper than Cloud Functions for some scenarios.</p>
<p>If you are excited, follow the example to <a href="https://github.com/pulumi/examples/blob/master/gcp-ts-cloudrun">Get Started with Cloud Run and Pulumi</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/containers" term="containers" label="Containers" />
                             
                                <category scheme="https://mikhail.io/tags/cloud-run" term="cloud-run" label="Cloud Run" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2019s]]></title>
            <link href="https://mikhail.io/2019/"/>
            <id>https://mikhail.io/2019/</id>
            
            <published>2019-12-19T00:00:00+00:00</published>
            <updated>2019-12-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Provisioned Concurrency: Avoiding Cold Starts in AWS Lambda]]></title>
            <link href="https://mikhail.io/2019/12/aws-lambda-provisioned-concurrency-no-cold-starts/"/>
            <id>https://mikhail.io/2019/12/aws-lambda-provisioned-concurrency-no-cold-starts/</id>
            
            <published>2019-12-19T00:00:00+00:00</published>
            <updated>2019-12-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>AWS recently announced the launch of Provisioned Concurrency, a new feature of AWS Lambda that intends to solve the problem of cold starts.</blockquote><p>AWS recently <a href="https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/">announced</a> the launch of <strong>Provisioned Concurrency</strong>, a new feature of AWS Lambda that intends to solve the problem of cold starts. In this article, we take another look at the problem of latency-critical serverless applications, and how the new feature impacts the status-quo.</p>
<h2 id="concurrency-model-of-aws-lambda">Concurrency Model of AWS Lambda</h2>
<p>Despite being serverless, AWS Lambda uses lightweight containers to process incoming requests. Every container, or worker, can process only a single request at any given time.</p>
<figure >
    
        <img src="executions.png"
            alt="Overlapping executions land on separate workers"
             />
        
    
    <figcaption>
        <h4>Overlapping executions land on separate workers</h4>
    </figcaption>
    
</figure>
<p>Because of this, the number of concurrent requests defines the number of required workers that a specific AWS Lambda function needs to serve a response at any given moment.</p>
<h2 id="cold-starts">Cold Starts</h2>
<p>How does AWS know how many workers it needs to run for a given function? Well, it doesn&rsquo;t know in advance. AWS allocates new workers on-demand as the Lambda gets invoked.</p>
<p>Whenever Lambda receives a request but it has no idle workers, the control plane assigns a new generic worker to it. The worker then has to download the custom code or binaries of your Lambda and load them into memory before it can service the request. This process takes time, which significantly increases response latency.</p>
<p>The issue of sporadically slow responses caused by the need to increase the pool of workers is known as <strong>Cold Start</strong>. Cold starts are consistently the top concern about the applicability of serverless technologies to latency-sensitive workloads. There are numerous articles about the problem, including many articles I have written in the <a href="https://mikhail.io/serverless/coldstarts/">Cold Starts</a> section on my website</p>
<h2 id="warming">Warming</h2>
<p>Cold starts don&rsquo;t occur for the majority of requests because AWS Lambda reuses workers between subsequent invocations. However, if a particular worker is idle for too long (usually, several minutes), AWS may decide to recycle and return it to the generic pool.</p>
<p>A trick known as <strong>Lambda Warmers</strong> uses kept-alive workers to reduce the frequency of cold starts. The idea is to have a CloudWatch timer that fires every few minutes and sends <code>N</code> parallel requests to the target Lambda. If all those requests land at the same time, AWS has to provision at least <code>N</code> workers to process them. The actual operation doesn&rsquo;t have to do any useful work, but it resets the recycling timer back to zero.</p>
<p>There are a few drawbacks with this approach:</p>
<ul>
<li>If a valid request comes at the same time as warming requests, it might hit a cold start.</li>
<li>Extra logic is needed in the Lambda code to detect warming requests and short-circuit them instead of doing useful work.</li>
<li>CloudWatch rules require additional setup and management.</li>
<li>The result is still best-effort: Lambda still recycles workers from time to time.</li>
</ul>
<p>You can find more details about how Pulumi can help with some of these issues in <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<h2 id="provisioned-concurrency">Provisioned Concurrency</h2>
<p>At re:Invent 2019, AWS introduced Lambda <strong>Provisioned Concurrency</strong>&mdash;a feature to work around cold starts. The base concurrency model doesn&rsquo;t change. However, you can request a given number of workers to be always-warm and dedicated to a specific Lambda.</p>
<p>Here is an example of configuring the provisioned concurrency with Pulumi in TypeScript:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> lambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;mylambda&#34;</span>, {
    code: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./app&#34;</span>),
    handler: <span style="color:#a31515">&#34;handler&#34;</span>,
    role: <span style="color:#2b91af">role.arn</span>,
    publish: <span style="color:#2b91af">true</span>,
});

<span style="color:#00f">const</span> fixed = <span style="color:#00f">new</span> aws.lambda.ProvisionedConcurrencyConfig(<span style="color:#a31515">&#34;fixed-concurrency&#34;</span>, {
  functionName: <span style="color:#2b91af">lambda.name</span>,
  qualifier: <span style="color:#2b91af">lambda.version</span>,
  provisionedConcurrentExecutions: <span style="color:#2b91af">2</span>,
});
</code></pre></div><p>The snippet sets the provisioned concurrency for <code>mylambda</code> to a fixed value of <code>2</code>. Note that concurrency is provisioned per Lambda version, and it can&rsquo;t be set for the <code>$LATEST</code> version alias. Therefore, I set the <code>publish</code> attribute of my Lambda to <code>true</code> and reference a specific version of the Lambda in provisioning.</p>
<h2 id="dynamic-provisioned-concurrency">Dynamic Provisioned Concurrency</h2>
<p>A fixed level of provisioned concurrency works well for stable workloads.</p>
<figure >
    
        <img src="steady.png"
            alt="Fixed provisioned concurrency for uniform workloads"
             />
        
    
    <figcaption>
        <h4>Fixed provisioned concurrency for uniform workloads</h4>
    </figcaption>
    
</figure>
<p>However, many workloads fluctuate a lot. Extreme elasticity and lack of configuration parameters have always been the essential benefits of AWS Lambda. It works great if you can tolerate the cold starts that come during scale-out. If not, you can explore more advanced scenarios for provisioning concurrency dynamically.</p>
<p>Instead of choosing a permanently fixed value, you can configure provisioned concurrency to autoscale. The first required component is the autoscaling target:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> resourceId = pulumi.interpolate<span style="color:#a31515">`function:</span><span style="color:#a31515">${</span>lambda.name<span style="color:#a31515">}</span><span style="color:#a31515">:</span><span style="color:#a31515">${</span>lambda.version<span style="color:#a31515">}</span><span style="color:#a31515">`</span>;
<span style="color:#00f">const</span> target = <span style="color:#00f">new</span> aws.appautoscaling.Target(<span style="color:#a31515">&#34;target&#34;</span>, {
    resourceId,
    serviceNamespace: <span style="color:#a31515">&#34;lambda&#34;</span>,
    scalableDimension: <span style="color:#a31515">&#34;lambda:function:ProvisionedConcurrency&#34;</span>,
    minCapacity: <span style="color:#2b91af">1</span>,
    maxCapacity: <span style="color:#2b91af">10</span>,
});
</code></pre></div><p>The second component is the autoscaling policy, which defines the conditions when scaling starts. There are two important ways to adapt your concurrency provisioning to traffic patterns.</p>
<h3 id="scheduled-profile">Scheduled profile</h3>
<p>Quite often, increases in request rates are partially predictable. For example, usage increases during business hours and decreases at night.</p>
<figure >
    
        <img src="scheduled.png"
            alt="Provisioned concurrency matches predictable workload changes"
             />
        
    
    <figcaption>
        <h4>Provisioned concurrency matches predictable workload changes</h4>
    </figcaption>
    
</figure>
<p>The following snippet sets two scheduled rules that switch between two levels of provisioned concurrency every day.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">function</span> scheduledConcurrency(name: <span style="color:#2b91af">string</span>, cron: <span style="color:#2b91af">string</span>, capacity: <span style="color:#2b91af">number</span>) {
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> aws.appautoscaling.ScheduledAction(<span style="color:#a31515">`schedule-</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
        resourceId,
        serviceNamespace: <span style="color:#a31515">&#34;lambda&#34;</span>,
        scalableDimension: <span style="color:#a31515">&#34;lambda:function:ProvisionedConcurrency&#34;</span>,
        scalableTargetAction: {
            minCapacity: <span style="color:#2b91af">capacity</span>,
            maxCapacity: <span style="color:#2b91af">capacity</span>,
        },
        schedule: <span style="color:#a31515">`cron(</span><span style="color:#a31515">${</span>cron<span style="color:#a31515">}</span><span style="color:#a31515">)`</span>,
    }, { dependsOn: [target]});
}

scheduledConcurrency(<span style="color:#a31515">&#34;day&#34;</span>, <span style="color:#a31515">&#34;0 8 * * ? *&#34;</span>, 10);
scheduledConcurrency(<span style="color:#a31515">&#34;night&#34;</span>, <span style="color:#a31515">&#34;0 18 * * ? *&#34;</span>, 2);
</code></pre></div><p>The example defines a helper function and calls it twice to set two schedules with different parameters.</p>
<h3 id="autoscaling-based-on-utilization">Autoscaling based on utilization</h3>
<p>If your workload pattern is less predictable, you can configure autoscaling for provisioned concurrency based on the measured utilization.</p>
<figure >
    
        <img src="variable.png"
            alt="Provisioned concurrency matches the variation in workload"
             />
        
    
    <figcaption>
        <h4>Provisioned concurrency matches the variation in workload</h4>
    </figcaption>
    
</figure>
<p>Here is a basic example of a dynamic scaling policy.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> scaledConcurrency = <span style="color:#00f">new</span> aws.appautoscaling.Policy(<span style="color:#a31515">&#34;autoscaling&#34;</span>, {
    resourceId,
    serviceNamespace: <span style="color:#a31515">&#34;lambda&#34;</span>,
    scalableDimension: <span style="color:#a31515">&#34;lambda:function:ProvisionedConcurrency&#34;</span>,
    policyType: <span style="color:#a31515">&#34;TargetTrackingScaling&#34;</span>,
    targetTrackingScalingPolicyConfiguration: {
        targetValue: <span style="color:#2b91af">0.8</span>,
        predefinedMetricSpecification: {
            predefinedMetricType: <span style="color:#a31515">&#34;LambdaProvisionedConcurrencyUtilization&#34;</span>,
        },
    },
}, { dependsOn: [target]});
</code></pre></div><p>Currently, there are issues with autoscaling based on the metrics, which makes provisioned concurrency scale less effectively than what the chart above shows. Hopefully, Amazon will fix these issues soon.</p>
<h2 id="pricing">Pricing</h2>
<p>While hand-crafted Lambda warmers are virtually free, provisioned concurrency can be costly. The new pricing is an integral part of the change: Instead of purely per-call model, AWS charges per hour for provisioned capacity.</p>
<p>You would pay $0.015 per hour per GB of provisioned worker memory, even if a worker handled zero requests.</p>
<p>The per-invocation price gets a discount: $0.035 per GB-hour instead of the regular $0.06 per GB-hour. This change means that fully-utilized workers would be cheaper if provisioned compared to on-demand workers.</p>
<figure >
    
        <img src="pricing.png"
            alt="Comparison of the cost of a 1GB worker for two billing models"
             />
        
    
    <figcaption>
        <h4>Comparison of the cost of a 1GB worker for two billing models</h4>
    </figcaption>
    
</figure>
<p>The equilibrium point is at 60% utilization. Note that because the billed duration is rounded up to the nearest 100 ms for each execution, the utilization is not limited to 100%. A series of sequential executions can be processed by a single worker. If each execution is 10 ms, the charge is still 100 ms, and the total utilization can be as high as 1000% in terms of the chart above!</p>
<p>Finally, be careful to clean up your resources after any experiments: Leaving running workers with provisioned concurrency can be expensive! Using <code>pulumi destroy</code> removes resources after you finish experimenting.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Provisioned concurrency brings the long-awaited solution to cold starts in AWS Lambda. However, it comes at a cost in terms of a direct impact on billing and management overhead to provision concurrency at optimal levels.</p>
<p>For high-load functions where utilization is continuously above a known level of requests, it makes sense to set the provisioned concurrency to that level to save money and have the guarantee of warm workers.</p>
<p>If your Lambda hosts a latency-sensitive API, especially with runtimes like Java and .NET, you should strive to find the right balance between the percentage of requests that result in a cold start and the money spent on concurrency. Autoscaling should help once AWS has fixed the initial glitches that slipped into their current services.</p>
<p>If you want to try this for yourself, we&rsquo;ve updated the <a href="https://github.com/pulumi/examples/blob/master/aws-ts-serverless-raw/">Serverless App example</a> to show off the scenario of configurable AWS Lambda provisioned concurrency.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Santa Brings Cloud to Every Developer]]></title>
            <link href="https://mikhail.io/2019/12/santa-brings-cloud-to-every-developer/"/>
            <id>https://mikhail.io/2019/12/santa-brings-cloud-to-every-developer/</id>
            
            <published>2019-12-01T00:00:00+00:00</published>
            <updated>2019-12-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>How Santa Cloud uses F# and Pulumi to bring cloud resources to the homes of software engineers.</blockquote><p><em>The post is a part of
<a href="https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/">F# Advent Calendar 2019</a>.
It&rsquo;s Christmas time!</em></p>
<p>Cloud is everywhere, and yet it&rsquo;s still inaccessible to millions of developers and IT pros. With shining yet cloudy eyes return they from KubeCon&rsquo;s and re:Invent&rsquo;s, just to pick up an issue in Jira, fix yet another <code>SingletonProxyFactoryBean</code> in their J2EE application, commit it to SVN, and hope it will be delivered in three months to the data center in their HQ basement.</p>
<p>But hey, it&rsquo;s December now, which means it&rsquo;s time to write a letter to Santa Cloud! Santa Cloud brings gifts to the homes of well-behaved engineers on the night of Christmas Eve. He accomplishes this feat with the aid of his elves, who make the toys in their glorious workshops in the State of Washington.</p>
<p><img src="elves.jpg" alt="Cloud Elves"></p>
<p>Here are some typical letters that Santa gets these days:</p>
<blockquote>I maintain 15 different apps, and each one has a custom deployment workflow. If only I had a <b>Kubernetes</b> cluster, I would stick all the apps in it and let the blue/green deployments do the magic!</blockquote>
<blockquote>My ten-year-old legacy application was designed for twenty users, and now it has to handle twenty thousand. It reached its performance limits! If only I could port it to <b>Serverless</b>, this would fix all my scalability issues.</blockquote>
<blockquote>
<p>We have 2 TB of data to analyze! Could you please send me a data lake and a <strong>Spark</strong> cluster? Actually, can I have two&hellip; or maybe three?</p>
</blockquote>
<p>There&rsquo;s enough cloud for everyone! Perhaps, a naughty engineer, who created a lot of technical debt and meeting invites, might only get a cozy single-core VM for their LAMP-stack website. But the lucky ones might get a whole Kubernetes cluster for themselves!</p>
<h2 id="behind-the-snow-curtains">Behind the Snow Curtains</h2>
<p>While cloud elves are good at provisioning the underlying hardware and providing services, Santa still needs to coordinate them to fulfill wishes. There are millions of aspiring cloud engineers, and all of them believe in Santa. It&rsquo;s not feasible to click the buttons in web portals or run ad-hoc scripts for each and every wishlist.</p>
<p>Luckily, Santa Cloud is good at programming and automation. He codes in <a href="https://fsharp.org">F#</a>, and he&rsquo;s one of the early customers of the .NET SDK for <a href="https://pulumi.com">Pulumi</a>. Hey, it&rsquo;s my fairy tale, so I choose the tech stack, thank you. Worry not, even if you are not familiar with F# or Pulumi, you will be able to follow along.</p>
<p>The workflow looks approximately like this:</p>
<ul>
<li>Represent a letter as a value in the program</li>
<li>Pick an elf which can fulfill the desire</li>
<li>Produce the resource</li>
<li>Wrap a note to the recipient</li>
</ul>
<h3 id="parsing-wishlists">Parsing wishlists</h3>
<p>Too bad, not every wishful engineer knows F#. Maybe, next year, Santa can make an online form that wouldn&rsquo;t accept anything but valid code. For now, it&rsquo;s the duty of Mrs. Cloud to read every letter and convert it to a list of records. Here is a sample:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> wishes = [
    { Recipient = Person <span style="color:#a31515">&#34;Kelsey&#34;</span> ; Resource = Kubernetes (Nodes 25) }
    { Recipient = Person <span style="color:#a31515">&#34;Jeff&#34;</span>   ; Resource = Serverless DotNet }
    { Recipient = Person <span style="color:#a31515">&#34;Satoshi&#34;</span>; Resource = Blockchain Public }
]
</code></pre></div><p>The value <code>wishes</code> is a strongly-typed list of records. Each record has multiple properties and utilizes &ldquo;algebraic data types&rdquo;: each type would have a label (&ldquo;Kubernetes&rdquo;) and an associated piece of data (25 nodes). The shape of data depends on the label: it&rsquo;s not possible to ask 25 nodes of serverless.</p>
<p>When the list is defined and compiled, it goes to production. But first, we need to assign an elf to each item in the list.</p>
<h3 id="elves">Elves</h3>
<p>All-powerful craftselves are represented as another F# type: a function type.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Elf</span> = Wish -&gt; (<span style="color:#2b91af">unit</span> -&gt; Identifier) option
</code></pre></div><p>Here is what it tells us:</p>
<ul>
<li>It&rsquo;s a function which expects a <code>Wish</code> as an input parameter</li>
<li>It may return nothing if this elf can&rsquo;t fulfill the wish (represented by <code>option</code> type)</li>
<li>If it returns something, it returns yet another function&mdash;of type <code>unit -&gt; Identifier</code></li>
<li>This resulting function has a side effect (it accepts <code>unit</code>): the production of a cloud resource</li>
<li>It returns an <code>Identifier</code> of that resource</li>
</ul>
<p>Whoa, that&rsquo;s a lot of information conveyed by a single-line type definition!</p>
<p>In our program, each elf is a function, for example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> aws (wish: Wish) =
    <span style="color:#00f">match</span> wish.Resource <span style="color:#00f">with</span>
    | Kubernetes k8s -&gt; makeEksCluster k8s |&gt; Some
    | Serverless s8s -&gt; makeAwsLambda s8s  |&gt; Some
    <span style="color:#008000">//|  ... more resource types
</span><span style="color:#008000"></span>    | _ -&gt; None
</code></pre></div><p>The function matches the type of the desired resource and calls a corresponding resource provisioning routine.</p>
<h3 id="resource-provisioning">Resource provisioning</h3>
<p>Pulumi handles the creation of cloud resources with its .NET SDKs spanning across all major cloud providers.</p>
<p>Here is a snippet which defines an Azure Function App:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> makeFunctionApp (Person name) (runtime: Runtime) () =
    <span style="color:#00f">let</span> app =
        FunctionApp(name,
            FunctionAppArgs
               (ResourceGroupName = io resourceGroup.Name,
                AppSettings = inputMap [<span style="color:#a31515">&#34;runtime&#34;</span>, runtime.ToString() |&gt; input],
                StorageConnectionString = io storageAccount.PrimaryConnectionString,
                Version = input <span style="color:#a31515">&#34;~2&#34;</span>))
    Url app.DefaultHostname
</code></pre></div><p>And here is a Google Kubernetes Engine cluster:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> makeGkeCluster (Person name) (Nodes nodeCount) () =
    <span style="color:#00f">let</span> masterVersion = input <span style="color:#a31515">&#34;1.15.1&#34;</span>
    <span style="color:#00f">let</span> cluster =
        Cluster(name,
            ClusterArgs
               (InitialNodeCount = input nodeCount,
                MinMasterVersion = masterVersion,
                NodeVersion = masterVersion,
                NodeConfig = input (
                    ClusterNodeConfigArgs
                       (MachineType = input <span style="color:#a31515">&#34;n1-standard-1&#34;</span>))))
    Url cluster.Endpoint

</code></pre></div><h3 id="production">Production</h3>
<p>The application logic of picking the right production facility and executing the order takes five lines of code.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> fulfill wish =
    <span style="color:#008000">// Randomize the order of elves, so that they get equal opportunities
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> elves = shuffle [aws; azure; gcp]

    <span style="color:#008000">// Find the first elf which agrees to produce a resource
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> make = elves |&gt; List.pick (<span style="color:#00f">fun</span> elf -&gt; elf wish)

    <span style="color:#008000">// Run the production!
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> identifier = make()

    wish.Recipient, identifier
</code></pre></div><p>We shuffle the order of elves to give them comparable workload, pick the first one in the list who agrees to execute the order, run the provisioning, and return the recipient name with their shiny resource identifier.</p>
<p>Go team Santa!</p>
<h3 id="delivery">Delivery</h3>
<p>For a nice personal touch, a card needs to be printed, gift-wrapped, and delivered by Santa and reindeers. Programming the reindeers is for another blog post. For now, we just return a list of tuples of person names and resource URLs:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> send (responses: Response <span style="color:#2b91af">list</span>) =
    responses
    |&gt; List.map (<span style="color:#00f">fun</span> (Person name, Url url) -&gt; name, url)
    |&gt; dict
</code></pre></div><p>Finally, another line combines the steps into the workflow:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> santaCloud () = wishes |&gt; (List.map fulfill) |&gt; send
</code></pre></div><p>And the last line defines the entry point of the application:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;EntryPoint&gt;]
<span style="color:#00f">let</span> main _ = Deployment.run santaCloud
</code></pre></div><p>Running the program produces an output like this:</p>
<p><img src="pulumiup.png" alt="Pulumi Console Output"></p>
<h2 id="retrospective">Retrospective</h2>
<p>Honestly, the system is brand-new this year, so we still need to see how it plays out on Christmas Eve. Regardless, we learned a thing or two today:</p>
<ul>
<li><strong>Types</strong>: F# has an excellent type system that allows expressing domain terms concisely and precisely.</li>
<li><strong>Logic</strong>: The program workflow is expressed in terms of function calls, pattern matching, and collections manipulation.</li>
<li><strong>Infrastructure</strong>: Cloud resources are defined in F# with Pulumi .NET SDK, which supports pretty much any resource in any cloud.</li>
<li><strong>Application</strong>: Blended together, these components enable writing expressive, cohesive, maintainable, cloud-native applications using familiar and loved tools and practices.</li>
</ul>
<p>Learn more about <a href="https://pulumi.com/dotnet">Pulumi for .NET</a> or browse <a href="https://github.com/mikhailshilkov/fsharp-advent-pulumi/tree/master/2019">the full code</a> for this example.</p>
<p>Merry Cloudsmas, ho-ho-ho!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Choosing the Best Deployment Tool for Your Serverless Applications]]></title>
            <link href="https://mikhail.io/2019/11/choosing-the-best-deployment-tool-for-serverless-applications/"/>
            <id>https://mikhail.io/2019/11/choosing-the-best-deployment-tool-for-serverless-applications/</id>
            
            <published>2019-11-12T00:00:00+00:00</published>
            <updated>2019-11-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Factors to consider while deploying cloud infrastructure for serverless apps.</blockquote><p>Function-as-a-Service solutions, such as AWS Lambda and Azure Functions, are a great way to build modern, scalable, and affordable cloud-native applications. By delegating routine work to cloud providers, serverless applications focus on custom logic to provide the ultimate business value. But, in fact, serverless is more than cloud functions. It&rsquo;s storage, HTTP gateways, databases, queues, monitoring, and security&mdash;and your serverless applications are likely to use multiple managed services and consist of many moving parts.</p>
<p>So how do you deploy such applications reliably? There are multiple cloud deployment tools out there! In this article, I identify several criteria to consider when making your decision.</p>
<h2 id="point-and-click-or-write-code">Point-and-Click or Write Code?</h2>
<p>Most tutorials and getting-started guides use web portals, like AWS Management Console or Microsoft Azure portal, to create basic serverless applications and required infrastructure. The visual flow of solutions like these is simple for newcomers to understand, and they can take shortcuts and assume some sensible defaults to streamline the process.</p>
<p>However, simply clicking through the online wizard won&rsquo;t produce reliable outcomes. Two weeks later, for example, you might not be able to follow the exact same sequence of steps to create a copy of your application when you need it. Now imagine if your colleague has to try!</p>
<p>Instead of the manual provisioning process, you can define the infrastructure as code. I&rsquo;m using “code” in a broad sense here: It can be any machine-readable format that is written by a human and executed by an automated tool.</p>
<p>I strongly recommend using the infrastructure-as-code approach for any application that&rsquo;s beyond simple demos and one-off trials. Put the code definition to the source control, and you&rsquo;ll get repeatable deployments, documentation, a history of changes, and reference materials&mdash;all in one step.</p>
<h2 id="procedural-or-desired-state">Procedural or Desired State?</h2>
<p>Bash or PowerShell scripts are the traditional way to automate. Every cloud provider has a command-line interface (CLI) to manage its resource portfolio, so it&rsquo;s entirely possible to script the provisioning of the entire infrastructure for your serverless application.</p>
<p>However, there are a few downsides to this approach. Naturally, scripts are very imperative: You describe the exact steps to execute, in the required order. And if you need to change the state of an existing environment, you&rsquo;ll likely need to manage update scripts that bring the infrastructure through transition steps. Finally, it&rsquo;s hard to manage failures: If a script fails in the middle, the exact state of your resources will often be unknown.</p>
<p>You can also try the declarative style of Desired State Configuration (DSC). The goal of DSC is to describe the complete state of the infrastructure, regardless of its current state. Then, it functions as an automated tool which reads the desired state, compares it to the current state, and figures out which steps to execute to reconcile the two.</p>
<p>All the tools that I mention in the rest of the article follow the desired-state philosophy.</p>
<h2 id="cloud-vendor-or-third-party">Cloud Vendor or Third Party?</h2>
<p>Each cloud vendor has its own native tool to manage infrastructure as code: <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a>, <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/template-deployment-overview">Azure Resource Manager (ARM) templates</a>, and <a href="https://cloud.google.com/deployment-manager/">Google Cloud Deployment Manager</a>, for example. These are all first-class services, so they have excellent feature coverage and are widely used, stable, and battle-tested.</p>
<p>However, while the vendor tools are robust, they have a set of constraints. Obviously, each tool focuses on one cloud, so mastering CloudFormation won&rsquo;t make you an ARM guru. Also, the deployment managers are fundamental infrastructure components of their respective clouds. Because they need to support every service and feature, they tend to operate at a quite low level. Additionally, they must avoid breaking changes, so legacy features and issues tend to pile up over time. The tools may feel rigid, and because they are closed source and vendor managed, you don&rsquo;t have much influence.</p>
<p>It&rsquo;s worth noting that numerous third-party infrastructure management tools try to compete with cloud vendors, and serverless apps can successfully use many of them. <a href="https://www.terraform.io/">HashiCorp&rsquo;s Terraform</a>, for example, is a cross-provider tool, which covers not only the majority of each cloud&rsquo;s features, but also the configuration of Kubernetes, Docker images, Kafka, monitoring tools, and some databases.</p>
<p>The downside of third-party tools is that they may not offer support for some cloud features. For example, while using Terraform&rsquo;s AzureRM provider, I definitely bumped into missing features, especially for newer services or niche configurations. To compensate for these kinds of issues, third-party tools are usually open source and accept contributions, so an active community is an essential factor.</p>
<h2 id="execution-on-the-client-or-in-the-cloud">Execution on the Client or in the Cloud?</h2>
<p>Cloud vendor tools take your definition files and run them from within the managed cloud service. For example, Azure creates a first-class object&mdash;deployment&mdash;which shows the deployed resources or problems that occurred. CloudFormation can automatically roll back a failed deployment to the previous well-known state.</p>
<p>In contrast, CLI scripts, Terraform, and <a href="https://www.pulumi.com/">Pulumi</a> run the deployment from the client machine where they are executed, or, even better, from CI/CD servers for most production deployments. This means they have more granular control over the execution flow and error handling, but also that they aren&rsquo;t able to reuse the built-in deployment features of the native-cloud managers.</p>
<p>Some tools, like <a href="https://github.com/cloudtools/troposphere">Troposphere</a>, <a href="https://aws.amazon.com/cdk/">AWS Cloud Development Kit (AWS CDK)</a>, and <a href="https://serverless.com/">Serverless Framework</a>, have their own way to describe the infrastructure. Still, they transpile these definitions to formats like CloudFormation to make use of the cloud deployment engines.</p>
<h2 id="general-purpose-or-specialized">General Purpose or Specialized?</h2>
<p>CloudFormation, ARM templates, and Terraform are all general-purpose tools: You can define serverless applications with them, but that&rsquo;s not their primary focus. Traditional infrastructure and networking solutions still have a far larger audience, so these scenarios attract more attention from the vendors. General-purpose tools lack higher-level abstractions, so the resource definitions tend to be verbose and repetitive.</p>
<p>Instead of general-purpose tools, you can use a specialized solution that is inherently modeled around the concepts of serverless. Serverless Framework is a multi-provider tool, while <a href="https://aws.amazon.com/serverless/sam/">AWS Serverless Application Model (SAM)</a> is a comparable option from AWS. Another example is Claudia.js&mdash;a deployment tool for node-based AWS Lambda.</p>
<p>It&rsquo;s easy to get started with specialized tools, and to use them for implementing basic scenarios. However, if your application is a combination of serverless features and traditional infrastructure, you may need to use more than one tool. In this case, make sure that the benefits of the specialized tools are enough to compensate for the complexity of using multiple options simultaneously.</p>
<p>I struggled with trying to use Serverless Framework for Azure: The experience was not ideal, and some features were missing. Understandably, AWS is Serverless Framework&rsquo;s primary target, so plugins targeting other clouds may lag behind.</p>
<h2 id="markup-or-programming-language">Markup or Programming Language?</h2>
<p>If you settle for a desired-state configuration tool, you will likely use a markup language to define the cloud infrastructure. CloudFormation, Google Cloud Deployment Manager, and Serverless Framework use YAML, while ARM templates are defined in JSON. Terraform invented its own markup language called HCL. This language makes the definitions more succinct and expressive, but requires you to master a new language and toolchain.</p>
<p>However, markup languages are lacking expressiveness to easily represent conditional shapes, multiple similar resources, or higher-level abstractions. They embed home-grown constructs inside YAML or JSON, or make you fall back to template-based generation. A newer class of tools is trying to address this limitation by using general-purpose programming languages to define cloud infrastructures. For example, AWS CDK uses TypeScript, Python, Java, and .NET to produce CloudFormation output.</p>
<p>Pulumi takes this idea even further by providing support for a similar set of languages to provision infrastructure for any cloud&mdash;and many tools beyond the cloud. Developers can use their existing skills and tools to define type-safe and testable infrastructure and to create higher level abstractions with classes, components, and functions.</p>
<p>Both Pulumi and AWS CDK have a set of components focusing on serverless applications, so they might be the best tools for providing both high-level expressive serverless components and low-level infrastructure resources, if needed.</p>
<p>In the past, I deployed Azure resources with templates containing five thousand lines of JSON. It wasn&rsquo;t a great developer experience to write all those lines, and the application was mostly a bunch of web servers and SQL databases&mdash;nothing too fancy. When I define a similar infrastructure with TypeScript, the code size reduction is at least tenfold. <a href="https://mikhail.io/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/">This post</a> gives an example of a similar transformation for an AWS serverless app.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Serverless applications consist of many components, combining multiple managed services into one cohesive flow. Regardless of your preference for a specific tool, you should use the infrastructure-as-code approach to make your serverless applications reliable and maintainable in the long run.</p>
<p>Cloud infrastructure tools are evolving quickly, and none of them are perfect just yet. To choose what&rsquo;s best for you, evaluate your needs and constraints, compare several options, and keep an eye on the field: More innovations are coming soon.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/infrastructure-as-code" term="infrastructure-as-code" label="Infrastructure as code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Infrastructure-as-Code]]></title>
            <link href="https://mikhail.io/tags/infrastructure-as-code/"/>
            <id>https://mikhail.io/tags/infrastructure-as-code/</id>
            
            <published>2019-11-12T00:00:00+00:00</published>
            <updated>2019-11-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[AWS Lambda vs. Azure Functions: 10 Major Differences]]></title>
            <link href="https://mikhail.io/2019/10/aws-lambda-vs-azure-functions-ten-major-differences/"/>
            <id>https://mikhail.io/2019/10/aws-lambda-vs-azure-functions-ten-major-differences/</id>
            
            <published>2019-10-20T00:00:00+00:00</published>
            <updated>2019-10-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>A comparison AWS Lambda with Azure Functions, focusing on their unique features and limitations.</blockquote><p>Forget about managing virtual machines or paying for idle hardware! Serverless compute brings unlimited scale and high availability to every company in the world, from small startups to multinational corporations. At least, that’s the vision of Amazon and Microsoft, today’s biggest cloud vendors.</p>
<p>AWS Lambda pioneered the Function as a Service (FaaS) application model in 2014. With Faas, a small piece of code—called a function—is deployed as a ZIP file and linked to a specific type of event, such as a queue or an HTTP endpoint. AWS runs this function every time a matching event occurs, be it once per day or a thousand times per second.</p>
<p>Since 2014, the serverless model has taken off, and every major cloud provider has introduced its flavor of an FaaS service: Azure Functions, Google Cloud Functions, and IBM Cloud Functions, to name a few. While the basic idea behind all the offerings is the same, there are many differences between these implementations.</p>
<p>Today, I’ll compare AWS Lambda with Azure Functions (Lambda’s equivalent in Azure cloud), focusing on their unique features and limitations. Here are the ten major differences between the two options.</p>
<h2 id="1-hosting-plans">1. Hosting Plans</h2>
<p>To put it simply, there is one way to run a serverless function in AWS: deploy it to the AWS Lambda service. Amazon’s strategy here is to make sure that this service covers as many customer scenarios as possible, ranging from hobby websites to enterprise-grade data processing systems.</p>
<p>Microsoft takes a different approach. They separated the notion of the Azure Functions programming model from the serverless operational model. With Azure Functions, I can deploy my functions to a pay-per-use, fully-managed Consumption plan. However, I can also use <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale">other hosting options</a> to run the same code:</p>
<ul>
<li>App Service plan: provides a predictable pay-per-hour price, but has limited auto-scaling behavior</li>
<li>Premium plan (preview): gives reserved capacity <em>and</em> elastic scaling, combined with advanced networking options, for a higher price</li>
<li>A Docker container: can run anywhere on self-managed infrastructure</li>
<li>Kubernetes-based event-driven architecture (KEDA, experimental): brings functions to Kubernetes, running in any cloud or on-premises</li>
</ul>
<p>The Consumption plan has the lowest management overhead and no fixed-cost component, which makes it the most serverless hosting option on the list. For that reason, I’m going to focus on AWS Lambda vs. Azure Functions Consumption plan for the rest of this article.</p>
<h2 id="2-configurability">2. Configurability</h2>
<p>When deploying <a href="https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html">an AWS Lambda function</a>, I need to define the maximum memory allocation, which has to be between 128MB and 3GB. The CPU power and cost of running the function are proportional to the allocated memory. It takes a bit of experimentation to define the optimal size, depending on the workload profile. Regardless of size, all instances run on Amazon Linux.</p>
<p>On the other hand, Azure Functions Consumption plan is one-size-fits-all. It comes with 1.5GB of memory and one low-profile virtual core. You can choose between Windows and Linux as a host operating system.</p>
<p><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan#plan-and-sku-settings">Azure Functions Premium plan</a> comes with multiple instance sizes, up to 14GB of memory, and four vCPUs. However, you have to pay a fixed per-hour fee for the reserved capacity.</p>
<h2 id="3-programming-languages">3. Programming Languages</h2>
<p>AWS Lambda natively supports JavaScript, Java, Python, Go, C#, F#, PowerShell, and Ruby code.</p>
<p>Azure Functions has runtimes for JavaScript, Java, Python, C#, F#, and PowerShell (preview). Azure lacks Go and Ruby—otherwise, the language options are very similar.</p>
<h2 id="4-programming-models">4. Programming Models</h2>
<p>Specifics vary between runtimes, but, overall, AWS Lambda has a <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html">straightforward programming model</a>. A function receives a JSON object as input and may return another JSON as output. The event type defines the schema of those objects, which are documented and defined in language SDKs.</p>
<p>Azure Functions has a more sophisticated model based on <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings">triggers and bindings</a>. A trigger is an event that the function listens to. The function may have any number of input and output bindings to pull and/or push extra data at the time of processing. For example, an HTTP-triggered function can also read a document from Azure Cosmos DB and send a queue message, all done declaratively via binding configuration.</p>
<p>The implementation details differ per language runtime. The binding system provides extra flexibility, but it also brings some complexity, in terms of both API and configuration.</p>
<h2 id="5-extensibility">5. Extensibility</h2>
<p>One drawback of all FaaS services on the market is the limited set of supported event types. For example, if you want to trigger your functions from a Kafka topic, you are out of luck on both AWS and Azure.</p>
<p>Other aspects of serverless functions are more customizable. AWS Lambda defines a <a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html">concept of layers</a>: a distribution mechanism for libraries, custom runtimes to support other languages, and other dependencies.</p>
<p>Azure Functions enables open <a href="https://github.com/Azure/azure-webjobs-sdk-extensions/wiki/Binding-Extensions-Overview">binding extensions</a> so that the community can create new types of bindings and bring them into Function Apps.</p>
<h2 id="6-concurrency-and-isolation">6. Concurrency and Isolation</h2>
<p>Both services can run multiple (potentially thousands) executions of the same function simultaneously, each handling one incoming event.</p>
<p>AWS Lambda always reserves a separate instance for a single execution. Each execution has its exclusive pool of memory and CPU cycles. Therefore, the performance is entirely predictable and stable.</p>
<p>Azure Functions allocates multiple concurrent executions to the same virtual node. If one execution is idle waiting for a response from the network, other executions may use resources which would otherwise be wasted. However, resource-hungry executions may fight for the pool of shared resources, harming the overall performance and processing time.</p>
<h2 id="7-cost">7. Cost</h2>
<p>Serverless pricing is based on a pay-per-usage model. Both services have two cost components: pay-per-call and pay-per-GB*seconds. The latter is a metric combining execution time and consumed memory.</p>
<p>Moreover, the price tag for both services is almost exactly the same: $0.20 per million requests and $16 per million GB*seconds ($16.67 for AWS). One million executions running for 100 ms each and consuming 1GB of memory cost less than $2. Since AWS Lambda was the first on the market, I assume Microsoft just copied the numbers.</p>
<p>There are some differences in the details, though:</p>
<ul>
<li>AWS Lambda charges for full provisioned memory capacity, while Azure Functions measures the actual average memory consumption of executions.</li>
<li>If Azure Function’s executions share the instance, the memory cost isn’t charged multiple times, but shared between executions, which may lead to noticeable reductions.</li>
<li>Both services charge for at least 100 ms and 128MB for each execution. AWS rounds the time up to the nearest 100 ms, while Azure rounds up to 1 ms.</li>
<li>CPU profiles are different for Lambda and Functions, which may lead to different durations for comparable workloads.</li>
</ul>
<p>I wrote more on how to measure the cost of Azure Functions <a href="https://mikhail.io/2019/08/how-to-measure-the-cost-of-azure-functions/">here</a>.</p>
<h2 id="8-http-integration">8. HTTP Integration</h2>
<p>AWS Lambda used to require Amazon API Gateway to listen to HTTP traffic, which came at a massive additional cost. Recently, Amazon introduced <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/alb-can-now-invoke-lambda-functions-to-serve-https-requests/">integration with Elastic Load Balancing</a>, which may be more cost efficient for high-load scenarios. However, the pricing is per hour, so good judgment is required.</p>
<p>Azure Functions comes with HTTP endpoint integration out of the box, and there is no additional cost for this integration.</p>
<h2 id="9-performance-and-scalability">9. Performance and Scalability</h2>
<p>AWS Lambda has been on the market longer than Azure Functions, and has a laser focus on the single-hosting model. Although there are no established industry-wide benchmarks, many claim that AWS Lambda is better for rapid scale-out and handling massive workloads, both for web APIs and queue-based applications. The bootstrapping delay effect—cold starts—are also less significant with Lambda.</p>
<p>Azure Functions has improved significantly in the last year or two, but Microsoft is still playing catch-up.</p>
<p>I published several comparison articles in the past:</p>
<ul>
<li><a href="https://mikhail.io/2019/serverless-at-scale-serving-stackoverflow-like-traffic/">Serverless at Scale: Serving StackOverflow-like Traffic</a></li>
<li><a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/">From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a></li>
<li><a href="https://mikhail.io/serverless/coldstarts/">Cold Starts in Serverless Functions</a></li>
</ul>
<h2 id="10-orchestrations">10. Orchestrations</h2>
<p>Serverless functions are nanoservices: small blocks of code doing just one thing. The question of how to build large applications and systems out of those tiny pieces is still open, but some composition patterns already exist.</p>
<p>Both AWS and Azure have dedicated services for workflow orchestration: AWS Step Functions and Azure Logic Apps. Quite often, functions are used as steps in those workflows, allowing them to stay independent but still solve significant tasks.</p>
<p>In addition, <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">Azure Durable Functions</a> is a library that brings workflow orchestration abstractions to code. It comes with several patterns to combine multiple serverless functions into stateful long-running flows. The library handles communication and state management robustly and transparently, while keeping the API surface simple.</p>
<h2 id="so-what-should-you-choose">So, What Should You Choose?</h2>
<p>AWS Lambda and Azure Functions are similar services, but the devil is in the details—and virtually every angle shows some essential distinctions between the two. My list of ten differences is certainly not exhaustive, and each aspect would need a separate article to cover it in full.</p>
<p>It’s unlikely that your choice will be driven purely by these differences. At the same time, whenever you have to choose one option over the other, or when <a href="https://www.iamondemand.com/blog/azure-user-heres-what-you-must-know-about-aws/">you switch between providers</a>, it’s crucial to adjust your thinking and practices to match the peculiarities.</p>
<p>In short, choose the option that fits you best!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How To Deploy a Function App with KEDA (Kubernetes-based Event-Driven Autoscaling)]]></title>
            <link href="https://mikhail.io/2019/10/how-to-deploy-a-function-app-with-keda/"/>
            <id>https://mikhail.io/2019/10/how-to-deploy-a-function-app-with-keda/</id>
            
            <published>2019-10-10T00:00:00+00:00</published>
            <updated>2019-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Hosting Azure Functions in Kubernetes: how it works and the simplest way to get started.</blockquote><p><strong>Azure Functions</strong> is a managed service for serverless applications in the Azure cloud. More broadly, Azure Functions is a runtime with multiple hosting possibilities. <strong>KEDA</strong> (<a href="https://cloudblogs.microsoft.com/opensource/2019/05/06/announcing-keda-kubernetes-event-driven-autoscaling-containers/">Kubernetes-based Event-Driven Autoscaling</a>) is an emerging option to host this runtime in <strong>Kubernetes</strong>.</p>
<p>In the first part of this post, I compare KEDA with cloud-based scaling and outline the required components. In the second part, I define infrastructure as code to deploy a sample KEDA application to an Azure Kubernetes Service (AKS) cluster.</p>
<p>The result is a fully working example and a high-level idea of how it works. Kubernetes expertise is not required!</p>
<h2 id="autoscaling-primer">Autoscaling Primer</h2>
<p>When you deploy an Azure Function, it runs within the Azure Functions runtime. The runtime is a host process which knows how to pull events from an <strong>event source</strong> (defined by the function trigger) and pass those to your function:</p>
<p><img src="runtime.png" alt="Azure Functions Runtime"></p>
<p>However, one instance of runtime rarely provides adequate processing capacity. If you only get one message per day, having an instance always running is wasteful. If you get thousands of events per second, one instance won’t be able to process all of them.</p>
<p>Automatic <strong>horizontal scaling</strong> solves the problem. At any point in time, several identical <strong>workers</strong> are crunching the events. The number N is optimized continuously to fit the current workload by adding new workers and removing underutilized ones.</p>
<figure >
    
        <img src="idea.png"
            alt="Components of an automatically scaled application"
             />
        
    
    <figcaption>
        <h4>Components of an automatically scaled application</h4>
    </figcaption>
    
</figure>
<p><strong>Instance Provisioner</strong> is an extra component in the auto-scaled system. It monitors the stream of metrics from the event source and decides to add or remove workers. A massive standby pool of idle generic workers provides the workforce. Such a generic worker pulls the <strong>artifact</strong> of the assigned function, plugs it into the runtime, and starts processing events.</p>
<h2 id="azure-functions-consumption-plan">Azure Functions Consumption Plan</h2>
<p><strong>Consumption Plan</strong> is the serverless hosting option where Azure manages all the scaling components internally. Let’s consider an example of an Azure Function triggered by a <strong>Storage Queue</strong>:</p>
<figure >
    
        <img src="functionapp.png"
            alt="Azure Functions Consumption Plan"
             />
        
    
    <figcaption>
        <h4>Azure Functions Consumption Plan</h4>
    </figcaption>
    
</figure>
<p>The deployment artifact is just the code packaged as a zip archive and uploaded to Blob Storage. <strong>Scale Controller</strong> is an internal Azure component that observes the target queue and allocates Function App instances based on the queue length fluctuations. Each instance bootstraps itself with a zip file, connects to the queue, and pulls messages to process.</p>
<p>The cloud provider manages all the components of the system, so developers can focus on writing business logic code. It can be <a href="/2019/08/ten-pearls-with-azure-functions-in-pulumi/">as simple as a JavaScript callback</a>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts">queue.onEvent(<span style="color:#a31515">&#34;MyHandler&#34;</span>, <span style="color:#00f">async</span> (context, msg) =&gt; {
   console.log(<span style="color:#a31515">&#34;New message: &#34;</span> + JSON.stringify(msg));
});
</code></pre></div><h2 id="keda">KEDA</h2>
<p>Over the last few years, Kubernetes has gained traction across many industries. KEDA is provides a way to design and run event-driven applications inside a Kubernetes cluster. KEDA implements the autoscaling components in terms of Kubernetes tools.</p>
<p>The target Function App is packaged together with the Azure Functions runtime into a custom <strong>Docker image</strong> and published to a <strong>registry</strong>. A Kubernetes <strong>deployment</strong> utilizes that image and configures the parameters to connect it to the target event source (for instance, a queue):</p>
<p><img src="k8s-deployment.png" alt="Kubernetes Deployment"></p>
<p>The application can then run on one or many instances, or <strong>pods</strong> in Kubernetes terms. Kubernetes has a built-in component to scale out the pods: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> (HPA). By default, HPA makes scaling decisions based on the processor utilization of existing pods. CPU turns out to be a poor metric for event-driven applications: many workloads are not CPU-bound, so the scale-out won’t be aggressive enough to keep the queue empty. Therefore, KEDA introduces a <strong>ScaledObject</strong>&mdash;a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource</a> which pulls metric values from the event source and feeds them as a custom metric to HPA:</p>
<figure >
    
        <img src="keda.png"
            alt="Kubernetes-based Event-Driven Autoscaling"
             />
        
    
    <figcaption>
        <h4>Kubernetes-based Event-Driven Autoscaling</h4>
    </figcaption>
    
</figure>
<p>At the time of the initial KEDA preview announcement and until Kubernetes version 1.16, Horizontal Pod Autoscaler wasn’t able to scale a deployment down to zero pods. Therefore, KEDA includes an extra controller to disable the deployment if the event source is empty and re-enable it when new events come in. This duty might be delegated back to HPA in the future versions.</p>
<p>Now, when we know what KEDA is and how it works, it’s time to deploy a Function App!</p>
<h2 id="deploy-a-function-app-with-keda-and-pulumi">Deploy a Function App with KEDA and Pulumi</h2>
<p>Here is the list of the components required to run a Function App in Kubernetes with KEDA:</p>
<ul>
<li>Azure Kubernetes Service cluster</li>
<li>Azure Container Registry</li>
<li>Helm chart keda-edge with KEDA service and custom resource definition</li>
<li>Azure Storage Queue as a sample event source</li>
<li>Docker image of the queue processor application</li>
<li>Deployment of this image</li>
<li>Scaled Object custom resource</li>
</ul>
<p>All these components can be defined and deployed within a single Pulumi program. Below I highlight the main blocks of the program. Navigate to the <a href="#reusable-components">following section</a> to look at a short and reusable component.</p>
<h3 id="aks-cluster">AKS Cluster</h3>
<p>KEDA can run on any Kubernetes cluster, but I choose to do so on managed AKS. My custom <code>AksCluster</code> component defines a cluster, including the required Active Directory and networking configuration. It makes multiple assumptions, so I only need to specify the main properties:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;keda-sample&#34;</span>);

<span style="color:#00f">const</span> aks = <span style="color:#00f">new</span> AksCluster(<span style="color:#a31515">&#34;keda-cluster&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    kubernetesVersion: <span style="color:#a31515">&#34;1.13.5&#34;</span>,
    vmSize: <span style="color:#a31515">&#34;Standard_B2s&#34;</span>,
    vmCount: <span style="color:#2b91af">3</span>,
});
</code></pre></div><p><code>aks.cluster</code> now has all the required output values, for instance, <code>aks.cluster.kubeConfigRaw</code> configuration.</p>
<h3 id="shared-cluster-components">Shared Cluster Components</h3>
<p>The next group of components needs to be deployed just once.</p>
<p>Azure Container Registry is not part of the AKS cluster. Its purpose is to host Docker images of applications.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> registry = <span style="color:#00f">new</span> azure.containerservice.Registry(<span style="color:#a31515">&#34;registry&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">args.resourceGroup.name</span>,
    adminEnabled: <span style="color:#2b91af">true</span>,
    sku: <span style="color:#a31515">&#34;Premium&#34;</span>,
});
</code></pre></div><p>A Helm chart <code>kedacore/keda-edge</code> deploys the KEDA service and the <code>ScaledObject</code> custom resource definition.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> keda = <span style="color:#00f">new</span> k8s.helm.v2.Chart(<span style="color:#a31515">&#34;keda-edge&#34;</span>, {
    repo: <span style="color:#a31515">&#34;kedacore&#34;</span>,
    chart: <span style="color:#a31515">&#34;keda-edge&#34;</span>,
    version: <span style="color:#a31515">&#34;0.0.1-2019.07.24.21.37.42-8ffd9a3&#34;</span>,
    values: {
        logLevel: <span style="color:#a31515">&#34;debug&#34;</span>,
    },
}, { providers: { kubernetes: <span style="color:#2b91af">aks.provider</span> } });
</code></pre></div><h3 id="event-source">Event Source</h3>
<p>KEDA supports multiple types of event sources, and the current list is available <a href="https://github.com/kedacore/keda#event-sources-and-scalers">here</a>. An event source is supported if there is a scaler which knows how to pull metrics out of it and turn them into a custom metric for HPA.</p>
<p>My example uses an Azure Storage Queue as the event source:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;kedasa&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
    accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
});

<span style="color:#00f">const</span> queue = <span style="color:#00f">new</span> azure.storage.Queue(<span style="color:#a31515">&#34;kedaqueue&#34;</span>, {
    storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
});
</code></pre></div><h3 id="docker-image">Docker Image</h3>
<p>The same Pulumi program is capable of building a Docker image and uploading it to the Container Registry.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> dockerImage = <span style="color:#00f">new</span> docker.Image(<span style="color:#a31515">&#34;image&#34;</span>, {
    imageName: <span style="color:#2b91af">pulumi.interpolate</span><span style="color:#a31515">`</span><span style="color:#a31515">${</span>registry.loginServer<span style="color:#a31515">}</span><span style="color:#a31515">/</span><span style="color:#a31515">${</span>args.queue.name<span style="color:#a31515">}</span><span style="color:#a31515">:v1.0.0`</span>,
    build: {
        context: <span style="color:#a31515">&#34;./functionapp&#34;</span>,
    },
    registry: {
        server: <span style="color:#2b91af">registry.loginServer</span>,
        username: <span style="color:#2b91af">registry.adminUsername</span>,
        password: <span style="color:#2b91af">registry.adminPassword</span>,
    },
});
</code></pre></div><p>The image refers to the folder with a <code>Dockerfile</code> in it and uses the <code>registry</code> variables to fill the credentials.</p>
<h3 id="deployment">Deployment</h3>
<p>Now, we can define a Deployment which uses the Docker image to run our Function App on Kubernetes pods.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> appLabels = { app: <span style="color:#2b91af">name</span> };
<span style="color:#00f">const</span> deployment = <span style="color:#00f">new</span> k8s.apps.v1.Deployment(name, {
    apiVersion: <span style="color:#a31515">&#34;apps/v1&#34;</span>,
    kind: <span style="color:#a31515">&#34;Deployment&#34;</span>,
    metadata: {
        labels: <span style="color:#2b91af">appLabels</span>,
    },
    spec: {
        selector: { matchLabels: <span style="color:#2b91af">appLabels</span> },
        template: {
            metadata: {
                labels: <span style="color:#2b91af">appLabels</span>,
            },
            spec: {
                containers: [{
                    name,
                    image: <span style="color:#2b91af">dockerImage.imageName</span>,
                    env: [{ name: <span style="color:#a31515">&#34;queuename&#34;</span>, value: <span style="color:#2b91af">args.queue.name</span> }],
                    envFrom: [{ secretRef: {name: <span style="color:#2b91af">secretQueue.metadata.name</span> } }],
                }],
                imagePullSecrets: [{ name: <span style="color:#2b91af">args.service.registrySecretName</span> }],
            },
        },
    },
}, { provider: <span style="color:#2b91af">aks.provider</span> });
</code></pre></div><p>The deployment refers to the secret value which stores the connection string to our target Storage Queue:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> secretQueue = <span style="color:#00f">new</span> k8s.core.v1.Secret(<span style="color:#a31515">&#34;queue-secret&#34;</span>, {
    data: {
        queueConnectionString:
            <span style="color:#2b91af">args.storageAccount.primaryConnectionString.apply</span>(c =&gt; Buffer.<span style="color:#00f">from</span>(c).toString(<span style="color:#a31515">&#34;base64&#34;</span>)),
    },
}, { provider: <span style="color:#2b91af">aks.provider</span> });
</code></pre></div><h3 id="scaled-object">Scaled Object</h3>
<p>Finally, we need to deploy an instance of the <code>ScaledObject</code> custom resource, which takes care of feeding the metrics from the queue to the Horizontal Pod Autoscaler.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> scaledObject = <span style="color:#00f">new</span> k8s.apiextensions.CustomResource(<span style="color:#a31515">&#34;scaledobject&#34;</span>, {
    apiVersion: <span style="color:#a31515">&#34;keda.k8s.io/v1alpha1&#34;</span>,
    kind: <span style="color:#a31515">&#34;ScaledObject&#34;</span>,
    metadata: {
        labels: { deploymentName: <span style="color:#2b91af">name</span> },
    },
    spec: {
        scaleTargetRef: { deploymentName: <span style="color:#2b91af">name</span> },
        triggers: [{
            <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;azure-queue&#34;</span>,
            metadata: {
                <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;queueTrigger&#34;</span>,
                connection: <span style="color:#a31515">&#34;queueConnectionString&#34;</span>,
                queueName: <span style="color:#2b91af">args.queue.name</span>,
                name: <span style="color:#a31515">&#34;myQueueItem&#34;</span>,
            },
        }],
    },
}, { provider: <span style="color:#2b91af">aks.provider</span> });
</code></pre></div><h2 id="reusable-components">Reusable Components</h2>
<p>We can simplify the program further by creating reusable Pulumi components for a Cluster, a Service, and a Function app. After those are done, here is all the code required to deploy a Kubernetes cluster with a Function App and KEDA.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Create an AKS K8s cluster
</span><span style="color:#008000"></span><span style="color:#00f">const</span> aks = <span style="color:#00f">new</span> AksCluster(<span style="color:#a31515">&#34;keda-cluster&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    kubernetesVersion: <span style="color:#a31515">&#34;1.13.5&#34;</span>,
    vmSize: <span style="color:#a31515">&#34;Standard_B2s&#34;</span>,
    vmCount: <span style="color:#2b91af">3</span>,
});

<span style="color:#008000">// Deploy shared components of KEDA (container registry, kedacore/keda-edge Helm chart)
</span><span style="color:#008000"></span><span style="color:#00f">const</span> service = <span style="color:#00f">new</span> KedaService(<span style="color:#a31515">&#34;keda-edge&#34;</span>, {
    resourceGroup,
    k8sProvider: <span style="color:#2b91af">aks.provider</span>,
});

<span style="color:#008000">// Deploy a Function App which subscribes to the Storage Queue
</span><span style="color:#008000"></span><span style="color:#00f">const</span> app = <span style="color:#00f">new</span> KedaStorageQueueHandler(<span style="color:#a31515">&#34;queue-handler&#34;</span>, {
    resourceGroup,
    storageAccount,
    queue,
    service,
    path: <span style="color:#a31515">&#34;./functionapp&#34;</span>,
});
</code></pre></div><p>You can find the full code of the components, the sample program, and steps to run it in <a href="https://github.com/pulumi/examples/tree/master/azure-ts-aks-keda">this example</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As of October 2019, the KEDA project is still in the experimental phase and should not be used for any production applications. Also, if the managed version of Azure Functions suits your needs, you should probably stick to that service. It requires less effort, provides high-level primitives, and enables unlimited elastic scaling.</p>
<p>However, if your company is betting on Kubernetes as the standard application platform, but you still see high value in event-driven fine-grained applications, KEDA might be an exciting option for the future.</p>
<p>Pulumi is a great tool to get a sample KEDA application running quickly and effortlessly. It combines Docker image creation, Kubernetes cluster provisioning, Helm chart installation, and KEDA deployment in the single program written in a familiar general-purpose language.</p>
<p>Get started with <a href="https://github.com/pulumi/examples/tree/master/azure-ts-aks-keda">Azure Kubernetes Service (AKS) Cluster and Azure Functions with KEDA</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/kubernetes" term="kubernetes" label="Kubernetes" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Cosmos DB]]></title>
            <link href="https://mikhail.io/tags/azure-cosmos-db/"/>
            <id>https://mikhail.io/tags/azure-cosmos-db/</id>
            
            <published>2019-09-24T00:00:00+00:00</published>
            <updated>2019-09-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How To Build Globally Distributed Applications with Azure Cosmos DB and Pulumi]]></title>
            <link href="https://mikhail.io/2019/09/how-to-build-globally-distributed-applications-with-azure-cosmos-db-and-pulumi/"/>
            <id>https://mikhail.io/2019/09/how-to-build-globally-distributed-applications-with-azure-cosmos-db-and-pulumi/</id>
            
            <published>2019-09-24T00:00:00+00:00</published>
            <updated>2019-09-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>A reusable component to build highly-available, low-latency applications on Azure</blockquote><p>In a <a href="/2019/07/globally-distributed-serverless-application-in-100-lines-of-code/">previous blog post</a>, I shared how easy it is to create a globally distributed, highly-available, low-latency application with Azure Functions, Azure Cosmos DB, and Pulumi.</p>
<p>Today, I want to show how the same approach can be generalized for any cloud compute service, including containers, virtual machines, and serverless functions.</p>
<h2 id="e-commerce-example">E-commerce Example</h2>
<p>Let’s consider an example of an e-commerce website which targets a worldwide audience. The architecture of such a website should address several challenges.</p>
<h3 id="multiple-teams">Multiple Teams</h3>
<p>A large e-commerce website is a sophisticated software application and consists of many subcomponents. Many engineers in multiple teams work on it simultaneously.</p>
<p>A typical approach is to organize engineers into smaller teams and make each team autonomous and responsible for one or several business domains. This strategy leads to microservices architecture when the application is built out of loosely coupled components. Each component represents a &ldquo;vertical slice&rdquo;: a team owns functionality across the vertical stack, from its user interface and service API to the data storage layer and underlying infrastructure.</p>
<p>Here is an example of such a breakdown:</p>
<p><img src="website.png" alt="Multiple components on a single web page"></p>
<figcaption>
    <h4>
        Multiple components on a single web page
    </h4>
</figcaption>
<h3 id="diversity-of-technology-decisions">Diversity of Technology Decisions</h3>
<p>One of the guiding principles of microservices architecture is that each team makes decisions regarding which tools and technologies to use. They are free to choose programming languages, databases, or cloud services that are the best fit for the service. In practice, some restrictions do apply&mdash;for instance, the teams might need to share the same cloud provider.</p>
<p>In our hypothetical example, the whole product is hosted on Azure cloud, however other aspects differ:</p>
<ul>
<li>Product Page team runs JavaScript serverless functions and stores data in a document database</li>
<li>Shopping Cart team uses C# and .NET Core, packages the code in a Docker container, and uses event sourcing to store every action ever made by users while shopping</li>
<li>Pricing Engine team has a mix of Java and C++ code with a bunch of third-party libraries and deploys the services to Ubuntu VMs that use MongoDB drivers for storage</li>
</ul>
<h3 id="global-presence">Global Presence</h3>
<p>Regardless of the tech, the user-facing services must serve customers across the globe and serve them fast. This requirement demands a geo-distributed infrastructure so that user requests could be handled by the nearest data center to minimize the latency and ensure excellent user experience.</p>
<p>Each team has to solve the following problems in a way which fits their tech stack:</p>
<ul>
<li>Distribute the data across the globe</li>
<li>Run their compute workload in each region next to the data</li>
<li>Manage all this infrastructure in a reliable way</li>
</ul>
<p>Let&rsquo;s consider these goals and possible solutions.</p>
<h2 id="azure-cosmos-db">Azure Cosmos DB</h2>
<p>Distributing data across many data centers while keeping them consistent and available is one of the hardest problems in computer science and engineering. Delegating this challenge to a specialized service or product is usually a sensible idea.</p>
<p>Azure Cosmos DB is the database of choice for global applications running in the Azure cloud. Cosmos DB provides a turn-key global distribution to any number of regions worldwide. Regions can be added or removed along the way while running production workloads and without having any impact on high availability. The accounts can be provisioned with a single write region or in multi-master mode with every region being writable, thus enabling writes that are local to each region.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="distributed-cosmos.svg"
            alt="Azure Cosmos DB with multiple locations"
             />
        
    
    <figcaption>
        <h4>Azure Cosmos DB with multiple locations</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Cosmos DB supports multiple models of data: key-value, document, column family, graph, with wire-protocol compatible APIs for MongoDB, Apache Cassandra, SQL, etcd, Gremlin, etc. It also supports multiple well-defined and practical consistency levels, from eventual to strong consistency, so that each application can strike the right balance between latency, availability, and consistency guarantees. All of this is offered as a fully-managed service, with the comprehensive SLAs covering latency, throughput, consistency, and 99.999% high availability for both reads and writes.</p>
<p>Sounds great, but how do we build applications to leverage this power?</p>
<h2 id="distributed-web-applications">Distributed Web Applications</h2>
<p>In contrast to Cosmos accounts, compute services in Azure are located in a single region. This holds true for Virtual Machines, Container Instances, Serverless Functions, and managed Azure Kubernetes Service.</p>
<p>If an application is deployed to a single location, we are not making good use of geographic redundancy of the database. If a user happens to be far from the application region, their request has to travel across the globe. Moreover, the application connects to the nearest region of Cosmos DB, while all other regions stay idle:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="distributed-cosmos-single-app.svg"
            alt="Application deployed in a single location"
             />
        
    
    <figcaption>
        <h4>Application deployed in a single location</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Instead, we should deploy a copy of application infrastructure to each of the target regions:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="distributed-app.svg"
            alt="Application deployed in multiple locations"
             />
        
    
    <figcaption>
        <h4>Application deployed in multiple locations</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Each application instance has to be configured to connect to the endpoint of the Cosmos DB in the same region to enjoy local single-digit-milliseconds latency for reads and writes.</p>
<p>While beneficial to end-users, such setup brings much extra complexity in terms of infrastructure management:</p>
<ul>
<li>Application regions must stay in sync with Cosmos DB regions, including the correct configuration of preferred locations.</li>
<li>With the growth of the number of regions, the traditional capacity planning gets harder and harder. This makes a dynamic auto-scaling configuration increasingly important.</li>
<li>A central routing service must be configured to flow the traffic from end-users to the closest application instance.</li>
<li>Each team must execute all these tasks, independently but coherently.</li>
</ul>
<p>Let’s see how infrastructure as code and a general-purpose programming language come to the rescue.</p>
<h2 id="search-for-a-reusable-abstraction">Search for a Reusable Abstraction</h2>
<p>In our example, three teams have a common goal of providing high-quality service to customers around the world, but they differ in the compute services they want to use.</p>
<p>Product Card team relies on serverless functions, so their infrastructure may look like this:</p>
<p><img src="functions.png" alt="Global application with Azure Functions"></p>
<figcaption>
    <h4>
        Global application with Azure Functions
    </h4>
</figcaption>
<p>Shopping Cart team packages their code as a Docker container, which they put into Azure Container Registry, and then use Azure Container Instances to run the application. The Registry is a global Azure resource, while Container Instances are deployed separately per region.</p>
<p><img src="containers.png" alt="Global application with Azure Container Instances"></p>
<figcaption>
    <h4>
        Global application with Azure Container Instances
    </h4>
</figcaption>
<p>Pricing Engine team runs their code on Infrastructure-as-a-Service (IaaS), so they have a task of creating Virtual Machine images, and then deploying those to an Azure Virtual Machine Scale Set, fronting it with a Load Balancer, and configuring all the networking infrastructure including Virtual Networks, Subnets, Public IPs, Network Security Groups, etc. Once again, this has to be done for each location independently.</p>
<p><img src="vmscalesets.png" alt="Global application with Virtual Machines"></p>
<figcaption>
    <h4>
        Global application with Virtual Machines
    </h4>
</figcaption>
<p>Each team has the same global structure of infrastructure, including a Traffic Manager front-end and a Cosmos DB data store. At the same time, the per-region compute infrastructure differs substantially.</p>
<p>Instead of copying the common pieces of infrastructure between the teams, we can think of a reusable abstraction which could be shared by every service.</p>
<p><img src="globalapp.png" alt="Global application pattern"></p>
<figcaption>
    <h4>
        Global application pattern
    </h4>
</figcaption>
<p>For this blog post, I created a custom Pulumi component called CosmosApp which implements such abstraction and simplifies the provisioning of global web applications. The component creates distributed Cosmos DB resources, as well as the front-end routing component, while allowing pluggable compute layer implementation.</p>
<h2 id="cosmos-app-with-azure-functions">Cosmos App with Azure Functions</h2>
<p>Let&rsquo;s look at an example of how the component can be used. Creating a globally-distributed application requires three steps:</p>
<h3 id="list-the-regions">List the Regions</h3>
<p>We define a configuration value to contain a comma-separated list of Azure regions that we’ll deploy the application to.</p>
<pre><code>$ pulumi config set locations WestUS,WestEurope,SouthEastAsia,SouthBrazil,AustraliaCentral
</code></pre><p>This value can be loaded and parsed into a JavaScript array:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> locations = <span style="color:#00f">new</span> pulumi.Config().<span style="color:#00f">require</span>(<span style="color:#a31515">&#34;locations&#34;</span>).split(<span style="color:#a31515">&#34;,&#34;</span>);
</code></pre></div><h3 id="build-a-region-template">Build a Region Template</h3>
<p>We define a factory function to create the infrastructure in each location. Its signature is a bit unusual in that we return a function from another function. I&rsquo;ll explain the motivation in the next example. For now, it&rsquo;s important to understand the following:</p>
<ul>
<li>We get a Cosmos account and a location as input arguments</li>
<li>We create a Function App in the specified location with connection settings pointing to the local Cosmos DB replica</li>
<li>We return the identifier of the Function App so that a Traffic Manager endpoint could point to it</li>
</ul>
<p>Here is the function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">function</span> buildProductApp({ cosmosAccount }: GlobalContext) {
    <span style="color:#00f">return</span> ({ location }: RegionalContext) =&gt; {
        <span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.ArchiveFunctionApp(<span style="color:#a31515">&#34;function-app&#34;</span>, {
            resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
            location,
            archive: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./app&#34;</span>),
            appSettings: {
                COSMOSDB_ENDPOINT: <span style="color:#2b91af">cosmosAccount.endpoint</span>,
                COSMOSDB_KEY: <span style="color:#2b91af">cosmosAccount.primaryMasterKey</span>,
                COSMOSDB_LOCATION: <span style="color:#2b91af">location</span>,
            },
        });

        <span style="color:#00f">return</span> {
            id: <span style="color:#2b91af">app.functionApp.id</span>,
        };
    };
}
</code></pre></div><h3 id="instantiate-a-cosmos-app">Instantiate a Cosmos App</h3>
<p>Finally, we use the <code>buildProductApp</code> factory function to create the geo-distributed application:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> products = <span style="color:#00f">new</span> CosmosApp(<span style="color:#a31515">&#34;products&#34;</span>, {
    resourceGroup,
    locations,
    databaseName: <span style="color:#a31515">&#34;productsdb&#34;</span>,
    containerName: <span style="color:#a31515">&#34;products&#34;</span>,
    factory: <span style="color:#2b91af">buildProductApp</span>,
});
</code></pre></div><p>[ <a href="https://github.com/pulumi/examples/tree/master/azure-ts-cosmosapp-component/functionApp.ts">Full Example</a> ]</p>
<h2 id="cosmos-app-with-container-instances">Cosmos App with Container Instances</h2>
<p>The Shopping Cart team will have to follow the same steps but define their compute infrastructure accordingly.</p>
<p>A significant difference to the Functions-based example is the fact that the Container-based application needs some infrastructure to be shared across regions. The top block of the factory function defines an instance of Azure Container Registry, builds a Docker image, and puts that image into the Registry:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">function</span> buildShoppingCartApp({ cosmosAccount, database, container }: GlobalContext) {

    <span style="color:#00f">const</span> registry = <span style="color:#00f">new</span> azure.containerservice.Registry(<span style="color:#a31515">&#34;global&#34;</span>, {
        resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
        adminEnabled: <span style="color:#2b91af">true</span>,
        sku: <span style="color:#a31515">&#34;Premium&#34;</span>,
    }, opts);

    <span style="color:#00f">const</span> dockerImage = <span style="color:#00f">new</span> docker.Image(<span style="color:#a31515">&#34;node-app&#34;</span>, {
        imageName: <span style="color:#2b91af">pulumi.interpolate</span><span style="color:#a31515">`</span><span style="color:#a31515">${</span>registry.loginServer<span style="color:#a31515">}</span><span style="color:#a31515">/mynodeapp:v1.0.0`</span>,
        build: {
            context: <span style="color:#a31515">&#34;./container&#34;</span>,
        },
        registry: {
            server: <span style="color:#2b91af">registry.loginServer</span>,
            username: <span style="color:#2b91af">registry.adminUsername</span>,
            password: <span style="color:#2b91af">registry.adminPassword</span>,
        },
    });

    <span style="color:#00f">return</span> ({ location }: RegionalContext) =&gt; {
        <span style="color:#008000">// ... see the next sample
</span><span style="color:#008000"></span>    };
}
</code></pre></div><p>Now, the factory function that builds per-region infrastructure can directly reference the Registry and the image to deploy an Azure Container Instances group. Note how <code>registry</code> and <code>dockerImage</code> are used as any other TypeScript variable inside the closure factory function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">function</span> buildShoppingCartApp({ cosmosAccount, database, container }: GlobalContext) {

    <span style="color:#00f">const</span> registry = <span style="color:#00f">new</span> azure.containerservice.Registry(<span style="color:#008000">/* ... */</span>);
    <span style="color:#00f">const</span> dockerImage = <span style="color:#00f">new</span> docker.Image(<span style="color:#008000">/* ... */</span>);

    <span style="color:#00f">return</span> ({ location }: RegionalContext) =&gt; {
        <span style="color:#00f">const</span> group = <span style="color:#00f">new</span> azure.containerservice.Group(<span style="color:#a31515">`aci-</span><span style="color:#a31515">${</span>location<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
            resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
            location,
            imageRegistryCredentials: [{
                server: <span style="color:#2b91af">registry.loginServer</span>,
                username: <span style="color:#2b91af">registry.adminUsername</span>,
                password: <span style="color:#2b91af">registry.adminPassword</span>,
            }],
            osType: <span style="color:#a31515">&#34;Linux&#34;</span>,
            containers: [
                {
                    cpu: <span style="color:#2b91af">0.5</span>,
                    image: <span style="color:#2b91af">dockerImage.imageName</span>,
                    memory: <span style="color:#2b91af">1.5</span>,
                    name: <span style="color:#a31515">&#34;web-server&#34;</span>,
                    ports: [{
                        port: <span style="color:#2b91af">80</span>,
                        protocol: <span style="color:#a31515">&#34;TCP&#34;</span>,
                    }],
                    environmentVariables: {
                        ENDPOINT: <span style="color:#2b91af">cosmosAccount.endpoint</span>,
                        MASTER_KEY: <span style="color:#2b91af">cosmosAccount.primaryMasterKey</span>,
                        DATABASE: <span style="color:#2b91af">database.name</span>,
                        COLLECTION: <span style="color:#2b91af">container.name</span>,
                        LOCATION: <span style="color:#2b91af">location</span>,
                    },
                },
            ],
            ipAddressType: <span style="color:#a31515">&#34;public&#34;</span>,
            dnsNameLabel: <span style="color:#a31515">`acishop-</span><span style="color:#a31515">${</span>location<span style="color:#a31515">}</span><span style="color:#a31515">`</span>,
        });

        <span style="color:#00f">return</span> {
            url: <span style="color:#2b91af">group.fqdn</span>,
        };
    };
}
</code></pre></div><p>Finally, a <code>CosmosApp</code> is defined:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> aci = <span style="color:#00f">new</span> CosmosApp(<span style="color:#a31515">&#34;aci&#34;</span>, {
    resourceGroup,
    locations,
    databaseName: <span style="color:#a31515">&#34;cartdb&#34;</span>,
    containerName: <span style="color:#a31515">&#34;items&#34;</span>,
    factory: <span style="color:#2b91af">buildShoppingCartApp</span>,
    enableMultiMaster: <span style="color:#2b91af">true</span>,
});
</code></pre></div><p>Shopping Cart workload expects both reads and writes from the end-users, so the team decides to enable multi-master support in their Cosmos DB instance.</p>
<p>[ <a href="https://github.com/pulumi/examples/tree/master/azure-ts-cosmosapp-component/aci.ts">Full Example</a> ]</p>
<h2 id="cosmos-app-with-virtual-machine-scale-sets">Cosmos App with Virtual Machine Scale Sets</h2>
<p>The Pricing Engine team has to go through the same steps but employ IaaS-based infrastructure to run application code. The factory function creates a dozen resources related to virtual machines, load balancing, networking, and auto-scaling. You may define as many resources as needed in the code, as long as it returns a pointer to the proper resource to link the endpoint.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">function</span> buildVMScaleSetApp({ cosmosAccount, database }: GlobalContext) {

    <span style="color:#008000">// Build a VM image here...
</span><span style="color:#008000"></span>
    <span style="color:#008000">// Define a MongoDB compatible collection
</span><span style="color:#008000"></span>    <span style="color:#00f">const</span> collection = <span style="color:#00f">new</span> azure.cosmosdb.MongoCollection(<span style="color:#a31515">`mongo-</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
        resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
        accountName: <span style="color:#2b91af">cosmosAccount.name</span>,
        databaseName: <span style="color:#2b91af">database.name</span>,
    });

    <span style="color:#00f">return</span> ({ location }: RegionalContext) =&gt; {

        <span style="color:#00f">const</span> publicIp = <span style="color:#00f">new</span> azure.network.PublicIp(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> loadBalancer = <span style="color:#00f">new</span> azure.lb.LoadBalancer(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> bpepool = <span style="color:#00f">new</span> azure.lb.BackendAddressPool(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> probe = <span style="color:#00f">new</span> azure.lb.Probe(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> rule = <span style="color:#00f">new</span> azure.lb.Rule(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> vnet = <span style="color:#00f">new</span> azure.network.VirtualNetwork(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> subnet = <span style="color:#00f">new</span> azure.network.Subnet(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> scaleSet = <span style="color:#00f">new</span> azure.compute.ScaleSet(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">const</span> autoscale = <span style="color:#00f">new</span> azure.monitoring.AutoscaleSetting(<span style="color:#008000">/* ... */</span>);

        <span style="color:#00f">return</span> {
            id: <span style="color:#2b91af">publicIp.id</span>,
        };
    };
}
</code></pre></div><p>The code also uses a collection with the MongoDB protocol compatibility because that’s what the application expects.</p>
<p>[ <a href="https://github.com/pulumi/examples/tree/master/azure-ts-cosmosapp-component/vms.ts">Full Example</a> ]</p>
<h2 id="conclusion">Conclusion</h2>
<p>Azure Cloud Platform offers an amazing array of capabilities for application developers. Azure Cosmos DB takes care of distributing the data globally, reliably, consistently, and with low-latency access anywhere around the world. Nonetheless, developers still need to take care of adequately provisioning the application layer for efficiently serving users around the globe.</p>
<p>Cosmos App component is an excellent example of the power of using a general-purpose programming language to define cloud infrastructure. Such a component can let you codify the best practices and let multiple teams or companies share the implementation. The component and applications can evolve independently, with tools like versioning, package managers, compilers, and automated tests helping to maintain high-quality standards.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure-cosmos-db" term="azure-cosmos-db" label="Azure Cosmos DB" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Epsagon]]></title>
            <link href="https://mikhail.io/tags/epsagon/"/>
            <id>https://mikhail.io/tags/epsagon/</id>
            
            <published>2019-08-23T00:00:00+00:00</published>
            <updated>2019-08-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How to Avoid Cost Pitfalls by Monitoring APIs in AWS Lambda]]></title>
            <link href="https://mikhail.io/2019/08/how-to-avoid-cost-pitfalls-by-monitoring-apis-in-aws-lambda/"/>
            <id>https://mikhail.io/2019/08/how-to-avoid-cost-pitfalls-by-monitoring-apis-in-aws-lambda/</id>
            
            <published>2019-08-23T00:00:00+00:00</published>
            <updated>2019-08-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>How to monitor your APIs using serverless technologies and an Epsagon dashboard.</blockquote><p>The modern software world runs on application programming interfaces, otherwise known as APIs. There is an API for everything: charging a credit card, sending an email, finding an address on a map, or hailing a taxi.</p>
<p>Likewise, the huge computing power of the cloud is entirely accessible via multiple layers of programming interfaces, including HTTP services and language-specific SDKs. Cloud vendors such as AWS have thousands of endpoints available to every developer in the world and modern businesses ought to embrace these existing services to stay productive. Instead of trying to reinvent the wheel, businesses should focus on the core values and differentiators for their specific market and simply purchase, adopt and reuse the remaining services required from third parties that have proven themselves in their given niche.</p>
<p>This principle stands at the core of the philosophy behind serverless architecture: focus on the crucial bits. Serverless tech has a low barrier of entry and was designed with the intention to require little boilerplate code to use. This helps to explain why so many reliable <a href="https://epsagon.com/blog/the-importance-and-impact-of-apis-in-serverless/">APIs are an inherent part</a> of serverless applications today. Lambda functions, for their part, are used by developers as the glue between cloud services and internal as well as external API calls.</p>
<h2 id="role-of-performance">Role of Performance</h2>
<p>One thing to keep in mind when adopting APIs, in general, is performance. Most API calls are usually synchronous requests over HTTP or HTTPS. They may be fast to execute, but they may also be slow and responsiveness is not manifested in the API definition itself. Moreover, the latency may vary over time, as APIs can crash at seemingly random intervals or under higher loads. Throttling strategies might also kick in once the workload passes a given threshold.</p>
<p>This aspect of performance is arguably more decisive in the serverless world than ever before, as slow downstream APs for serverless platforms have numerous implications:</p>
<ul>
<li>Each Lambda function has a maximum duration threshold and it&rsquo;s going to time out and fail if the total duration of API calls goes above that value.</li>
<li>AWS charges organizations for execution times for Lambda invocations. Even when a function is idle while waiting for the HTTP response, you pay for each 100 milliseconds consumed. Slow API calls lead to the inevitable <a href="https://epsagon.com/blog/how-much-does-aws-lambda-cost/">extra cost of the serverless application</a>.</li>
<li>For synchronous invocations, when a user is waiting for the Lambda to complete, substantial slowdowns lead to poor user experience, as unhappy customers and lose time, and often, revenue.</li>
</ul>
<p>Luckily, a lot can be done to prevent and mitigate such issues.</p>
<h2 id="instrumenting-api-performance">Instrumenting API Performance</h2>
<p>There are several strategies to assess the impact that the performance of a third-party API has on your serverless application.</p>
<p>While just getting started with a new API, it makes sense to research if the supplier has provided a performance SLA or not. Regardless of the answer, the next step is to execute the target calls and measure the latency profile. You should strive to do so well in advance, before signing up for a new service. Finally, it&rsquo;s worth running a load test to emulate your target workload over an extended period.</p>
<p>Your job is still not done even after an application is successfully deployed to production. For any non-trivial application, it&rsquo;s worth investing in monitoring tools and best practices. A monitoring toolkit provides both real-time and historical perspectives on the performance of your dependencies as well as their impact on application usability and cost.</p>
<p>The following section gives a sneak peek of such activities.</p>
<h2 id="example-geo-alerting-application">Example: Geo-Alerting Application</h2>
<p>Let&rsquo;s consider a sample application that processes telemetry from connected vehicles. Each vehicle sends periodic messages, which end up in the AWS cloud. A single AWS Lambda receives a message and executes the following steps:</p>
<ul>
<li>Decodes the payload and extracts data, including geo-coordinates and sensor values.</li>
<li>Reverse geocodes the coordinates to infer the address with a call to the <a href="https://geocode.xyz/">xyz</a> service.</li>
<li>Saves a log entry containing the message properties and addresses to an <a href="https://aws.amazon.com/dynamodb/">AWS DynamoDB</a>.</li>
<li>Compares the values against thresholds to check whether to send an alert.</li>
<li>If needed, loads a suitable email template from an <a href="https://aws.amazon.com/s3/">AWS S3</a> bucket.</li>
<li>Sends an email notification using the <a href="https://sendgrid.com/">SendGrid</a> mailing service.</li>
</ul>
<p>It&rsquo;s a reasonably straightforward application and yet it depends on four external APIs: two AWS services and two third-party endpoints. Here, we will implement a prototype of such an application and link it to an <a href="https://epsagon.com/">Epsagon</a> dashboard to see what kind of insights we can get about its performance.</p>
<h2 id="timeline-of-a-single-invocation">Timeline of a Single Invocation</h2>
<p>Let&rsquo;s start by calling my AWS Lambda once, then find that invocation in the Epsagon dashboard and display the <a href="https://epsagon.com/blog/introducing-the-timeline-view/">timeline view</a>:</p>
<p><img src="timeline.png" alt="The Epsagon dashboard: A Timeline view of a single call."></p>
<figcaption><h4>The Epsagon dashboard: A Timeline view of a single call.</h4></figcaption>
<p>You can see that this single invocation took 827 milliseconds to complete and that this time was entirely spent on waiting for the API calls to complete. You can also clearly see the breakdown of the execution time between the four dependencies and that the geocoding and email sending is slower than the calls to DynamoDB and S3.</p>
<p>The timeline view looks very familiar to any developer who has ever worked with performance and network tabs as part of a browser&rsquo;s developer tools. And it&rsquo;s beneficial for a similar purpose: hunting down the culprit of slow execution.</p>
<p>It&rsquo;s worth mentioning that you don&rsquo;t have to track the calls manually. All the data is collected automatically by the Epsagon library once you plug it into the Lambda.</p>
<h2 id="tracking-invocations-over-time">Tracking Invocations Over Time</h2>
<p>The above was just one measurement, which might not be very representative of reality. The next step is to see how the target AWS Lambda behaves over time. Let&rsquo;s set up a test to execute the function several hundred times over several minutes. It&rsquo;s easiest to use AWS API Gateway and invoke the function via HTTP. Alternatively, you could use an SQS queue or another asynchronous trigger, but monitoring behaves similarly in all cases.</p>
<p>After the test, go ahead and navigate to the Architecture Map page:</p>
<p><img src="architecturemap.png" alt="Architecture Map - Epsagon dashboard."></p>
<figcaption><h4>Architecture Map - Epsagon dashboard.</h4></figcaption>
<p>The preceding Epsagon dashboard gives you an aggregate view of all recent function executions. In this case, the numbers happen to be quite similar to the previous test: about 800 milliseconds of invocation time on average with geocoding being the slowest call. SendGrid calls seem to go faster on average, but the red border around it indicates that some of the requests failed. This also points to a potential problem, which you would need to investigate. You may want to take some concrete actions based on the collected data, such as trying another geocoding service to see if it is faster for a comparable price.</p>
<p>You can estimate the cost of your application based on the predicted number of invocations multiplied by the price for 900 milliseconds of execution time. This function spends all the time waiting for I/O operations to complete and it&rsquo;s not CPU- or memory-heavy. Therefore, it makes sense to provision smaller sizes of AWS Lambda instances.</p>
<p>Finally, you may want to break down your &ldquo;monolith&rdquo; function into multiple functions that are interconnected with queues. Smaller single-purpose functions <a href="https://epsagon.com/blog/the-right-way-to-distribute-messages-effectively-in-serverless-applications/">have several benefits</a>, including higher resilience and potentially lower cost.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Almost every serverless function relies on one or many API calls and a combination of other cloud services and third-party applications. The latency and stability of those calls directly affect the Lambda function, both in terms of performance and cost.</p>
<p>Be sure to test the performance of any newly adopted API before relying on it. In addition, you need to monitor the latency, reliability and <a href="https://epsagon.com/blog/finding-serverless-hidden-costs/">cost of the API calls in production</a>. It is important that you optimize the structure of your application to avoid long-running calls on the synchronous path that is observable by end-users. And for better cost manageability and higher resilience, complete the executions as fast as possible and delegate the remaining work to queue- or event-based functions.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/epsagon" term="epsagon" label="Epsagon" />
                             
                                <category scheme="https://mikhail.io/tags/monitoring" term="monitoring" label="Monitoring" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Monitoring]]></title>
            <link href="https://mikhail.io/tags/monitoring/"/>
            <id>https://mikhail.io/tags/monitoring/</id>
            
            <published>2019-08-23T00:00:00+00:00</published>
            <updated>2019-08-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Ten Pearls With Azure Functions in Pulumi]]></title>
            <link href="https://mikhail.io/2019/08/ten-pearls-with-azure-functions-in-pulumi/"/>
            <id>https://mikhail.io/2019/08/ten-pearls-with-azure-functions-in-pulumi/</id>
            
            <published>2019-08-21T00:00:00+00:00</published>
            <updated>2019-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Ten bite-sized code snippets that use Pulumi to build serverless applications with Azure Functions and infrastructure as code.</blockquote><p>In this post, we&rsquo;ll take a look at 10 &ldquo;pearls&rdquo;—bite-sized code snippets—that demonstrate using Pulumi to build serverless applications with Azure Functions and infrastructure as code. These pearls are organized into four categories, each demonstrating a unique scenario:</p>
<ul>
<li><strong>Function App Deployment</strong>: Deploy an existing Azure Functions application using infrastructure as code.</li>
<li><strong>HTTP Functions as Callbacks</strong>: Mix JavaScript or TypeScript functions with your infrastructure definition to produce strongly-typed, self-contained, serverless HTTP endpoints.</li>
<li><strong>Cloud Event Handling</strong>: Leverage a variety of event sources available to Azure Functions with lightweight event handlers.</li>
<li><strong>Data Flows with Function Bindings</strong>: Take advantage of function bindings—declarative connectors to Azure services.</li>
</ul>
<p>Our Azure Functions pearls are:</p>
<p><a href="#function-app-deployment"><strong>Function App Deployment</strong></a></p>
<ul>
<li><a href="#1-deploy-a-net-function-app">Deploy a Function App written in .NET or any other supported runtime</a></li>
<li><a href="#2-run-functions-using-an-elastic-premium-plan">Configure Functions to run on an Elastic Premium Plan</a></li>
</ul>
<p><a href="#http-functions-as-callbacks"><strong>HTTP Functions as Callbacks</strong></a></p>
<ul>
<li><a href="#3-define-node-js-functions-as-inline-callbacks">Define Node.js Functions as inline callbacks</a></li>
<li><a href="#4-rest-apis-as-multiple-functions">Implement REST APIs as multiple Functions</a></li>
<li><a href="#5-function-warming-with-a-timer-function">&ldquo;Warm&rdquo; the Functions to avoid Cold Starts</a></li>
</ul>
<p><a href="#cloud-event-handling"><strong>Cloud Event Handling</strong></a></p>
<ul>
<li><a href="#6-process-events-from-azure-event-hubs">Process events from Azure Event Hub</a></li>
<li><a href="#7-subscribe-to-azure-event-grid">Subscribe to Blob creation with Azure Event Grid</a></li>
<li><a href="#8-respond-to-resource-level-events">Run a Function every time an Azure resource is modified</a></li>
</ul>
<p><a href="#data-flows-with-function-bindings"><strong>Data Flows with Function Bindings</strong></a></p>
<ul>
<li><a href="#9-output-bindings">Push a message to a Storage Queue with an output binding</a></li>
<li><a href="#10-input-bindings">Pull a Storage Table row with an input binding</a></li>
</ul>
<h2 id="function-app-deployment">Function App Deployment</h2>
<p>Azure Functions can be written in many languages, and Pulumi supports whatever choice you make. You can take any existing serverless application and deploy it using infrastructure-as-code.</p>
<h3 id="1-deploy-a-net-function-app">1. Deploy a .NET Function App</h3>
<p>[ <a href="https://github.com/pulumi/examples/tree/master/azure-ts-functions-raw">Runnable Example</a> ]</p>
<p>Many Function Apps are .NET applications created with native tooling like Visual Studio or the Functions CLI. Pulumi can simply deploy such an application in only a few lines of JavaScript:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> dotnetApp = <span style="color:#00f">new</span> azure.appservice.ArchiveFunctionApp(<span style="color:#a31515">&#34;http-dotnet&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   archive: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./app/bin/Debug/netcoreapp2.1/publish&#34;</span>),
   appSettings: { runtime: <span style="color:#a31515">&#34;dotnet&#34;</span> },
});
</code></pre></div><p>There are only four things required:</p>
<ul>
<li>the Function App name (&ldquo;http-dotnet&rdquo;)</li>
<li>the resource group it belongs to</li>
<li>the path to the compiled .NET assemblies, and</li>
<li>the desired runtime</li>
</ul>
<p>Pulumi takes care of the rest for you. It handles the following tasks:</p>
<ul>
<li>Creating a Storage Account and a Blob Container</li>
<li>Zipping up the binaries and uploading them to the blob container</li>
<li>Calculating a SAS token</li>
<li>Preparing a Consumption Plan and a Function App using this Consumption Plan</li>
<li>Configuring the required application settings, including a reference to the zip archive with the SAS token</li>
</ul>
<p><img src="console.png" alt="Serverless Application deployed by Pulumi"></p>
<p>While a few values are required, you are not restricted to the default behavior and can change any setting.</p>
<h3 id="2-run-functions-using-an-elastic-premium-plan">2. Run Functions using an Elastic Premium Plan</h3>
<p>While the Consumption Plan is the ultimate serverless option, there are other ways to host Azure Functions. Azure also offers the Premium Plan: a combination of the power and guarantees of a fixed App Service Plan with the elasticity of Consumption.</p>
<p>If you want to take advantage of a Premium Plan, go ahead and define it as a Pulumi resource, then link to it in the Function App definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> premiumPlan = <span style="color:#00f">new</span> azure.appservice.Plan(<span style="color:#a31515">&#34;my-premium&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   sku: {
       tier: <span style="color:#a31515">&#34;Premium&#34;</span>,
       size: <span style="color:#a31515">&#34;EP1&#34;</span>,
   },
   maximumElasticWorkerCount: <span style="color:#2b91af">20</span>,
});

<span style="color:#00f">const</span> javaApp = <span style="color:#00f">new</span> azure.appservice.ArchiveFunctionApp(<span style="color:#a31515">&#34;http-java&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   plan: <span style="color:#2b91af">premiumPlan</span>,
   archive: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./java/target/functions/fabrikam&#34;</span>),
   appSettings: { runtime: <span style="color:#a31515">&#34;java&#34;</span> },
});
</code></pre></div><p>In this example, I deployed a Java application. The Premium Plan has a couple of configuration knobs: the <em>instance size</em> and the <em>maximum scale-out limit</em>.</p>
<h2 id="http-functions-as-callbacks">HTTP Functions as Callbacks</h2>
<p>Node.js is another runtime supported by Azure Functions. Like before, you can use the <code>ArchiveFunctionApp</code> class to deploy the application from an external folder. However, Pulumi&rsquo;s Node.js SDK provides a way to mix the code of your functions directly into your infrastructure definition.</p>
<h3 id="3-define-nodejs-functions-as-inline-callbacks">3. Define Node.js Functions as Inline Callbacks</h3>
<p>[ <a href="https://github.com/pulumi/examples/tree/master/azure-ts-functions">Runnable Example</a> ]</p>
<p>A TypeScript or a JavaScript function becomes an Azure Function deployed to the cloud:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> greeting = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#39;greeting&#39;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
       <span style="color:#00f">return</span> {
           status: <span style="color:#2b91af">200</span>,
           body: <span style="color:#a31515">`Hello </span><span style="color:#a31515">${</span>req.query[<span style="color:#a31515">&#39;name&#39;</span>] || <span style="color:#a31515">&#39;World&#39;</span><span style="color:#a31515">}</span><span style="color:#a31515">!`</span>,
       };
   }
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> url = greeting.url;
</code></pre></div><p>You are free to use any NPM module inside the callback, and the dependencies will be automatically packaged into the deployment artifact. Alternatively, you can extract the callback function into a separate file or package and import it from your infrastructure code.</p>
<p>While mixing infrastructure and application code in the same file may seem counterintuitive, it provides many benefits:</p>
<ul>
<li>Combined code binaries (data plane) and infrastructure (control plane) as a single unit of deployment</li>
<li>Eliminate the need for boilerplate configuration like <code>host.json</code> and <code>function.json</code> files</li>
<li>Robust typing out-of-the-box: For instance, you can flawlessly &ldquo;dot into&rdquo; the <code>content</code> and <code>req</code> object above.</li>
</ul>
<p>You can read more about the motivation in <a href="https://mikhail.io/2019/05/serverless-as-simple-callbacks-with-pulumi-and-azure-functions/">Serverless as Simple Callbacks with Pulumi and Azure Functions</a>.</p>
<p>The previous example deployed a Function App with a single Function. However, Azure supports applications with multiple Functions bundled together.</p>
<h3 id="4-rest-apis-as-multiple-functions">4. REST APIs as Multiple Functions</h3>
<p>It&rsquo;s common to use Azure Functions for implementing RESTful APIs. As an API may consist of multiple endpoints, we may need to combine several HTTP Functions into a single deployment unit.</p>
<p>To achieve that, we define an <code>HttpFunction</code> object per Azure Function, each with its callback and settings. Then, we pass an array of these objects into a <code>MultiCallbackFunctionApp</code> constructor. Each Function definition may have a specific route and HTTP methods to handle:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> <span style="color:#00f">get</span> = <span style="color:#00f">new</span> azure.appservice.HttpFunction(<span style="color:#a31515">&#34;Read&#34;</span>, {
   route: <span style="color:#a31515">&#34;items&#34;</span>,
   methods: [<span style="color:#a31515">&#34;GET&#34;</span>],
   callback: <span style="color:#2b91af">async</span> (context, request) =&gt; {
       <span style="color:#00f">const</span> items = <span style="color:#00f">await</span> repository.list();
       <span style="color:#00f">return</span> { status: <span style="color:#2b91af">200</span>, body: <span style="color:#2b91af">items</span> };
   },
});

<span style="color:#00f">const</span> post = <span style="color:#00f">new</span> azure.appservice.HttpFunction(<span style="color:#a31515">&#34;Add&#34;</span>, {
   route: <span style="color:#a31515">&#34;items&#34;</span>,
   methods: [<span style="color:#a31515">&#34;POST&#34;</span>],
   callback: <span style="color:#2b91af">async</span> (context, request) =&gt; {
       <span style="color:#00f">const</span> id = <span style="color:#00f">await</span> repository.add(request.body);
       <span style="color:#00f">return</span> { status: <span style="color:#2b91af">201</span>, body: { id }  };
   },
});

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.MultiCallbackFunctionApp(<span style="color:#a31515">&#34;multi-app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   functions: [<span style="color:#00f">get</span>, post],
});
</code></pre></div><p>You can combine as many Functions as you want, including functions of different types, as shown in the next example.</p>
<h3 id="5-function-warming-with-a-timer-function">5. Function &ldquo;Warming&rdquo; with a Timer Function</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/blob/master/examples/http-multi">Runnable Example</a> ]</p>
<p>Scheduled jobs are another frequent use case for serverless functions. It&rsquo;s possible to define a <a href="https://docs.microsoft.com/azure/azure-functions/functions-bindings-timer#cron-expressions"><em>cron expression</em></a> and get the code executed at designated intervals.</p>
<p>The Consumption Plan disposes a worker if no Function runs in about 20 minutes. After disposal, the next execution will cause a <a href="https://mikhail.io/serverless/coldstarts/define/"><em>cold start</em></a>, and the response latency, will increase.</p>
<p>In order to avoid this disposal, we can combine a target HTTP Function with a Timer Function. The body of the Timer Function is empty, since its sole purpose is to trigger the Function App and keep the worker &ldquo;warm&rdquo;:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> warmer = <span style="color:#00f">new</span> azure.appservice.TimerFunction(<span style="color:#a31515">&#34;warmer&#34;</span>, {
   schedule: <span style="color:#a31515">&#34;0 */5 * * * *&#34;</span>,
   callback: <span style="color:#2b91af">async</span> () =&gt; {},
});

<span style="color:#00f">const</span> http = <span style="color:#00f">new</span> azure.appservice.HttpFunction(<span style="color:#a31515">&#34;hello&#34;</span>, {
   callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
       <span style="color:#00f">return</span> {
           status: <span style="color:#2b91af">200</span>,
           body: <span style="color:#a31515">&#34;Hello World!&#34;</span>,
       };
   }
});

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.MultiCallbackFunctionApp(<span style="color:#a31515">&#34;always-warm-app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   functions: [http, warmer],
});
</code></pre></div><p>It&rsquo;s easy to imagine a custom component <code>WarmedFunctionApp</code> which appends a standard Timer Function to an array of Functions passed to its constructor.</p>
<h2 id="cloud-event-handling">Cloud Event Handling</h2>
<p>While HTTP is a widespread use case, Azure Functions support many other trigger types too. <a href="https://mikhail.io/2019/05/serverless-as-simple-callbacks-with-pulumi-and-azure-functions/">The previous post</a> featured Storage Queues and ServiceBus Topics. Pulumi supports Timers, Events Hubs, Event Grid, Storage Blobs, Service Bus Queues, and Cosmos DB Change Feed events, too! Let&rsquo;s see how that looks using Azure Event Hubs.</p>
<h3 id="6-process-events-from-azure-event-hubs">6. Process Events from Azure Event Hubs</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/tree/master/examples/eventhub">Runnable Example</a> ]</p>
<p>Azure Event Hubs is a fully-managed log-based messaging service comparable to Apache Kafka. In contrast to a self-hosted Kafka cluster, it only takes a few lines of JavaScript to create an Event Hub and start processing events:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> eventHub = <span style="color:#00f">new</span> eventhub.EventHub(<span style="color:#a31515">&#34;my-hub&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   namespaceName: <span style="color:#2b91af">namespace.name</span>,
   partitionCount: <span style="color:#2b91af">2</span>,
   messageRetention: <span style="color:#2b91af">7</span>,
});

eventHub.onEvent(<span style="color:#a31515">&#34;MyHubEvent&#34;</span>, <span style="color:#00f">async</span> (context, msg) =&gt; {
   console.log(<span style="color:#a31515">&#34;Event Hub message: &#34;</span> + JSON.stringify(msg));
});
</code></pre></div><p>Every time a new event comes in, be it once per hour or a thousand times a second, the Function is executed. Azure manages the scale-out for you.</p>
<h3 id="7-subscribe-to-azure-event-grid">7. Subscribe to Azure Event Grid</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/blob/master/examples/eventgrid">Runnable Example</a> ]</p>
<p>Azure Event Grid is another trigger type for Azure Functions. It is a dispatcher service for distributing events from many other Azure services as well as external data sources.</p>
<p>A classic example is subscribing to events from Azure Blob Storage. The Event Grid subscription connects to a given Storage Account and provides several handy options to filter the event stream. The following example subscribes to all JPG files created in any container of the account:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;eventgridsa&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
   accountKind: <span style="color:#a31515">&#34;StorageV2&#34;</span>,
});

azure.eventgrid.events.onGridBlobCreated(<span style="color:#a31515">&#34;OnNewBlob&#34;</span>, {
   storageAccount,
   subjectFilter: {
       caseSensitive: <span style="color:#2b91af">false</span>,
       subjectEndsWith: <span style="color:#a31515">&#34;.jpg&#34;</span>,
   },
   callback: <span style="color:#2b91af">async</span> (context, event) =&gt; {
       context.log(<span style="color:#a31515">`Subject: </span><span style="color:#a31515">${</span>event.subject<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
       context.log(<span style="color:#a31515">`File size: </span><span style="color:#a31515">${</span>event.data.contentLength<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
   },
});
</code></pre></div><p>The <code>event</code> object is strongly typed: The snippet above logs the file size, but there are many other properties at your disposal. Code completion makes the discovery process painless.</p>
<p>It&rsquo;s worth mentioning that Pulumi does much of the work behind the scenes here, allowing you to focus on the important parts of the task at hand. In particular, Pulumi retrieves the appropriate secret key from Azure Functions ARM API and creates an Event Grid subscription which points to a specific webhook containing that key.</p>
<h3 id="8-respond-to-resource-level-events">8. Respond to Resource-level Events</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/blob/master/examples/eventgrid">Runnable Example</a> ]</p>
<p>Here is a good illustration of the power of Event Grid. A callback function gets triggered for each change to any resource belonging to the target Resource Group:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;eventgrid-rg&#34;</span>);

eventgrid.events.onResourceGroupEvent(<span style="color:#a31515">&#34;OnResourceChange&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   callback: <span style="color:#2b91af">async</span> (context, event) =&gt; {
       context.log(<span style="color:#a31515">`Subject: </span><span style="color:#a31515">${</span>event.subject<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
       context.log(<span style="color:#a31515">`Event Type: </span><span style="color:#a31515">${</span>event.eventType<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
       context.log(<span style="color:#a31515">`Data: </span><span style="color:#a31515">${</span>JSON.stringify(event.data)<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
   },
});
</code></pre></div><p>This simple piece of code is an easy launching point for many automation, auditing, and governance scenarios.</p>
<h2 id="data-flows-with-function-bindings">Data Flows with Function Bindings</h2>
<p>Azure Functions come with a powerful system of bindings. So far, we only saw examples of using event sources as trigger bindings. However, Azure also supports a powerful set of so called &ldquo;input&rdquo; and &ldquo;output&rdquo; bindings.</p>
<h3 id="9-output-bindings">9. Output Bindings</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/blob/master/examples/queue">Runnable Example</a> ]</p>
<p>Output bindings enable developers to easily forward the data from an Azure Function to an arbitrary destination in a declarative manner. For instance, if a queue handler needs to send a message to another queue, we don&rsquo;t have to use cloud SDKs. Instead, we can return the message-to-be-sent from the callback and wire it to the output queue. Here is a quick example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> incoming = <span style="color:#00f">new</span> azure.storage.Queue(<span style="color:#a31515">&#34;queue-in&#34;</span>, {
  storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
});

<span style="color:#00f">const</span> outgoing = <span style="color:#00f">new</span> azure.storage.Queue(<span style="color:#a31515">&#34;queue-out&#34;</span>, {
   storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
});

incoming.onEvent(<span style="color:#a31515">&#34;NewMessage&#34;</span>,  {
   outputs: [outgoing.output(<span style="color:#a31515">&#34;queueOut&#34;</span>)],
   callback: <span style="color:#2b91af">async</span> (context, person) =&gt; {
       <span style="color:#00f">return</span> {
           queueOut: <span style="color:#a31515">`</span><span style="color:#a31515">${</span>person.name<span style="color:#a31515">}</span><span style="color:#a31515"> logged into the system`</span>,
       };
   },
});
</code></pre></div><p>Two elements play together here:</p>
<ul>
<li>The <code>outputs</code> property defines an output binding with the name <code>queueOut</code> and the destination to <code>outgoing</code></li>
<li>The <code>queueOut</code> property of the resulting object contains the output message</li>
</ul>
<p>Note that the binding name must match the output property.</p>
<h3 id="10-input-bindings">10. Input Bindings</h3>
<p>[ <a href="https://github.com/pulumi/pulumi-azure/tree/master/examples/table">Runnable Example</a> ]</p>
<p>Input bindings pull extra bits of information and pass them as input parameters to the callback. The exact usage depends on the trigger and binding types and might be tricky to get right with JSON configuration files. Here is one example of wiring done in a Pulumi program:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> values = <span style="color:#00f">new</span> azure.storage.Table(<span style="color:#a31515">&#34;values&#34;</span>, {
   storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
});

<span style="color:#00f">const</span> getFunc = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#39;get-value&#39;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   route: <span style="color:#a31515">&#34;{key}&#34;</span>,
   inputs: [
       values.input(<span style="color:#a31515">&#34;entry&#34;</span>, { partitionKey: <span style="color:#a31515">&#34;lookup&#34;</span>, rowKey: <span style="color:#a31515">&#34;{key}&#34;</span> }),
   ],
   callback: <span style="color:#2b91af">async</span> (context, request, entry) =&gt; {
       <span style="color:#00f">return</span> {
           status: <span style="color:#2b91af">200</span>,
           body: <span style="color:#2b91af">entry.value</span>,
       };
   },
});
</code></pre></div><p>There are three crucial bits here:</p>
<ul>
<li>The <code>route</code> property of this HTTP Function contains a template parameter <code>key</code>. The runtime extracts the actual <code>key</code> value from the HTTP request.</li>
<li>The <code>inputs</code> option contains a reference to a Storage Table with a hardcoded <code>partitionKey</code> and a <code>rowKey</code> bound to the <code>key</code> template parameter. At execution time, a row is retrieved based on the combination of the keys. The entry, if found, is passed to the callback.</li>
<li>The <code>callback</code> has three input parameters, while all previous examples had two. The third parameter contains the retrieved row.</li>
</ul>
<p>It&rsquo;s possible to have multiple input and output bindings, and any combination of those.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>In this post, we&rsquo;ve seen some of the exciting things you can do with Azure Functions in Pulumi. Developers use serverless functions as a glue between managed cloud services. Pulumi offers a compelling way to define the links between these pieces of cloud infrastructure in a simple and expressive way.</p>
<p>For a streamlined Pulumi walkthrough, including language runtime installation and cloud configuration, see the  <a href="https://www.pulumi.com/docs/quickstart/azure/">Azure Quickstart</a>, check more examples at <a href="https://github.com/pulumi/pulumi-azure/tree/master/examples">Pulumi Azure GitHub</a>, and join the <a href="https://slack.pulumi.com/">Pulumi Community Slack</a>.</p>]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[AWS Lambda Cold Starts After 10 Minutes]]></title>
            <link href="https://mikhail.io/2019/08/aws-lambda-cold-starts-after-10-minutes/"/>
            <id>https://mikhail.io/2019/08/aws-lambda-cold-starts-after-10-minutes/</id>
            
            <published>2019-08-20T00:00:00+00:00</published>
            <updated>2019-08-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>How AWS Lambda changed the policy of recycling idle instances</blockquote><p>I&rsquo;ve just released an update to the <a href="/serverless/coldstarts/">Serverless Cold Starts</a> section of my website. The most significant change to the previous dataset seems to be in how AWS treats idle instances of AWS Lambda.</p>
<p>Cold starts are expensive, so all cloud providers preserve a warm instance of a cloud function even when the application is idle. If the function stays unused for an extended period, such idle instance might eventually be recycled.</p>
<p>Azure Functions recycles its instances after 20 minutes of idling. Google Cloud Functions has no fixed value, but most often, instances may survive several hours of inactivity. The policy of AWS Lambda has recently changed.</p>
<h2 id="pre-july-between-25-and-60-minutes">Pre-July: between 25 and 60 minutes</h2>
<p>The behavior of AWS Lambda used to look a bit random: an average idle instance would typically survive between 25 and 60 minutes:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'ID');
  data.addColumn('number', 'Value');
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows([[38.526995191666664,0.1550262,"point {fill-color: red}"],[49.383118026666665,0.3873159,"point {fill-color: blue}"],[30.627794798333333,0.3444636,"point {fill-color: blue}"],[74.55351005666667,0.6604886999999999,"point {fill-color: blue}"],[40.02867755333333,0.44541949999999997,"point {fill-color: blue}"],[80.70089967333334,0.3074113,"point {fill-color: blue}"],[13.919134966666666,0.2557926,"point {fill-color: red}"],[20.954349756666666,0.1430216,"point {fill-color: red}"],[69.30654145,1.7465974,"point {fill-color: blue}"],[51.437736711666666,0.8658656,"point {fill-color: blue}"],[41.848015896666666,0.0581395,"point {fill-color: red}"],[73.16705693,0.2018015,"point {fill-color: blue}"],[83.82289175666666,0.5951442,"point {fill-color: blue}"],[3.2707585133333335,0.0964497,"point {fill-color: red}"],[66.16240756833334,0.4297529,"point {fill-color: blue}"],[59.17943319166667,0.1006936,"point {fill-color: red}"],[5.27709279,0.13207929999999998,"point {fill-color: red}"],[69.47966145666666,1.7406161999999998,"point {fill-color: blue}"],[59.19029861333333,0.1015944,"point {fill-color: red}"],[27.499207288333334,0.49999869999999996,"point {fill-color: blue}"],[22.710786873333333,0.09022949999999999,"point {fill-color: red}"],[48.95806383833333,0.11274819999999999,"point {fill-color: red}"],[65.48154354666667,14.947999699999999,"point {fill-color: blue}"],[1.4133973316666666,0.15462019999999999,"point {fill-color: red}"],[9.55894851,0.0874689,"point {fill-color: red}"],[67.69212707333334,3.7460364999999998,"point {fill-color: blue}"],[57.763618925,0.1694528,"point {fill-color: red}"],[25.383047783333335,0.11548,"point {fill-color: red}"],[15.135814975,0.0829861,"point {fill-color: blue}"],[86.21106092833334,2.1164294999999997,"point {fill-color: blue}"],[64.66950564666666,0.5459723,"point {fill-color: blue}"],[38.61166446666667,0.0641221,"point {fill-color: red}"],[55.90309728333334,1.9127231,"point {fill-color: blue}"],[42.49044799166667,0.5686247999999999,"point {fill-color: blue}"],[24.975154416666665,0.0692699,"point {fill-color: red}"],[52.56358164,0.3999735,"point {fill-color: blue}"],[26.981866243333332,0.0788804,"point {fill-color: red}"],[69.17019360833334,0.2850802,"point {fill-color: blue}"],[9.011017508333333,0.061772299999999995,"point {fill-color: red}"],[70.70602261333333,0.37219169999999996,"point {fill-color: blue}"],[5.3830482716666666,0.1564117,"point {fill-color: red}"],[41.183333123333334,4.3570378,"point {fill-color: blue}"],[49.20586052666667,0.6072678,"point {fill-color: blue}"],[57.739191868333336,1.0766117,"point {fill-color: red}"],[68.35231534333333,0.5408453,"point {fill-color: blue}"],[78.83988125333333,0.2447595,"point {fill-color: blue}"],[41.262252188333335,0.2634827,"point {fill-color: blue}"],[86.75830183333333,0.42455919999999997,"point {fill-color: blue}"],[59.96241439666667,0.3954815,"point {fill-color: blue}"],[38.77283253833333,0.7081769,"point {fill-color: red}"],[52.34286770333333,0.29376979999999997,"point {fill-color: blue}"],[21.98877415666667,0.1383248,"point {fill-color: red}"],[50.37356623833333,0.2773712,"point {fill-color: blue}"],[12.080005235,0.0981435,"point {fill-color: red}"],[29.329113341666666,3.4658368,"point {fill-color: blue}"],[17.092130383333334,0.1038376,"point {fill-color: red}"],[74.917163235,2.5994357999999997,"point {fill-color: blue}"],[8.132405245,0.0736616,"point {fill-color: red}"],[52.931433815,0.2546939,"point {fill-color: blue}"],[75.70268996333334,1.6510258,"point {fill-color: blue}"],[45.389382411666666,0.24255459999999998,"point {fill-color: blue}"],[26.962280723333333,0.6419199,"point {fill-color: red}"],[39.424155168333336,3.5855287999999996,"point {fill-color: blue}"],[76.90400380333334,4.4713025,"point {fill-color: blue}"],[30.403184581666668,0.127333,"point {fill-color: red}"],[29.021802965,0.0987057,"point {fill-color: blue}"],[34.48582949666667,3.4538837,"point {fill-color: blue}"],[41.205893455,0.12239499999999999,"point {fill-color: red}"],[1.8639237333333334,0.0576263,"point {fill-color: red}"],[41.067952205,0.0847106,"point {fill-color: red}"],[1.8757833433333333,0.039630399999999996,"point {fill-color: red}"],[48.392306383333334,0.14035599999999998,"point {fill-color: red}"],[40.47558774,9.3439129,"point {fill-color: blue}"],[66.45981046333334,1.5875199999999998,"point {fill-color: blue}"],[82.95569771,2.0429387,"point {fill-color: blue}"],[1.271768535,0.14090919999999998,"point {fill-color: red}"],[66.21468290166666,0.33384949999999997,"point {fill-color: blue}"],[49.21878872,0.0773193,"point {fill-color: red}"],[34.057152085,0.0710756,"point {fill-color: red}"],[80.80053234500001,0.3747753,"point {fill-color: blue}"],[60.04510325333334,0.0874165,"point {fill-color: red}"],[22.034956168333334,0.0904994,"point {fill-color: red}"],[46.157779061666666,0.0744843,"point {fill-color: red}"],[85.419151815,0.2379674,"point {fill-color: blue}"],[12.9599633,0.274729,"point {fill-color: red}"],[16.034895655,0.1649873,"point {fill-color: red}"],[35.555929825,0.0453263,"point {fill-color: red}"],[49.593340945,0.2723506,"point {fill-color: blue}"],[79.36076043666667,0.316059,"point {fill-color: blue}"],[69.01801610833334,0.8283473,"point {fill-color: blue}"],[70.063744925,0.4620835,"point {fill-color: blue}"],[80.1751643,0.2826442,"point {fill-color: blue}"],[1.1729676583333333,0.048190199999999996,"point {fill-color: red}"],[42.33278996166667,0.3490057,"point {fill-color: red}"],[4.419674148333334,0.1804642,"point {fill-color: red}"],[60.28483970166667,0.14762119999999998,"point {fill-color: blue}"],[25.991137336666668,0.7594624999999999,"point {fill-color: red}"],[32.78963815666667,0.1544057,"point {fill-color: red}"],[22.052811168333335,0.8696495999999999,"point {fill-color: red}"],[69.418732085,0.24269259999999998,"point {fill-color: blue}"],[32.022322808333335,6.205000699999999,"point {fill-color: blue}"],[8.766117276666668,0.0791263,"point {fill-color: red}"],[82.04198705333333,1.4059723,"point {fill-color: blue}"],[77.825930185,1.6483149,"point {fill-color: blue}"],[5.624886218333334,0.1566382,"point {fill-color: red}"],[73.98357983,0.5510028,"point {fill-color: blue}"],[17.60873581,0.4956273,"point {fill-color: red}"],[20.550943963333335,0.4935788,"point {fill-color: red}"],[45.674281875,0.4088925,"point {fill-color: blue}"],[42.545045435,2.4441832,"point {fill-color: blue}"],[19.882126030000002,0.6622591,"point {fill-color: red}"],[42.6391371,3.4835228999999996,"point {fill-color: blue}"],[76.755180255,0.3556185,"point {fill-color: blue}"],[88.62716236,1.3676065,"point {fill-color: blue}"],[77.31318927333334,0.28793219999999997,"point {fill-color: blue}"],[8.070663053333334,0.1375013,"point {fill-color: red}"],[17.057905591666668,0.09323659999999999,"point {fill-color: red}"],[38.93715281666667,0.25168409999999997,"point {fill-color: red}"],[14.149482925000001,0.2588968,"point {fill-color: red}"],[24.336118831666667,0.055414599999999994,"point {fill-color: red}"],[58.186720385,0.46007719999999996,"point {fill-color: blue}"],[45.53523862666667,3.5395277999999997,"point {fill-color: blue}"],[12.969594291666667,0.2101442,"point {fill-color: red}"],[18.79669068,0.149012,"point {fill-color: red}"],[41.91367900833333,0.0839695,"point {fill-color: red}"],[18.242984068333335,0.0901901,"point {fill-color: red}"],[21.996763806666667,0.1241394,"point {fill-color: red}"],[69.65529542333333,0.7016737,"point {fill-color: blue}"],[49.79530868166667,0.0648183,"point {fill-color: red}"],[51.0066732,0.3343772,"point {fill-color: blue}"],[27.38611686666667,0.2376876,"point {fill-color: red}"],[47.009798626666665,0.07447089999999999,"point {fill-color: red}"],[17.167833645,0.9254233,"point {fill-color: red}"],[15.69020994,0.14488009999999998,"point {fill-color: red}"],[21.131497565,0.1447976,"point {fill-color: red}"],[14.074645466666666,0.0842025,"point {fill-color: red}"],[83.59170068,15.5666993,"point {fill-color: blue}"],[77.83909701500001,0.22315449999999998,"point {fill-color: blue}"],[22.995626943333335,0.1252585,"point {fill-color: red}"],[25.53520855,0.1670047,"point {fill-color: red}"],[52.68116263833333,0.2983606,"point {fill-color: blue}"],[5.8015912033333334,0.0770951,"point {fill-color: red}"],[19.280252418333333,0.08207629999999999,"point {fill-color: red}"],[63.85613409166667,1.32148,"point {fill-color: blue}"],[69.616274215,9.464854299999999,"point {fill-color: blue}"],[68.24069932333333,0.3779628,"point {fill-color: blue}"],[13.993746851666668,0.0789617,"point {fill-color: red}"],[8.878349793333333,0.6475008,"point {fill-color: red}"],[67.668255225,1.9237023,"point {fill-color: blue}"],[6.326890916666667,0.07884379999999999,"point {fill-color: red}"],[24.749510005,0.10695189999999999,"point {fill-color: red}"],[84.21048494666667,0.44550069999999997,"point {fill-color: blue}"],[88.96865076666667,0.29153809999999997,"point {fill-color: blue}"],[48.511442693333336,1.7128995999999999,"point {fill-color: blue}"],[79.66668867166666,0.4309491,"point {fill-color: blue}"],[64.39583149833334,0.90518,"point {fill-color: blue}"],[29.685448296666667,0.0826042,"point {fill-color: red}"],[43.29882289333334,3.8454314999999997,"point {fill-color: blue}"],[0.474605485,0.1236186,"point {fill-color: red}"],[56.049815931666664,0.051351999999999995,"point {fill-color: red}"],[81.99091987833333,0.29411349999999997,"point {fill-color: blue}"],[63.07512019833334,0.3395738,"point {fill-color: blue}"],[29.345477113333335,0.140897,"point {fill-color: red}"],[49.89055974166667,0.30003209999999997,"point {fill-color: blue}"],[83.60091419333334,2.1698577,"point {fill-color: blue}"],[42.50274455,3.9732217,"point {fill-color: blue}"],[12.140651748333333,0.169569,"point {fill-color: red}"],[44.77874430833334,0.2017753,"point {fill-color: red}"],[10.051442241666667,0.1366813,"point {fill-color: blue}"],[21.661612018333333,0.07703,"point {fill-color: red}"],[13.758143198333334,0.2562397,"point {fill-color: red}"],[44.45710445,0.37225959999999997,"point {fill-color: blue}"],[38.24003102666667,0.24395419999999998,"point {fill-color: red}"],[79.88764414833334,0.38577649999999997,"point {fill-color: blue}"],[64.15617373833334,0.40928139999999996,"point {fill-color: blue}"],[57.39915237333334,19.868931099999998,"point {fill-color: blue}"],[10.345730388333333,0.0971137,"point {fill-color: red}"],[65.45581997166667,0.4287224,"point {fill-color: blue}"],[22.55339774,0.0855411,"point {fill-color: red}"],[65.40696494333334,0.3989004,"point {fill-color: blue}"],[79.52298223833334,3.4590891,"point {fill-color: blue}"],[43.76374572333334,0.0729218,"point {fill-color: red}"],[39.038809193333336,0.08668379999999999,"point {fill-color: red}"],[50.86184635666667,0.3091078,"point {fill-color: blue}"],[86.21345749666666,1.8218668,"point {fill-color: blue}"],[59.12925654833334,0.41616359999999997,"point {fill-color: blue}"],[51.09478328833333,0.1282289,"point {fill-color: red}"],[2.871335323333333,0.0803813,"point {fill-color: red}"],[58.85896658833333,0.07109309999999999,"point {fill-color: red}"],[58.11024778666667,0.5860803,"point {fill-color: blue}"],[26.98715775666667,0.21246949999999998,"point {fill-color: red}"],[56.097461368333335,0.0885328,"point {fill-color: red}"],[42.03146309,0.2326391,"point {fill-color: red}"],[45.36751881,0.0840113,"point {fill-color: red}"],[87.11906116166666,3.5698512,"point {fill-color: blue}"],[58.55965826,0.34712879999999996,"point {fill-color: blue}"],[25.816009206666667,0.2552513,"point {fill-color: red}"],[17.404054948333332,0.0900733,"point {fill-color: red}"],[65.73017756833333,0.5520629,"point {fill-color: blue}"],[14.399627151666667,0.149383,"point {fill-color: red}"],[83.27538464333334,3.3537849,"point {fill-color: blue}"],[70.74300259166667,0.237814,"point {fill-color: blue}"],[88.65498865166667,0.2842586,"point {fill-color: blue}"],[38.991331716666664,3.5091847,"point {fill-color: blue}"],[38.14694393333333,0.9561502,"point {fill-color: red}"],[71.10140489833333,0.3424193,"point {fill-color: blue}"],[58.56162732833334,0.16513619999999998,"point {fill-color: red}"],[21.959824858333334,0.0590597,"point {fill-color: red}"],[53.137709226666665,0.0997979,"point {fill-color: red}"],[18.013297526666666,0.2582125,"point {fill-color: red}"],[69.18486378666667,0.4516975,"point {fill-color: blue}"],[13.351460756666667,0.1327903,"point {fill-color: red}"],[85.00581744666667,1.8761225,"point {fill-color: blue}"],[39.07636569,0.0808726,"point {fill-color: red}"],[51.594023211666666,0.2955469,"point {fill-color: blue}"],[5.350187821666667,0.16918629999999998,"point {fill-color: red}"],[58.34890635166667,0.3161403,"point {fill-color: blue}"],[89.686224735,0.8639465,"point {fill-color: blue}"],[18.323332263333334,0.0954989,"point {fill-color: red}"],[34.87626021333333,0.3901031,"point {fill-color: blue}"],[27.952431695,0.1634298,"point {fill-color: red}"],[74.40671932833334,0.281557,"point {fill-color: blue}"],[72.76571325500001,0.38391939999999997,"point {fill-color: blue}"],[7.33391814,0.11623779999999999,"point {fill-color: red}"],[5.975228706666667,0.10757929999999999,"point {fill-color: red}"],[85.54390559833334,0.26751369999999997,"point {fill-color: blue}"],[49.00373446333334,0.8579922,"point {fill-color: blue}"],[16.763886736666667,1.5631433,"point {fill-color: red}"],[67.04111677666667,0.33007749999999997,"point {fill-color: blue}"],[42.47902446,1.13818,"point {fill-color: red}"],[46.97548657833333,0.5071719,"point {fill-color: red}"],[86.612332575,1.7822205999999998,"point {fill-color: blue}"],[2.4378143233333334,0.1488296,"point {fill-color: red}"],[27.40071021,0.7840035999999999,"point {fill-color: red}"],[73.74944608166666,0.27938409999999997,"point {fill-color: blue}"],[31.61127565,0.3431031,"point {fill-color: blue}"],[34.180652725,0.2551906,"point {fill-color: red}"],[37.12643061666667,1.6426549,"point {fill-color: blue}"],[1.3390968633333333,0.0552043,"point {fill-color: red}"],[6.727067206666667,0.09216139999999999,"point {fill-color: red}"],[25.780578365,0.6253052,"point {fill-color: blue}"],[79.14681766333334,5.0709165,"point {fill-color: blue}"],[69.817400345,0.3040488,"point {fill-color: blue}"],[10.202245455,0.13399239999999998,"point {fill-color: red}"],[4.346626401666667,0.0546916,"point {fill-color: red}"],[42.897410746666665,0.26411,"point {fill-color: blue}"],[89.076378615,0.3723878,"point {fill-color: blue}"],[21.729789506666666,0.0486503,"point {fill-color: red}"],[35.79139018166667,1.7933103,"point {fill-color: blue}"],[23.373755308333333,0.0816527,"point {fill-color: red}"],[26.64472644,0.2566372,"point {fill-color: red}"],[28.746672148333335,0.322054,"point {fill-color: blue}"],[89.08841646500001,9.4912413,"point {fill-color: blue}"],[37.482617760000004,0.09839439999999999,"point {fill-color: red}"],[29.311321578333335,0.32794029999999996,"point {fill-color: blue}"],[5.457457761666666,0.0734413,"point {fill-color: red}"],[85.213222265,1.6167749999999999,"point {fill-color: blue}"],[61.404976751666666,0.6279275,"point {fill-color: blue}"],[9.051504411666667,0.1886284,"point {fill-color: red}"],[63.24859332166667,4.039847,"point {fill-color: blue}"],[9.03212719,0.0734992,"point {fill-color: red}"],[40.51894098333334,0.8137154,"point {fill-color: blue}"],[55.42688147166667,0.32365459999999996,"point {fill-color: blue}"],[75.122186805,0.4001491,"point {fill-color: blue}"],[70.62906459833333,0.6791537,"point {fill-color: blue}"],[1.3122326683333334,0.0831619,"point {fill-color: red}"],[59.76903205166667,0.08785119999999999,"point {fill-color: red}"],[25.388045336666668,0.29014239999999997,"point {fill-color: red}"],[0.920790815,0.6097923,"point {fill-color: red}"],[83.51527025166666,4.7549209999999995,"point {fill-color: blue}"],[83.44897519833333,0.9816033,"point {fill-color: blue}"],[27.450029371666666,0.2180827,"point {fill-color: blue}"],[17.821980085,0.0455135,"point {fill-color: red}"],[22.403803613333334,0.16736199999999998,"point {fill-color: red}"],[15.961839651666667,0.14143709999999998,"point {fill-color: red}"],[19.012584058333335,0.3111471,"point {fill-color: red}"],[31.62366229166667,3.7095799,"point {fill-color: blue}"],[1.8704391166666667,0.057992999999999996,"point {fill-color: red}"],[71.42424322333333,0.27396929999999997,"point {fill-color: blue}"],[83.85850077333333,0.49724549999999995,"point {fill-color: blue}"],[62.26884993666667,0.2524341,"point {fill-color: blue}"],[20.45903944166667,0.1138647,"point {fill-color: red}"],[3.086111831666667,0.043722,"point {fill-color: red}"],[71.28978561333334,0.37220909999999996,"point {fill-color: blue}"],[28.870723481666666,0.4992834,"point {fill-color: red}"],[2.959983771666667,0.14606249999999998,"point {fill-color: red}"],[85.65798081,0.2530921,"point {fill-color: blue}"],[34.65570706166667,0.1287787,"point {fill-color: red}"],[13.682872056666668,0.21241089999999999,"point {fill-color: red}"],[34.115176395,0.1617923,"point {fill-color: red}"],[25.928021343333334,0.32191949999999997,"point {fill-color: blue}"],[21.103599826666667,0.0800965,"point {fill-color: red}"],[85.10701037166668,0.4554698,"point {fill-color: blue}"],[69.37371559,0.3305394,"point {fill-color: blue}"],[73.361643245,0.8546243,"point {fill-color: blue}"],[1.922387565,0.0474819,"point {fill-color: red}"],[10.94825157,0.8821517999999999,"point {fill-color: red}"],[60.22903840666667,0.6054992,"point {fill-color: blue}"],[62.832648585,1.776465,"point {fill-color: blue}"]]);
  options.hAxis = {
    title: 'minutes'
  };
  options.vAxis = {
    title: 'seconds'
  };
  return new google.visualization.ScatterChart(document.getElementById('chart_div_aws-lambda-10-minutes\/old_scatter'));
});
</script>
<figure>
  <div id="chart_div_aws-lambda-10-minutes/old_scatter"></div>
  <figcaption class="imageCaption"><h4>Examples of AWS Lambda idle instances lifecycle in June 2019</h4></figcaption>
</figure>
<p>This chart plots the response duration (Y-axis) by the interval since the previous requests (X-axis). Each point represents a single request in the dataset. Blue points are cold starts, and red points are responses from warm instances.</p>
<p>While some cold starts happen after 10 minutes or so, many instances survive up to 1 hour.</p>
<h2 id="post-july-the-fixed-lifespan-of-10-minutes">Post-July: the fixed lifespan of 10 minutes</h2>
<p>The chart has changed in July:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'ID');
  data.addColumn('number', 'Value');
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows([[5.205542855,0.0729751,"point {fill-color: red}"],[2.950550171666667,0.10641429999999999,"point {fill-color: red}"],[21.773925093333332,0.6577742,"point {fill-color: blue}"],[17.136447813333334,4.2055996,"point {fill-color: blue}"],[8.678438453333333,0.08126789999999999,"point {fill-color: red}"],[7.511579011666667,0.09276709999999999,"point {fill-color: red}"],[9.738658645000001,0.11244219999999999,"point {fill-color: red}"],[55.04185414,3.9711586,"point {fill-color: blue}"],[21.243548555,2.8069797999999997,"point {fill-color: blue}"],[38.209268525,4.2607133,"point {fill-color: blue}"],[55.89774716833333,0.5142095,"point {fill-color: blue}"],[32.63548630166667,3.4478115999999996,"point {fill-color: blue}"],[5.314496326666667,0.142228,"point {fill-color: red}"],[18.875097523333334,0.3599982,"point {fill-color: blue}"],[21.034865136666667,0.2502255,"point {fill-color: blue}"],[2.1307685133333334,0.1471817,"point {fill-color: red}"],[27.960624361666667,0.7032609,"point {fill-color: blue}"],[5.02703192,0.1417984,"point {fill-color: red}"],[47.290301013333334,4.3324842,"point {fill-color: blue}"],[58.04031498,0.236867,"point {fill-color: blue}"],[33.783128768333334,0.3956788,"point {fill-color: blue}"],[27.090651733333335,0.42828099999999997,"point {fill-color: blue}"],[22.565576651666667,1.5347891,"point {fill-color: blue}"],[32.63414636,0.3581791,"point {fill-color: blue}"],[24.458362688333334,0.23919769999999999,"point {fill-color: blue}"],[20.523905925,0.6085069,"point {fill-color: blue}"],[24.79081742,0.5613693,"point {fill-color: blue}"],[16.008860071666668,0.4109767,"point {fill-color: blue}"],[13.163823415,0.9173602,"point {fill-color: blue}"],[38.091357423333335,1.5530088,"point {fill-color: blue}"],[21.81296898,0.2500099,"point {fill-color: blue}"],[26.801957608333336,1.5911244,"point {fill-color: blue}"],[35.680861621666665,0.3148438,"point {fill-color: blue}"],[16.67575253,1.5898845,"point {fill-color: blue}"],[7.109732576666667,0.056506499999999994,"point {fill-color: red}"],[1.9068108266666668,0.0918271,"point {fill-color: red}"],[15.357300865000001,4.3314313,"point {fill-color: blue}"],[3.885373285,0.068304,"point {fill-color: red}"],[26.202351248333333,0.28664629999999997,"point {fill-color: blue}"],[23.011914403333332,1.6756772999999998,"point {fill-color: blue}"],[1.9420863933333334,0.25440019999999997,"point {fill-color: red}"],[29.297437353333333,0.264167,"point {fill-color: blue}"],[29.40653468,0.354613,"point {fill-color: blue}"],[9.99938658,0.5570834,"point {fill-color: red}"],[7.18001534,0.6364238999999999,"point {fill-color: red}"],[7.807944281666667,0.0748373,"point {fill-color: red}"],[43.669602233333336,0.3297212,"point {fill-color: blue}"],[17.135989770000002,0.2279042,"point {fill-color: blue}"],[49.769005220000004,1.6557754999999998,"point {fill-color: blue}"],[48.00394794166667,0.2524152,"point {fill-color: blue}"],[5.506282056666667,0.1191433,"point {fill-color: red}"],[28.164396455000002,0.35999549999999997,"point {fill-color: blue}"],[33.04928259166667,0.757203,"point {fill-color: blue}"],[5.682673811666667,0.0683164,"point {fill-color: red}"],[3.21177282,0.1632293,"point {fill-color: red}"],[21.07322926666667,0.3272121,"point {fill-color: blue}"],[0.9020255466666667,0.2184137,"point {fill-color: red}"],[9.788402721666667,0.0699385,"point {fill-color: red}"],[9.333410851666667,0.7360896,"point {fill-color: red}"],[38.19660054166667,0.2631861,"point {fill-color: blue}"],[12.876818468333333,0.24644329999999998,"point {fill-color: blue}"],[14.977115771666668,5.248785499999999,"point {fill-color: blue}"],[30.234662506666666,1.3478455999999999,"point {fill-color: blue}"],[2.5742192966666666,0.0633237,"point {fill-color: red}"],[13.865113048333333,0.22283519999999998,"point {fill-color: blue}"],[10.794890818333334,2.1035478,"point {fill-color: blue}"],[28.932961621666667,0.9330444,"point {fill-color: blue}"],[2.8972817833333333,1.1923922999999998,"point {fill-color: red}"],[3.14905917,0.0735439,"point {fill-color: red}"],[14.62542796,0.3699729,"point {fill-color: blue}"],[28.146624486666667,0.2253889,"point {fill-color: blue}"],[57.90674627333333,1.3434393,"point {fill-color: blue}"],[26.862139135,0.2592239,"point {fill-color: blue}"],[33.539368198333335,0.2819228,"point {fill-color: blue}"],[3.3368404883333334,0.1401955,"point {fill-color: red}"],[10.948546381666667,0.2810572,"point {fill-color: blue}"],[12.40320724,4.5088441999999995,"point {fill-color: blue}"],[11.584143055,0.5768689,"point {fill-color: blue}"],[16.923735803333333,1.2714741,"point {fill-color: blue}"],[28.787613108333336,0.6679575999999999,"point {fill-color: blue}"],[52.41602769,1.9783807,"point {fill-color: blue}"],[21.883031556666666,0.47135439999999995,"point {fill-color: blue}"],[30.547405513333334,0.5419035999999999,"point {fill-color: blue}"],[27.964696043333333,0.43265149999999997,"point {fill-color: blue}"],[17.70462578,0.3022551,"point {fill-color: blue}"],[49.44523039833334,1.5939056,"point {fill-color: blue}"],[9.012838056666666,0.0721861,"point {fill-color: red}"],[20.959079543333335,0.3322603,"point {fill-color: blue}"],[48.506768575,1.2003183,"point {fill-color: blue}"],[19.94375859,4.4317548,"point {fill-color: blue}"],[22.395982421666666,0.2636794,"point {fill-color: blue}"],[25.350514098333335,0.2810781,"point {fill-color: blue}"],[43.09774109833334,4.1969408,"point {fill-color: blue}"],[26.227272545,0.2525319,"point {fill-color: blue}"],[44.967660001666665,4.2895959999999995,"point {fill-color: blue}"],[15.747613823333333,2.2996567,"point {fill-color: blue}"],[11.4916439,0.2540919,"point {fill-color: blue}"],[4.66845564,0.39803,"point {fill-color: red}"],[15.619132365,0.2643333,"point {fill-color: blue}"],[10.560933736666668,1.681524,"point {fill-color: blue}"],[4.895317276666667,0.0667763,"point {fill-color: red}"],[11.710955435,4.5365003,"point {fill-color: blue}"],[42.15546887166667,0.2732461,"point {fill-color: blue}"],[55.932061585,0.5618080999999999,"point {fill-color: blue}"],[5.7023475716666665,0.2815375,"point {fill-color: red}"],[4.538830501666666,0.1070778,"point {fill-color: red}"],[55.03088436,0.5004191,"point {fill-color: blue}"],[0.236882975,0.066214,"point {fill-color: red}"],[52.800519040000005,1.8903378,"point {fill-color: blue}"],[0.012698408333333334,0.0777326,"point {fill-color: red}"],[12.825518370000001,0.3066105,"point {fill-color: blue}"],[17.521510253333332,0.474263,"point {fill-color: blue}"],[46.33441173166667,0.2527368,"point {fill-color: blue}"],[4.665199341666667,0.0459963,"point {fill-color: red}"],[22.036639306666668,0.2759649,"point {fill-color: blue}"],[4.6440072416666665,0.08288949999999999,"point {fill-color: red}"],[9.667525656666667,0.13132359999999998,"point {fill-color: red}"],[10.925694471666667,0.3007049,"point {fill-color: blue}"],[7.546159071666667,0.06749909999999999,"point {fill-color: red}"],[38.92562516666667,0.36546409999999996,"point {fill-color: blue}"],[15.11285829,10.3576766,"point {fill-color: blue}"],[54.970940925,0.3952174,"point {fill-color: blue}"],[10.460719738333333,1.5359456999999999,"point {fill-color: blue}"],[20.071385555,0.2511376,"point {fill-color: blue}"],[13.375347798333333,0.290476,"point {fill-color: blue}"],[15.230204370000001,0.6791967999999999,"point {fill-color: blue}"],[9.594194141666668,0.2098504,"point {fill-color: red}"],[43.56448059,0.4917996,"point {fill-color: blue}"],[17.381496876666667,0.1719635,"point {fill-color: blue}"],[4.090363793333333,0.08397009999999999,"point {fill-color: red}"],[31.032993763333334,0.219763,"point {fill-color: blue}"],[12.852999543333334,1.6204977999999999,"point {fill-color: blue}"],[13.009825766666667,0.2275748,"point {fill-color: blue}"],[44.51589496333333,0.24906709999999999,"point {fill-color: blue}"],[48.29526980833334,9.4752444,"point {fill-color: blue}"],[28.11104407166667,4.3849355999999995,"point {fill-color: blue}"],[44.79007943166667,3.7983647,"point {fill-color: blue}"],[19.523992656666668,1.0391688,"point {fill-color: blue}"],[28.648346773333333,0.2724181,"point {fill-color: blue}"],[4.781787685,0.11107399999999999,"point {fill-color: red}"],[27.014539446666667,0.3432175,"point {fill-color: blue}"],[24.142719325,1.7235786,"point {fill-color: blue}"],[23.134774085,0.2685766,"point {fill-color: blue}"],[25.259148943333333,0.9360737,"point {fill-color: blue}"],[21.726234818333335,0.45835929999999997,"point {fill-color: blue}"],[4.732111293333333,0.0694786,"point {fill-color: red}"],[17.285875865,0.3481688,"point {fill-color: blue}"],[17.878259521666667,0.6408619,"point {fill-color: blue}"],[3.2312401833333335,0.0841282,"point {fill-color: red}"],[3.6145708216666668,0.21329199999999998,"point {fill-color: red}"],[35.61125183833333,0.7937889,"point {fill-color: blue}"],[26.891271548333332,0.39790719999999996,"point {fill-color: blue}"],[28.840341006666666,0.2858997,"point {fill-color: blue}"],[17.986112015,0.2457246,"point {fill-color: blue}"],[29.708526795,1.1692529,"point {fill-color: blue}"],[29.849828813333335,0.5814541,"point {fill-color: blue}"],[17.218032018333332,0.73374,"point {fill-color: blue}"],[7.6352141216666665,0.0787389,"point {fill-color: red}"],[46.09706425833333,0.34692599999999996,"point {fill-color: blue}"],[24.77022902166667,0.4238134,"point {fill-color: blue}"],[32.04818470833333,0.453159,"point {fill-color: blue}"],[39.297754938333334,0.3214896,"point {fill-color: blue}"],[54.77747788333333,0.5726214,"point {fill-color: blue}"],[4.967630998333333,0.0706193,"point {fill-color: red}"],[30.491208241666666,0.4764519,"point {fill-color: blue}"],[0.45684577833333334,0.0727438,"point {fill-color: red}"],[25.982041776666666,0.33197309999999997,"point {fill-color: blue}"],[57.58344489666667,0.213691,"point {fill-color: blue}"],[25.960420346666666,0.21276109999999998,"point {fill-color: blue}"],[29.374855788333335,0.2368525,"point {fill-color: blue}"],[8.095142546666667,0.0777544,"point {fill-color: red}"],[10.730351386666667,0.46126399999999995,"point {fill-color: blue}"],[22.310034181666666,0.3214547,"point {fill-color: blue}"],[45.88233271166667,4.3751836,"point {fill-color: blue}"],[6.267874396666667,0.7806938,"point {fill-color: red}"],[41.767281925,0.24014039999999998,"point {fill-color: blue}"],[7.488845013333333,0.0759958,"point {fill-color: red}"],[52.925085450000005,0.41132949999999996,"point {fill-color: blue}"],[6.531748058333333,0.44180929999999996,"point {fill-color: red}"],[21.633974655,3.5737514999999997,"point {fill-color: blue}"],[25.751592221666666,0.3723629,"point {fill-color: blue}"],[57.413430051666666,0.23598,"point {fill-color: blue}"],[48.22025750833333,2.1119955,"point {fill-color: blue}"],[2.4710389033333335,0.44573219999999997,"point {fill-color: red}"],[2.8164372933333333,0.11949109999999999,"point {fill-color: red}"],[29.739453695,1.3511077,"point {fill-color: blue}"],[59.85920454,3.6903542,"point {fill-color: blue}"],[5.759704396666667,0.0836941,"point {fill-color: red}"],[21.05319593,0.45559279999999996,"point {fill-color: blue}"],[20.98677186,0.3672477,"point {fill-color: blue}"],[14.277599351666668,1.5439053999999999,"point {fill-color: blue}"],[43.123278498333335,0.26792279999999996,"point {fill-color: blue}"],[7.085624431666667,0.074601,"point {fill-color: red}"],[12.454860156666667,5.1216139,"point {fill-color: blue}"],[13.457299926666668,0.5611008,"point {fill-color: blue}"],[30.474265956666667,0.20301829999999998,"point {fill-color: blue}"],[1.8632459883333334,0.07394099999999999,"point {fill-color: red}"],[4.897079163333333,0.1449926,"point {fill-color: red}"],[26.580458235000002,0.8365707,"point {fill-color: blue}"],[15.221323648333334,3.8955243,"point {fill-color: blue}"],[14.704649895000001,0.2722969,"point {fill-color: blue}"],[41.260677810000004,1.4169186,"point {fill-color: blue}"],[54.903552536666666,1.9525209,"point {fill-color: blue}"],[8.192975275,0.0726103,"point {fill-color: red}"],[31.99967567166667,0.2081352,"point {fill-color: blue}"],[36.805746365,3.8747236999999997,"point {fill-color: blue}"],[7.194568875,0.0575447,"point {fill-color: red}"],[17.264154758333333,0.272285,"point {fill-color: blue}"],[14.121679611666666,3.2790531,"point {fill-color: blue}"],[17.39999362,0.2981274,"point {fill-color: blue}"],[27.284415751666668,0.5823134,"point {fill-color: blue}"],[26.671761775,1.7537691,"point {fill-color: blue}"],[9.129264043333334,0.3617609,"point {fill-color: red}"],[3.6000143816666665,0.13209959999999998,"point {fill-color: red}"],[26.62542948,2.2690422999999997,"point {fill-color: blue}"],[0.4851755716666667,0.1090476,"point {fill-color: red}"],[17.10813361,3.6408894999999997,"point {fill-color: blue}"],[47.54190605,0.44462009999999996,"point {fill-color: blue}"],[45.536380155,0.26159869999999996,"point {fill-color: blue}"],[33.611594231666665,8.890205,"point {fill-color: blue}"],[41.142801545,0.23901019999999998,"point {fill-color: blue}"],[9.964273803333333,0.13344799999999998,"point {fill-color: red}"],[5.773736171666667,0.072779,"point {fill-color: red}"],[4.383194816666666,0.0799802,"point {fill-color: red}"],[2.3502029483333335,0.0455345,"point {fill-color: red}"],[28.327197445,0.3577012,"point {fill-color: blue}"],[21.279258655,0.2260094,"point {fill-color: blue}"],[5.166607976666667,0.08948779999999999,"point {fill-color: red}"],[18.53387572666667,4.200319299999999,"point {fill-color: blue}"],[18.792424438333335,4.4998143,"point {fill-color: blue}"],[2.407105663333333,0.152107,"point {fill-color: red}"],[14.70829689,1.97461,"point {fill-color: blue}"],[12.500928955000001,0.35082199999999997,"point {fill-color: blue}"],[37.29890278,0.2830051,"point {fill-color: blue}"],[5.763145691666667,0.08747669999999999,"point {fill-color: red}"],[10.517388195,0.2602002,"point {fill-color: blue}"],[19.645800401666666,1.9795189999999998,"point {fill-color: blue}"],[3.022166921666667,0.06750969999999999,"point {fill-color: red}"],[48.79278951333333,0.28050939999999996,"point {fill-color: blue}"],[18.927243258333334,0.3267169,"point {fill-color: blue}"],[47.107094028333336,5.2347157,"point {fill-color: blue}"],[26.29755139,0.2246791,"point {fill-color: blue}"],[10.797636368333334,3.6805765,"point {fill-color: blue}"],[34.219281305,2.0022965999999998,"point {fill-color: blue}"],[56.41317888666667,4.1769321999999995,"point {fill-color: blue}"],[4.5590787950000005,1.4886850999999999,"point {fill-color: red}"],[24.624643006666666,0.3440535,"point {fill-color: blue}"],[13.489810871666666,0.7923926,"point {fill-color: blue}"],[53.48551212,0.20091299999999998,"point {fill-color: blue}"],[1.24179509,0.08562399999999999,"point {fill-color: red}"],[3.9594451250000002,0.1512782,"point {fill-color: red}"],[26.92009459166667,0.3054511,"point {fill-color: blue}"],[25.25817851,1.9681855,"point {fill-color: blue}"],[17.85744654666667,0.3193709,"point {fill-color: blue}"],[46.36070672,0.2474633,"point {fill-color: blue}"],[58.90165328166667,0.2810061,"point {fill-color: blue}"],[18.364332798333333,0.4058022,"point {fill-color: blue}"],[12.203593395,10.4270149,"point {fill-color: blue}"],[21.78381891,0.24661989999999998,"point {fill-color: blue}"],[44.33441240166667,2.3375190999999997,"point {fill-color: blue}"],[48.22288827333333,0.3076607,"point {fill-color: blue}"],[25.653182428333334,0.3227139,"point {fill-color: blue}"],[6.282574893333334,0.07130449999999999,"point {fill-color: red}"],[28.147908983333334,4.9963152,"point {fill-color: blue}"],[3.3192418766666667,0.086087,"point {fill-color: red}"],[30.014725375,0.3139358,"point {fill-color: blue}"],[49.40947267333333,0.35711339999999997,"point {fill-color: blue}"],[13.388344886666667,1.9394331999999999,"point {fill-color: blue}"],[17.28361115,0.3440081,"point {fill-color: blue}"],[25.255335603333332,1.3016328,"point {fill-color: blue}"],[48.93193205,0.2049797,"point {fill-color: blue}"],[9.406192488333334,0.26661799999999997,"point {fill-color: red}"],[17.51506733,0.5035815,"point {fill-color: blue}"],[58.26915884666667,0.3745981,"point {fill-color: blue}"],[16.35743708,3.3837067,"point {fill-color: blue}"],[55.661343255,3.7016237999999997,"point {fill-color: blue}"],[26.425328968333332,0.3398334,"point {fill-color: blue}"],[12.009492093333334,1.2590331,"point {fill-color: blue}"],[13.432812096666668,0.2782039,"point {fill-color: blue}"],[18.260913696666666,2.4154196,"point {fill-color: blue}"],[9.698259533333333,0.25290809999999997,"point {fill-color: red}"],[16.484434635,0.4616938,"point {fill-color: blue}"],[5.092524091666667,0.2843421,"point {fill-color: red}"],[38.69841969666667,0.6556539,"point {fill-color: blue}"],[12.461642073333334,4.2124334999999995,"point {fill-color: blue}"],[11.512802061666667,0.2357192,"point {fill-color: blue}"],[22.023669928333334,2.3117810999999997,"point {fill-color: blue}"],[21.585668296666668,0.2358108,"point {fill-color: blue}"],[9.936649116666667,0.1610255,"point {fill-color: red}"],[43.849753195,5.2367669,"point {fill-color: blue}"],[4.758206058333333,0.14549969999999998,"point {fill-color: red}"],[3.2772610983333332,0.1238232,"point {fill-color: red}"],[51.32073028833334,1.9391188,"point {fill-color: blue}"],[24.802597218333332,0.2987158,"point {fill-color: blue}"],[30.024298853333335,0.2029456,"point {fill-color: blue}"],[23.994000766666666,0.662655,"point {fill-color: blue}"],[3.722260416666667,0.0834248,"point {fill-color: red}"],[45.91238661166667,0.8396652,"point {fill-color: blue}"],[5.921298566666667,0.0813429,"point {fill-color: red}"]]);
  options.hAxis = {
    title: 'minutes'
  };
  options.vAxis = {
    title: 'seconds'
  };
  return new google.visualization.ScatterChart(document.getElementById('chart_div_aws-lambda-10-minutes\/new_scatter'));
});
</script>
<figure>
  <div id="chart_div_aws-lambda-10-minutes/new_scatter"></div>
  <figcaption class="imageCaption"><h4>Examples of AWS Lambda idle instances lifecycle in August 2019</h4></figcaption>
</figure>
<p>There is no ambiguity anymore: every instance lives for precisely 10 minutes after the last request.</p>
<p>This behavior is similar to Azure Functions, but the value is two times shorter: 10 minutes for AWS versus 20 minutes for Azure.</p>
<h2 id="month-over-month-view">Month-over-month view</h2>
<p>I broke down the data set by month and calculated the periods when an idle instance is recycled with 10%, 50%, and 90% probabilities. Here is the chart that I produced, the higher line means the higher duration of survival:</p>




  







<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('string', 'Time');

  const points = [{"items":[["March",29],["April",28],["May",29],["June",21],["July",11],["August",10]],"name":"90% survive"},{"items":[["March",44],["April",44],["May",46],["June",37],["July",15],["August",10]],"name":"50% survive"},{"items":[["March",59],["April",59],["May",60],["June",58],["July",16],["August",11]],"name":"10% survive"}];
  const seriesCount = points.length;
  for (var j = 0; j < seriesCount; j++) {
    data.addColumn('number', points[j].name);
  }

  const rows = [];
  for (var i = 0; i < points[0].items.length; i++)
  {
    const item = [points[0].items[i][0]];
    for (var j = 0; j < seriesCount; j++) {
      item.push(points[j].items[i][1]);
    }
    rows.push(item);
  }

  data.addRows(rows);
  console.debug(rows);
  options.lineWidth = 3;
  options.hAxis = {
    title: '2019 by month'
  };
  options.vAxis = {
    title: 'minutes',
    maxValue: 1.0
  };
  options.legend = { position: 'top' };
  options.series = [{"color":"green"},{"color":"yellow"},{"color":"red"}];
  return new google.visualization.AreaChart(document.getElementById('chart_div_aws-lambda-10-minutes\/interval_history'));
});
</script>
<figure>
  <div id="chart_div_aws-lambda-10-minutes/interval_history"></div>
  <figcaption class="imageCaption"><h4>The typical lifetime of an idle AWS Lambda</h4></figcaption>
</figure>
<p>Once again, we can see how the broader range of probabilistic lifespans collapsed into the deterministic 10-minute threshold.</p>
<h2 id="practical-takeaway">Practical Takeaway</h2>
<p>Applications with infrequent invocations running in AWS Lambda might experience more cold starts compared to pre-July time.</p>
<p>Cold starts are a major perceived pain point of serverless services. While cloud providers are fighting the cold starts on many fronts, it&rsquo;s interesting to see a change in AWS Lambda, which seemingly goes the opposite direction.</p>
<p>On the other hand, the predictability of a fixed lifespan makes the keep-it-warm workarounds more reliable. Whenever you feel a cold start might be a problem for your Lambda, you might want to issue a warm-up request every 5 minutes. You can find an example of doing so in a structured way in <a href="/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<p>Read more about <a href="/serverless/coldstarts/aws/">Cold Starts in AWS Lambda</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cost]]></title>
            <link href="https://mikhail.io/tags/cost/"/>
            <id>https://mikhail.io/tags/cost/</id>
            
            <published>2019-08-07T00:00:00+00:00</published>
            <updated>2019-08-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How to Measure the Cost of Azure Functions]]></title>
            <link href="https://mikhail.io/2019/08/how-to-measure-the-cost-of-azure-functions/"/>
            <id>https://mikhail.io/2019/08/how-to-measure-the-cost-of-azure-functions/</id>
            
            <published>2019-08-07T00:00:00+00:00</published>
            <updated>2019-08-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Azure pricing can be complicated—to get the most value out of your cloud platform, you need to know how to track spend and measure the costs incurred by Azure Functions.</blockquote><p>Azure Functions can be hosted in multiple ways: there&rsquo;s an App Service plan with a fixed cost per hour, a new Premium plan with both fixed and variable price components, not to mention self-managed options based on container technologies. Instead of any of those three, this article focuses on Consumption plan—the classic serverless offering billed purely based on the actual usage.</p>
<h2 id="how-is-serverless-different">How Is Serverless Different?</h2>
<p>Serverless Functions have three important properties, each of them having a significant impact on the way we deal with the cost of an application:</p>
<ul>
<li>
<p><strong>Low management overhead</strong>: the cloud provider manages the service. The total cost of ownership is minimal: developers create code to solve business problems—the rest is taken care of.</p>
</li>
<li>
<p><strong>Pay-per-use</strong>: Functions are charged per actual executions. Nothing is reserved in advance, so the cost of running a Function App grows linearly with the application demand.</p>
</li>
<li>
<p><strong>Elastically scalable</strong>: when a Function is idle, Azure scales the infrastructure down to zero with no associated cost. Whenever the workload grows, Azure brings enough capacity to serve all the demand.</p>
</li>
</ul>
<h4 id="why-does-this-matter">Why does this matter?</h4>
<p>In the past, the cost of infrastructure running an application and the value that the application provides were separated. A company would run multiple applications, each having numerous components and services, on a shared infrastructure of dedicated hardware, a pool of VMs, or IaaS services in the cloud. It&rsquo;s quite complicated to tear the expenses apart and determine the exact cost of each application, let alone a particular component. Moreover, the investments are planned and executed in advance, so the infrastructure can&rsquo;t follow the elasticity of workload and ends up overprovisioned and underutilized.</p>
<p>Today, the application portfolio can run on serverless functions, each component becoming a separate Function App. In turn, each App may contain multiple functions. Since the company pays per actual use, they can understand and manage the exact cost of each component.</p>
<p>This ability enables the business to:</p>
<ul>
<li>See which features are most profitable, and which ones are too expensive;</li>
<li>Optimize the Functions that have high impact on the invoice and ignore the rest;</li>
<li>Make informed decisions of whether to spend engineering time on such optimizations or create new business value while paying a premium to the cloud provider.</li>
</ul>
<p>Note that the cost information is retrospective: the actual numbers come after the fact of spending. The lack of budgeting makes decision makers nervous: they are used to plan the cost of infrastructure long in advance. Therefore, our goal is to understand the cost structure and be able to predict the changes in invoices as applications and business evolve.</p>
<h2 id="billing-model-of-the-consumption-plan">Billing Model of the Consumption Plan</h2>
<p>Let&rsquo;s dissect the structure of the Consumption plan. There are two core components of the cost of serverless Functions in Azure: <strong>Execution Count</strong> and <strong>Execution Time</strong>.</p>
<p><em>Execution Count</em> is straightforward. Each Function defines a trigger—an event which causes the code to execute. It can be an incoming HTTP request or a message in a given queue. Every call counts: you get charged $0.20 per million executions. This component of the cost can be substantially reduced if you batch events: process several events in a single execution.</p>
<p>The second cost component is called <em>Execution Time</em> on the pricing page, which isn&rsquo;t exactly correct: it depends on both completion time and memory consumption and is metered in <em>GB-seconds</em>. You pay $16 per million GB-seconds. That is, if a Function runs 1 million times, it always consumes 1 GB of memory and completes in 1 second, you pay $16.</p>
<p>The memory consumption is always rounded up to the next 128 MB, and the minimum time charge is 100 milliseconds. Therefore, the minimal time charge is again $0.20 per million executions.</p>
<p>Holistically, there are other cost components of running serverless applications not tied directly to the Azure Functions service: I&rsquo;ll briefly touch on those at the end of the article.</p>
<h2 id="azure-bill-and-cost-analysis">Azure Bill and Cost Analysis</h2>
<p>The prominent place to see the cost of operating Azure Functions is the monthly bill. Open the Azure portal and navigate to your subscription&rsquo;s page, choose <em>Invoices</em> and then a period to look at. Here is a sample report from a subscription of mine:</p>
<p><img src="invoiced-units.png" alt="Consumed Function Units on an Azure invoice"></p>
<figcaption><h4>Consumed Function Units on an Azure invoice</h4></figcaption>
<p>You should be able to see the incurred change for the two metrics we discussed above: <em>Total Executions</em> and <em>Execution Time</em> (the metric in GB-seconds).</p>
<p>With <em>Cost Analysis</em> tool, you can see the billing data at per day granularity. Depending on your analysis goal, you might need more details than that. For instance, you may wonder how the cost is spread over periods within a day or be able to predict the future cost based on short trials before the application (or its newer version) goes into production.</p>
<p>Let&rsquo;s see how to dig deeper into the details.</p>
<h2 id="azure-monitor-metrics">Azure Monitor Metrics</h2>
<p><strong>Azure Monitor</strong> is a service for collecting, analyzing, and acting on telemetry from applications running in the Azure cloud. While mostly focusing on performance, it also collects some useful data related to service consumption.</p>
<p>Azure Functions issue two cost-related metrics into Azure Monitor: <em>Function Execution Count</em> and <em>Function Execution Units</em>. Each metric emits a value once every minute.</p>
<p>To see these metrics in the Azure portal, navigate to the <em>Monitor</em> service and select the <em>Metrics</em> item on the left. Click <em>Select a Resource</em> button and find the Function App that you want to investigate. Note that the <em>Resource Type</em> should be set to <em>App Service</em>: there&rsquo;s no value called <em>Function App</em> in that dropdown.</p>
<p>Select <em>Function Execution Count</em> in the Metrics dropdown, <em>Sum</em> as the aggregation type, and adjust the period selector as needed. You should now see a chart similar to this one:</p>
<p><img src="monitor-execution-count.png" alt="Function Execution Count in Azure Monitor"></p>
<figcaption><h4>Function Execution Count in Azure Monitor</h4></figcaption>
<p>In this particular case, there were about 4,940 executions in the last 30 minutes. You can view the stats per minute; in this example, all the executions come from a single spike—something that I might want to investigate.</p>
<p>Now, switch the metric to <em>Function Execution Units</em>. Alternatively, you can add it to the same chart, but the scale of the two metrics is so different that you won&rsquo;t be able to see both lines at the same time:</p>
<p><img src="monitor-execution-units.png" alt="Function Execution Units in Azure Monitor"></p>
<figcaption><h4>Function Execution Units in Azure Monitor</h4></figcaption>
<p>The value conversion gets a bit tricky here. The chart shows a total of 634.13 million <em>Function Execution Units</em> consumed in the last hour. These are not the GB-seconds mentioned above, though: the metric is nominated in MB-milliseconds. To convert this to GB-seconds, divide it by 1,024,000. So, in this case, my Function App consumed 634,130,000 / 1,024,000 = 619 GB-seconds in the last half-an-hour.</p>
<h2 id="sum-it-up">Sum It Up</h2>
<p>Let&rsquo;s estimate the monthly cost of the application based on the metrics above. We start with the  half-an-hour calculation:</p>
<pre><code>Execution Count = 4,940 * $0.20 / 1,000,000 = $0.000988
Execution Time = 634,130,000 / 1,024,000 * $16 / 1,000,000 = $0.009908
30 Min Total = Execution Count + Execution Time = $0.010896
</code></pre><p>If I keep the same average workload over a month, the cost is going to be:</p>
<pre><code>Monthly Cost = 30 Min Total * 2 * 24 * 30 = $15.69
</code></pre><p>My application costs about 2 cents an hour, or 15 bucks a month.</p>
<h2 id="dashboards">Dashboards</h2>
<p>If you only need to look at the data once, the <em>Metrics</em> screen above should be sufficient.</p>
<p>For continuous monitoring of the metrics, you can put the same charts onto your Azure Dashboard. On the same screen, Click <em>Pin to dashboard</em> button and then navigate to the <em>Dashboard</em> menu item of the portal. You should see your chart added:</p>
<p><img src="monitoring-dashboard.png" alt="Monitoring Dashboard"></p>
<figcaption><h4>Monitoring Dashboard</h4></figcaption>
<p>You might have several Function Apps to monitor at this point. You can either add a separate chart for each one of them, or add several lines to the same chart, or a combination of both. To customize the name of the dashboard item, click on the chart, edit the name, and click <em>Update Dashboard</em> button.</p>
<p>The dashboard has a period selector at the top, which allows changing the visible time interval of all charts on the fly—very handy to zoom in and zoom out to go from overview to a more nuanced view of the cost breakdown and back.</p>
<h2 id="api">API</h2>
<p>The user interface of the Azure portal is helpful, but you might want to integrate the data into other tools used in your organization. For that, you can still use metrics from Azure Monitor but retrieve the values programmatically. One way to do so is to request information from the REST API periodically. In the following template, replace the parameters in curly braces with your actual Azure resources:</p>
<pre><code>GET /subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.Web/sites/{function-app-name}/providers/microsoft.insights/metrics?api-version=2018-01-01&amp;metricnames=FunctionExecutionUnits,FunctionExecutionCount
Host: management.azure.com
Authorization: Bearer {access-token}
</code></pre><p>The access token can be obtained from the Azure Command Line Interface (CLI) with the command <code>az account get-access-token</code>.</p>
<p>By the way, you can also get the same metric values within the CLI itself:</p>
<pre><code>az monitor metrics list --resource /subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.Web/sites/{function-app-name} --metric FunctionExecutionUnits,FunctionExecutionCount --aggregation Total --interval PT1M
</code></pre><p>In both cases, you receive a JSON response with time series data. Here is a snippet to illustrate the most useful bits:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="">...</span>
<span style="color:#a31515">&#34;name&#34;</span><span style="">:</span> {
    &#34;localizedValue&#34;: <span style="color:#a31515">&#34;Function Execution Units&#34;</span>,
    &#34;value&#34;: <span style="color:#a31515">&#34;FunctionExecutionUnits&#34;</span>
}<span style="">,</span>
<span style="color:#a31515">&#34;timeseries&#34;</span><span style="">:</span> [
{
    &#34;data&#34;: [
    {
        &#34;timeStamp&#34;: <span style="color:#a31515">&#34;2019-07-05T10:23:00+00:00&#34;</span>,
        &#34;total&#34;: 127.0
    },
    {
        &#34;timeStamp&#34;: <span style="color:#a31515">&#34;2019-07-05T10:24:00+00:00&#34;</span>,
        &#34;total&#34;: 34.1
    }
    <span style="">...</span>
</code></pre></div><p>The reported time interval and granularity are adjustable with HTTP query and command parameters.</p>
<p>Please note that Azure Monitor has a retention period of 30 days, which is as far as you can go to observe the historical data. At the moment, I couldn&rsquo;t find a built-in capability to stream the Function App execution metrics into long-term storage. Therefore, to store a copy of the data, you need to implement the integration based on periodic calls to Metrics API and persist the response into the storage of your choice. Azure Table Storage could be one pragmatic solution for this purpose.</p>
<h2 id="application-insights-metrics">Application Insights Metrics</h2>
<p>Azure Monitor is a great tool in addition to observing the monthly bill. However, its resolution is still limited in two ways:</p>
<ul>
<li>It reports aggregated values with a resolution of one minute</li>
<li>It combines metrics coming from all the Functions belonging to the same Function App into a single value</li>
</ul>
<p>There&rsquo;s currently no way to get the cost of <em>GB-seconds</em> consumption per single execution. However, you can get the duration of each execution from a tool called <strong>Application Insights</strong>. Since the metrics are reported individually per each Function and each execution, they should help you estimate the cost structure for a given Function App.</p>
<p>The best tool to explore this data is <em>Application Insights Logs</em>. Select the Application Insights account associated with your Function App and click <em>Logs (Analytics)</em> in the toolbar and put the following query into the editor:</p>
<pre><code>customMetrics
| where name contains &quot;AvgDurationMs&quot;
| limit 100
</code></pre><p>The query retrieves a hundred sample metric values with the <em>name</em> column reflecting the Azure Function name, <em>Target</em> in this case, and <em>value</em> showing the execution duration in milliseconds:</p>
<p><img src="duration-logs.png" alt="Duration in Application Insights Logs"></p>
<figcaption><h4>Duration in Application Insights Logs</h4></figcaption>
<p>The same metric can be used to plot the duration distribution in time; here is a sample query:</p>
<pre><code>customMetrics
| where name contains &quot;AvgDurationMs&quot;
| summarize sum(value) by name, bin(timestamp, 1m)
| render timechart
</code></pre><p><img src="duration-chart.png" alt="Duration over Time in Application Insights Logs"></p>
<figcaption><h4>Duration over Time in Application Insights Logs</h4></figcaption>
<p>You can see that the Function plotted in blue has spent much more execution time than the green one.</p>
<h2 id="cost-beyond-functions">Cost Beyond Functions</h2>
<p>It&rsquo;s important to note that we only discussed the direct cost of Azure Functions executions. There are several other potential costs associated with running an Azure Function App:</p>
<ul>
<li>Application Insights. Depending on the event volume and sampling settings, the cost of this monitoring service can become quite substantial and exceed the cost of Azure Functions themselves. Be careful and test your configuration before and soon after going to production.</li>
<li>Network traffic. If your Functions serve the traffic to the outside world, the networking fees apply. They are usually relatively low, but the cost may build up for high-volume Functions serving bulk data.</li>
<li>Storage. Azure Functions use a Storage Account for internal state and coordination. In my experience, these costs are negligible.</li>
</ul>
<p>I hope this article gives you enough perspective to start looking into the real cost of your serverless applications. May your Functions stay performant and the bill low!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/cost" term="cost" label="Cost" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[7 Ways to Deal with Application Secrets in Azure]]></title>
            <link href="https://mikhail.io/2019/07/7-ways-to-deal-with-application-secrets-in-azure/"/>
            <id>https://mikhail.io/2019/07/7-ways-to-deal-with-application-secrets-in-azure/</id>
            
            <published>2019-07-26T00:00:00+00:00</published>
            <updated>2019-07-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>From config files to Key Vault and role-based access, learn how infrastructure as code helps manage application secrets in Azure.</blockquote><p>Every non-trivial application relies on configuration values that may depend on the current execution environment. Some of these values contain sensitive information that shouldn&rsquo;t be shared publicly. In general, the fewer parties that have access to those secret values, the safer the application will be—in fact, in an ideal world, no one would be granted direct access to those secrets.</p>
<p>Examples of secret configuration values include:</p>
<ul>
<li>A connection string to a message bus or a database</li>
<li>A SAS Token to an Azure Storage account</li>
<li>An access key for a third-party service</li>
</ul>
<p>There&rsquo;s no one universal way to manage secrets, as a lot depends on the context in which they are used. In this article, I go through seven ways to use secret values in a .NET Core application running in Azure. I start with naively hard-coded strings and build up from there to more secure options.</p>
<p>While the concepts are universally applicable, my code samples focus on a .NET application running in <strong>Azure App Service</strong> and configured with Pulumi.</p>
<h2 id="1-hard-coded-secrets">1. Hard-coded Secrets</h2>
<p>Whenever you want to try a new API requiring a secret access token, it&rsquo;s natural to copy-paste that secret into your code and run it—simply to make sure that the setup works.</p>
<p>For example, you are testing integration with a payment service:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> apiKey = <span style="color:#a31515">&#34;payment-service-key&#34;</span>;
<span style="color:#2b91af">var</span> paymentService = <span style="color:#00f">new</span> PaymentService(apiKey);
</code></pre></div><p>or you want to start sending messages to an <strong>Azure Storage Queue</strong>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> connectionString = <span style="color:#a31515">&#34;DefaultEndpointsProtocol=https;AccountName=account-name;AccountKey=account-key&#34;</span>;
<span style="color:#2b91af">var</span> storageAccount = CloudStorageAccount.Parse(connectionString);
<span style="color:#2b91af">var</span> queueClient = storageAccount.CreateCloudQueueClient();
</code></pre></div><p>Both snippets are fine for a Hello-World application with a lifespan of two hours. However, I would strongly discourage doing so in any code that can potentially be checked into a source control system, even for a 10-minute experiment. Don&rsquo;t copy-paste secrets into files which are part of a git repository. One accidental <code>git commit &amp; git push</code>—and the secrets are compromised.</p>
<p>Avoid the &ldquo;Secrets as Code&rdquo; practice: there are bots scanning your GitHub repository for those.</p>
<h2 id="2-configuration-files">2. Configuration Files</h2>
<p>Traditionally, putting secrets in a configuration file is considered more secure. For classic .NET applications, this would be an <code>app.config</code> or a <code>web.config</code> file. The idea is that the values on a developer machine are different from the values in the production environment. Non-sensitive development settings are kept on disk and maybe in source control, while the real secrets are only injected by a deployment script or a CI/CD system, so they are not exposed publicly.</p>
<p>In the .NET Core world, a configuration is usually stored in an <code>appsettings.json</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   &#34;MyConfig&#34;: {
       &#34;PaymentApiKey&#34;: <span style="color:#a31515">&#34;payment-service-key&#34;</span>,
       &#34;StorageConnectionString&#34;: <span style="color:#a31515">&#34;DefaultEndpointsProtocol=https;AccountName=account-name;AccountKey=account-key&#34;</span>
  }
}
</code></pre></div><p>which is then mapped to a plain C# object in code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">MyConfig</span>
{
   <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> PaymentApiKey { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
   <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> StorageConnectionString { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>With some configuration not shown here, the properties are filled at startup time and can be used in the code as a plain object:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> apiKey = config.PaymentApiKey;
<span style="color:#2b91af">var</span> paymentService = <span style="color:#00f">new</span> PaymentService(apiKey);
<span style="color:#008000">// ...
</span><span style="color:#008000"></span>
<span style="color:#2b91af">var</span> connectionString = config.StorageConnectionString;
<span style="color:#2b91af">var</span> storageAccount = CloudStorageAccount.Parse(connectionString);
<span style="color:#2b91af">var</span> queueClient = storageAccount.CreateCloudQueueClient();
</code></pre></div><p>In my experience, such setup still poses substantial risks. <code>appsettings.production.json</code> may still be accidentally checked into the source control. Developers tend to use real cloud resources for their local and test environments, which do contain sensitive information that can be exploited when leaked.</p>
<p>On top of that, it doesn&rsquo;t feel right to mix configuration—how many threads to run, or how big the message batches should be—with secret connection strings and API keys in the same file. These are separate kinds of configuration, and they warrant different workflows.</p>
<h2 id="3-environment-variables-and-application-settings">3. Environment Variables and Application Settings</h2>
<p>One alternative approach is to read the secret values from environment variables. .NET Core configuration system can parse environment variables instead of, or in addition to the settings files. Likewise, one could read such values with a one-liner in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> apiKey = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;PAYMENT_API_KEY&#34;</span>);
<span style="color:#008000">// ...
</span><span style="color:#008000"></span>
<span style="color:#2b91af">var</span> connectionString = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;STORAGE_CONNECTION_STRING&#34;</span>);
<span style="color:#008000">// ...
</span></code></pre></div><p>Your CI/CD system should inject those values as part of the deployment pipeline.</p>
<p>The App Service gives you the ability to set environment variables via <strong>Application Settings</strong>. Here is a snippet of a Pulumi program that passes both secret values to <code>appSettings</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> cfg = <span style="color:#00f">new</span> pulumi.Config();
<span style="color:#00f">const</span> paymentApiKey = cfg.requireSecret(<span style="color:#a31515">&#34;paymentApiKey&#34;</span>);

<span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;storage&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
});

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,
   appSettings: {
       <span style="color:#a31515">&#34;PAYMENT_API_KEY&#34;</span>: paymentApiKey,
       <span style="color:#a31515">&#34;STORAGE_CONNECTION_STRING&#34;</span>: storageAccount.primaryConnectionString,
   },
});
</code></pre></div><p>The Storage connection string is produced by the Pulumi program directly, so it doesn&rsquo;t have to be placed anywhere outside the program itself.</p>
<p>The payment service key is provided by a third party, so its encrypted value is stored in Pulumi configuration. Read <a href="https://www.pulumi.com/blog/managing-secrets-with-pulumi/">Managing Secrets with Pulumi</a> to learn about security options available for secrets in Pulumi config.</p>
<h2 id="4-azure-key-vault">4. Azure Key Vault</h2>
<p>In the previous example, both secrets end up in Application Settings. Every person with sufficient permissions may go to the App Service and see them in clear text. While this can be restricted, it&rsquo;s a good idea to grant full read access to the application developers and operators of the App Service to give them the full picture when troubleshooting issues.</p>
<p>In some cases, compliance to a certain standard may <em>require</em> the use of a certified key management service offering enhanced security for sensitive secrets.</p>
<p>Azure has a dedicated service for storing secrets, <strong>Azure Key Vault</strong>. You can create and populate a Key Vault with all the secrets from the same Pulumi program:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> vault = <span style="color:#00f">new</span> azure.keyvault.KeyVault(<span style="color:#a31515">&#34;vault&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   sku: {
       name: <span style="color:#a31515">&#34;standard&#34;</span>,
   },
   tenantId: <span style="color:#2b91af">tenantId</span>,
   accessPolicies: [{
       tenantId,
       <span style="color:#008000">// The current principal has to be granted permissions to Key Vault so that it can actually add and then remove
</span><span style="color:#008000"></span>       <span style="color:#008000">// secrets to/from the Key Vault. Otherwise, &#39;pulumi up&#39; and &#39;pulumi destroy&#39; operations will fail.
</span><span style="color:#008000"></span>       objectId: <span style="color:#2b91af">currentPrincipal</span>,
       secretPermissions: [<span style="color:#a31515">&#34;delete&#34;</span>, <span style="color:#a31515">&#34;get&#34;</span>, <span style="color:#a31515">&#34;list&#34;</span>, <span style="color:#a31515">&#34;set&#34;</span>],
   }],
});

<span style="color:#00f">const</span> secret = <span style="color:#00f">new</span> azure.keyvault.Secret(<span style="color:#a31515">&#34;paymentApiKey&#34;</span>, {
   keyVaultId: <span style="color:#2b91af">vault.id</span>,
   value: <span style="color:#2b91af">paymentApiKey</span>,
});
</code></pre></div><p>Additionally, one or many <strong>Service Principals</strong> (SP) should be configured to access the Key Vault. One SP which has been granted secret management access deploys the infrastructure by running the Pulumi program. That&rsquo;s why the snippet above assigns an access policy to <code>currentPrincipal</code>.</p>
<p>Another SP is used by the application itself to read the secret values. The Client ID and Client Secret parameters are placed in the application configuration. Finally, .NET Core configuration gets hooked to the Key Vault at startup:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs">builder.AddAzureKeyVault(
   <span style="color:#a31515">$&#34;https://{config[&#34;</span>azureKeyVault:vault<span style="color:#a31515">&#34;]}.vault.azure.net/&#34;</span>,
   config[<span style="color:#a31515">&#34;azureKeyVault:clientId&#34;</span>],
   config[<span style="color:#a31515">&#34;azureKeyVault:clientSecret&#34;</span>]
);
</code></pre></div><p>This solution is not entirely satisfying though, since we&rsquo;ve traded storing the secrets for storing SP credentials. Is that a big enough win?  Luckily, it&rsquo;s easy to get rid of those credentials with Managed identities.</p>
<h2 id="5-accessing-key-vault-with-managed-identities">5. Accessing Key Vault with Managed Identities</h2>
<p>With <strong>Managed identities</strong>, Azure takes care of creating a Service Principal, passing the credentials, rotating secrets, and so on. Enabling a managed identity on App Service is just an extra option:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,
   <span style="color:#008000">// A system-assigned managed identity
</span><span style="color:#008000"></span>   identity: {
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;SystemAssigned&#34;</span>,
   },
});
</code></pre></div><p>On top of that, the managed principal must be granted access to the Key Vault:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> principalId = app.identity.apply(id =&gt; id.principalId);

<span style="color:#008000">// Grant App Service access to KV secrets
</span><span style="color:#008000"></span><span style="color:#00f">new</span> azure.keyvault.AccessPolicy(<span style="color:#a31515">&#34;app-policy&#34;</span>, {
   keyVaultId: <span style="color:#2b91af">vault.id</span>,
   tenantId: <span style="color:#2b91af">tenantId</span>,
   objectId: <span style="color:#2b91af">principalId</span>,
   secretPermissions: [<span style="color:#a31515">&#34;get&#34;</span>],
});
</code></pre></div><p>Now, the configuration block in the .NET Core app doesn&rsquo;t need to retrieve any secrets. <code>AzureServiceTokenProvider</code> helps with the authentication process:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> azureServiceTokenProvider = <span style="color:#00f">new</span> AzureServiceTokenProvider();
<span style="color:#2b91af">var</span> keyVaultClient = <span style="color:#00f">new</span> KeyVaultClient(
   <span style="color:#00f">new</span> KeyVaultClient.AuthenticationCallback(
       azureServiceTokenProvider.KeyVaultTokenCallback));
builder.AddAzureKeyVault(
   keyVaultEndpoint, keyVaultClient, <span style="color:#00f">new</span> DefaultKeyVaultSecretManager());
</code></pre></div><p>That&rsquo;s quite a bit of a boilerplate, but there is a way to get rid of it.</p>
<h2 id="6-accessing-key-vault-from-application-settings">6. Accessing Key Vault from Application Settings</h2>
<p>App Service has a neat feature of integrating its Application Settings with Key Vault. It allows us to combine #3 and #5&rsquo;s approaches and get the best of both:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Produce a URI of the KV secret defined above
</span><span style="color:#008000"></span><span style="color:#00f">const</span> secretUri = pulumi.interpolate<span style="color:#a31515">`</span><span style="color:#a31515">${</span>secret.vaultUri<span style="color:#a31515">}</span><span style="color:#a31515">secrets/</span><span style="color:#a31515">${</span>secret.name<span style="color:#a31515">}</span><span style="color:#a31515">/</span><span style="color:#a31515">${</span>secret.version<span style="color:#a31515">}</span><span style="color:#a31515">`</span>;

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,
   <span style="color:#008000">// A system-assigned managed identity
</span><span style="color:#008000"></span>   identity: {
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;SystemAssigned&#34;</span>,
   },
   appSettings: {
       <span style="color:#008000">// The setting points directly to the KV setting
</span><span style="color:#008000"></span>       <span style="color:#a31515">&#34;PAYMENT_API_KEY&#34;</span>: pulumi.interpolate<span style="color:#a31515">`@Microsoft.KeyVault(SecretUri=</span><span style="color:#a31515">${</span>secretUri<span style="color:#a31515">}</span><span style="color:#a31515">)`</span>,
   }
});
</code></pre></div><p>With that, the API key is loaded into the App Service environment variable without its value being publicly exposed anywhere!</p>
<h2 id="7-role-based-access-control">7. Role-based Access Control</h2>
<p>What is the most secure way to deal with secrets? <em>Have no secrets</em>. The Storage Account connection string is a great example when it&rsquo;s possible to avoid storing and reading the sensitive value altogether.</p>
<p><strong>Role-based access control</strong> (RBAC) of <strong>Azure Active Directory</strong> (AAD) is a great tool to manage permissions in a declarative way. Let&rsquo;s assume our application only needs to send messages to one Storage Queue. Instead of storing a full connection string with an access token, the connection string should point to the account, and the identity behind the App Service should be granted write permissions to the required Storage Queue:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> permission = <span style="color:#00f">new</span> azure.role.Assignment(<span style="color:#a31515">&#34;send&#34;</span>, {
   principalId,
   scope: <span style="color:#2b91af">pulumi.interpolate</span><span style="color:#a31515">`</span><span style="color:#a31515">${</span>storageAccount.id<span style="color:#a31515">}</span><span style="color:#a31515">/queueServices/default/queues/</span><span style="color:#a31515">${</span>queue.name<span style="color:#a31515">}</span><span style="color:#a31515">`</span>,
   roleDefinitionName: <span style="color:#a31515">&#34;Storage Queue Data Message Sender&#34;</span>,
});
</code></pre></div><p>We can put the queue URL into Application Settings because there&rsquo;s nothing secret in it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> queueUri = pulumi.interpolate<span style="color:#a31515">`</span><span style="color:#a31515">${</span>storageAccount.primaryQueueEndpoint<span style="color:#a31515">}${</span>queue.name<span style="color:#a31515">}</span><span style="color:#a31515">`</span>;

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;app&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,
   identity: {
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;SystemAssigned&#34;</span>
   },
   appSettings: {
       <span style="color:#a31515">&#34;STORAGE_QUEUE_URL&#34;</span>: queueUri,
   },
</code></pre></div><p>It takes a bit of C# boilerplate to send a message with role-based authorization:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> provider = <span style="color:#00f">new</span> AzureServiceTokenProvider();
<span style="color:#2b91af">string</span> accessToken = <span style="color:#00f">await</span> provider.GetAccessTokenAsync(<span style="color:#a31515">&#34;https://storage.azure.com/&#34;</span>);
<span style="color:#2b91af">var</span> tokenCredential = <span style="color:#00f">new</span> TokenCredential(accessToken);
<span style="color:#2b91af">var</span> storageCredentials = <span style="color:#00f">new</span> StorageCredentials(tokenCredential);
<span style="color:#2b91af">var</span> url = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;StorageBlobUrl&#34;</span>);
<span style="color:#2b91af">var</span> queue = <span style="color:#00f">new</span> CloudQueue(<span style="color:#00f">new</span> Uri(url), storageCredentials);
queue.AddMessage(<span style="color:#00f">new</span> CloudQueueMessage(<span style="color:#a31515">&#34;Hello&#34;</span>));
</code></pre></div><p>Let&rsquo;s hope another quality-of-life improvement is on the way.</p>
<h2 id="role-of-infrastructure-as-code">Role of Infrastructure as Code</h2>
<p>While security practices may vary depending on application requirements, Pulumi plays an essential role in the appropriate setup of service configuration and environment:</p>
<ul>
<li>It links an output of one resource to another one&rsquo;s input, avoiding the need to store the values.</li>
<li>It provides a built-in mechanism to manage external secrets.</li>
<li>It is a great way to take advantage of Azure features like Managed identities and RBAC in a cohesive way.</li>
</ul>
<p>Infrastructure as Code helps make your applications secure and reliable. Refer to <a href="https://github.com/pulumi/examples/tree/master/azure-ts-msi-keyvault-rbac">this full example</a> of using Key Vault, Managed identities, RBAC with App Service and Pulumi.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/security" term="security" label="Security" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Security]]></title>
            <link href="https://mikhail.io/tags/security/"/>
            <id>https://mikhail.io/tags/security/</id>
            
            <published>2019-07-26T00:00:00+00:00</published>
            <updated>2019-07-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Load-Testing Azure Functions with Loader.io]]></title>
            <link href="https://mikhail.io/2019/07/load-testing-azure-functions-with-loaderio/"/>
            <id>https://mikhail.io/2019/07/load-testing-azure-functions-with-loaderio/</id>
            
            <published>2019-07-04T00:00:00+00:00</published>
            <updated>2019-07-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Verifying your Function App as a valid target for the cloud load testing.</blockquote><p>When Azure Functions team presented the new <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-premium-plan">Premium plan</a>, they made a series of demos which compared the response time of a Function App running on the Consumption plan vs. an App running on the Premium plan. Both apps would receive a rapid growth of incoming requests, and then the percentiles of response latencies were compared.</p>
<p>The demos used a tool called <a href="https://loader.io/">Loader</a>, also referred to as &ldquo;Loader.io&rdquo;. The tool is visual and easy-to-use, but there is one step which might be just a little bit tricky to go through: verification. To avoid DDoS-ing somebody else&rsquo;s site, Loader requires users to verify each Target host by placing a key on a given URL of that domain.</p>
<p>If you want to follow along, it&rsquo;s time to create an account at <a href="https://loader.io/">loader.io</a>—they have a free plan. After signing in, go to <a href="https://loader.io/targets/new">New target host</a> and enter the URL of your Function App:</p>
<p><img src="new-target-host.png" alt="Azure Function App as a new target host"></p>
<figcaption><h4>Azure Function App as a new target host</h4></figcaption>
<p>Click &ldquo;Next: Verify&rdquo; button, and you are given a token to put inside your Function App to prove the ownership of it:</p>
<p><img src="target-verification.png" alt="Target Verification of Azure Function App"></p>
<figcaption><h4>Target Verification of Azure Function App</h4></figcaption>
<p>I know two ways to achieve verification.</p>
<h2 id="option-1-function-proxies">Option 1: Function Proxies</h2>
<p>With <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-proxies">Azure Functions Proxies</a>, you can specify endpoints in your Function App which aren&rsquo;t handled by Functions directly but redirect the requests to other endpoints or respond with a hard-coded response.</p>
<p>The later is exactly what we need. Create a <code>proxies.json</code> file in the root of your Function App, next to the <code>host.json</code> file, and put the following content in there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;$schema&#34;: <span style="color:#a31515">&#34;http://json.schemastore.org/proxies&#34;</span>,
    &#34;proxies&#34;: {
        &#34;loaderio-verifier&#34;: {
            &#34;matchCondition&#34;: {
                &#34;methods&#34;: [ <span style="color:#a31515">&#34;GET&#34;</span> ],
                &#34;route&#34;: <span style="color:#a31515">&#34;/loaderio-your-token-goes-here&#34;</span>
            },
            &#34;responseOverrides&#34;: {
                &#34;response.body&#34;: <span style="color:#a31515">&#34;loaderio-your-token-goes-here&#34;</span>,
                &#34;response.headers.Content-Type&#34;: <span style="color:#a31515">&#34;text/plain&#34;</span>
            }
        }
    }
}
</code></pre></div><p>Don&rsquo;t forget to replace <code>loaderio-your-token-goes-here</code> value with the token that you received at the previous step. Now, the hard-coded response is returned at the endpoint which Loader uses for verification.</p>
<p>If your Function App is a .NET project, include <code>proxies.json</code> to the <code>csproj</code>/<code>fsproj</code> file, so that it becomes a part of the deployment artifact:</p>
<pre><code>...
      &lt;None Update=&quot;proxies.json&quot;&gt;
         &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
      &lt;/None&gt;
   &lt;/ItemGroup&gt;
&lt;/Project&gt;
</code></pre><p>After re-publishing the Function App, you should be able to see a new proxy in the portal:</p>
<p><img src="loader-verifier-proxy.png" alt="Loader Verifier Proxy in the Azure Portal"></p>
<figcaption><h4>Loader Verifier Proxy in the Azure Portal</h4></figcaption>
<p>You are all set to go! Click the &ldquo;Verify&rdquo; button to see</p>
<p><img src="target-verification-congrats.png" alt="Target Verification Completed of Azure Function App"></p>
<h2 id="option-2-another-function">Option 2: Another Function</h2>
<p>Instead of dealing with Proxies, you could create another Azure Function in the same App and let it return the token.</p>
<p>I&rsquo;ll give you an example of defining such a Function with TypeScript and Pulumi.</p>
<p>Say, I want to load-test the following target HTTP function (it just pauses for 500 ms):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> target = <span style="color:#00f">new</span> azure.appservice.HttpFunction(<span style="color:#a31515">&#34;Target&#34;</span>, {
    callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
        <span style="color:#00f">await</span> <span style="color:#00f">new</span> Promise(resolve =&gt; setTimeout(resolve, 500));
        <span style="color:#00f">return</span> {
            status: <span style="color:#2b91af">200</span>,
            body: <span style="color:#a31515">&#34;Hello World!&#34;</span>,
        };
    },
});
</code></pre></div><p>Now, I can add another HTTP function to handle the verification requests:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> verifier = <span style="color:#00f">new</span> azure.appservice.HttpFunction(<span style="color:#a31515">&#34;Verifier&#34;</span>, {
    route: <span style="color:#a31515">&#34;loaderio-your-token-goes-here&#34;</span>,
    callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
        <span style="color:#00f">return</span> {
            status: <span style="color:#2b91af">200</span>,
            body: <span style="color:#a31515">&#34;loaderio-your-token-goes-here&#34;</span>,
        };
    },
});
</code></pre></div><p>Finally, I put both Functions into a Function App and deploy them together:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.MultiCallbackFunctionApp(<span style="color:#a31515">&#34;loaderio&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    functions: [target, verifier],
    hostSettings: { extensions: { http: { routePrefix: <span style="color:#a31515">&#34;&#34;</span> } } },
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> endpoint = app.endpoint;
</code></pre></div><p>Note that I set <code>routePrefix</code> to an empty string to avoid <code>api</code> prefix in my URLs (I usually do it anyway). Now, as soon as Pulumi deploys my App, I can start testing it with Loader.</p>
<p>You can find a full example involving the <code>MultiCallbackFunctionApp</code> component <a href="https://github.com/pulumi/pulumi-azure/blob/master/examples/http-multi/index.ts">here</a>.</p>
<h2 id="running-the-load-tests">Running the Load Tests</h2>
<p>Regardless of which path you chose, you should now be good to start using Loader to run performance tests on your Azure Functions.</p>
<p>Here is a chart of a sample test that I ran to make sure the setup works:</p>
<p><img src="loader-results.png" alt="Loader.io Test Results"></p>
<figcaption><h4>Loader.io Test Results</h4></figcaption>
<p>Which test parameters to use and what kind of results you might get depends a lot on your application. That&rsquo;s a good topic for another post someday.</p>
<p>For now, happy testing!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure CLI]]></title>
            <link href="https://mikhail.io/tags/azure-cli/"/>
            <id>https://mikhail.io/tags/azure-cli/</id>
            
            <published>2019-07-03T00:00:00+00:00</published>
            <updated>2019-07-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How Azure CLI Manages Your Access Tokens]]></title>
            <link href="https://mikhail.io/2019/07/how-azure-cli-manages-access-tokens/"/>
            <id>https://mikhail.io/2019/07/how-azure-cli-manages-access-tokens/</id>
            
            <published>2019-07-03T00:00:00+00:00</published>
            <updated>2019-07-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Azure CLI is a powerful tool to manage your cloud resources. Where does it store the sensitive information and why might you want to care?</blockquote><p>Azure has several tools available to create and manage cloud resources. The Azure command-line interface (CLI) is probably the second most-used tool after the web portal. The CLI runs on any platform and covers a wide variety of actions. Here is a command to create a new Virtual Machine:</p>
<pre><code>az vm create --resource-group Sample --name VM1 --image UbuntuLTS --generate-ssh-keys
</code></pre><p>If you use Azure for your day-to-day job or hobby projects, it&rsquo;s quite likely that you already have the Azure CLI installed on your computer.</p>
<h2 id="azure-login">Azure login</h2>
<p>The very first command that you run after installing the CLI on your development machine is going to be</p>
<pre><code>az login
</code></pre><p>There are several sign-in flows, but most typically the CLI opens the default browser asking you to log in there. The login operation has a unique session identifier. Once you sign in with this session ID, the CLI receives a notification on its back channel. The notification contains a JWT access token.</p>
<p>From this point on, the access token is used by most other CLI commands to access Azure Management REST API. API uses OAuth protocol where the access token is passed in the <code>Authorization</code> HTTP header.</p>
<h2 id="storing-tokens-on-the-local-disk">Storing tokens on the local disk</h2>
<p>However, there is one problem. The CLI is a short-lived program: it runs for the duration of a single command execution and then quits. The process dies, so there&rsquo;s no way to keep access tokens in memory between the executions. To avoid continually asking for user credentials, the CLI keeps its state on disk. If you go to the <code>~/.azure/</code> directory (<code>%HOMEPATH%/.azure/</code> in Windows command-line) you can find several of such state files:</p>
<pre><code>~/.azure&gt; ls

accessTokens.json
az.json
az.sess
azureProfile.json
clouds.configtelemetry.txt
</code></pre><p>Two of these files contain some relevant information. <code>azureProfile.json</code> lists the properties of your Azure subscriptions and users (no passwords or tokens).</p>
<p><code>accessTokens.json</code> is more interesting. As the name suggests, it contains all the tokens from the Azure CLI, right there in plain text. Here is the shape of JSON entries in there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
   &#34;tokenType&#34;: <span style="color:#a31515">&#34;Bearer&#34;</span>,
   &#34;expiresIn&#34;: 3599,
   &#34;expiresOn&#34;: <span style="color:#a31515">&#34;2019-05-05 00:22:01.577315&#34;</span>,
   &#34;resource&#34;: <span style="color:#a31515">&#34;https://management.core.windows.net/&#34;</span>,
   &#34;accessToken&#34;: <span style="color:#a31515">&#34;...&#34;</span>,
   &#34;refreshToken&#34;: <span style="color:#a31515">&#34;...&#34;</span>,
   &#34;identityProvider&#34;: <span style="color:#a31515">&#34;live.com&#34;</span>,
   &#34;userId&#34;: <span style="color:#a31515">&#34;...&#34;</span>,
   &#34;isMRRT&#34;: <span style="color:#00f">true</span>,
   &#34;_clientId&#34;: <span style="color:#a31515">&#34;...&#34;</span>,
   &#34;_authority&#34;: <span style="color:#a31515">&#34;https://login.microsoftonline.com/common&#34;</span>
}
</code></pre></div><p>There are multiple entries like this for different combinations of <code>resource</code>, <code>_authority</code>, and others. Obviously, I removed all the sensitive values from this snippet, but you could see yours in plain text. Go ahead and copy-paste a value of <code>accessToken</code> property to &ldquo;Encoded&rdquo; the text box on <a href="https://jwt.io">https://jwt.io</a> page. The site decodes the token and shows you the properties:</p>
<p><img src="azure-jwt-decoded.png" alt="Azure access token decoded with JWT.io"></p>
<figcaption><h4>Azure access token decoded with JWT.io</h4></figcaption>
<p>The access token has a limited lifespan—mine are all 60 minutes. To avoid requiring to login after access expiration, there is another powerful token—a refresh token. Whenever an access token expires, CLI goes to the authentication service, presents the refresh token, and asks for a new access token. The lifetime of a refresh token is longer, and it&rsquo;s managed on the service side. There are some configurable policies to expire it: for instance, Azure might invalidate a token if it was inactive for more than X days. It can also be revoked manually at any time.</p>
<p>The refresh tokens are stored inside the same <code>accessTokens.json</code> file, right next to the access token (see the snippet above). It&rsquo;s not a JWT token: it is an opaque blob sent from Azure AD whose contents are not known to any client components. You cannot see what’s inside a refresh token but Azure can.</p>
<h2 id="token-reuse-by-other-tools">Token reuse by other tools</h2>
<p>We learned that access tokens are not specific to the Azure CLI and aren&rsquo;t used exclusively by it. Let&rsquo;s run a short experiment:</p>
<ol>
<li>
<p>Run <code>az login</code> or any other Azure CLI command to make sure there&rsquo;s a current access token.</p>
</li>
<li>
<p>Open <code>azureProfile.json</code> and copy-paste the subscription <code>id</code> field into the following URL: <code>https://management.azure.com/subscriptions/{subscription-id}/resourcegroups?api-version=2019-05-10</code>.</p>
</li>
<li>
<p>Open the <code>accessTokens.json</code> file, find the latest entry, and copy the <code>accessToken</code> field.</p>
</li>
</ol>
<p>Now, build the following cURL command out of these two values:</p>
<pre><code>curl --header &quot;Authorization: Bearer {access-token}&quot; \
https://management.azure.com/subscriptions/{subscription-id}/resourcegroups?api-version=2019-05-10
</code></pre><p>Run the command, and you get the list of resource groups in your subscription!</p>
<p>By the way, you can also find both properties with the Azure CLI commands <code>az account list</code> and <code>az account get-access-token</code>. It doesn&rsquo;t feel as hacky as copy-pasting from JSON files, but it is more convenient :)</p>
<p>Multiple third-party tools use the fact that the Azure CLI can log in to Azure and then provide access tokens. Both Terraform and Pulumi have a default method of authenticating into Azure with the Azure CLI. They probably delegate to the CLI instead of accessing the <code>accessTokens.json</code> file directly, but that&rsquo;s mostly convenience and not the hard requirement. Effectively, they are able to reuse the tokens created by the Azure CLI for their own purpose.</p>
<h2 id="security-risks-of-token-exposure">Security risks of token exposure</h2>
<p>This is all handy and good. However, there is a dark side of convenience. The existence of a file with clear-text Azure access tokens means that you should be careful not to expose that file to anyone. Don&rsquo;t share your <code>~/.azure/</code> folder with anybody, don&rsquo;t put it on GitHub, don&rsquo;t upload it to random file-sharing applications.</p>
<p>Access tokens file could potentially become an attack vector. Say, you install a new shiny CLI tool from NPM (many developers have dozens of them). You run it from your user account, the tool does its job but also silently uploads <code>accessTokens.json</code> file to somebody&rsquo;s FTP. A month later, you get surprised while looking at your Azure bill.</p>
<p>I checked this scenario by uploading my <code>~/.azure</code> folder to a brand-new VM. A fresh installation of the Azure CLI happily welcomes me without the need for <code>az login</code>:</p>
<p><img src="cli-on-vm.png" alt="Azure CLI open on a new VM with copied credentials"></p>
<figcaption><h4>Azure CLI open on a new VM with copied credentials</h4></figcaption>
<h2 id="conclusion">Conclusion</h2>
<p>Know your tools, reuse the power of Azure CLI for those Azure Management REST API crunching sessions, and keep your machine safe.</p>
<p>Happy hacking!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-cli" term="azure-cli" label="Azure CLI" />
                             
                                <category scheme="https://mikhail.io/tags/security" term="security" label="Security" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Globally-distributed Serverless Application in 100 Lines of Code. Infrastructure Included!]]></title>
            <link href="https://mikhail.io/2019/07/globally-distributed-serverless-application-in-100-lines-of-code/"/>
            <id>https://mikhail.io/2019/07/globally-distributed-serverless-application-in-100-lines-of-code/</id>
            
            <published>2019-07-02T00:00:00+00:00</published>
            <updated>2019-07-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Building a serverless application on Azure with both the data store and the HTTP endpoint located close to end users for fast response time.</blockquote><p>Pulumi is excellent at connecting multiple cloud components into a cohesive application. In my <a href="https://blog.pulumi.com/serverless-as-simple-callbacks-with-pulumi-and-azure-functions">previous post</a>, I introduced the way to mix JavaScript or TypeScript serverless functions directly into the cloud infrastructure programs.</p>
<p>Today, I will build a serverless application with both the data store and the HTTP endpoint located close to end users to ensure prompt response time. The entire application runs on top of managed Azure services and is defined as a single Pulumi program in TypeScript.</p>
<h2 id="baseline">Baseline</h2>
<p>I&rsquo;m going to build a URL shortener: a simple HTTP endpoint which accepts a shortcode in the URL and then redirects a user to the full URL associated with the given short code.</p>
<p>I start simple and make a non-distributed version of the application first. It consists of two main components: a Cosmos DB container to store URL mappings and two Azure Functions to handle HTTP requests:</p>
<p><img src="pulumi-azure-url-shortener-basic.png" alt="The URL Shortener app deployed to a single location"></p>
<figcaption><h4>The URL Shortener app deployed to a single location</h4></figcaption>
<p>Let&rsquo;s define this infrastructure in a Pulumi program.</p>
<h3 id="cosmos-db-collection">Cosmos DB Collection</h3>
<p>Arguably, Cosmos DB is overkill in such a simple scenario: we just need a key-value store, so Table Storage would suffice. However, Cosmos DB comes handy at the multi-region setup later on.</p>
<p>Next to a standard resource group definition, I create a Cosmos DB account with location set to a single region and <code>Session</code> consistency level:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> pulumi <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/pulumi&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> azure <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> cosmos <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@azure/cosmos&#34;</span>;

<span style="color:#00f">const</span> location = <span style="color:#a31515">&#34;westus&#34;</span>; <span style="color:#008000">// To be changed to multiple configurable locations
</span><span style="color:#008000"></span>
<span style="color:#00f">let</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;UrlShorterner&#34;</span>, {
   location,
});

<span style="color:#00f">let</span> cosmosdb = <span style="color:#00f">new</span> azure.cosmosdb.Account(<span style="color:#a31515">&#34;UrlStore&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   location,
   geoLocations: [{ location, failoverPriority: <span style="color:#2b91af">0</span> }],
   offerType: <span style="color:#a31515">&#34;Standard&#34;</span>,
   consistencyPolicy: {
       consistencyLevel: <span style="color:#a31515">&#34;Session&#34;</span>,
       maxIntervalInSeconds: <span style="color:#2b91af">5</span>,
       maxStalenessPrefix: <span style="color:#2b91af">100</span>,
   },
});
</code></pre></div><p>Cosmos DB has a hierarchical structure of Accounts, Databases, and Containers. Unfortunately, Cosmos DB containers can’t be defined as Pulumi resources yet. As a workaround, I defined a helper function <code>getContainer</code> to create a database and a container using Cosmos SDK, see <a href="https://github.com/pulumi/examples/blob/master/azure-ts-serverless-url-shortener-global/cosmosclient.ts">here</a>.</p>
<h3 id="function-app">Function App</h3>
<p>Serverless Azure Functions are going to handle the HTTP layer in my application. I use the technique of <a href="https://blog.pulumi.com/serverless-as-simple-callbacks-with-pulumi-and-azure-functions">serverless functions as callbacks</a> to define Azure Functions inline inside my Pulumi program:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> fn = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#34;GetUrl&#34;</span>, {
   resourceGroup,
   location,
   route: <span style="color:#a31515">&#34;{key}&#34;</span>,
   callback: <span style="color:#2b91af">async</span> (_, request: <span style="color:#2b91af">azure.appservice.HttpRequest</span>) =&gt; {
       <span style="color:#00f">const</span> endpoint = cosmosdb.endpoint.<span style="color:#00f">get</span>();
       <span style="color:#00f">const</span> masterKey = cosmosdb.primaryMasterKey.<span style="color:#00f">get</span>();

       <span style="color:#00f">const</span> container = <span style="color:#00f">await</span> getContainer(endpoint, masterKey);

       <span style="color:#00f">const</span> key = request.params[<span style="color:#a31515">&#39;key&#39;</span>];
       <span style="color:#00f">try</span> {
           <span style="color:#00f">const</span> response = <span style="color:#00f">await</span> container.item(key.toString()).read();
           <span style="color:#00f">return</span> {
               status: <span style="color:#2b91af">301</span>,
               headers: { <span style="color:#a31515">&#34;location&#34;</span>: response.body.url },
               body: <span style="color:#a31515">&#39;&#39;</span>,
           };
       } <span style="color:#00f">catch</span> (e) {
           <span style="color:#00f">return</span> { status: <span style="color:#2b91af">404</span>, body: <span style="color:#a31515">&#39;Short URL not found&#39;</span> }
       }
   }
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> url = fn.url;
</code></pre></div><p>I supply the template <code>{key}</code> as <code>route</code> parameter so that the function accepts wildcard URLs and extract the wildcard value as a <code>key</code> parameter available inside the <code>request</code> object. Then, I use the received key to look up the full URL. The URL is returned to the client as a header of the <code>301</code> Redirect response. <code>404</code> Not Found is returned in case the requested document does not exist—the naughty Cosmos SDK throws an error in this scenario.</p>
<p>Note how the Cosmos DB parameters <code>endpoint</code> and <code>primaryMasterKey</code> are used directly inside the function. There is no need to manage the keys manually or explicitly put them into application settings: the generated keys are injected into the code at the time of deployment. In practice, a better idea is to store the key in a KeyVault and read it via application settings, but I&rsquo;ll leave this for a separate article.</p>
<p>A function to add a URL to the database looks quite similar to the above:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> fn = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#34;AddUrl&#34;</span>, {
   resourceGroup,
   methods: [<span style="color:#a31515">&#34;POST&#34;</span>],
   callback: <span style="color:#2b91af">async</span> (_, request: <span style="color:#2b91af">azure.appservice.HttpRequest</span>) =&gt; {
       <span style="color:#00f">const</span> endpoint = cosmosdb.endpoint.<span style="color:#00f">get</span>();
       <span style="color:#00f">const</span> masterKey = cosmosdb.primaryMasterKey.<span style="color:#00f">get</span>();
       <span style="color:#00f">const</span> container = <span style="color:#00f">await</span> getContainer(endpoint, masterKey, location);

       <span style="color:#00f">await</span> container.items.create(request.body);
       <span style="color:#00f">return</span> { status: <span style="color:#2b91af">200</span>, body: <span style="color:#a31515">&#39;Short URL saved&#39;</span> };
   }
});
<span style="color:#00f">export</span> <span style="color:#00f">const</span> addEndpoint = fn.url;
</code></pre></div><p>The key differences are:</p>
<ul>
<li>Using <code>POST</code> as the accepted HTTP method</li>
<li>Creating a new item in Cosmos DB</li>
<li>Using <code>request.body</code> as the payload to save</li>
</ul>
<p>I might need to add some validation in a real application—skipped for now.</p>
<h3 id="trying-it-out">Trying It Out</h3>
<p>Now, when a user navigates to an address like <code>https://geturl-xyz.azurewebsites.net/api/URLKEY</code>, they get redirected to the full URL associated with the key. The URL is not quite short yet, but we could make it so with a custom domain configuration.</p>
<p>Presumably, we expect our URL shortener to be popular around the world. Therefore, one key aspect is to make sure that the service responds fast and enables a smooth user experience. Let&rsquo;s measure the response time of our first version from different spots around the world:</p>
<table class="table table-striped">
<thead>
<tr>
<th>Location</th>
<th>Response time</th>
</tr>
</thead>
<tbody>
<tr>
<td>San Francisco</td>
<td>133 ms</td>
</tr>
<tr>
<td>New York</td>
<td>272 ms</td>
</tr>
<tr>
<td>London</td>
<td>383 ms</td>
</tr>
<tr>
<td>Frankfurt</td>
<td>420 ms</td>
</tr>
<tr>
<td>Tel-Aviv</td>
<td>470 ms</td>
</tr>
<tr>
<td>Hong-Kong</td>
<td>437 ms</td>
</tr>
<tr>
<td>Brisbane</td>
<td>449 ms</td>
</tr>
</tbody>
</table>
<p>The further we get from the West US region, the slower the responses become.</p>
<p>How can we do better?</p>
<h2 id="bring-compute-and-data-closer-to-users">Bring Compute and Data Closer to Users</h2>
<p>It&rsquo;s hard to beat the speed of light, so if we want to respond fast, we need to bring both code and data close to any target user.</p>
<p>Luckily, the most involved aspect of that—the data distribution—can be handled by Cosmos DB multi-region accounts. We need to take care of deploying the code in multiple locations and routing the traffic to the nearest one. Here is the plan:</p>
<p><img src="pulumi-azure-url-shortener-distributed.png" alt="The multi-region URL Shortener app"></p>
<figcaption><h4>The multi-region URL Shortener app</h4></figcaption>
<p>I don&rsquo;t care too much about the latency of Add URL function, so I left it out of the picture.</p>
<h3 id="configuring-the-locations">Configuring the locations</h3>
<p>The above picture shows three Azure regions, but since I&rsquo;m using a general-purpose programming language, I can handle any number of locations in the same way. Actually, I put the list of target regions in Pulumi config file and read it at execution time:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> config = <span style="color:#00f">new</span> pulumi.Config();
<span style="color:#00f">const</span> locations = config.<span style="color:#00f">require</span>(<span style="color:#a31515">&#34;locations&#34;</span>).split(<span style="color:#a31515">&#39;,&#39;</span>);
<span style="color:#00f">const</span> primaryLocation = locations[0];
</code></pre></div><p>The order of regions defines the priority.</p>
<h3 id="multi-region-cosmos-db-account">Multi-region Cosmos DB Account</h3>
<p>Since <code>locations</code> is a simple array, I can <code>map</code> this array to produce the array of <code>geoLocations</code> to be set for my Cosmos DB:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">let</span> cosmosdb = <span style="color:#00f">new</span> azure.cosmosdb.Account(<span style="color:#a31515">&#34;url-cosmos&#34;</span>, {
   location: <span style="color:#2b91af">primaryLocation</span>,
   geoLocations: <span style="color:#2b91af">locations.map</span>((location, failoverPriority) =&gt; ({
       location,
       failoverPriority
   })),
   <span style="color:#008000">/* ... other properties stay the same ... */</span>
</code></pre></div><p>Right here, the full power of a programming language at my fingertips!</p>
<h3 id="multi-region-serverless-app">Multi-region Serverless App</h3>
<blockquote>
<p>Azure Traffic Manager is a DNS-based traffic load balancer that enables you to distribute traffic optimally to services across global Azure regions while providing high availability and responsiveness.</p>
</blockquote>
<p>Sounds like what I need for my global application! Let&rsquo;s define a Traffic Manager profile:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> profile = <span style="color:#00f">new</span> azure.trafficmanager.Profile(<span style="color:#a31515">&#34;UrlShortEndpoint&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   trafficRoutingMethod: <span style="color:#a31515">&#39;Performance&#39;</span>,
   dnsConfigs: [{
       relativeName: <span style="color:#a31515">&#34;shorturls&#34;</span>,
       ttl: <span style="color:#2b91af">60</span>,
   }],
   monitorConfigs: [{
       protocol: <span style="color:#a31515">&#39;HTTP&#39;</span>,
       port: <span style="color:#2b91af">80</span>,
       path: <span style="color:#a31515">&#39;/api/ping&#39;</span>,
   }]
});
</code></pre></div><p>I set the routing method to Performance:</p>
<blockquote>
<p>Select Performance when you have endpoints in different geographic locations and you want end users to use the &ldquo;closest&rdquo; endpoint in terms of the lowest network latency.</p>
</blockquote>
<p>Now, I need to create a Function App in each of the target regions. That&rsquo;s easy to achieve with a <code>for..of</code> loop:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">for</span> (<span style="color:#00f">const</span> location <span style="color:#00f">of</span> locations) {
   <span style="color:#00f">const</span> fn = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">`GetUrl-</span><span style="color:#a31515">${</span>location<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
       resourceGroup,
       location,
       route: <span style="color:#a31515">&#34;{key}&#34;</span>,
       callback: <span style="color:#008000">/* ... the function stays the same ... */</span>
   });
}
</code></pre></div><p>Finally, I set up an Endpoint for each Function App to bind them to the Traffic Manager. I do so in the same loop as above:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">for</span> (<span style="color:#00f">const</span> location <span style="color:#00f">of</span> locations) {
   <span style="color:#00f">const</span> fn = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">`GetUrl-</span><span style="color:#a31515">${</span>location<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
       <span style="color:#008000">/* ... function definition as shown above ... */</span>
   });

   <span style="color:#00f">const</span> app = fn.functionApp;

   <span style="color:#00f">new</span> azure.trafficmanager.Endpoint(<span style="color:#a31515">`tme</span><span style="color:#a31515">${</span>location<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
       resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
       profileName: <span style="color:#2b91af">profile.name</span>,
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#39;azureEndpoints&#39;</span>,
       targetResourceId: <span style="color:#2b91af">app.id</span>,
       target: <span style="color:#2b91af">app.defaultHostname</span>,
       endpointLocation: <span style="color:#2b91af">app.location</span>,
   });
}
</code></pre></div><p>I assign the properties of each Function App to the corresponding Endpoint with a simple object initializer expression.</p>
<p>Everything is in place. I can export the Traffic Manager endpoint:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">export</span> <span style="color:#00f">const</span> endpoint = pulumi.interpolate <span style="color:#a31515">`http://</span><span style="color:#a31515">${</span>profile.fqdn<span style="color:#a31515">}</span><span style="color:#a31515">/api/{key}`</span>;
</code></pre></div><p>Time to re-run the latency tests.</p>
<h2 id="results">Results</h2>
<p>I deployed the distributed infrastructure to five Azure regions: West US, East US, West Europe, East Asia, and Australia East. The results are shown in the table below, next to the values from the initial run:</p>
<table class="table table-striped">
<thead>
<tr>
<th>Location</th>
<th>Single region</th>
<th>Multiple regions</th>
</tr>
</thead>
<tbody>
<tr>
<td>San Francisco</td>
<td>133 ms</td>
<td>140 ms</td>
</tr>
<tr>
<td>New York</td>
<td>272 ms</td>
<td>152 ms</td>
</tr>
<tr>
<td>London</td>
<td>383 ms</td>
<td>150 ms</td>
</tr>
<tr>
<td>Frankfurt</td>
<td>420 ms</td>
<td>130 ms</td>
</tr>
<tr>
<td>Tel-Aviv</td>
<td>470 ms</td>
<td>307 ms</td>
</tr>
<tr>
<td>Hong-Kong</td>
<td>437 ms</td>
<td>149 ms</td>
</tr>
<tr>
<td>Brisbane</td>
<td>449 ms</td>
<td>529 ms</td>
</tr>
</tbody>
</table>
<p>We got a much smoother distribution of response time. There&rsquo;s no region close enough to Tel-Aviv, so its latency is still high-ish. I checked Brisbane, and the traffic was somehow directed to the East US region, so the latency hasn&rsquo;t improved at all. For today, I guess I&rsquo;ll stick to the hypothesis &ldquo;Australian internet is slow&rdquo; and advise to always test your target scenarios without trusting the providers blindly.</p>
<p>More importantly, I walked you through a scenario of developing an end-to-end application with Pulumi. I highlighted the power of leveraging familiar techniques coming from the TypeScript realm. You can find the full code in <a href="https://github.com/pulumi/examples/tree/master/azure-ts-serverless-url-shortener-global">Pulumi examples</a>.</p>
<p>Cloud brings superpowers to developer&rsquo;s hands. You just need to use those efficiently.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/infrastructure-as-code" term="infrastructure-as-code" label="Infrastructure as Code" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/azure-cosmos-db" term="azure-cosmos-db" label="Azure Cosmos DB" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Hosting a Static Website on Azure with Pulumi]]></title>
            <link href="https://mikhail.io/2019/06/hosting-a-static-website-on-azure-with-pulumi/"/>
            <id>https://mikhail.io/2019/06/hosting-a-static-website-on-azure-with-pulumi/</id>
            
            <published>2019-06-27T00:00:00+00:00</published>
            <updated>2019-06-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Static websites are back in the mainstream these days. Setting up the infrastructure to serve a static website in Azure is a task where Pulumi shines.</blockquote><p>Static websites are back in the mainstream these days. Website generators like Jekyll, Hugo, or Gatsby, make it fairly easy to combine templates and markdown pages to produce static HTML files. Static assets are the simplest thing to serve and cache, so the whole setup ends up being fast and cost-efficient.</p>
<p>Many platforms offer services to host such static websites. This post explains the steps to create the infrastructure to do so on Microsoft Azure.</p>
<p>Setting up the infrastructure to serve a static website doesn&rsquo;t sound like it would be all that difficult, but when you consider HTTPS certificates, content distribution networks, and attaching it to a custom domain, integrating all the components can be quite daunting.</p>
<p>Fortunately, this is a task where Pulumi shines. Pulumi&rsquo;s code-centric approach not only makes configuring cloud resources easier to do and maintain, but it also eliminates the pain of integrating multiple services.</p>
<h2 id="overview">Overview</h2>
<p>Our goal is to create a static website with a custom domain—I&rsquo;ll use an imaginary <code>demo.pulumi.com</code> for this article. In 2019, my website has to support HTTPS, so we need to create a custom TLS certificate too.</p>
<p>The final solution consists of several Azure services:</p>
<ul>
<li>Static files will be stored in a <strong>Blob Container</strong> inside a <strong>Storage Account</strong></li>
<li>The Storage Account will have <strong>Static Website feature</strong> enabled to have some basic URL rewrite rules</li>
<li>We&rsquo;ll put an <strong>Azure CDN Endpoint</strong> in front of the container to support the custom domain over TLS</li>
<li>Azure CDN will self-manage the TLS certificate</li>
<li>Our custom DNS provider will have the rule to point to the CDN endpoint (that&rsquo;s a manual step)</li>
</ul>
<p>The diagram below outlines the interaction of these components:</p>
<p><img src="pulumi-azure-static-website.png" alt="Static website running on Azure and defined in Pulumi"></p>
<figcaption><h4>Static website running on Azure and defined in Pulumi</h4></figcaption>
<p>Let&rsquo;s break down how to configure each component using Pulumi.</p>
<h2 id="resource-group">Resource Group</h2>
<p>Let&rsquo;s start a new Pulumi program, import the Pulumi packages, and define a new resource group:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> pulumi <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/pulumi&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> azure <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure&#34;</span>;

<span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;demo-rg&#34;</span>, {
   location: <span style="color:#2b91af">azure.WestEurope</span>,
});
</code></pre></div><h2 id="storage-account">Storage Account</h2>
<p>The Storage Account will contain our static website&rsquo;s assets:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;demopulumi&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
   accountKind: <span style="color:#a31515">&#34;StorageV2&#34;</span>,
});
</code></pre></div><p>The only trick here is to make sure that the account kind is V2; otherwise, it won&rsquo;t support the static website feature.</p>
<h2 id="static-website-hosting-in-azure-storage">Static Website Hosting in Azure Storage</h2>
<p>Any storage container could be exposed as a web endpoint. However, that&rsquo;s not flexible enough. The URL would always include the container name and the exact file name, so the user would have to ask for <code>https://demo.pulumi.com/containername/index.html</code> instead of simply <code>https://demo.pulumi.com/</code>.</p>
<p>Enabling Static Website Hosting in Azure Storage improves this experience. A dedicated container <strong><code>$web</code></strong> gets automatically created, which also has special treatment for <code>index</code> and <code>404</code> documents.</p>
<p>The bad news is that Static Website Hosting is not a part of Azure Resource Manager API, and therefore, it&rsquo;s not available out-of-the-box in ARM templates, Terraform, or Pulumi. We can enable this feature with Azure CLI, so the solution is to create a dynamic Pulumi resource which enables Pulumi experience while delegating the work to the CLI. You can find the full source code for the dynamic resource in <a href="https://github.com/pulumi/examples/blob/master/azure-ts-static-website/staticWebsite.ts">this example</a>, but the usage is quite trivial:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> staticWebsite = <span style="color:#00f">new</span> StorageStaticWebsite(<span style="color:#a31515">&#34;demopulumi-static&#34;</span>,
   { accountName: <span style="color:#2b91af">storageAccount.name</span> },
   { parent: <span style="color:#2b91af">storageAccount</span> });

<span style="color:#008000">// Web endpoint to the website
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">const</span> staticEndpoint = staticWebsite.endpoint;
</code></pre></div><p>The last line exports the static website endpoint. It will look something like <code>https://demopulumi01234abc.z6.web.core.windows.net/</code>—no custom domain yet, but already functional—once we deploy some static files in there.</p>
<h2 id="static-files">Static Files</h2>
<p>Now, it&rsquo;s time to upload the static files to the <code>$web</code> Blob Container.</p>
<p>For this demo, I&rsquo;ve created a folder <code>wwwroot</code> with two files in it: <code>index.html</code> and <code>404.html</code>. I can upload those files with the following Pulumi snippet:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts">[<span style="color:#a31515">&#34;index.html&#34;</span>, <span style="color:#a31515">&#34;404.html&#34;</span>].map(name =&gt;
   <span style="color:#00f">new</span> azure.storage.Blob(name, {
       name,
       resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
       storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
       storageContainerName: <span style="color:#2b91af">staticWebsite.webContainerName</span>,
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;block&#34;</span>,
       source: <span style="color:#a31515">`./wwwroot/</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">`</span>,
       contentType: <span style="color:#a31515">&#34;text/html&#34;</span>,
   })
);
</code></pre></div><p>In practice, your static website might contain hundreds or thousands of files. At that point, you might want to split the file upload operation from Pulumi and do it as a separate step in your CI/CD pipeline.</p>
<h2 id="azure-cdn">Azure CDN</h2>
<p>To make the static website files available over our custom domain and HTTPS, we need to create an Azure CDN Endpoint and to point it to the storage account:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> cdn = <span style="color:#00f">new</span> azure.cdn.Profile(<span style="color:#a31515">&#34;demo-cdn&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   sku: <span style="color:#a31515">&#34;Standard_Microsoft&#34;</span>,
});

<span style="color:#00f">const</span> endpoint = <span style="color:#00f">new</span> azure.cdn.Endpoint(<span style="color:#a31515">&#34;demo-cdn-ep&#34;</span>, {
   name: <span style="color:#a31515">&#34;demopulumi&#34;</span>,
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   profileName: <span style="color:#2b91af">cdn.name</span>,
   isHttpAllowed: <span style="color:#2b91af">true</span>,
   isHttpsAllowed: <span style="color:#2b91af">true</span>,
   originHostHeader: <span style="color:#2b91af">staticWebsite.hostName</span>,
   origins: [{
       name: <span style="color:#a31515">&#34;blobstorage&#34;</span>,
       hostName: <span style="color:#2b91af">staticWebsite.hostName</span>,
   }],
});

<span style="color:#008000">// CDN endpoint to the website. Allow it some time after the deployment to get ready.
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">const</span> cdnEndpoint = pulumi.interpolate<span style="color:#a31515">`https://</span><span style="color:#a31515">${</span>endpoint.hostName<span style="color:#a31515">}</span><span style="color:#a31515">/`</span>;
</code></pre></div><p>I specified an explicit name for the CDN Endpoint: <code>demopulumi</code>. This name has to be globally unique because it is a part of the endpoint URL <code>https://demopulumi.azureedge.net</code>. Please pick a custom name before running the program.</p>
<p>I pointed the CDN origin to the static website name with <code>staticWebsite.hostName</code>. As usual, it&rsquo;s easy to link resources in Pulumi code!</p>
<p>You might need to wait a few minutes before your content is visible as the CDN configuration is not immediately executed. I&rsquo;ve set <code>isHttpAllowed</code> to <code>true</code> because HTTP is available sooner than HTTPS; feel free to switch it off for your production configuration.</p>
<h2 id="configure-a-domain-dns-rule">Configure a Domain DNS Rule</h2>
<p>You&rsquo;ve probably registered your domain with some third-party provider. Follow the instructions of your provider to configure a CNAME rule for the website&rsquo;s DNS. For a custom domain <code>demo.pulumi.com</code>, the CNAME entry <code>demo</code> would be linked to the endpoint <code>demopulumi.azureedge.net</code>.</p>
<p>CNAME entries don&rsquo;t support &ldquo;naked&rdquo; domains like <code>pulumi.com</code>. If you want to set up a top-level domain to be served from the static Azure website, you&rsquo;d have to use an <a href="https://support.dnsimple.com/articles/alias-record/">Alias DNS record</a>, which isn&rsquo;t supported by some DNS providers. Please check with your provider for available options.</p>
<p>The following step assumes that a CNAME record is configured; otherwise, it would fail with a validation error.</p>
<h2 id="custom-domain-and-tls">Custom Domain and TLS</h2>
<p>The final step is to point our custom domain to the CDN endpoint and provision a TLS certificate to enable HTTPS support. Once again, these operations are not parts of the ARM API surface, so another <a href="https://github.com/pulumi/examples/blob/master/azure-ts-dynamicresource/cdnCustomDomain.ts">dynamic resource</a> was created to support them.</p>
<p>The usage is quite straightforward, just make sure to use your own domain in the following snippet:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> customDomain = <span style="color:#00f">new</span> CDNCustomDomainResource(<span style="color:#a31515">&#34;cdn-custom-domain&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   <span style="color:#008000">// Ensure that there is a CNAME record for demo pointing.pulumi.com to demopulumi.azureedge.net.
</span><span style="color:#008000"></span>   <span style="color:#008000">// You would do that in your domain registrar&#39;s portal.
</span><span style="color:#008000"></span>   customDomainHostName: <span style="color:#a31515">&#34;demo.pulumi.com&#34;</span>,
   profileName: <span style="color:#2b91af">cdn.name</span>,
   endpointName: <span style="color:#2b91af">endpoint.name</span>,
   <span style="color:#008000">// This will enable HTTPS through Azure&#39;s one-click automated certificate deployment.
</span><span style="color:#008000"></span>   <span style="color:#008000">// The certificate is fully managed by Azure from provisioning to automatic renewal
</span><span style="color:#008000"></span>   <span style="color:#008000">// at no additional cost to you.
</span><span style="color:#008000"></span>   httpsEnabled: <span style="color:#2b91af">true</span>,
}, { parent: <span style="color:#2b91af">endpoint</span> });
</code></pre></div><h2 id="bring-it-live">Bring It Live!</h2>
<p>And we are done! Run <code>pulumi up</code> and make sure that all resources get created successfully.</p>
<p>Start testing with <code>staticEndpoint</code>—it&rsquo;s the first one to become available. <code>cdnEndpoint</code> might return some <code>404</code>&rsquo;s at first, be patient. Then, try your custom domain with <code>HTTP</code>. Finally, the TLS certificate takes quite a while to be registered, so come back in an hour or so to test <code>HTTPS</code>.</p>
<p>While a &ldquo;static website&rdquo; may sound simple, we went through a rather complicated process to wire all the components together. Some Azure services and features are more straightforward to automate than others, but in the end, we combined all of them into one cohesive Pulumi program to host a static website, served over HTTPS and from a worldwide CDN. That&rsquo;s the power of a general-purpose programming language applied to the task of sophisticated infrastructure automation. Once the reusable components are in place, reliable and reproducible deployments become a reality.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/static-website" term="static-website" label="Static Website" />
                             
                                <category scheme="https://mikhail.io/tags/infrastructure-as-code" term="infrastructure-as-code" label="Infrastructure-as-Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Static Website]]></title>
            <link href="https://mikhail.io/tags/static-website/"/>
            <id>https://mikhail.io/tags/static-website/</id>
            
            <published>2019-06-27T00:00:00+00:00</published>
            <updated>2019-06-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless as Simple Callbacks with Pulumi and Azure Functions]]></title>
            <link href="https://mikhail.io/2019/05/serverless-as-simple-callbacks-with-pulumi-and-azure-functions/"/>
            <id>https://mikhail.io/2019/05/serverless-as-simple-callbacks-with-pulumi-and-azure-functions/</id>
            
            <published>2019-05-07T00:00:00+00:00</published>
            <updated>2019-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The simplest way to take a Node.js function and deploy it to Azure cloud as an HTTP endpoint using Pulumi.</blockquote><p>Serverless compute services, like Azure Functions, offer an amazing power to application developers to leverage: highly available, automatically scaled, low-ceremony, pay-per-value functions created in several lines of code.</p>
<p>So, what&rsquo;s <strong>the simplest</strong> way to take a Node.js function and deploy it to Azure cloud as an HTTP endpoint? How about this little tutorial:</p>
<h4 id="1-create-a-new-pulumi-program">1. Create a new Pulumi program:</h4>
<pre><code>$ pulumi new azure-typescript
</code></pre><h4 id="2-define-an-http-endpoint-in-indexts">2. Define an HTTP endpoint in <code>index.ts</code>:</h4>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> azure <span style="color:#00f">from</span> <span style="color:#a31515">&#39;@pulumi/azure&#39;</span>;

<span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#39;example&#39;</span>, { location: <span style="color:#a31515">&#39;West US&#39;</span> });

<span style="color:#00f">const</span> greeting = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#39;greeting&#39;</span>, {
 resourceGroup,
 callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
   <span style="color:#00f">return</span> {
     status: <span style="color:#2b91af">200</span>,
     body: <span style="color:#a31515">`Hello </span><span style="color:#a31515">${</span>req.query[<span style="color:#a31515">&#39;name&#39;</span>] || <span style="color:#a31515">&#39;World&#39;</span><span style="color:#a31515">}</span><span style="color:#a31515">!`</span>
   };
 }
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> url = greeting.url;
</code></pre></div><h4 id="3-deploy">3. Deploy:</h4>
<pre><code>$ pulumi up

Updating:
   Type
 + pulumi:pulumi:Stack
 + ├─ azure:appservice:HttpEventSubscription
 + │  ├─ azure:storage:Account
 + │  ├─ azure:appservice:Plan
 + │  ├─ azure:storage:Container
 + │  ├─ azure:storage:ZipBlob
 + │  └─ azure:appservice:FunctionApp
 + └─ azure:core:ResourceGroup

Outputs:
   url: &quot;https://greetingc21a23fe.azurewebsites.net/api/greeting&quot;
Resources:
   + 8 created
</code></pre><h4 id="4-access-your-function-via-http">4. Access your function via HTTP:</h4>
<pre><code>$ curl https://greetingc21a23fe.azurewebsites.net/api/greeting?name=Pulumi
Hello Pulumi!
</code></pre><p>With 12 lines of code and two CLI commands, I&rsquo;ve created all Azure resources required to host a serverless function without an explicit configuration of Azure services. Okay, I had to define a location for my resource group, but that could also be accomplished via <code>pulumi config</code>.</p>
<p>Pulumi compiled my TypeScript function, serialized it to a JavaScript file, created the bindings in a <code>function.json</code> file, hosting configuration in a <code>host.json</code> file, uploaded all these assets to Blob Storage, and configured a Consumption Plan and a Function App to run my function. An automated and reproducible deployment in less than two minutes.</p>
<h2 id="beyond-hello-world">Beyond Hello-World</h2>
<p>The power of Node.js comes from the richness of its library ecosystem. There&rsquo;s an NPM package for everything, so you definitely want to use those.</p>
<p>Serverless-function-as-callback imports dependencies in a transparent way. Install the NPM packages of your choice and use them inside the callback:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> moment <span style="color:#00f">from</span> <span style="color:#a31515">&#39;moment&#39;</span>;

<span style="color:#00f">const</span> greeting = <span style="color:#00f">new</span> azure.appservice.HttpEventSubscription(<span style="color:#a31515">&#39;greeting&#39;</span>, {
 resourceGroup,
 callback: <span style="color:#2b91af">async</span> (context, req) =&gt; {
   <span style="color:#00f">return</span> {
     status: <span style="color:#2b91af">200</span>,
     body: <span style="color:#a31515">`Hello </span><span style="color:#a31515">${</span>req.query[<span style="color:#a31515">&#39;name&#39;</span>] || <span style="color:#a31515">&#39;World&#39;</span><span style="color:#a31515">}</span><span style="color:#a31515"> at </span><span style="color:#a31515">${</span>moment().format(<span style="color:#a31515">&#39;LLLL&#39;</span>)<span style="color:#a31515">}</span><span style="color:#a31515">!`</span>
   };
 }
});
</code></pre></div><p>The packages get zipped up inside the deployment artifact automatically so that the Function App can find them at the startup. So there&rsquo;s no need to manually figure out how to produce the archive, get it uploaded, and maintain it as your libraries get updated.</p>
<p>Stay tuned for another blog post with a full implementation of a serverless URL shortener application deployed into multiple Azure regions for fast response time and improved resiliency.</p>
<h2 id="not-only-http">Not Only HTTP</h2>
<p>Your application might not be a bunch of HTTP functions. You probably want to leverage queues for asynchronous message passing. How about defining a callback on the queue object itself:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;storage&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   location: <span style="color:#2b91af">resourceGroup.location</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
});

<span style="color:#00f">const</span> queue = <span style="color:#00f">new</span> azure.storage.Queue(<span style="color:#a31515">&#34;myqueue&#34;</span>, {
  resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
  storageAccountName: <span style="color:#2b91af">storageAccount.name</span>
});

queue.onEvent(<span style="color:#a31515">&#34;newMessage&#34;</span>,  <span style="color:#00f">async</span> (context, msg) =&gt; {
   <span style="color:#008000">// code to process &#39;msg&#39; however you want here
</span><span style="color:#008000"></span>   console.log(<span style="color:#a31515">&#34;Message received: &#34;</span> + msg.toString());
});
</code></pre></div><p>Alternatively, define a ServiceBus topic and immediately subscribe to the messages:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> servicebus <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure/eventhub&#34;</span>;

<span style="color:#00f">const</span> <span style="color:#00f">namespace</span> = <span style="color:#00f">new</span> servicebus.Namespace(<span style="color:#a31515">&#34;test&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   sku: <span style="color:#a31515">&#34;standard&#34;</span>,
});

<span style="color:#00f">const</span> topic = <span style="color:#00f">new</span> servicebus.Topic(<span style="color:#a31515">&#34;mytopic&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   namespaceName: <span style="color:#2b91af">namespace.name</span>,
});

<span style="color:#00f">export</span> <span style="color:#00f">const</span> subscription = topic.onEvent(<span style="color:#a31515">&#34;mysubscription&#34;</span>, <span style="color:#00f">async</span> (context, msg) =&gt; {
   console.log(<span style="color:#a31515">&#34;Received: &#34;</span> + msg.toString());
});
</code></pre></div><p>In addition, get notified when a new PNG image is uploaded to a Blob Container:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageContainer = <span style="color:#00f">new</span> azure.storage.Container(<span style="color:#a31515">&#34;images-container&#34;</span>, {
  resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
  storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
  name: <span style="color:#a31515">&#34;images&#34;</span>,
});

storageContainer.onBlobEvent(<span style="color:#a31515">&#34;newImage&#34;</span>, {
   callback: <span style="color:#2b91af">async</span> (context, blob) =&gt; {
       console.log(<span style="color:#a31515">&#34;File size: &#34;</span> + context.bindingData.properties.length);
   },
   filterSuffix: <span style="color:#a31515">&#34;.png&#34;</span>,
});
</code></pre></div><p>In all these examples, you get a fully configured Function App on Consumption  Plan ramped up and bound to the trigger of choice. Your callback runs in the cloud handling every event with no manual work of hooking these different components together.</p>
<p>Using a general-purpose language like TypeScript provides one consistent approach to defining and delivering serverless applications and infrastructure as one cohesive application.</p>
<p>We strive to make cloud development as simple as everyday JavaScript development that made the language so successful. A lot is happening behind the scenes, but the code still looks like &ldquo;normal code&rdquo;. Composition of cloud resources should be as straightforward as hooking up components in any traditional application.</p>
<h2 id="looking-ahead">Looking Ahead</h2>
<p>Pulumi serverless programming model for Azure Functions is just ramping up. There is only a handful of trigger types supported right now, and some configuration parameters are not exposed yet.</p>
<p>So, today is the perfect time to chime in and join the discussion! Help us answer the questions:</p>
<ul>
<li>Is this programming model beneficial to your scenarios?</li>
<li>Which trigger types do you want to be supported?</li>
<li>How should we package multiple functions into Function App(s)?</li>
<li>Do you need input and output bindings to be supported, and if yes, in which shape?</li>
</ul>
<p>Feel free to leave a comment below, create an issue on <a href="https://github.com/pulumi/pulumi-azure/">GitHub</a>, tag me on <a href="https://twitter.com/MikhailShilkov">Twitter</a>, or join <a href="https://slack.pulumi.io/">Pulumi community Slack channel</a>.</p>
<p>Happy serverless programming!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/infrastructure-as-code" term="infrastructure-as-code" label="Infrastructure-as-Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure App Service]]></title>
            <link href="https://mikhail.io/tags/azure-app-service/"/>
            <id>https://mikhail.io/tags/azure-app-service/</id>
            
            <published>2019-05-06T00:00:00+00:00</published>
            <updated>2019-05-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Level up your Azure Platform as a Service applications with Pulumi]]></title>
            <link href="https://mikhail.io/2019/05/level-up-your-azure-platform-as-a-service-applications-with-pulumi/"/>
            <id>https://mikhail.io/2019/05/level-up-your-azure-platform-as-a-service-applications-with-pulumi/</id>
            
            <published>2019-05-06T00:00:00+00:00</published>
            <updated>2019-05-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Leverage Pulumi for continuous delivery of code and infrastructure to Azure PaaS. An ASP.NET Core application running on App Service and DevOps pipelines.</blockquote><p>Pulumi enables developers to define cloud infrastructure using general purpose programming languages. Pulumi works with multiple cloud providers and has first-class support for all services in Microsoft Azure.</p>
<p>Today I want to guide you through the process of developing Pulumi programs to leverage Azure <a href="https://azure.microsoft.com/overview/what-is-paas/">Platform-as-a-Service</a> (PaaS) services. My language of choice is TypeScript—a powerful and expressive typed language, which is very familiar to many Azure users.</p>
<h2 id="azure-platform-as-a-service">Azure Platform as a Service</h2>
<p>Azure consists of dozens of cloud services, from VMs to Kubernetes to Serverless. In my experience, a lot of customers choose Azure for its strong portfolio of PaaS-level services.</p>
<p><a href="https://azure.microsoft.com/services/app-service/">Azure App Service</a> is a well-established managed compute offering to run web applications, RESTful APIs, or background workers. <a href="https://azure.microsoft.com/services/sql-database/">Azure SQL Database</a> is a fully managed service to run relational databases with features like high availability and backups available out-of-the-box. Enriched by services like <a href="https://azure.microsoft.com/services/devops/">Azure DevOps</a> for CI/CD and <a href="https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview">Application Insights</a> for APM, PaaS is a powerful way to get the benefits of the cloud without the need to fully re-architect software solutions.</p>
<p>The power of relying on PaaS is evidenced by significant customer adoption. App Service is among the most popular compute services in Azure:</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">If you use automation (ARM, scripts, TF, ...) to define and deploy Azure infrastructure, which services are your primary target? Vote &amp; RT!</p>&mdash; Mikhail Shilkov (@MikhailShilkov) <a href="https://twitter.com/MikhailShilkov/status/1120592994351099904?ref_src=twsrc%5Etfw">April 23, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Nonetheless, PaaS services pose different challenges to application developers. In particular, the usage of multiple cloud services demands an investment in infrastructure automation. That&rsquo;s where Pulumi comes to the rescue.</p>
<h2 id="a-sample-application">A Sample Application</h2>
<p>For this walkthrough, I took an existing application from Azure Samples GitHub: <a href="https://github.com/azure-samples/dotnetcore-sqldb-tutorial">.NET Core MVC sample for Azure App Service</a>. Predictably enough, it&rsquo;s a Todo List application, and this time it is a web app built with ASP.NET Core, Entity Framework Core and a SQL database. <a href="https://docs.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-dotnetcore-sqldb">Build an ASP.NET Core and SQL Database app in Azure App Service</a> describes how to deploy such application to Azure App Service by means of clicking buttons in the Azure portal.</p>
<blockquote>
<p>Friends don&rsquo;t let friends right-click publish</p>
</blockquote>
<p>Instead, I suggest relying on infrastructure as code. I built a Pulumi program and integrated it into a fully automated build and deployment pipeline in Azure DevOps.</p>
<p>Here is a sketch of the solution:</p>
<p><img src="pulumi-app-service.png" alt="Pulumi and Azure PaaS - Application Diagram"></p>
<figcaption><h4>Pulumi and Azure PaaS - Application Diagram</h4></figcaption>
<p>Let&rsquo;s get started building together!</p>
<h2 id="solution-structure">Solution Structure</h2>
<p>The following snippet shows the essential elements of the solution:</p>
<pre><code>\infra                   # Cloud infrastructure definition goes here
   \index.ts             # Pulumi program in TypeScript
\src
   \Controllers          # \
   \Models               #  ASP.NET Core web app
   \Views                # /
   \Data                 # EF Core Data Context
   \wwwroot              # Static assets (JavaScript/CSS/Images)
\azure-pipelines.yml     # Azure DevOps pipeline definition
</code></pre><p>As a first step, I cloned the <a href="https://github.com/azure-samples/dotnetcore-sqldb-tutorial">Todo List app</a> into the <code>src</code> folder. There&rsquo;s nothing specific to Pulumi here: it&rsquo;s just an ASP.NET Core app. It could be your application instead.</p>
<h2 id="bootstrapping-a-pulumi-program">Bootstrapping a Pulumi Program</h2>
<p>The Pulumi development experience is powered by the <a href="https://pulumi.io/reference/commands.html">Pulumi CLI</a>. After <a href="https://pulumi.io/quickstart/install.html">installing the CLI</a>, I jump into an empty <code>infra</code> folder and run <code>pulumi new azure-typescript</code> accepting all the default answers. The CLI bootstraps a skeleton of a TypeScript NodeJS application. The code looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> pulumi <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/pulumi&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> azure <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure&#34;</span>;

<span style="color:#008000">// Create an Azure Resource Group
</span><span style="color:#008000"></span><span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;resourceGroup&#34;</span>, {
   location: <span style="color:#a31515">&#34;WestUS&#34;</span>,
});

<span style="color:#008000">// Create an Azure resource (Storage Account)
</span><span style="color:#008000"></span><span style="color:#00f">const</span> account = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;storage&#34;</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   location: <span style="color:#2b91af">resourceGroup.location</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
});

<span style="color:#008000">// Export the connection string for the storage account
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">const</span> connectionString = account.primaryConnectionString;
</code></pre></div><p>The infrastructure pieces are defined by instantiating objects of appropriate types: <code>ResourceGroup</code> and <code>Account</code> in this example.</p>
<h2 id="stacks">Stacks</h2>
<p>If an application is developed to run and evolve for months and years, it&rsquo;s smart to invest in practices like Continuous Integration and Deployment (CI/CD) and Infrastructure as Code (IaC). It&rsquo;s quite likely that such an application will run in multiple environments: production, staging, development, and so on.</p>
<p>Pulumi comes with a handy concept of <a href="https://pulumi.io/reference/stack.html">stacks</a>— isolated, independently configurable instances of a Pulumi program. A separate stack can be designated for each deployment environment.</p>
<p>We can take the notion of the stack into the program and apply the stack name to definitions of infrastructure resources:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Use first 10 characters of the stackname as prefix for resource names
</span><span style="color:#008000"></span><span style="color:#00f">const</span> prefix = pulumi.getStack().substring(0, 9);

<span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-rg`</span>, {
   location: <span style="color:#a31515">&#34;WestUS&#34;</span>,
});

<span style="color:#00f">const</span> resourceGroupArgs = {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   location: <span style="color:#2b91af">resourceGroup.location</span>,
};

<span style="color:#008000">// Storage Account name must be lowercase and cannot have any dash characters
</span><span style="color:#008000"></span><span style="color:#00f">const</span> storageAccountName = <span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix.toLowerCase().replace(<span style="color:#a31515">/-/g</span>, <span style="color:#a31515">&#34;&#34;</span>)<span style="color:#a31515">}</span><span style="color:#a31515">sa`</span>;
<span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(storageAccountName, {
   ...resourceGroupArgs,

   accountKind: <span style="color:#a31515">&#34;StorageV2&#34;</span>,
   accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
   accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
});
</code></pre></div><p>Note how I use the power of the general purpose programming language to</p>
<ul>
<li>Interact with the environment by reading the stack name</li>
<li>Encode custom rules for resource naming</li>
<li>Work around the shortcomings of the cloud, namely, the restricted set of characters to use in Storage Accounts</li>
<li>Extract the value <code>resourceGroupArgs</code> to reuse the same definition for upcoming resources.</li>
</ul>
<p>As a result, the stacks <code>production</code> and <code>dev</code> will be deployed to separate resource groups with clean and consistent naming throughout the resources.</p>
<h2 id="deploying-the-application-to-app-service">Deploying the Application to App Service</h2>
<p>Now, it&rsquo;s time to define the infrastructure to host my ASP.NET Core app. There are three pieces of the puzzle to fit together.</p>
<h3 id="1-app-service-plan">1. App Service Plan</h3>
<p>An App Service Plan defines the pricing tier, instance size and other parameters related to performance, scalability, and cost of the hosted application. My definition is relatively straightforward:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> appServicePlan = <span style="color:#00f">new</span> azure.appservice.Plan(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-asp`</span>, {
   ...resourceGroupArgs,

   kind: <span style="color:#a31515">&#34;App&#34;</span>,

   sku: {
       tier: <span style="color:#a31515">&#34;Basic&#34;</span>,
       size: <span style="color:#a31515">&#34;B1&#34;</span>,
   },
});
</code></pre></div><p>In a more advanced scenario, I could provision different performance tiers based on the target environment identified by the stack name.</p>
<h3 id="2-deployment-artifact">2. Deployment Artifact</h3>
<p>App Service is a mature Azure service with a long history, so it has numerous options for deployment methods. Arguably, the newest <a href="https://github.com/Azure/app-service-announcements/issues/110">Run from Package</a> is the most friendly way to practice Infrastructure as Code automation.</p>
<p>Essentially, I prepare a zip file with the published .NET Core assemblies and upload it to Azure Blob Storage:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageContainer = <span style="color:#00f">new</span> azure.storage.Container(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-c`</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
   containerAccessType: <span style="color:#a31515">&#34;private&#34;</span>,
});

<span style="color:#00f">const</span> blob = <span style="color:#00f">new</span> azure.storage.ZipBlob(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-b`</span>, {
   resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
   storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
   storageContainerName: <span style="color:#2b91af">storageContainer.name</span>,
   <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;block&#34;</span>,

   content: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;../src/bin/Debug/netcoreapp2.1/publish.zip&#34;</span>)
});
</code></pre></div><h3 id="3-app-service">3. App Service</h3>
<p>Now, I can define an App Service and instruct it to use this package to run the website. I do so by linking its application settings to the shared access signature of the blob:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> codeBlobUrl = signedBlobReadUrl(blob, storageAccount, storageContainer);

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-as`</span>, {
   ...resourceGroupArgs,
   appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,

   appSettings: {
       <span style="color:#a31515">&#34;WEBSITE_RUN_FROM_PACKAGE&#34;</span>: codeBlobUrl,
   },
});
</code></pre></div><p>At startup, App Service downloads the zip and mounts it as a local read-only disk to store the application binaries.</p>
<h2 id="adding-a-sql-database">Adding a SQL Database</h2>
<p>The application host is now defined, but I also need a relational database to store and query Todo Items. Staying true to the PaaS path, I&rsquo;m using Azure SQL Database service.</p>
<h3 id="reading-configuration-parameters">Reading Configuration Parameters</h3>
<p>Setting up a SQL Server requires a couple of parameter values that might change between execution environments, for instance, a username and a password for the connection string. Pulumi provides <a href="https://pulumi.io/reference/config.html">a way to configure</a> the program&rsquo;s parameters per stack.</p>
<p>The configuration itself will happen in my CI/CD pipeline. For now, I can query the values with <code>pulumi.Config</code> helper tool:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#008000">// Get the username and the password to use for SQL from config.
</span><span style="color:#008000"></span><span style="color:#00f">const</span> config = <span style="color:#00f">new</span> pulumi.Config();
<span style="color:#00f">const</span> username = config.<span style="color:#00f">require</span>(<span style="color:#a31515">&#34;sqlUsername&#34;</span>);
<span style="color:#00f">const</span> pwd = config.<span style="color:#00f">require</span>(<span style="color:#a31515">&#34;sqlPassword&#34;</span>);
</code></pre></div><h3 id="azure-sql-server-and-database">Azure SQL Server and Database</h3>
<p>I&rsquo;m all set to code the SQL infrastructure. Two resources need to be defined:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> sqlServer = <span style="color:#00f">new</span> azure.sql.SqlServer(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-sql`</span>, {
   ...resourceGroupArgs,

   administratorLogin: <span style="color:#2b91af">username</span>,
   administratorLoginPassword: <span style="color:#2b91af">pwd</span>,
   version: <span style="color:#a31515">&#34;12.0&#34;</span>,
});

<span style="color:#00f">const</span> database = <span style="color:#00f">new</span> azure.sql.Database(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-db`</span>, {
   ...resourceGroupArgs,
   serverName: <span style="color:#2b91af">sqlServer.name</span>,
   requestedServiceObjectiveName: <span style="color:#a31515">&#34;S0&#34;</span>
});
</code></pre></div><p>Note that <code>requestedServiceObjectiveName</code> defines the performance tier and the price of the Azure SQL Database.</p>
<h3 id="wiring-app-service-to-the-database">Wiring App Service to the Database</h3>
<p>By default, Azure SQL Database is configured to deny any incoming connections for security reasons. One approach is to allow access to all Azure services. However, a more secure method is to white-list the Outbound IPs of the App Service:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> firewallRules = app.outboundIpAddresses.apply(
   ips =&gt; ips.split(<span style="color:#a31515">&#39;,&#39;</span>).map(
       ip =&gt; <span style="color:#00f">new</span> azure.sql.FirewallRule(<span style="color:#a31515">`FR</span><span style="color:#a31515">${</span>ip<span style="color:#a31515">}</span><span style="color:#a31515">`</span>, {
           endIpAddress: <span style="color:#2b91af">ip</span>,
           resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
           serverName: <span style="color:#2b91af">sqlServer.name</span>,
           startIpAddress: <span style="color:#2b91af">ip</span>,
       })
   ));
</code></pre></div><p>Note that the actual IPs of the service are unknown at the time of writing the program. Nonetheless, the combination of <code>apply</code>, <code>split</code>, and <code>map</code> functions enables me to wire the runtime value of IPs to the proper set of firewall rules.</p>
<p>Additional firewall rules may be implemented to allow administrative access from outside the App Service.</p>
<p>Finally, our ASP.NET Core application expects a connection string with <code>MyDbConnection</code>. To create one, I join the database server and the database name to produce the connection string and add it to the App Service configuration:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-as`</span>, {
   <span style="color:#008000">// ... other parameters as defined above
</span><span style="color:#008000"></span>
   connectionStrings: [{
       name: <span style="color:#a31515">&#34;MyDbConnection&#34;</span>,
       value:
           <span style="color:#2b91af">pulumi.all</span>([sqlServer.name, database.name]).apply(([server, db]) =&gt;
               <span style="color:#a31515">`Server=tcp:</span><span style="color:#a31515">${</span>server<span style="color:#a31515">}</span><span style="color:#a31515">.database.windows.net;initial catalog=</span><span style="color:#a31515">${</span>db<span style="color:#a31515">}</span><span style="color:#a31515">;user ID=</span><span style="color:#a31515">${</span>username<span style="color:#a31515">}</span><span style="color:#a31515">;password=</span><span style="color:#a31515">${</span>pwd<span style="color:#a31515">}</span><span style="color:#a31515">;Min Pool Size=0;Max Pool Size=30;Persist Security Info=true;`</span>),
       <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;SQLAzure&#34;</span>
   }]
});
</code></pre></div><h2 id="collecting-metrics-with-application-insights">Collecting Metrics with Application Insights</h2>
<p>Application Insights is an Application Performance Management (APM) service to be used for collecting metrics from cloud applications.</p>
<p>After adding Application Insights NuGet packages into my ASP.NET solution, I can go ahead and define the App Insights resource and link it to the App Service with an instrumentation key:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> appInsights = <span style="color:#00f">new</span> azure.appinsights.Insights(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-ai`</span>, {
   ...resourceGroupArgs,

   applicationType: <span style="color:#a31515">&#34;Web&#34;</span>
});

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>prefix<span style="color:#a31515">}</span><span style="color:#a31515">-as`</span>, {
   <span style="color:#008000">// ... other parameters as defined above
</span><span style="color:#008000"></span>
   appSettings: {
       <span style="color:#a31515">&#34;ApplicationInsights:InstrumentationKey&#34;</span>: appInsights.instrumentationKey,
        <span style="color:#008000">// ... other settings as defined above
</span><span style="color:#008000"></span>   },
});
</code></pre></div><h2 id="continuous-deployment-with-azure-devops">Continuous Deployment with Azure DevOps</h2>
<p>While Pulumi CLI works great during development; a production deployment should rather be executed in a CI/CD pipeline. This time I&rsquo;m using Azure DevOps—a managed CI/CD service to build, test, and deploy cloud applications.</p>
<p>In particular, I defined an Azure Pipeline consisting of three steps:</p>
<ul>
<li>Build &amp; Publish the .NET Core application;</li>
<li>Restore NPM packages for the infrastructure program;</li>
<li>Provision the cloud infrastructure with Pulumi.</li>
</ul>
<h3 id="build--publish-the-net-application">Build &amp; Publish the .NET Application</h3>
<p>The first step utilizes a built-in task which triggers the .NET Core CLI to build the source code in the <code>src</code> folder and publish the assemblies as a zip file.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml">steps:
- task: DotNetCoreCLI@2
 inputs:
   command: <span style="color:#a31515">&#39;publish&#39;</span>
   projects: <span style="color:#a31515">&#39;src&#39;</span>
 displayName: <span style="color:#a31515">&#39;Build and publish the ASP.NET Core app&#39;</span>
</code></pre></div><h3 id="restore-npm-packages-for-the-infrastructure-program">Restore NPM packages for the infrastructure program</h3>
<p>The second step is a simple <code>npm install</code> step to restore the NodeJS dependencies:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"> - task: Npm@1
   inputs:
     command: <span style="color:#a31515">&#39;install&#39;</span>
     workingDir: <span style="color:#a31515">&#39;infra&#39;</span> 
   displayName: <span style="color:#a31515">&#39;NPM install packages&#39;</span>
</code></pre></div><h3 id="install-pulumi-and-run-infrastructure-code">Install Pulumi and Run Infrastructure Code</h3>
<p>There is a Pulumi task available in Azure Marketplace: <a href="https://marketplace.visualstudio.com/items?itemName=pulumi.build-and-release-task">Pulumi Azure Pipelines Task</a>. After installing it to your organization, you should be able to utilize a simple task like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml"> - task: Pulumi@0
   inputs:
     azureSubscription: <span style="color:#a31515">&#39;Your Azure Subscription(aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee)&#39;</span>
     command: <span style="color:#a31515">&#39;up&#39;</span>
     args: <span style="color:#a31515">&#39;--yes&#39;</span>
     cwd: <span style="color:#a31515">&#39;infra&#39;</span>
     stack: <span style="color:#a31515">&#39;dev&#39;</span>
   displayName: <span style="color:#a31515">&#39;Install pulumi and run infra code&#39;</span>
   name: pulumi
</code></pre></div><p>Alternatively, if you can&rsquo;t install third-party tasks from Azure Marketplace, you can call the Pulumi CLI from a custom shell script, see <a href="https://github.com/pulumi/examples/tree/master/azure-ts-appservice-devops/alternative-pipeline/">this pipeline definition</a>.</p>
<p>When everything is wired correctly, I see this screen in Azure DevOps:</p>
<p><img src="pulumi-devops-build.png" alt="Green Build of a Pulumi Program in DevOps"></p>
<figcaption><h4>Green Build of a Pulumi Program in Azure DevOps</h4></figcaption>
<p>The newly created resource group contains six resources:</p>
<p><img src="pulumi-resource-group.png" alt="Azure Resource created by Pulumi"></p>
<figcaption><h4>Azure Resources created by Pulumi</h4></figcaption>
<p>The application is up and running:</p>
<p><img src="pulumi-todo-app.png" alt="Todo List App deployed to Azure with Pulumi"></p>
<figcaption><h4>Application Screenshot</h4></figcaption>
<p>The telemetry is flowing into Application Insights:</p>
<p><img src="pulumi-application-map.png" alt="Application Map from App Insights"></p>
<figcaption><h4>Application Map from Azure App Insights</h4></figcaption>
<p>You can find the full code of the application, infrastructure definition, and deployment pipeline in <a href="https://github.com/pulumi/examples/tree/master/azure-ts-appservice-devops/">Pulumi Examples repository</a>.</p>
<h2 id="pulumi--azure-paas">Pulumi ❤️ Azure PaaS</h2>
<p>Azure App Service and friends are a great way to deploy web applications and APIs without worrying about the nitty-gritty details of the underlying hardware.</p>
<p>Writing a TypeScript program to compose an application out of the cloud building blocks feels like a breeze to me. I can reuse all my skills and stay productive by defining cloud resources as code in a familiar language.</p>
<p>You can get going with these resources:</p>
<ul>
<li><a href="https://pulumi.io/quickstart/">Getting Started with Pulumi</a></li>
<li><a href="https://pulumi.io/quickstart/azure/setup.html">Setup Pulumi to work with Azure</a></li>
<li><a href="https://pulumi.io/quickstart/azure/index.html">Walkthroughs and Examples</a></li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-app-service" term="azure-app-service" label="Azure App Service" />
                             
                                <category scheme="https://mikhail.io/tags/infrastructure-as-code" term="infrastructure-as-code" label="Infrastructure-as-Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Less Frequent Cold Starts in Google Cloud Functions]]></title>
            <link href="https://mikhail.io/2019/04/less-frequent-cold-starts-in-google-cloud-functions/"/>
            <id>https://mikhail.io/2019/04/less-frequent-cold-starts-in-google-cloud-functions/</id>
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Google keeps idle instances of Cloud Functions alive for many hours.</blockquote><p>Several days ago, I released an update to the <a href="/serverless/coldstarts/">Serverless Cold Starts</a> section of my website. The most significant change to the previous dataset seems to be in how GCP treats idle instances of Google Cloud Functions.</p>
<p>Cold starts are expensive, so all cloud providers preserve a warm instance of a cloud function even when the application is idle. If the function stays unused for an extended period, such idle instance might eventually be recycled.</p>
<p>Azure Functions recycles its instances after 20 minutes of idling. AWS Lambda has no fixed value, but in practice, it&rsquo;s between 25 and 65 minutes of idling.</p>
<h2 id="recycling-of-idle-instances-in-google-cloud-functions">Recycling of Idle Instances in Google Cloud Functions</h2>
<p>The behavior of Google Cloud Functions used to look quite random: an idle instance could survive for 5 hours or die after 5 minutes.</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'ID');
  data.addColumn('number', 'Value');
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows([[87.223692415,1.0566834,"point {fill-color: blue}"],[137.43873814833333,0.1484199,"point {fill-color: red}"],[74.57167498833333,1.1565178999999999,"point {fill-color: blue}"],[146.68497998166666,0.1531141,"point {fill-color: red}"],[139.61548119166667,0.1174578,"point {fill-color: red}"],[176.277201315,0.1454798,"point {fill-color: red}"],[24.758321308333333,0.41548179999999996,"point {fill-color: red}"],[249.36039055833334,0.11653999999999999,"point {fill-color: red}"],[43.489343070000004,0.19265649999999998,"point {fill-color: red}"],[43.41242365,0.15041849999999998,"point {fill-color: red}"],[76.81456954833334,0.16244519999999998,"point {fill-color: red}"],[169.590103985,1.228108,"point {fill-color: blue}"],[136.26404786,0.1420197,"point {fill-color: red}"],[242.24137742,1.4241983,"point {fill-color: blue}"],[95.82704468,0.1570009,"point {fill-color: red}"],[296.88630615333335,1.0368719,"point {fill-color: blue}"],[269.39025931000003,0.24895779999999998,"point {fill-color: red}"],[269.5663538983333,0.154046,"point {fill-color: red}"],[110.24344410333333,0.184665,"point {fill-color: red}"],[287.08118408,0.1465811,"point {fill-color: red}"],[182.57397214,0.1486031,"point {fill-color: red}"],[135.42568823666667,0.1216621,"point {fill-color: red}"],[56.938099396666665,0.1625989,"point {fill-color: red}"],[271.71979930000003,0.15939309999999998,"point {fill-color: red}"],[205.55865500000002,0.1577032,"point {fill-color: red}"],[108.23185526666667,0.1475897,"point {fill-color: red}"],[135.89114960166668,0.1184686,"point {fill-color: red}"],[169.26750183166666,0.1807258,"point {fill-color: red}"],[258.737404565,0.1196749,"point {fill-color: red}"],[226.63771318833335,0.2045243,"point {fill-color: red}"],[262.84643950000003,0.22819299999999998,"point {fill-color: red}"],[293.13809461166665,0.1507894,"point {fill-color: red}"],[182.968682575,0.22145299999999998,"point {fill-color: red}"],[34.215864195,0.1764276,"point {fill-color: red}"],[295.15763217,1.1933494999999998,"point {fill-color: blue}"],[150.96640485833333,0.199534,"point {fill-color: red}"],[207.39917468166666,0.15333259999999999,"point {fill-color: red}"],[118.16966400166667,0.3628223,"point {fill-color: red}"],[61.919341743333334,0.2657411,"point {fill-color: red}"],[181.75221923666666,0.36230989999999996,"point {fill-color: red}"],[68.03008951,1.3742744999999998,"point {fill-color: blue}"],[147.19282296166668,1.0319287,"point {fill-color: blue}"],[126.44732700833333,1.0576056999999999,"point {fill-color: blue}"],[256.82481317500003,1.0792119,"point {fill-color: blue}"],[82.73822200333333,0.9017957999999999,"point {fill-color: blue}"],[52.60437289,1.1126448,"point {fill-color: blue}"],[173.00318578833333,1.1043473,"point {fill-color: blue}"],[250.74365347666668,1.7999507,"point {fill-color: blue}"],[36.959685485,0.9355346,"point {fill-color: blue}"],[67.08367706166666,1.0031276999999998,"point {fill-color: blue}"],[214.060766645,1.0029251,"point {fill-color: blue}"],[274.704438615,1.5827122,"point {fill-color: blue}"],[297.27548389833333,1.0178862,"point {fill-color: blue}"],[288.79592982500003,1.3866809999999998,"point {fill-color: blue}"],[162.93987092166668,1.0028139,"point {fill-color: blue}"],[88.22329438833333,1.0690956999999999,"point {fill-color: blue}"],[252.26362830833335,1.7261803,"point {fill-color: blue}"],[43.80041404666667,1.135055,"point {fill-color: blue}"],[176.38321055,1.6311339,"point {fill-color: blue}"],[166.31704522333334,1.0412077,"point {fill-color: blue}"],[268.71009862333335,0.9906495,"point {fill-color: blue}"],[140.79376768666668,1.0657199,"point {fill-color: blue}"],[13.77464807,0.2530493,"point {fill-color: red}"],[49.03659896333333,1.1385117,"point {fill-color: blue}"],[227.057888775,1.0844878,"point {fill-color: blue}"],[96.44963171833334,1.1102583,"point {fill-color: blue}"],[28.147415018333334,0.2045428,"point {fill-color: red}"],[33.67353681833333,0.1552207,"point {fill-color: red}"],[194.540898155,1.2751652999999998,"point {fill-color: blue}"],[70.188071235,1.1101413,"point {fill-color: blue}"],[287.72277695,1.1031204,"point {fill-color: blue}"],[117.14435473833333,1.4884906999999998,"point {fill-color: blue}"],[146.13995391666666,1.1824472,"point {fill-color: blue}"],[134.848917125,1.1503706,"point {fill-color: blue}"],[6.198198615,0.1200643,"point {fill-color: red}"],[272.22082465833336,1.8174244,"point {fill-color: blue}"],[272.076837705,0.9099334,"point {fill-color: blue}"],[254.25126218333335,1.4116381999999998,"point {fill-color: blue}"],[3.5562576150000003,0.20559239999999998,"point {fill-color: red}"],[241.24663554666668,1.0376678,"point {fill-color: blue}"],[36.133117425,0.1855914,"point {fill-color: red}"],[127.88293258,1.1324634999999998,"point {fill-color: blue}"],[72.64254592,1.1979092999999998,"point {fill-color: blue}"],[239.33737744166666,1.5689089999999999,"point {fill-color: blue}"],[17.293930013333334,0.1762161,"point {fill-color: red}"],[259.06780773,1.0922498,"point {fill-color: blue}"],[153.27434680333334,1.0615386,"point {fill-color: blue}"],[30.384787253333332,0.1454522,"point {fill-color: red}"],[68.49212499666667,1.4926811,"point {fill-color: blue}"],[74.86667819166667,0.18071959999999998,"point {fill-color: red}"],[116.54087381333333,1.0320007,"point {fill-color: blue}"],[72.80991923833334,1.1393955,"point {fill-color: blue}"],[36.416669435,0.116703,"point {fill-color: red}"],[280.0228915416667,0.9303334,"point {fill-color: blue}"],[175.68889323,1.0084794,"point {fill-color: blue}"],[212.01485436333334,1.0117771,"point {fill-color: blue}"],[193.552334275,0.1980939,"point {fill-color: red}"],[164.46697412833333,1.0523803,"point {fill-color: blue}"],[209.78850674333333,1.4035106,"point {fill-color: blue}"],[204.98935508833333,1.4994581999999999,"point {fill-color: blue}"],[138.03401118666667,0.7861942,"point {fill-color: blue}"],[33.46733526166667,1.1454414,"point {fill-color: blue}"],[172.42431594833334,0.9291914,"point {fill-color: blue}"],[29.078624281666666,0.11532049999999999,"point {fill-color: red}"],[154.01439743666668,1.572323,"point {fill-color: blue}"],[167.65361145166668,1.236796,"point {fill-color: blue}"],[137.95378280833333,1.3952571999999999,"point {fill-color: blue}"],[28.3963188,0.1842472,"point {fill-color: red}"],[297.96170652166666,1.2337631999999998,"point {fill-color: blue}"],[269.0737857266667,1.053574,"point {fill-color: blue}"],[19.886915153333334,0.176781,"point {fill-color: red}"],[130.82699470666668,1.066201,"point {fill-color: blue}"],[284.098351775,1.2528922,"point {fill-color: blue}"],[178.32934875166666,1.6881826,"point {fill-color: blue}"],[85.90160536500001,0.1221119,"point {fill-color: red}"],[97.33458936666666,0.2349732,"point {fill-color: red}"],[21.844685518333335,0.1500091,"point {fill-color: red}"],[289.79008637,1.2539475,"point {fill-color: blue}"],[74.59488160833334,0.1467291,"point {fill-color: red}"],[244.024279135,1.3677051,"point {fill-color: blue}"],[227.55774270333333,1.443601,"point {fill-color: blue}"],[256.68853661833333,1.1714261,"point {fill-color: blue}"],[222.25834139333332,1.2432119,"point {fill-color: blue}"],[238.951189995,0.1795877,"point {fill-color: red}"],[226.38173262666666,1.5045482,"point {fill-color: blue}"],[294.67955841,1.8456959,"point {fill-color: blue}"],[99.18901662166667,0.12764129999999999,"point {fill-color: red}"],[278.0834623466667,1.741083,"point {fill-color: blue}"],[7.885343441666667,0.1728361,"point {fill-color: red}"],[132.85020800833334,0.1453505,"point {fill-color: red}"],[291.766222225,1.1987883,"point {fill-color: blue}"],[60.31657987166667,0.1488767,"point {fill-color: red}"],[204.11946903,1.4050178,"point {fill-color: blue}"],[148.49542282666667,1.2760323999999998,"point {fill-color: blue}"],[205.053457925,1.1489859999999998,"point {fill-color: blue}"],[275.93359578166667,0.2407251,"point {fill-color: red}"],[289.6393913283333,1.1696649,"point {fill-color: blue}"],[252.21820598833335,1.177824,"point {fill-color: blue}"],[113.748031235,0.12556509999999999,"point {fill-color: red}"],[255.34628755666668,1.0744639999999999,"point {fill-color: blue}"],[36.74066517,0.17008189999999998,"point {fill-color: red}"],[65.204398995,0.1508945,"point {fill-color: red}"],[270.79117415,0.1551312,"point {fill-color: red}"],[274.1876650766667,0.23926399999999998,"point {fill-color: red}"],[149.2896922,0.1408284,"point {fill-color: red}"],[225.5523986,1.0925729,"point {fill-color: blue}"],[255.08454178166667,0.11930769999999999,"point {fill-color: red}"],[25.89336998,0.1916564,"point {fill-color: red}"],[53.060380305,0.1555498,"point {fill-color: red}"],[22.638511823333335,0.14676509999999998,"point {fill-color: red}"],[250.4308637,1.0357667,"point {fill-color: blue}"],[296.71878613166666,1.6848896,"point {fill-color: blue}"],[96.12677505666667,0.22341409999999998,"point {fill-color: red}"],[60.99219259833333,0.23702569999999998,"point {fill-color: red}"],[256.08434925333336,1.0510951,"point {fill-color: blue}"],[232.84060575,0.9961182,"point {fill-color: blue}"],[193.792260715,1.079712,"point {fill-color: blue}"],[227.11498299166666,0.13687349999999998,"point {fill-color: red}"],[205.03767552666667,0.17410679999999998,"point {fill-color: red}"],[208.71672330833334,1.4457699,"point {fill-color: blue}"],[49.20881353666667,0.15569439999999998,"point {fill-color: red}"],[26.572466488333333,0.1615075,"point {fill-color: red}"],[253.27134801833333,0.9896400999999999,"point {fill-color: blue}"],[14.550253915,0.22347689999999998,"point {fill-color: red}"],[251.430971375,1.023903,"point {fill-color: blue}"],[241.509584625,1.0028181999999999,"point {fill-color: blue}"],[50.858438168333336,0.1484766,"point {fill-color: red}"],[20.344326083333335,0.1212916,"point {fill-color: red}"],[185.39100854833333,0.9185295,"point {fill-color: blue}"],[191.149828305,0.9071406,"point {fill-color: blue}"],[262.28844481833335,1.1471826,"point {fill-color: blue}"],[237.73719925333333,0.205426,"point {fill-color: red}"],[231.25718542666667,1.2968628,"point {fill-color: blue}"],[257.90647813333334,1.0902935,"point {fill-color: blue}"],[178.65077327333333,1.2822594,"point {fill-color: blue}"],[11.301954238333334,0.1603836,"point {fill-color: red}"],[28.448314925000002,0.11855929999999999,"point {fill-color: red}"],[128.696263445,0.1841691,"point {fill-color: red}"],[163.056119335,0.1605257,"point {fill-color: red}"],[63.73438352666667,0.1544774,"point {fill-color: red}"],[176.20631566666668,1.0941598,"point {fill-color: blue}"],[165.77568253166666,0.16040089999999999,"point {fill-color: red}"],[234.37890517166667,0.1544748,"point {fill-color: red}"],[2.377874686666667,0.1444221,"point {fill-color: red}"],[135.08549020666666,0.37593279999999996,"point {fill-color: red}"],[19.96737894,0.1507161,"point {fill-color: red}"],[231.42486994,1.1990307,"point {fill-color: blue}"],[189.52361637,1.3003206999999999,"point {fill-color: blue}"],[73.909583905,0.1869411,"point {fill-color: red}"],[258.52357387666666,1.096418,"point {fill-color: blue}"],[81.82668368666667,0.21103159999999999,"point {fill-color: red}"],[46.96224949,0.1642302,"point {fill-color: red}"],[236.200729345,1.5489064,"point {fill-color: blue}"],[190.525326435,0.9660270999999999,"point {fill-color: blue}"],[152.95266797833332,1.4237691,"point {fill-color: blue}"],[146.73557154666668,1.0967350999999999,"point {fill-color: blue}"],[81.84676565333334,1.0320433,"point {fill-color: blue}"],[199.31243362,1.1142024,"point {fill-color: blue}"],[162.43113410333333,1.7961365,"point {fill-color: blue}"],[152.42296678166667,1.0829096,"point {fill-color: blue}"],[260.8737841016667,1.3939412,"point {fill-color: blue}"],[83.15109560833334,0.1481192,"point {fill-color: red}"],[4.754378658333334,0.1164431,"point {fill-color: red}"],[230.05418345666666,1.3426825,"point {fill-color: blue}"],[97.95829273833334,0.9152414,"point {fill-color: blue}"],[254.96736956666666,1.2632387,"point {fill-color: blue}"],[71.00711805333333,0.9282672,"point {fill-color: blue}"],[149.44824248333333,1.5421642,"point {fill-color: blue}"],[10.964756038333334,0.12348819999999999,"point {fill-color: red}"],[113.28522191333333,0.9677684999999999,"point {fill-color: blue}"],[111.04770184833333,0.8861671,"point {fill-color: blue}"],[90.77294012,1.0757079,"point {fill-color: blue}"],[162.21318272333335,2.2409909,"point {fill-color: blue}"],[280.9290627583333,1.0695527,"point {fill-color: blue}"],[207.02659628666666,1.3483219,"point {fill-color: blue}"],[201.39565389,1.6691806999999999,"point {fill-color: blue}"],[20.94819503,0.23510419999999999,"point {fill-color: red}"],[239.73240256333335,1.2233766,"point {fill-color: blue}"],[27.619186505000002,0.17290519999999998,"point {fill-color: red}"],[46.553182801666665,0.13262569999999999,"point {fill-color: red}"],[56.34648819833333,1.1173647,"point {fill-color: blue}"],[256.62954586666666,1.3539396,"point {fill-color: blue}"],[62.831499021666666,0.1505993,"point {fill-color: red}"],[275.6406538416667,1.0714481,"point {fill-color: blue}"],[45.538203165,0.2169278,"point {fill-color: red}"],[0.8791947266666666,0.1221535,"point {fill-color: red}"],[272.39604312166665,1.2284472,"point {fill-color: blue}"],[62.40914327,0.18116949999999998,"point {fill-color: red}"],[225.56585732833335,2.1722106,"point {fill-color: blue}"],[99.052418505,1.265823,"point {fill-color: blue}"],[16.871225671666668,0.1770967,"point {fill-color: red}"],[123.94542724333334,1.288328,"point {fill-color: blue}"],[18.31800064666667,0.14440429999999999,"point {fill-color: red}"],[263.280077865,1.3339486,"point {fill-color: blue}"],[21.436637155,0.18642889999999998,"point {fill-color: red}"],[94.34754320333333,1.4543393,"point {fill-color: blue}"],[117.11585929166667,1.579083,"point {fill-color: blue}"],[55.707697638333336,0.159154,"point {fill-color: red}"],[8.979015023333334,0.2419159,"point {fill-color: red}"],[103.80108434666667,1.2468008,"point {fill-color: blue}"],[64.35289877833334,1.4614425,"point {fill-color: blue}"],[60.88839642333333,1.6228939,"point {fill-color: blue}"],[58.26925609166667,1.260687,"point {fill-color: blue}"],[17.559311201666667,1.3031960999999999,"point {fill-color: blue}"],[29.71336813,1.5216835,"point {fill-color: blue}"],[68.59336281,1.3916693,"point {fill-color: blue}"],[49.31385596166667,0.1928222,"point {fill-color: red}"],[87.86838761833333,1.199423,"point {fill-color: blue}"],[20.8073951,0.18657579999999999,"point {fill-color: red}"],[9.728972058333333,0.1812152,"point {fill-color: red}"],[37.32750240166667,0.16315949999999999,"point {fill-color: red}"],[2.435776011666667,0.2014597,"point {fill-color: red}"],[98.49926338333333,1.3995349,"point {fill-color: blue}"],[3.312975311666667,0.1625811,"point {fill-color: red}"],[48.003949785,1.2099632,"point {fill-color: blue}"],[41.49195675666667,0.21326679999999998,"point {fill-color: red}"],[69.02042072500001,0.1828999,"point {fill-color: red}"],[42.921844271666664,1.5311708,"point {fill-color: blue}"],[11.416963851666667,0.4844766,"point {fill-color: red}"],[96.02089690166667,1.3422952,"point {fill-color: blue}"],[77.82533934,1.3206909999999998,"point {fill-color: blue}"],[41.08763415333333,0.21799559999999998,"point {fill-color: red}"],[72.87805657166666,0.1853696,"point {fill-color: red}"],[61.59633903666667,0.20174219999999998,"point {fill-color: red}"],[5.371281215,0.1731836,"point {fill-color: red}"],[103.329508625,1.2128503,"point {fill-color: blue}"],[7.731169465,0.1785291,"point {fill-color: red}"],[12.499948448333333,0.1760302,"point {fill-color: red}"],[27.205666341666667,0.1528651,"point {fill-color: red}"],[4.478275166666667,0.1201376,"point {fill-color: red}"],[90.247879785,1.2016961,"point {fill-color: blue}"],[17.030285595,0.1538317,"point {fill-color: red}"],[44.22267518333334,1.2068003,"point {fill-color: blue}"],[56.06800488166667,0.221573,"point {fill-color: red}"],[112.187806425,1.2410694,"point {fill-color: blue}"],[12.938699230000001,0.1768649,"point {fill-color: red}"],[78.61715465166667,1.2571913,"point {fill-color: blue}"],[57.56987962,0.170096,"point {fill-color: red}"],[107.38113058500001,1.2842683,"point {fill-color: blue}"],[114.92149421,1.4791484,"point {fill-color: blue}"],[85.92122537333333,0.2157517,"point {fill-color: red}"],[106.57325469,1.110651,"point {fill-color: blue}"],[95.087340755,1.6116157,"point {fill-color: blue}"],[58.16652578166667,0.1537105,"point {fill-color: red}"],[110.46850410666667,1.2520277,"point {fill-color: blue}"],[111.87444235166667,1.2963,"point {fill-color: blue}"],[83.92497317333334,1.2266662,"point {fill-color: blue}"],[24.970793208333333,0.1525029,"point {fill-color: red}"],[60.58494389166667,1.1332303,"point {fill-color: blue}"],[110.44966615666667,1.1732307,"point {fill-color: blue}"],[39.15555298833333,0.1905078,"point {fill-color: red}"],[74.14524927333333,1.3574221,"point {fill-color: blue}"],[44.85535599666667,0.2110926,"point {fill-color: red}"],[33.47827237166667,0.2085166,"point {fill-color: red}"],[26.261190368333335,0.22051959999999998,"point {fill-color: red}"],[23.586615993333332,0.151187,"point {fill-color: red}"],[108.43388421166667,1.2534212,"point {fill-color: blue}"],[117.38849716666667,0.1715093,"point {fill-color: red}"],[12.16588387,0.2208625,"point {fill-color: red}"],[290.60522265000003,0.1548714,"point {fill-color: red}"]]);
  options.hAxis = {
    title: 'minutes'
  };
  options.vAxis = {
    title: 'seconds'
  };
  return new google.visualization.ScatterChart(document.getElementById('chart_div_less-frequent-cold-starts-in-google-cloud-functions\/old_scatter'));
});
</script>
<figure>
  <div id="chart_div_less-frequent-cold-starts-in-google-cloud-functions/old_scatter"></div>
  <figcaption class="imageCaption"><h4>Examples of GCF idle instances lifecycle in February 2019</h4></figcaption>
</figure>
<p>This chart plots the response duration (Y-axis) by the interval since the previous requests (X-axis). Each point represents a single request in the dataset. Blue points are cold starts, and red points are responses from warm instances.</p>
<p>The chart has changed in April:</p>




  





<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('number', 'ID');
  data.addColumn('number', 'Value');
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows([[105.81351983166667,0.0756384,"point {fill-color: red}"],[96.59532183333333,0.1220762,"point {fill-color: red}"],[82.52338920333334,0.030621699999999998,"point {fill-color: red}"],[92.53022229,0.037190299999999996,"point {fill-color: red}"],[49.42072908666667,0.1941552,"point {fill-color: red}"],[25.237515588333334,0.0778483,"point {fill-color: red}"],[113.05062448,0.1585164,"point {fill-color: red}"],[148.71092069333335,0.10826279999999999,"point {fill-color: red}"],[161.80196170166667,0.0821906,"point {fill-color: red}"],[61.79145604833334,0.0837918,"point {fill-color: red}"],[104.493495525,0.0686647,"point {fill-color: red}"],[267.7564307283333,0.1744695,"point {fill-color: red}"],[277.05766174166666,0.0600386,"point {fill-color: red}"],[48.45887947666667,0.06616659999999999,"point {fill-color: red}"],[28.87558840666667,0.10678259999999999,"point {fill-color: red}"],[17.322959245,0.0730233,"point {fill-color: red}"],[50.60919381666667,0.08554289999999999,"point {fill-color: red}"],[29.05453872,0.0727881,"point {fill-color: red}"],[194.61954192333334,0.1215207,"point {fill-color: red}"],[198.44057850333334,0.07615469999999999,"point {fill-color: red}"],[42.062566546666666,0.0598208,"point {fill-color: red}"],[99.04740697,0.1055122,"point {fill-color: red}"],[250.83990179,0.055309,"point {fill-color: red}"],[186.19912747166666,0.0986976,"point {fill-color: red}"],[174.37558632333332,0.1468729,"point {fill-color: red}"],[128.72715744333334,0.0805317,"point {fill-color: red}"],[9.008557521666667,0.0900523,"point {fill-color: red}"],[90.82616570500001,0.031956,"point {fill-color: red}"],[227.20806170666668,0.07217219999999999,"point {fill-color: red}"],[278.90462053833335,0.06417819999999999,"point {fill-color: red}"],[146.42299223333333,0.0648044,"point {fill-color: red}"],[128.49140270166666,0.08831399999999999,"point {fill-color: red}"],[11.661693435,0.0358562,"point {fill-color: red}"],[33.35804462833333,0.1572796,"point {fill-color: red}"],[13.464274306666667,0.08522819999999999,"point {fill-color: red}"],[65.31287226,0.17993779999999998,"point {fill-color: red}"],[89.57953647666668,0.09941599999999999,"point {fill-color: red}"],[89.44180861833334,0.0357816,"point {fill-color: red}"],[7.539028968333334,0.0741588,"point {fill-color: red}"],[19.235294858333333,0.0291206,"point {fill-color: red}"],[14.664884098333333,0.0339854,"point {fill-color: red}"],[4.666798855,6.980028099999999,"point {fill-color: blue}"],[256.80711591333335,0.09284329999999999,"point {fill-color: red}"],[254.14269916833334,0.1315284,"point {fill-color: red}"],[2.810462406666667,0.1026319,"point {fill-color: red}"],[91.52725614166667,0.0323332,"point {fill-color: red}"],[23.913188358333333,0.0982982,"point {fill-color: red}"],[239.61879878333335,0.0689915,"point {fill-color: red}"],[143.94349646666666,0.031173299999999998,"point {fill-color: red}"],[285.88628766666665,0.07416099999999999,"point {fill-color: red}"],[46.74573943833333,0.03362,"point {fill-color: red}"],[185.33489473166668,0.0900142,"point {fill-color: red}"],[131.54717144833333,0.0687513,"point {fill-color: red}"],[0.4547042333333333,0.0433171,"point {fill-color: red}"],[84.41282356833334,0.0642108,"point {fill-color: red}"],[20.79435291,0.0906395,"point {fill-color: red}"],[85.97549925,0.1084324,"point {fill-color: red}"],[28.659735068333333,0.079486,"point {fill-color: red}"],[52.35472729166667,0.1346619,"point {fill-color: red}"],[82.98398236,0.0371929,"point {fill-color: red}"],[1.3526582516666668,0.0913151,"point {fill-color: red}"],[54.75896879333334,0.19168249999999998,"point {fill-color: red}"],[293.9280581483333,0.0962229,"point {fill-color: red}"],[68.94369074333333,0.0978478,"point {fill-color: red}"],[75.60866382,0.07683269999999999,"point {fill-color: red}"],[202.33829910333333,0.081773,"point {fill-color: red}"],[121.30490491,0.18148599999999998,"point {fill-color: red}"],[37.13650702166667,0.1053168,"point {fill-color: red}"],[190.06912669833335,0.061008,"point {fill-color: red}"],[58.60327119166667,0.0332668,"point {fill-color: red}"],[2.8519092383333335,0.059394999999999996,"point {fill-color: red}"],[28.325070738333334,0.27229729999999996,"point {fill-color: red}"],[58.603558103333334,0.037759999999999995,"point {fill-color: red}"],[66.35570874833333,0.050254099999999996,"point {fill-color: red}"],[238.27509040666666,6.5182281,"point {fill-color: blue}"],[200.938462915,0.0299812,"point {fill-color: red}"],[65.55312997333334,0.029154,"point {fill-color: red}"],[190.12284801333334,0.1362965,"point {fill-color: red}"],[118.76757765333333,0.07271559999999999,"point {fill-color: red}"],[6.294241638333333,0.0368406,"point {fill-color: red}"],[167.25433674,0.06291519999999999,"point {fill-color: red}"],[33.319838510000004,0.0922081,"point {fill-color: red}"],[31.099837768333334,0.1234913,"point {fill-color: red}"],[50.805670795000005,0.095215,"point {fill-color: red}"],[74.38146856666667,0.0835172,"point {fill-color: red}"],[69.08117308666667,0.337584,"point {fill-color: red}"],[152.90356230833333,7.0634695999999995,"point {fill-color: blue}"],[281.59012889666667,2.7039446,"point {fill-color: blue}"],[39.28964179,0.09857819999999999,"point {fill-color: red}"],[265.620545325,0.0808412,"point {fill-color: red}"],[232.68172477666667,2.656344,"point {fill-color: blue}"],[69.8941448,6.5705288,"point {fill-color: blue}"],[89.23974576833334,0.09917559999999999,"point {fill-color: red}"],[0.11678145666666667,0.030070399999999997,"point {fill-color: red}"],[45.84248363333334,0.0748359,"point {fill-color: red}"],[62.602150665,0.038969199999999996,"point {fill-color: red}"],[291.207324705,0.030794,"point {fill-color: red}"],[89.33692783000001,0.0452252,"point {fill-color: red}"],[289.40809041833336,0.07129529999999999,"point {fill-color: red}"],[9.629405533333333,0.1372096,"point {fill-color: red}"],[87.47228188333334,0.0468071,"point {fill-color: red}"],[5.910260966666667,0.0381505,"point {fill-color: red}"],[34.517435375,0.0393636,"point {fill-color: red}"],[69.07650127166667,0.033339799999999996,"point {fill-color: red}"],[205.29608801666666,0.1650988,"point {fill-color: red}"],[125.51832391666667,0.031141099999999998,"point {fill-color: red}"],[121.45256027833334,0.0735676,"point {fill-color: red}"],[110.02986198666667,0.059318199999999995,"point {fill-color: red}"],[231.31997054333334,0.1615936,"point {fill-color: red}"],[74.23321002333333,0.049579899999999996,"point {fill-color: red}"],[36.708360685,0.0401793,"point {fill-color: red}"],[168.56717707,0.0903011,"point {fill-color: red}"],[55.10216827833334,0.0286761,"point {fill-color: red}"],[134.78821275833334,0.0861863,"point {fill-color: red}"],[83.05524857166667,0.0576843,"point {fill-color: red}"],[271.048924885,0.1313359,"point {fill-color: red}"],[214.3299092,0.0390118,"point {fill-color: red}"],[106.01154154166667,0.0725082,"point {fill-color: red}"],[159.82971452166666,0.10186469999999999,"point {fill-color: red}"],[43.648102978333334,0.0321774,"point {fill-color: red}"],[275.1847367666667,0.07704799999999999,"point {fill-color: red}"],[170.71661992166668,0.11647769999999999,"point {fill-color: red}"],[73.730849805,0.13615739999999998,"point {fill-color: red}"],[43.474220425,0.0817236,"point {fill-color: red}"],[106.73321274666667,0.2084662,"point {fill-color: red}"],[110.01190759833334,0.11550389999999999,"point {fill-color: red}"],[28.577336391666666,0.06347649999999999,"point {fill-color: red}"],[87.38391993,0.0876777,"point {fill-color: red}"],[53.46984533,0.1550376,"point {fill-color: red}"],[23.178145048333334,0.0749867,"point {fill-color: red}"],[255.28641159,0.0738566,"point {fill-color: red}"],[67.88033142500001,0.030962999999999997,"point {fill-color: red}"],[8.106229165,0.0616259,"point {fill-color: red}"],[43.638769276666665,0.07224599999999999,"point {fill-color: red}"],[218.93553511833335,0.2332842,"point {fill-color: red}"],[119.78381243,0.0319818,"point {fill-color: red}"],[2.3599741583333333,0.0670926,"point {fill-color: red}"],[108.38747658666666,0.11651009999999999,"point {fill-color: red}"],[70.59545352333333,0.0803028,"point {fill-color: red}"],[12.77326927,0.1274871,"point {fill-color: red}"],[64.41290596,0.0681489,"point {fill-color: red}"],[122.67954707166668,0.0865753,"point {fill-color: red}"],[145.48412965333333,0.13749609999999998,"point {fill-color: red}"],[152.58686381666666,0.1228137,"point {fill-color: red}"],[20.062284493333333,0.0743832,"point {fill-color: red}"],[46.22047467666667,0.0348724,"point {fill-color: red}"],[242.49706731,0.073849,"point {fill-color: red}"],[140.83951085833334,0.030178,"point {fill-color: red}"],[39.258356801666665,0.0796812,"point {fill-color: red}"],[21.402082025000002,0.0740144,"point {fill-color: red}"],[26.288567381666667,0.0338959,"point {fill-color: red}"],[129.705454585,0.10233239999999999,"point {fill-color: red}"],[28.582631195,0.1118455,"point {fill-color: red}"],[87.26183483333334,0.0603161,"point {fill-color: red}"],[71.37898861000001,0.0736233,"point {fill-color: red}"],[184.275699095,0.0336586,"point {fill-color: red}"],[89.53028749833334,0.0643133,"point {fill-color: red}"],[111.17367596333334,0.2814329,"point {fill-color: red}"],[88.51148966666666,0.0680727,"point {fill-color: red}"],[91.28765077666667,0.0327099,"point {fill-color: red}"],[90.21515986166666,0.0334349,"point {fill-color: red}"],[50.89858688166667,0.1992212,"point {fill-color: red}"],[112.94588925333333,0.0390063,"point {fill-color: red}"],[109.74530879833334,0.14959509999999998,"point {fill-color: red}"],[207.89940760000002,0.1019056,"point {fill-color: red}"],[83.06639447166667,0.031587,"point {fill-color: red}"],[27.896309353333333,0.07082809999999999,"point {fill-color: red}"],[294.96954953666665,0.0330855,"point {fill-color: red}"],[9.821824455,0.0336211,"point {fill-color: red}"],[108.79262868333333,0.031684199999999996,"point {fill-color: red}"],[57.92157143833334,0.1813988,"point {fill-color: red}"],[81.56983929666667,0.0928734,"point {fill-color: red}"],[197.52008209166667,0.19999229999999998,"point {fill-color: red}"],[4.2182380083333335,0.13001759999999998,"point {fill-color: red}"],[137.65967530166668,0.07651379999999999,"point {fill-color: red}"],[163.81422214666668,0.07674109999999999,"point {fill-color: red}"],[139.857681185,0.0337813,"point {fill-color: red}"],[78.58233377666667,0.10925359999999999,"point {fill-color: red}"],[60.320831160000004,0.1105591,"point {fill-color: red}"],[23.282000846666666,0.0864105,"point {fill-color: red}"],[174.62606653,0.0876941,"point {fill-color: red}"],[60.899337360000004,0.0912352,"point {fill-color: red}"],[111.52586705,0.0971148,"point {fill-color: red}"],[27.709486526666666,0.09527369999999999,"point {fill-color: red}"],[41.14815743333333,0.0693879,"point {fill-color: red}"],[87.26228773333334,0.0631099,"point {fill-color: red}"],[62.65977429666667,0.3168045,"point {fill-color: red}"],[268.7295370416667,0.032436,"point {fill-color: red}"],[234.550810375,0.1829933,"point {fill-color: red}"],[58.31415157166667,0.0716393,"point {fill-color: red}"],[90.18688921333333,0.0731839,"point {fill-color: red}"],[95.03349283333334,0.0384339,"point {fill-color: red}"],[51.45620064166667,0.039131,"point {fill-color: red}"],[41.723880065,0.1274523,"point {fill-color: red}"],[176.55088172833334,0.060309699999999994,"point {fill-color: red}"],[93.56903528833334,0.0623134,"point {fill-color: red}"],[78.03119414833334,0.0296536,"point {fill-color: red}"],[85.15402862333333,0.0334091,"point {fill-color: red}"],[73.89229103833334,0.059188399999999995,"point {fill-color: red}"],[118.84997997,0.063896,"point {fill-color: red}"],[142.58135959166668,6.0924115,"point {fill-color: blue}"],[137.46751286333333,0.0342234,"point {fill-color: red}"],[203.777611675,0.0738489,"point {fill-color: red}"],[236.29880228000002,6.3002049,"point {fill-color: blue}"],[55.065055156666666,0.0334912,"point {fill-color: red}"],[246.47420958666666,0.0662546,"point {fill-color: red}"],[173.37935109166668,0.0356946,"point {fill-color: red}"],[223.66577324666667,0.033801899999999996,"point {fill-color: red}"],[115.40981442666667,0.0327512,"point {fill-color: red}"],[64.06819050666667,0.0329217,"point {fill-color: red}"],[0.9286138583333333,0.0335447,"point {fill-color: red}"],[73.22287179166666,0.043589199999999995,"point {fill-color: red}"],[78.34373812,0.0695628,"point {fill-color: red}"],[94.09217755,0.0567748,"point {fill-color: red}"],[97.37234852666667,0.030153899999999997,"point {fill-color: red}"],[1.698437135,0.061472599999999995,"point {fill-color: red}"],[58.79156018333333,0.1026291,"point {fill-color: red}"],[98.97854800333333,0.0304702,"point {fill-color: red}"],[120.18787187833334,0.0305964,"point {fill-color: red}"],[194.53364270666668,0.1206409,"point {fill-color: red}"],[226.27955292000001,0.117092,"point {fill-color: red}"],[27.553007660000002,0.0607601,"point {fill-color: red}"],[57.903099553333334,0.0933208,"point {fill-color: red}"],[254.07820787166668,0.1338666,"point {fill-color: red}"],[24.944446166666665,0.0319367,"point {fill-color: red}"],[116.57615356333334,0.10434099999999999,"point {fill-color: red}"],[26.112880526666668,0.030970099999999997,"point {fill-color: red}"],[91.77510080333333,0.08029879999999999,"point {fill-color: red}"],[104.97564567333333,0.0717674,"point {fill-color: red}"],[84.70984694333333,0.0701931,"point {fill-color: red}"],[286.99773266166665,0.1295696,"point {fill-color: red}"],[2.486252855,0.0409411,"point {fill-color: red}"],[83.22916459333334,0.1168819,"point {fill-color: red}"],[108.305834295,0.0309126,"point {fill-color: red}"],[12.426002753333334,0.09964179999999999,"point {fill-color: red}"],[99.22689499,0.0663468,"point {fill-color: red}"],[287.7314649016667,0.0690353,"point {fill-color: red}"],[75.18027952166666,0.084482,"point {fill-color: red}"],[21.594071166666666,0.0317514,"point {fill-color: red}"],[232.58802146,0.082247,"point {fill-color: red}"],[259.43584583166665,0.06594169999999999,"point {fill-color: red}"],[35.609715625,0.074631,"point {fill-color: red}"],[232.028917645,0.1386652,"point {fill-color: red}"],[94.04295043,0.0821861,"point {fill-color: red}"],[176.30368083166667,0.0625462,"point {fill-color: red}"],[67.16896412166666,0.0312915,"point {fill-color: red}"],[70.81787476666666,0.5033162,"point {fill-color: red}"],[158.06874901833334,0.059058299999999994,"point {fill-color: red}"],[184.45501218500002,0.1037413,"point {fill-color: red}"],[39.86585632166667,0.0825002,"point {fill-color: red}"],[252.97296709,0.0618168,"point {fill-color: red}"],[130.48510006666666,0.2066771,"point {fill-color: red}"],[53.98758316,0.089819,"point {fill-color: red}"],[78.41117205,0.0321674,"point {fill-color: red}"],[155.796365075,3.444598,"point {fill-color: blue}"],[62.37979291166667,0.0435409,"point {fill-color: red}"],[36.828457155,0.09497219999999999,"point {fill-color: red}"],[107.28061628,0.0323972,"point {fill-color: red}"],[72.95397739,0.09296979999999999,"point {fill-color: red}"],[99.9561877,6.3112324,"point {fill-color: blue}"],[66.28731935666667,0.0317737,"point {fill-color: red}"],[14.904075666666667,0.08947469999999999,"point {fill-color: red}"],[64.45072178666666,0.0308092,"point {fill-color: red}"],[114.92463681,0.1229376,"point {fill-color: red}"],[274.20573200166666,0.1003845,"point {fill-color: red}"],[40.549934633333336,0.0434837,"point {fill-color: red}"],[145.70529646666668,0.0372578,"point {fill-color: red}"],[11.509618235,0.1150585,"point {fill-color: red}"],[68.15808847333334,0.19566599999999998,"point {fill-color: red}"],[85.93793673333333,0.0403523,"point {fill-color: red}"],[42.64762271833333,0.0314727,"point {fill-color: red}"],[243.59423202,0.9177206,"point {fill-color: blue}"],[156.29282349,0.10082819999999999,"point {fill-color: red}"],[116.76705087,0.032882999999999996,"point {fill-color: red}"],[39.818408555,0.063784,"point {fill-color: red}"],[259.16311091833336,0.10827869999999999,"point {fill-color: red}"],[62.44099923,0.0757595,"point {fill-color: red}"],[237.23580263333335,1.3148431999999999,"point {fill-color: blue}"],[299.5862769716667,0.06334,"point {fill-color: red}"],[230.721367455,0.1531416,"point {fill-color: red}"],[102.057633025,0.1124321,"point {fill-color: red}"],[268.4158676033333,0.0656916,"point {fill-color: red}"],[242.78164158166666,2.0297392,"point {fill-color: blue}"],[87.21862182166667,0.1239132,"point {fill-color: red}"],[234.23839701166668,6.368559599999999,"point {fill-color: blue}"],[127.67222922833334,0.15714999999999998,"point {fill-color: red}"],[1.4884433283333334,0.0300426,"point {fill-color: red}"],[88.98313027,0.0810384,"point {fill-color: red}"],[178.53583606,0.0954443,"point {fill-color: red}"],[102.22187692166666,0.0338313,"point {fill-color: red}"],[11.321818361666667,0.066672,"point {fill-color: red}"],[20.171261223333335,0.1244933,"point {fill-color: red}"],[243.64260562166666,0.10686029999999999,"point {fill-color: red}"],[32.73472273666667,0.0861528,"point {fill-color: red}"],[225.19487173000002,0.10225419999999999,"point {fill-color: red}"],[45.27914278,6.4535791,"point {fill-color: blue}"],[223.28896599666666,0.0844892,"point {fill-color: red}"],[39.470883595000004,0.0352363,"point {fill-color: red}"],[283.02859960333336,0.062008799999999996,"point {fill-color: red}"],[34.10210403166667,0.0335551,"point {fill-color: red}"]]);
  options.hAxis = {
    title: 'minutes'
  };
  options.vAxis = {
    title: 'seconds'
  };
  return new google.visualization.ScatterChart(document.getElementById('chart_div_less-frequent-cold-starts-in-google-cloud-functions\/new_scatter'));
});
</script>
<figure>
  <div id="chart_div_less-frequent-cold-starts-in-google-cloud-functions/new_scatter"></div>
  <figcaption class="imageCaption"><h4>Examples of GCF idle instances lifecycle in April 2019</h4></figcaption>
</figure>
<p>It seems that this chart shows fewer points, but that&rsquo;s deceptive: the points are just much denser and almost all of the responses are warm! Even after 4-5 hours of idling, the instance survives most of the time.</p>
<p>The following chart estimates the probability (Y-axis) of a cold start by the interval between two subsequent requests (X-axis). A higher line means higher chances of an instance being recycled:</p>




  







<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('string', 'Time');

  const points = [{"items":[[0,0],[20,0.05434782608695652],[40,0.19753086419753085],[60,0.2768361581920904],[80,0.4968944099378882],[100,0.5942028985507246],[120,0.7128112936964913],[140,0.7128112936964913],[160,0.7128112936964913],[180,0.7128112936964913],[200,0.8402777777777778],[220,0.8402777777777778],[240,0.8573666308628715],[260,0.8573666308628715],[280,0.8573666308628715],[300,0.8573666308628715]],"name":"February 2019"},{"items":[[0,0],[20,0.017605633802816902],[40,0.02041190237905736],[60,0.02041190237905736],[80,0.026619343389529725],[100,0.040280210157618214],[120,0.043941411451398134],[140,0.07396449704142012],[160,0.08843537414965986],[180,0.09942753945359485],[200,0.09942753945359485],[220,0.09942753945359485],[240,0.146006600660066],[260,0.146006600660066],[280,0.15975440222428175],[300,0.15975440222428175]],"name":"April 2019"}];
  const seriesCount = points.length;
  for (var j = 0; j < seriesCount; j++) {
    data.addColumn('number', points[j].name);
  }

  const rows = [];
  for (var i = 0; i < points[0].items.length; i++)
  {
    const item = [points[0].items[i][0]];
    for (var j = 0; j < seriesCount; j++) {
      item.push(points[j].items[i][1]);
    }
    rows.push(item);
  }

  data.addRows(rows);
  console.debug(rows);
  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability',
    maxValue: 1.0
  };
  options.legend = { position: 'top' };
  options.series = [{"color":"#3572A5"},{"color":"#339933"}];
  return new google.visualization.AreaChart(document.getElementById('chart_div_less-frequent-cold-starts-in-google-cloud-functions\/bytime_interval'));
});
</script>
<figure>
  <div id="chart_div_less-frequent-cold-starts-in-google-cloud-functions/bytime_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start happening before minute X in February vs. April</h4></figcaption>
</figure>
<p>While the chance of surviving 5 hours of inactivity used to be about 15%, now it&rsquo;s flipped, and an idle instance survives for 5 hours in almost 85% of cases.</p>
<h2 id="what-are-the-other-factors">What Are The Other Factors?</h2>
<p>It looks like larger instances in terms of provisioned memory are more likely to survive than smaller instances:</p>




  







<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('string', 'Time');

  const points = [{"items":[[0,0],[20,0.0821917808219178],[40,0.0802936834335126],[60,0.0802936834335126],[80,0.0802936834335126],[100,0.12101449275362319],[120,0.12101449275362319],[140,0.21212121212121213],[160,0.2124637155297533],[180,0.2124637155297533],[200,0.2124637155297533],[220,0.2124637155297533],[240,0.26444927253197925],[260,0.26444927253197925],[280,0.26444927253197925],[300,0.26444927253197925]],"name":"128 MB"},{"items":[[0,0],[20,0],[40,0.004329004329004329],[60,0.00819672131147541],[80,0.013215859030837005],[100,0.029045643153526972],[120,0.045454545454545456],[140,0.05316606929510155],[160,0.05316606929510155],[180,0.13080886675657918],[200,0.13080886675657918],[220,0.13080886675657918],[240,0.1966800356506239],[260,0.1966800356506239],[280,0.1966800356506239],[300,0.1966800356506239]],"name":"256 MB"},{"items":[[0,0],[20,0],[40,0.00425531914893617],[60,0.00975609756097561],[80,0.012605042016806723],[100,0.02358490566037736],[120,0.037037037037037035],[140,0.03840508806262231],[160,0.03840508806262231],[180,0.06191417937953207],[200,0.06191417937953207],[220,0.06191417937953207],[240,0.06191417937953207],[260,0.06191417937953207],[280,0.15384615384615385],[300,0.15384615384615385]],"name":"512 MB"},{"items":[[0,0],[20,0.0044444444444444444],[40,0.0044444444444444444],[60,0.010275250265528774],[80,0.010275250265528774],[100,0.010275250265528774],[120,0.023809523809523808],[140,0.025],[160,0.05864696374971139],[180,0.05864696374971139],[200,0.06411201179071481],[220,0.06411201179071481],[240,0.10268317853457172],[260,0.10268317853457172],[280,0.1261904761904762],[300,0.1261904761904762]],"name":"1 GB"},{"items":[[0,0],[20,0],[40,0],[60,0.011192616660354416],[80,0.011192616660354416],[100,0.011875968992248062],[120,0.011875968992248062],[140,0.03076923076923077],[160,0.04830188679245283],[180,0.04830188679245283],[200,0.05616483881688268],[220,0.05616483881688268],[240,0.10339281266786597],[260,0.10339281266786597],[280,0.10339281266786597],[300,0.10339281266786597]],"name":"2 GB"}];
  const seriesCount = points.length;
  for (var j = 0; j < seriesCount; j++) {
    data.addColumn('number', points[j].name);
  }

  const rows = [];
  for (var i = 0; i < points[0].items.length; i++)
  {
    const item = [points[0].items[i][0]];
    for (var j = 0; j < seriesCount; j++) {
      item.push(points[j].items[i][1]);
    }
    rows.push(item);
  }

  data.addRows(rows);
  console.debug(rows);
  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability',
    maxValue: 1.0
  };
  options.legend = { position: 'top' };
  options.series =  null ;
  return new google.visualization.AreaChart(document.getElementById('chart_div_less-frequent-cold-starts-in-google-cloud-functions\/bymemory_interval'));
});
</script>
<figure>
  <div id="chart_div_less-frequent-cold-starts-in-google-cloud-functions/bymemory_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start by instance size</h4></figcaption>
</figure>
<p>After 5 hours of idling, an instance of 128 MB is 2.5x more likely to be recycled compared to a 2 GB instance.</p>
<p>A similar effect is observed for language runtimes:</p>




  







<script type="text/javascript">
addChart((data, options) => {
  data.addColumn('string', 'Time');

  const points = [{"items":[[0,0],[20,0],[40,0],[60,0.0199203187250996],[80,0.0211864406779661],[100,0.03769291053773812],[120,0.03769291053773812],[140,0.05],[160,0.11067193675889328],[180,0.11067193675889328],[200,0.14390851449275363],[220,0.14390851449275363],[240,0.14533799533799535],[260,0.14533799533799535],[280,0.1917808219178082],[300,0.23333333333333334]],"name":"Python"},{"items":[[0,0],[20,0],[40,0.004347826086956522],[60,0.010575341486090253],[80,0.010575341486090253],[100,0.01282051282051282],[120,0.04827586206896552],[140,0.07206273258904837],[160,0.07206273258904837],[180,0.07206273258904837],[200,0.09540318438623524],[220,0.09540318438623524],[240,0.13775780010576413],[260,0.13775780010576413],[280,0.16354723707664884],[300,0.16354723707664884]],"name":"JavaScript"},{"items":[[0,0],[20,0],[40,0.008620689655172414],[60,0.011042649465678251],[80,0.011042649465678251],[100,0.019982737425784006],[120,0.019982737425784006],[140,0.03115998952605394],[160,0.03115998952605394],[180,0.0573719885161873],[200,0.0573719885161873],[220,0.0573719885161873],[240,0.0573719885161873],[260,0.0573719885161873],[280,0.0573719885161873],[300,0.0573719885161873]],"name":"Go"}];
  const seriesCount = points.length;
  for (var j = 0; j < seriesCount; j++) {
    data.addColumn('number', points[j].name);
  }

  const rows = [];
  for (var i = 0; i < points[0].items.length; i++)
  {
    const item = [points[0].items[i][0]];
    for (var j = 0; j < seriesCount; j++) {
      item.push(points[j].items[i][1]);
    }
    rows.push(item);
  }

  data.addRows(rows);
  console.debug(rows);
  options.lineWidth = 3;
  options.hAxis = {
    title: 'Time since the previous invocation (minutes)'
  };
  options.vAxis = {
    title: 'probability',
    maxValue: 1.0
  };
  options.legend = { position: 'top' };
  options.series = [{"color":"#3572A5"},{"color":"#F1E05A"},{"color":"#339933"}];
  return new google.visualization.AreaChart(document.getElementById('chart_div_less-frequent-cold-starts-in-google-cloud-functions\/bylanguage_interval'));
});
</script>
<figure>
  <div id="chart_div_less-frequent-cold-starts-in-google-cloud-functions/bylanguage_interval"></div>
  <figcaption class="imageCaption"><h4>Probability of a cold start by instance size by language</h4></figcaption>
</figure>
<p>While Python and Javascript are in the same ballpark, Go functions seem to survive longer and more reliably.</p>
<h2 id="practical-takeaway">Practical Takeaway</h2>
<p>Cloud providers are fighting the cold starts on many fronts. Now, it seems that Google found a way to keep warm container instances idle for extended periods without too much overhead for their infrastructure.</p>
<p>With this recent improvement, the practice of having a dummy timer-based trigger to keep function instances warm gets less useful. Which is a good thing: less hand-holding and management means more time on developing useful features! One tiny step closer to a serverless nirvana.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Binaris]]></title>
            <link href="https://mikhail.io/tags/binaris/"/>
            <id>https://mikhail.io/tags/binaris/</id>
            
            <published>2019-04-03T00:00:00+00:00</published>
            <updated>2019-04-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Dataviz]]></title>
            <link href="https://mikhail.io/tags/dataviz/"/>
            <id>https://mikhail.io/tags/dataviz/</id>
            
            <published>2019-04-03T00:00:00+00:00</published>
            <updated>2019-04-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Maps]]></title>
            <link href="https://mikhail.io/tags/maps/"/>
            <id>https://mikhail.io/tags/maps/</id>
            
            <published>2019-04-03T00:00:00+00:00</published>
            <updated>2019-04-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Visualizing (Absence Of) Cold Starts in Binaris]]></title>
            <link href="https://mikhail.io/2019/04/visualizing-absence-of-cold-starts-binaris/"/>
            <id>https://mikhail.io/2019/04/visualizing-absence-of-cold-starts-binaris/</id>
            
            <published>2019-04-03T00:00:00+00:00</published>
            <updated>2019-04-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Running visual map experiment on Binaris cloud functions.</blockquote><p>In my previous article, <a href="/2019/03/visualizing-cold-starts/">Visualizing Cold Starts</a>, I visually demonstrated the impact of <a href="/serverless/coldstarts/">serverless cold starts</a> on map loading time. When the serverless function serving the map tiles is cold, users experience a noticeable delay before they can see the selected map view.</p>
<p>I then ran the map loading test on  AWS Lambda, Google Cloud Functions, and Azure Functions. The duration of the perceptible delay varied between cloud providers, ranging from 1.5 to 8 seconds.</p>
<p>Is this effect unfortunately inherent to all Function-as-a-Service offerings? Or is it possible to avoid cold starts altogether?</p>
<h2 id="binaris">Binaris</h2>
<p><a href="https://www.binaris.com/">Binaris</a> is a new player in the serverless functions market. Binaris removes performance barriers to improve developer productivity and enable any application to be built on top of cloud functions.</p>
<p>One of the clear benefits and differentiators from other providers is the absence of cold starts in Binaris functions:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Another great post by Mikhail. Cold start is a deal breaker for many apps, especially anything user facing. Check out <a href="https://twitter.com/gobinaris?ref_src=twsrc%5Etfw">@gobinaris</a> if you&#39;re looking for serverless but want to get rid of cold starts altogether. <a href="https://t.co/3MGH9gEr4c">https://t.co/3MGH9gEr4c</a></p>&mdash; Avner (@avnerbraverman) <a href="https://twitter.com/avnerbraverman/status/1106616656602685440?ref_src=twsrc%5Etfw">March 15, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>In light of this, I decided to rerun my map visualization using Binaris functions to illustrate the difference:</p>




<figure style="cursor: pointer">
    <img src="binaris.png" alt="Map loads from Binaris backend" data-alt="binaris.gif">
    <figcaption><h4>Map loads from Binaris backend (click to play or replay)</h4></figcaption>
</figure>
<p>The initial map loads immediately after redeployment of the Binaris function, so it would cause a cold start for other providers. There’s no noticeable delay for the Binaris function: All map tiles are loaded after 0.4 seconds, similar to the loading times on subsequent zoom levels during the same test.</p>
<p>In case the warm-up happens during the deployment, I reran the experiment after several hours of inactivity. The idle function would have been &ldquo;frozen&rdquo; by other cloud providers,  but I got no cold start for my Binaris function.</p>
<p>Note that the map tiles are stored in AWS S3. For fairness, the S3 resides in the same region where the Binaris function runs (us-east-1). The client ran on a VM in Azure’s US East region.</p>
<h2 id="how-does-binaris-handle-parallel-requests">How does Binaris handle parallel requests?</h2>
<p>The map control fires 12 parallel requests. Similar to the technique used in my initial <a href="/2019/03/visualizing-cold-starts/">Visualizing Cold Starts</a> article, I modified the function to color-code each tile based on the function’s instance ID and to overlay that ID on the image. This way, it’s easy to see how parallel requests were spread over the instances.</p>
<p>As explained in <a href="/2019/03/concurrency-and-isolation-in-serverless-functions/">Serverless Concurrency Models</a>, Binaris functions can run in two concurrency modes: concurrent or exclusive.</p>
<h3 id="concurrent-mode">Concurrent Mode</h3>
<p>When a Binaris function is configured to run in the concurrent mode, the same instance (a.k.a. <em>function unit</em>) can handle up to 10 requests at the same time. For I/O-bound functions, this can lead to more efficient resource utilization and lower cost.</p>
<p>When running in concurrent mode, a single instance may handle the entire color-coded map:</p>
<p><img src="binaris-concurrent-mode.png" alt="Concurrent Mode"></p>
<figcaption><h4>The same function unit served all 12 requests concurrently</h4></figcaption>
<p>The maximum concurrency level is currently 10, so some of the tiles in the picture above must have been processed sequentially. Indeed, sometimes it happens that two instances return the tiles for the same view:</p>
<p><img src="binaris-concurrent-mode-mixed.png" alt="Concurrent Mode Mixed"></p>
<figcaption><h4>Two function units served 12 parallel requests</h4></figcaption>
<p>How is the exclusive mode different?</p>
<h3 id="exclusive-mode">Exclusive Mode</h3>
<p>When a Binaris function is set to run in the exclusive mode, each instance handles a single request at a time. Therefore, multiple instances are needed to process concurrent requests.</p>
<p>As expected, the map gets more colorful for functions in the exclusive mode:</p>
<p><img src="binaris-exclusive-mode.png" alt="Exclusive Mode"></p>
<figcaption><h4>Four function units served 12 parallel requests</h4></figcaption>
<p>However, the number of instances doesn’t get close to the number of tiles: 12 tiles are usually served by 3-5 function units. I suspect this is partially explained by the fact that Binaris functions use HTTP/1.1 protocol, and the browser would only open a limited number of connections to the same host:</p>
<p><img src="binaris-http-connections.png" alt="HTTP Connections to Binaris function"></p>
<figcaption><h4>12 requests are cascaded by Chrome because they connect to the same host</h4></figcaption>
<p>A switch to HTTP/2 would shave some extra latency off the overall map loading time.</p>
<h2 id="conclusions">Conclusions</h2>
<p>The cold start issue is getting less significant for all cloud providers over time. Binaris may be ahead of others in this race.</p>
<p>My simple test does not necessarily prove the complete absence of cold starts in Binaris functions, but it looks optimistic. I look forward to a world where developers would be able to build applications, including user-facing latency-sensitive ones, out of composable managed serverless functions.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/binaris" term="binaris" label="Binaris" />
                             
                                <category scheme="https://mikhail.io/tags/maps" term="maps" label="Maps" />
                             
                                <category scheme="https://mikhail.io/tags/dataviz" term="dataviz" label="Dataviz" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Reducing Cold Start Duration in Azure Functions]]></title>
            <link href="https://mikhail.io/2019/03/reducing-azure-functions-cold-start-time/"/>
            <id>https://mikhail.io/2019/03/reducing-azure-functions-cold-start-time/</id>
            
            <published>2019-03-27T00:00:00+00:00</published>
            <updated>2019-03-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The influence of the deployment method, application insights, and more on Azure Functions cold starts.</blockquote><p>Back in February, I published the first version of <a href="/serverless/coldstarts/azure/">Cold Starts in Azure Functions</a>—the detailed analysis of cold start durations in serverless Azure. The article showed the following numbers for C# and JavaScript functions:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["C#",4.940131306,"Median: 4.9s",3.5689002093333335,8.574869002666652,3.5689002093333335,8.574869002666652,"{color: #178600; fill-color: #178600}"],["JavaScript",7.546261614666666,"Median: 7.5s",5.676060145333333,11.99859984266667,5.676060145333333,11.99859984266667,"{color: #F1E05A; fill-color: #F1E05A}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_reducing-azure-functions-cold-start-time\/languages'));
  
});
</script>
<figure>
  <div id="chart_div_reducing-azure-functions-cold-start-time/languages"></div>
  <figcaption class="imageCaption"><h4>Typical cold start durations per language (February 2019)</h4></figcaption>
</figure>
<p>Note that I amended the format of the original chart: the range shows the most common 67% of values, and the dot shows the median value. This change makes the visual comparison easier for the rest of today&rsquo;s post.</p>
<p>My numbers triggered several discussions on twitter. In one of them, Jeff Hollan (a program manager on Functions team) responded:</p>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">The numbers in <a href="https://twitter.com/MikhailShilkov?ref_src=twsrc%5Etfw">@MikhailShilkov</a> blog are significantly higher than what I see in our automated reports.  I&#39;m following up as well with <a href="https://twitter.com/MikhailShilkov?ref_src=twsrc%5Etfw">@MikhailShilkov</a> to validate all settings.  But the rest of convo is valid :)</p>&mdash; Jeff Hollan (@jeffhollan) <a href="https://twitter.com/jeffhollan/status/1100474640026132480?ref_src=twsrc%5Etfw">February 26, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>The team collects the stats of cold starts internally, and their numbers were lower than mine. We started an e-mail thread to reconcile the results. I won&rsquo;t publish any messages from the private thread, but I&rsquo;m posting the main findings below.</p>
<h2 id="check-the-deployment-method">Check the deployment method</h2>
<p>For my tests, I&rsquo;ve been using &ldquo;Run from external package&rdquo; deployment method, where the function deployment artifact is stored as a zip file on blob storage. This method is the most friendly for automation and infrastructure-as-code pipelines.</p>
<p>Apparently, it also increases the cold start duration. I believe the situation already improved since my original article, but here are the current numbers from mid-March.</p>
<p>.NET:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No Zip",2.3750335866666665,"Median: 2.4s",2.1337158226666664,2.673247721333334,2.1337158226666664,2.673247721333333,"{color: #008F95; fill-color: #008F95}"],["Local Zip",2.6825472653333335,"Median: 2.7s",2.4944191559999997,2.9249525773333334,2.4944191559999997,2.9249525773333334,"{color: #E9B000; fill-color: #E9B000}"],["External Zip",3.089673053333333,"Median: 3.1s",2.6488860746666667,3.755905496,2.6488860746666667,3.755905496,"{color: #E24E42; fill-color: #E24E42}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_reducing-azure-functions-cold-start-time\/deploymentcs'));
  
});
</script>
<figure>
  <div id="chart_div_reducing-azure-functions-cold-start-time/deploymentcs"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per deployment method for C# functions (March 2019)</h4></figcaption>
</figure>
<p>Node.js:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No Zip",3.9645696746666665,"Median: 4.0s",3.0125055733333332,5.792564922666666,3.0125055733333332,5.792564922666666,"{color: #008F95; fill-color: #008F95}"],["Local Zip",4.180329562666666,"Median: 4.2s",3.302554292,5.856474070666667,3.302554292,5.856474070666667,"{color: #E9B000; fill-color: #E9B000}"],["External Zip",5.0813236066666665,"Median: 5.1s",3.8377469759999996,7.825438031999999,3.8377469759999996,7.825438031999999,"{color: #E24E42; fill-color: #E24E42}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_reducing-azure-functions-cold-start-time\/deploymentjs'));
  
});
</script>
<figure>
  <div id="chart_div_reducing-azure-functions-cold-start-time/deploymentjs"></div>
  <figcaption class="imageCaption"><h4>Cold start durations per deployment method for JavaScript functions (March 2019)</h4></figcaption>
</figure>
<p>Run-from-external-zip deployment increases the cold start by approximately 1 second.</p>
<h2 id="application-insights">Application Insights</h2>
<p>I always configured my Function Apps to write telemetry to Application Insights. However, this adds a second to the cold start:</p>





  






<script type="text/javascript">
addChart((data, options) => {
  const points = [["No AppInsights",2.6825472653333335,"Median: 2.7s",2.4944191559999997,2.9249525773333334,2.4944191559999997,2.9249525773333334,"{color: #5BA3F1; fill-color: #5BA3F1}"],["With AppInsights",3.488305096666666,"Median: 3.5s",3.0500858440000003,4.097649741333333,3.0500858440000003,4.097649741333333,"{color: #003D8B; fill-color: #003D8B}"]];

  data.addColumn('string', 'x');
  data.addColumn('number', 'values');
  
  
  data.addColumn({type: 'string', role: 'tooltip'});
  
  
  
  options.series[0].tooltip = true;
  
  data.addColumn({id:'i0', type:'number', role:'interval'});
  data.addColumn({id:'i1', type:'number', role:'interval'});
  data.addColumn({id:'i2', type:'number', role:'interval'});
  data.addColumn({id:'i3', type:'number', role:'interval'});
  data.addColumn({ 'type': 'string', 'role': 'style' });
  data.addRows(points);

  options.lineWidth = 0;
  options.intervals = { 'style':'boxes' };
  options.vAxis = {
    title: 'seconds',
    viewWindow: { min: 0 }
  };

  
  return new google.visualization.ScatterChart(document.getElementById('chart_div_reducing-azure-functions-cold-start-time\/appinsights'));
  
});
</script>
<figure>
  <div id="chart_div_reducing-azure-functions-cold-start-time/appinsights"></div>
  <figcaption class="imageCaption"><h4>Cold start durations with and without Application Insights integration</h4></figcaption>
</figure>
<p>I can&rsquo;t really recommend &ldquo;Don&rsquo;t use Application Insights&rdquo; because the telemetry service is vital in most scenarios. Anyway, keep this fact in mind and watch <a href="https://github.com/Azure/azure-functions-host/issues/4183">the corresponding issue</a>.</p>
<h2 id="keep-bugging-the-team">Keep bugging the team</h2>
<p>While you can use the information above, the effect is still going to be limited. Obviously, the power to reduce the cold starts lies within the Azure Functions engineering team.</p>
<p>Coincidence or not, the numbers have already significantly improved since early February, and <a href="https://github.com/Azure/azure-functions-host/issues/4184">more work</a> is <a href="https://github.com/Azure/azure-functions-host/commit/792bb463b4bc48d67570d5b44b69c89b9d43f86d">in progress</a>.</p>
<p>I consider this to be a part of my mission: spotlighting the issues in public gives that nudge to prioritize performance improvements over other backlog items.</p>
<p>Meanwhile, the data in <a href="/serverless/coldstarts/azure/">Cold Starts in Azure Functions</a> and <a href="/serverless/coldstarts/big3/">Comparison of Cold Starts in Serverless Functions across AWS, Azure, and GCP</a> are updated: I&rsquo;m keeping them up-to-date as promised before.</p>
<h4 id="ps">P.S.</h4>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">Nice post and appreciate you circling back and helping keep us honest.</p>&mdash; Jeff Hollan (@jeffhollan) <a href="https://twitter.com/jeffhollan/status/1110905674714669059?ref_src=twsrc%5Etfw">March 27, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Concurrency and Isolation in Serverless Functions]]></title>
            <link href="https://mikhail.io/2019/03/concurrency-and-isolation-in-serverless-functions/"/>
            <id>https://mikhail.io/2019/03/concurrency-and-isolation-in-serverless-functions/</id>
            
            <published>2019-03-24T00:00:00+00:00</published>
            <updated>2019-03-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Exploring approaches to sharing or isolating resources between multiple executions of the same cloud function and the associated trade-offs.</blockquote><p>Serverless vendors have different approaches when it comes to sharing or isolating resources between multiple executions of the same cloud function. In this article, I&rsquo;ll explore the execution concurrency models of three FaaS offerings and the associated trade-offs.</p>
<h2 id="aws-lambda">AWS Lambda</h2>
<p>AWS Lambda executions are always entirely isolated from each other. Simple enough, right?</p>
<p>A function execution maps 1:1 to a function instance. Each execution runs on a separate host, i.e., a dedicated container with its own instance of the runtime. All the resources of this host (CPU, RAM, scratch disk space) are dedicated solely to this execution.</p>
<p>AWS Lambda spins up as many hosts as needed to process all concurrent requests:</p>
<p><img src="isolated-executions.png" alt="Isolated Executions in AWS Lambda"></p>
<figcaption><h4>3 overlapping executions running on 3 isolated hosts</h4></figcaption>
<p>This model has a benefit of predictability: The resource manager allocates the same amount of resources to each execution. Therefore, the variability between executions is low (with the cold start being a remarkable exception).</p>
<p>Resource allocation has to be preconfigured by selecting the memory limit. CPU cycles are allocated proportionally to RAM. Performance variation might increase for smaller instance sizes: 128 MB instances are empirically known to be less consistent.</p>
<p>In terms of pricing, the executions are independent. The bill consists of two parts: a fixed fee per execution and a variable charge for execution duration, measured in GB-seconds:</p>
<pre><code>&lt;allocated instance size in GB&gt; * &lt;duration of execution rounded up to 100ms&gt;
</code></pre><p>In the example above, if each execution has 1 GB of allocated RAM and runs for 1 second, the total bill will be 3 GB-seconds.</p>
<p>This model works but is definitely better for some workloads than others.</p>
<h2 id="io-bound-workloads">I/O Bound Workloads</h2>
<p>Typically, enterprise workloads are not very demanding to CPU (not much computation other than serialization and conversions) and RAM (mostly consumed by the runtime and libraries since functions are stateless). Many functions end up calling other resources synchronously over the network: managed cloud services, databases, and web APIs. This means the execution duration is mainly determined by the response time of those resources.</p>
<p>AWS would charge for the full execution time, even if 90% of the time were spent waiting for an external response. In this case, the resources dedicated to the execution are basically wasted.</p>
<p>Ben Kehoe argued for <a href="https://read.acloud.guru/the-need-for-asynchronous-rpc-architecture-in-serverless-systems-ff168f1c8785">the need for asynchronous FaaS call chains in serverless systems</a> almost 2 years ago, but, unfortunately, the situation is still the same.</p>
<h2 id="resource-pooling">Resource Pooling</h2>
<p>In the ideal world of greenfield projects, serverless functions would only be purely transforming inputs to outputs or would make external calls that respond fast and don’t require keeping any state between the requests.</p>
<p>However, in the real world, you’re often required to access traditional SQL databases, such as Postgres or SQL Server. Database connection protocols were not designed for today’s serverless services with hundreds of stateless short-lived single-execution hosts. Connections are expensive to establish. There is a limit of how many of them the database can handle. Therefore, the client should strive to reuse the connections as much as possible.</p>
<p>Good examples of these issues and possible workarounds are shown in these great posts by Jeremy Daly:</p>
<ul>
<li><a href="https://www.jeremydaly.com/reuse-database-connections-aws-lambda/">How To: Reuse Database Connections in AWS Lambda</a></li>
<li><a href="https://www.jeremydaly.com/manage-rds-connections-aws-lambda/">How To: Manage RDS Connections from AWS Lambda Serverless Functions</a></li>
<li><a href="https://github.com/jeremydaly/serverless-mysql">Serverless MySQL</a>.</li>
</ul>
<p>A smaller scale version of the same problem applies to HTTP-based communication: reuse of DNS lookups, TCP connections, etc.</p>
<p>Obviously, it would help if multiple executions of a function shared the pool of database and TCP connections.</p>
<h2 id="azure-functions">Azure Functions</h2>
<p>Azure Functions tried to address this issue by separating the notion of executions and instances. In the Azure world, an instance is a host with dedicated resources (both CPU and RAM allocations are fixed and not currently configurable). Each instance is then capable of running multiple executions at the same time and reusing the resources for all of them.</p>
<p>If we apply the Azure model to the example of the 3 overlapping executions we discussed in the AWS section, we can quickly see how they differ:</p>
<p><img src="concurrent-executions.png" alt="Concurrent Executions in Azure Functions"></p>
<figcaption><h4>3 overlapping executions running on a single host</h4></figcaption>
<p>In this case, the executions share the common pool of resources by running at the same host.</p>
<p>The potential efficiency is also reflected in the reduced bill: You&rsquo;re charged for a merged window of the parallel invocations. If 1 GB of memory is consumed (regardless if 1, 2, or 3 executions are active), then the total bill for 3 executions is 1.5 GB-seconds, where 1.5s is the time between the start of the first execution and the end of the last execution. That&rsquo;s 2x cheaper compared to AWS Lambda.</p>
<p>This calculation might not be accurate for very short executions or many concurrent executions. The minimum time charge is always 100 ms, and 0.125 GB is the minimum memory charge. For that reason, a single execution can’t be less than 0.0125 GB-seconds.</p>
<p>In the example above, sharing resources is lean and beneficial to the end customer. However, it might be problematic in the case of CPU-bound workloads; see <a href="https://blog.binaris.com/from-0-to-1000-instances/#azure-2">Bcrypt/Azure example in How Serverless Providers Scale Queue Processing</a>.</p>
<p>Another benefit of concurrent executions is the potential for reuse of SQL connection pools and HTTP clients. For instance, an Azure Function implemented in C# shares the same .NET process for concurrent executions. Therefore, any static objects are reused automatically.</p>
<h2 id="configuring-concurrency-in-azure-functions">Configuring Concurrency in Azure Functions</h2>
<p>We have now established a clear trade-off between resource allocation efficiency and performance guarantees. Now that we understand the basics of concurrent executions, you might be wondering how Azure Functions decide how many executions to put into a single instance.</p>
<p>The truth is, there isn’t one clear answer. It’s a combination of decisions made by the <a href="https://blog.binaris.com/from-0-to-1000-instances/#azure">Scale Controller</a> and configuration knobs. It also depends on the type of the event triggering the execution.</p>
<p>For Azure Functions that are triggered by queue messages, there are settings <code>batchSize</code> and <code>newBatchThreshold</code>. The maximum number of concurrent executions is then derived from the simple equation <code>batchSize + newBatchThreshold</code>.</p>
<p>Cosmos DB and Event Hubs triggers invoke one function execution per batch of items. Each batch is tied to one partition in the event source. The concurrency is then determined at runtime by the Scale Controller based on factors, such as the number of partitions and the metrics reported from existing instances.</p>
<p>HTTP Functions are the marriage of these two approaches. There is a setting, <code>maxConcurrentRequests</code>, that can be used to limit the concurrency explicitly. It defaults to <code>200</code> concurrent requests, which is quite generous. In practice, it’s not likely to reach that level of execution concurrency unless they are idle for minutes. Most commonly, the Scale Controller creates new instances of the function before the maximum limit is exhausted, which may improve the response time but will incur a higher bill.</p>
<h2 id="binaris">Binaris</h2>
<p>Binaris has support for both modes: AWS Lambda-esque exclusive invocations or concurrent invocations similar to Azure Functions. The user decides which one to use by changing a simple setting in the <code>binaris.yml</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yml" data-lang="yml">functions:
  NewCreate:
    file: function.js
    entrypoint: handler
    executionModel: concurrent
    runtime: node8
</code></pre></div><p>When <code>executionModel</code> is set to <code>exclusive</code>, the Binaris runtime limits the concurrency of each &ldquo;function unit&rdquo; (container) to one.</p>
<p>Alternatively, when <code>executionModel</code> is set to <code>concurrent</code>, Binaris enables the shared execution model, which allows for re-entrant invocations on the same function unit. The current model allows for up to 10 concurrent invocations within a single unit (the number will be configurable in the future). All re-entrant invocations share the same memory, disk space, and available vCPU.</p>
<p>Binaris charges a flat rate for each millisecond of running time. The rate does not depend on whether a single execution is running or if there are multiple executions running on the same instance.</p>
<p>There is no configuration for instance CPU or RAM allocation. The price of 1 second of execution is equivalent to what AWS charges for 1 GB instances. In the example above, the customer is charged for 1.5 seconds of execution time.</p>
<p>The granularity is always 1 ms, and there is no minimum charge, which may make a big difference for short executions compared to the minimum of 100 ms for AWS and Azure. This makes Binaris very competitive for quick functions: If 10 parallel executions complete within 10 ms, the total charge is 100 times less compared to AWS Lambda.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Sharing compute resources between concurrent executions of serverless functions can be beneficial for I/O-bound workloads. During the periods when some executions are idle waiting for a response from the network, other executions may continue to use the allocated resources. This shareability also applies to assets, such as database connections and libraries loaded into the instance’s memory.</p>
<p>The shared execution model allows for more efficient use of hardware resources, which, in turn, leads to a lower bill.</p>
<p>However, concurrency can also lead to resource contention for CPU-intensive workloads, which might negatively affect the performance of serverless functions. Thus, until cloud providers come up with a perfect method of optimizing concurrency at runtime, it&rsquo;s essential to give the function owner control over the concurrency mode. Given a simple knob, they can make a judgment call between concurrency and isolation.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/binaris" term="binaris" label="Binaris" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Visualizing Cold Starts]]></title>
            <link href="https://mikhail.io/2019/03/visualizing-cold-starts/"/>
            <id>https://mikhail.io/2019/03/visualizing-cold-starts/</id>
            
            <published>2019-03-14T00:00:00+00:00</published>
            <updated>2019-03-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Serverless cold starts illustrated with animated GIFs.</blockquote><p>I <a href="/serverless/coldstarts/">wrote a lot</a> about cold starts of serverless functions. The articles are full of charts and numbers which are hopefully useful but might be hard to internalize. I decided to come up with a way to represent colds starts visually.</p>
<p>I created HTTP functions that serve geographic maps (map credit <a href="https://www.openstreetmap.org">Open Street Map</a>). The map is a combination of small square tiles; each tile is 256 by 256 pixels. My selected map view consists of 12 tiles, so 12 requests are made to the serverless function to load a single view.</p>
<p>During each experiment, I load the map and then zoom-in three times. The very first view hits  the function in a cold state. Subsequently, the zoomed views are loaded from the warm function. There is a timer next to the map which shows the total time elapsed since the beginning until the last tile arrives.</p>
<h2 id="cold-starts-visualized">Cold Starts Visualized</h2>
<p>All functions are implemented in Node.js and run in the geographically closest region to me (West Europe).</p>
<p>The functions load map tiles from the cloud storage (AWS S3, Google Cloud Storage, and Azure Blob Storage). So the duration is increased by loading SDK at startup and the storage read latency.</p>
<h3 id="aws-lambda">AWS Lambda</h3>
<p>The following GIF movie is a recording of the experiment against an AWS Lambda:</p>




<figure style="cursor: pointer">
    <img src="aws.png" alt="Map loads from AWS Lambda backend" data-alt="aws.gif">
    <figcaption><h4>Map loads from AWS Lambda backend (click to play or replay)</h4></figcaption>
</figure>
<p>The cold view took 1.9 seconds to load, while the warm views were between 200 and 600 milliseconds. The distinction is fairly visible but not extremely annoying: the first load feels like a small network glitch.</p>
<h3 id="google-cloud-functions">Google Cloud Functions</h3>
<p>This GIF shows the experiment against a Google Cloud Function:</p>




<figure style="cursor: pointer">
    <img src="gcp.png" alt="Map loads from Google Cloud Functions backend" data-alt="gcp.gif">
    <figcaption><h4>Map loads from Google Cloud Functions backend (click to play or replay)</h4></figcaption>
</figure>
<p>Loading the initial view took an extra second compared to AWS. It&rsquo;s not a dealbreaker, but the delay of 3 seconds is often quoted as psychologically important.</p>
<p>The tiles seem to appear more gradually; read more on that below.</p>
<h3 id="azure-functions">Azure Functions</h3>
<p>Here is another movie, this time for an Azure Function:</p>




<figure style="cursor: pointer">
    <img src="azure.png" alt="Map loads from Azure Functions backend" data-alt="azure.gif">
    <figcaption><h4>Map loads from Azure Functions backend (click to play or replay)</h4></figcaption>
</figure>
<p>As expected from my previously published measurements, Azure Functions take significantly longer to start. A user has enough time to start wondering whether the map is broken.</p>
<p>I expect better results from Functions implemented in C#, but that would not be an apples-to-apples comparison.</p>
<h2 id="how-do-providers-handle-parallel-requests">How do providers handle parallel requests?</h2>
<p>The map control fires 12 requests in parallel. All functions talk HTTP/2, so the old limit on the number of connections does not apply. Let&rsquo;s compare how those parallel requests are processed.</p>
<h3 id="aws-lambda-1">AWS Lambda</h3>
<p>Each instance of AWS Lambda can handle a single request at a time. So, instead of hitting just one cold start, we hit 12 cold starts in parallel. To illustrate this, I’ve modified the function to color-code each tile based on the Lambda instance ID and to print that ID on the image:</p>
<p><img src="aws-colored.png" alt="AWS Map Colored"></p>
<figcaption><h4>AWS Lambda provisioned 12 instances to serve 12 requests in parallel</h4></figcaption>
<p>Effectively, the measured durations represent the roundtrip time for <em>the slowest out of 12 parallel requests</em>. It&rsquo;s not the average or median duration of the cold start.</p>
<h3 id="google-cloud-functions-1">Google Cloud Functions</h3>
<p>Google uses the same one-execution-per-instance model, so I expected GCP Cloud Functions to behave precisely the same as AWS Lambda. However, I was wrong:</p>
<p><img src="gcp-colored.png" alt="GCP Map Colored"></p>
<figcaption><h4>Google Cloud Function provisioned 3 instances to serve 12 parallel requests</h4></figcaption>
<p>Only three instances were created, and two of them handled multiple requests. It looks like GCP serializes the incoming requests and spreads them through a limited set of instances.</p>
<h3 id="azure-functions-1">Azure Functions</h3>
<p>Azure Functions have a different design: each instance of a function can handle multiple parallel requests at the same time. Thus, in theory, all 12 tiles could be served by the first instance created after the cold start.</p>
<p>In practice, multiple instances are created. The picture looks very similar to GCP:</p>
<p><img src="azure-colored.png" alt="Azure Map Colored"></p>
<figcaption><h4>Azure Function provisioned 4 instances to serve 12 parallel requests</h4></figcaption>
<p>There were four active instances, but the same one handled 9 out of 12 requests. This behavior seems to be quite consistent between multiple runs.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope that these visualizations are useful to get a better feel of the cold starts in serverless functions.</p>
<p>However, they are just examples. Don&rsquo;t treat the numbers as exact statistics for a given cloud provider. If you are curious, you can learn more in <a href="/serverless/coldstarts/">Serverless Cold Starts</a> series of articles.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/maps" term="maps" label="Maps" />
                             
                                <category scheme="https://mikhail.io/tags/dataviz" term="dataviz" label="Dataviz" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Evergreen Serverless Performance Reviews]]></title>
            <link href="https://mikhail.io/2019/02/evergreen-serverless-performance-reviews/"/>
            <id>https://mikhail.io/2019/02/evergreen-serverless-performance-reviews/</id>
            
            <published>2019-02-24T00:00:00+00:00</published>
            <updated>2019-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Automating cloud infrastructure to delivery always-up-to-date performance metrics.</blockquote><p>In the past 6 months I published several blog posts under the same theme of comparing the serverless services of the top cloud providers in terms of their performance and scalability properties:</p>
<ul>
<li><a href="https://mikhail.io/2018/08/serverless-cold-start-war/">Serverless: Cold Start War</a></li>
<li><a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/">From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a></li>
<li><a href="https://mikhail.io/2019/serverless-at-scale-serving-stackoverflow-like-traffic/">Serverless at Scale: Serving StackOverflow-like Traffic</a></li>
</ul>
<p>I received some very positive feedback on those posts (readers are fantastic!). On top of that, people always had great suggestions about improving and extending the contents of those articles. I&rsquo;ve been thinking a lot about them.</p>
<h2 id="limitations-of-a-blog-post">Limitations of a blog post</h2>
<p>However, the format of a blog post is limited in several important ways:</p>
<ul>
<li>
<p><strong>Read-only</strong>. Once it&rsquo;s published, it&rsquo;s published. Except for some typos, I never make changes or extend the past articles. Despite the lack of technical limitations, re-writing blog posts doesn&rsquo;t seem to be a part of the genre. If I want to add another cloud, do I change the old post or create an entirely new one?</p>
</li>
<li>
<p><strong>Point-in-time</strong>. All those articles are heavily data-driven: numbers, charts, comparisons. The value of the data decreases as time goes. Readers can&rsquo;t trust the 2-years-old numbers in the ever-changing world of the cloud.</p>
</li>
<li>
<p><strong>Wall of text</strong>. A blog post is just a chunk of text interrupted by images and charts. People read it from top to bottom. There&rsquo;s no other structure to it. So, I have to come up with the best sequence of material to present. I have to balance the length of the post to give enough insight but not to be too long and tedious. There&rsquo;s no good way to summarize something and then send curious readers to the details.</p>
</li>
<li>
<p><strong>Same for everyone</strong>. Readers might have different backgrounds. Somebody only cares about one cloud provider or one language because that&rsquo;s what they work with. Others look for brief comparison and industry-wide trends. Nonetheless, they all have to read the same text.</p>
</li>
<li>
<p><strong>Not reproducible</strong>. Because of the previous points, there is no real incentive to make the experiments reproducible. It&rsquo;s enough to run it once, publish the results, and forget. However, this means that the effort is lost, and there is no open code that others could use.</p>
</li>
</ul>
<p>I decided to have a shot at addressing these issues.</p>
<h2 id="how-to-solve-these-problems">How to solve these problems</h2>
<p>I believe I can solve the issues inherent to the format of the blog by doing the following steps:</p>
<ul>
<li>
<p><strong>Automate the experiments</strong>. Provision the required infrastructure, run the workload, collect metrics, record the data, aggregate them, and publish the charts. All programmatically, without significant human intervention.</p>
</li>
<li>
<p><strong>Run more often</strong>. Re-do the experiments every month or so. Compare the results over time, detect any trends.</p>
</li>
<li>
<p><strong>Website not blog</strong>. Publish the results as a set of pages with different perspectives on the related data. Make it compelling for multiple types of audience.</p>
</li>
<li>
<p><strong>Keep it up-to-date</strong>. Make sure that people can trust the data as being actual, not old or obsolete.</p>
</li>
<li>
<p><strong>Open everything</strong>. Publish the code behind the experiments, the raw data, the aggregated data. Enable people to find bugs, flaws, and suggest improvements—if they want to.</p>
</li>
</ul>
<h2 id="whats-available-today">What&rsquo;s available today</h2>
<p>I&rsquo;ve completed all the suggestions for the topic of <a href="/serverless/coldstarts/">Cold Starts</a>:</p>
<a href="/serverless/coldstarts">
<figure >
    
        <img src="coldstarts-screen.jpg"
            alt="Cold Starts landing page screenshot"
             />
        
    
    <figcaption>
        <h4>Cold Starts landing page screenshot</h4>
    </figcaption>
    
</figure>
</a>
<p>Two dozens of cloud functions span across three providers. The experiments run for a week and then the results are saved as JSON files. A script aggregates the data and produces charts in several seconds.</p>
<p>I commit to running this experiment and updating the data at least once in 2 months for as long as it would make sense, to my judgment.</p>
<p>The corresponding section of the website consists of 16 pages with different focus and level of details.</p>
<p>All the code and data are <a href="/serverless/open/">open</a>.</p>
<p>I invite you to give it a try, <a href="https://www.twitter.com/MikhailShilkov">follow me on Twitter</a> and <a href="https://github.com/mikhailshilkov/mikhailio-hugo/issues/5">leave the feedback on GitHub</a>.</p>
<p><a href="/serverless/coldstarts/">Cold Starts in Serverless Functions</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                            
                        
                    
                
            
        </entry>
    
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[From YAML to TypeScript: Developer's View on Cloud Automation]]></title>
            <link href="https://mikhail.io/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/"/>
            <id>https://mikhail.io/2019/02/from-yaml-to-typescript-developers-view-on-cloud-automation/</id>
            
            <published>2019-02-14T00:00:00+00:00</published>
            <updated>2019-02-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>An expressive and powerful way to design cloud-native and serverless infrastructure</blockquote><p>The rise of managed cloud services, cloud-native and serverless applications brings both new possibilities and challenges. More and more practices from software development process like version control, code review, continuous integration, and automated testing are applied to the cloud infrastructure automation.</p>
<p>Most existing tools suggest defining infrastructure in text-based markup formats, YAML being the favorite. In this article, I&rsquo;m making a case for using real programming languages like TypeScript instead. Such a change makes even more software development practices applicable to the infrastructure realm.</p>
<h2 id="sample-application">Sample Application</h2>
<p>It&rsquo;s easier to make a case given a specific example. For this essay, I define a URL Shortener application, a basic clone of tinyurl.com or bit.ly. There is an administrative page where one can define short aliases for long URLs:</p>
<p><img src="url-shortener.png" alt="URL Shortener sample app"></p>
<figcaption><h4><h4>URL Shortener sample app</h4></figcaption>
<p>Now, whenever a visitor goes to the base URL of the application + an existing alias, they get redirected to the full URL.</p>
<p>This app is simple to describe but involves enough moving parts to be representative of some real-world issues. As a bonus, there are many existing implementations on the web to compare with.</p>
<h2 id="serverless-url-shortener">Serverless URL Shortener</h2>
<p>I&rsquo;m a big proponent of the serverless architecture: the style of cloud applications being a combination of serverless functions and managed cloud services. They are fast to develop, effortless to run and cost pennies unless the application gets lots of users. However, even serverless applications have to deal with infrastructure, like databases, queues, and other sources of events and destinations of data.</p>
<p>My examples are going to use Amazon AWS, but this could be Microsoft Azure or Google Cloud Platform too.</p>
<p>So, the gist is to store URLs with short names as key-value pairs in Amazon DynamoDB and use AWS Lambdas to run the application code. Here is the initial sketch:</p>
<p><img src="lambda-dynamodb.png" alt="URL Shortener with AWS Lambda and DynamoDB"></p>
<figcaption><h4>URL Shortener with AWS Lambda and DynamoDB</h4></h4></figcaption>
<p>The Lambda at the top receives an event when somebody decides to add a new URL. It extracts the name and the URL from the request and saves them as an item in the DynamoDB table.</p>
<p>The Lambda at the bottom is called whenever a user navigates to a short URL. The code reads the full URL based on the requested path and returns a 301 response with the corresponding location.</p>
<p>Here is the implementation of the <code>Open URL</code> Lambda in JavaScript:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#00f">const</span> aws = require(<span style="color:#a31515">&#39;aws-sdk&#39;</span>);
<span style="color:#00f">const</span> table = <span style="color:#00f">new</span> aws.DynamoDB.DocumentClient();

exports.handler = <span style="color:#00f">async</span> (event) =&gt; {
  <span style="color:#00f">const</span> name = event.path.substring(1);

  <span style="color:#00f">const</span> params = { TableName: <span style="color:#a31515">&#34;urls&#34;</span>, Key: { <span style="color:#a31515">&#34;name&#34;</span>: name } };
  <span style="color:#00f">const</span> value = <span style="color:#00f">await</span> table.get(params).promise();

  <span style="color:#00f">const</span> url = value &amp;&amp; value.Item &amp;&amp; value.Item.url;
  <span style="color:#00f">return</span> url
    ? { statusCode: 301, body: <span style="color:#a31515">&#34;&#34;</span>, headers: { <span style="color:#a31515">&#34;Location&#34;</span>: url } }
    : { statusCode: 404, body: name + <span style="color:#a31515">&#34; not found&#34;</span> };
};
</code></pre></div><p>That&rsquo;s 11 lines of code. I&rsquo;ll skip the implementation of <code>Add URL</code> function because it&rsquo;s very similar. Considering a third function to list the existing URLs for UI, we might end up with 30-40 lines of JavaScript in total.</p>
<p>So, how do we deploy the application?</p>
<p>Well, before we do that, we should realize that the above picture was an over-simplification:</p>
<ul>
<li>AWS Lambda can&rsquo;t handle HTTP requests directly, so we need to add AWS API Gateway in front of it.</li>
<li>We also need to serve some static files for the UI, which we&rsquo;ll put into AWS S3 and proxy it with the same API Gateway.</li>
</ul>
<p>Here is the updated diagram:</p>
<p><img src="apigateway-lambda-dynamodb-s3.png" alt="API Gateway, Lambda, DynamoDB, and S3"></p>
<figcaption><h4>API Gateway, Lambda, DynamoDB, and S3</h4></figcaption>
<p>This is a viable design, but the details are even more complicated:</p>
<ul>
<li>API Gateway is a complex beast which needs Stages, Deployments, and REST Endpoints to be appropriately configured.</li>
<li>Permissions and Policies need to be defined so that API Gateway could call Lambda and Lambda could access DynamoDB.</li>
<li>Static Files should go to S3 Bucket Objects.</li>
</ul>
<p>So, the actual setup involves a couple of dozen objects to be configured in AWS:</p>
<p><img src="apigateway-lambda-dynamodb-s3-details.png" alt="All cloud resources to be provisioned"></p>
<figcaption><h4>All cloud resources to be provisioned</h4></figcaption>
<p>How do we approach this task?</p>
<h2 id="options-to-provision-the-infrastructure">Options to Provision the Infrastructure</h2>
<p>There are many options to provision a cloud application, each one has its trade-offs. Let&rsquo;s quickly go through the list of possibilities to understand the landscape.</p>
<h2 id="aws-web-console">AWS Web Console</h2>
<p>AWS, like any other cloud, has a <a href="https://console.aws.amazon.com">web user interface</a> to configure its resources:</p>
<p><img src="aws-web-console.png" alt="AWS Web Console"></p>
<figcaption><h4>AWS Web Console</h4></figcaption>
<p>That&rsquo;s a decent place to start—good for experimenting, figuring out the available options, following the tutorials, i.e., for exploration.</p>
<p>However, it doesn&rsquo;t suit particularly well for long-lived ever-changing applications developed in teams. A manually clicked deployment is pretty hard to reproduce in the exact manner, which becomes a maintainability issue pretty fast.</p>
<h2 id="aws-command-line-interface">AWS Command Line Interface</h2>
<p>The <a href="https://aws.amazon.com/cli/">AWS Command Line Interface</a> (CLI) is a unified tool to manage all AWS services from a command prompt. You write the calls like</p>
<pre><code class="language-console" data-lang="console">aws apigateway create-rest-api --name 'My First API' --description 'This is my first API'

aws apigateway create-stage --rest-api-id 1234123412 --stage-name 'dev' --description 'Development stage' --deployment-id a1b2c3
</code></pre><p>The initial experience might not be as smooth as clicking buttons in the browser, but the huge benefit is that you can <em>reuse</em> commands that you once wrote. You can build scripts by combining many commands into cohesive scenarios. So, your colleague can benefit from the same script that you created. You can provision multiple environments by parameterizing the scripts.</p>
<p>Frankly speaking, I&rsquo;ve never done that for several reasons:</p>
<ul>
<li>CLI scripts feel too imperative to me. I have to describe &ldquo;how&rdquo; to do things, not &ldquo;what&rdquo; I want to get in the end.</li>
<li>There seems to be no good story for updating existing resources. Do I write small delta scripts for each change? Do I have to keep them forever and run the full suite every time I need a new environment?</li>
<li>If a failure occurs mid-way through the script, I need to manually repair everything to a consistent state. This gets messy real quick, and I have no desire to exercise this process, especially in production.</li>
</ul>
<p>To overcome such limitations, the notion of the <strong>Desired State Configuration</strong> (DSC) was invented. Under this paradigm, one describes the desired layout of the infrastructure, and then the tooling takes care of either provisioning it from scratch or applying the required changes to an existing environment.</p>
<p>Which tool provides DSC model for AWS? There are legions.</p>
<h2 id="aws-cloudformation">AWS CloudFormation</h2>
<p><a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> is the first-party tool for Desired State Configuration management from Amazon. CloudFormation templates use YAML to describe all the infrastructure resources of AWS.</p>
<p>Here is a snippet from <a href="https://aws.amazon.com/blogs/compute/build-a-serverless-private-url-shortener/">a private URL shortener example</a> kindly provided at AWS blog:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">Resources:
  S3BucketForURLs:
    Type: <span style="color:#a31515">&#34;AWS::S3::Bucket&#34;</span>
    DeletionPolicy: Delete
    Properties:
      BucketName: !If [ &#34;CreateNewBucket&#34;, !Ref &#34;AWS::NoValue&#34;, !Ref S3BucketName ]
      WebsiteConfiguration:
        IndexDocument: <span style="color:#a31515">&#34;index.html&#34;</span>
      LifecycleConfiguration:
        Rules:
          -
            Id: DisposeShortUrls
            ExpirationInDays: !Ref URLExpiration
            Prefix: <span style="color:#a31515">&#34;u&#34;</span>
            Status: Enabled
</code></pre></div><p>This is just a very short fragment: the complete example consists of 317 lines YAML. That&rsquo;s an order of magnitude more than the actual JavaScript code that we have in the application!</p>
<p>CloudFormation is a powerful tool, but it demands quite some learning to be done to master it. Moreover, it&rsquo;s specific to AWS: you won&rsquo;t be able to transfer the skill to other cloud providers.</p>
<p>Wouldn&rsquo;t it be great if there was a universal DSC format? Meet Terraform.</p>
<h2 id="terraform">Terraform</h2>
<p><a href="https://www.terraform.io/">HashiCorp Terraform</a> is an open source tool to define infrastructure in declarative configuration files. It has a pluggable architecture, so the tool supports all major clouds and even hybrid scenarios.</p>
<p>The custom text-based Terraform <code>.tf</code> format is used to define the configurations. The templating language is quite powerful, and once you learn it, you can use it for different cloud providers.</p>
<p>Here is a snippet from <a href="https://github.com/jamesridgway/aws-lambda-short-url">AWS Lambda Short URL Generator</a> example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-tf" data-lang="tf"><span style="color:#00f">resource</span> <span style="color:#a31515">&#34;aws_api_gateway_rest_api&#34;</span> <span style="color:#a31515">&#34;short_urls_api_gateway&#34;</span> {
  name        = <span style="color:#a31515">&#34;Short URLs API&#34;</span>
  description = <span style="color:#a31515">&#34;API for managing short URLs.&#34;</span>
}
<span style="color:#00f">resource</span> <span style="color:#a31515">&#34;aws_api_gateway_usage_plan&#34;</span> <span style="color:#a31515">&#34;short_urls_admin_api_key_usage_plan&#34;</span> {
  name         = <span style="color:#a31515">&#34;Short URLs admin API key usage plan&#34;</span>
  description  = <span style="color:#a31515">&#34;Usage plan for the admin API key for Short URLS.&#34;</span>
  api_stages {
    api_id = <span style="color:#a31515">&#34;</span><span style="color:#a31515">${</span>aws_api_gateway_rest_api.short_urls_api_gateway.id<span style="color:#a31515">}</span><span style="color:#a31515">&#34;</span>
    stage  = <span style="color:#a31515">&#34;</span><span style="color:#a31515">${</span>aws_api_gateway_deployment.short_url_api_deployment.stage_name<span style="color:#a31515">}</span><span style="color:#a31515">&#34;</span>
  }
}
</code></pre></div><p>This time, the complete example is around 450 lines of textual templates. Are there ways to reduce the size of the infrastructure definition?</p>
<p>Yes, by raising the level of abstraction. It&rsquo;s possible with Terraform&rsquo;s modules, or by using other, more specialized tools.</p>
<h2 id="serverless-framework-and-sam">Serverless Framework and SAM</h2>
<p><a href="https://serverless.com/">The Serverless Framework</a> is an infrastructure management tool focused on serverless applications. It works across cloud providers (AWS support is the strongest though) and only exposes features related to building applications with cloud functions.</p>
<p>The benefit is that it&rsquo;s much more concise. Once again, the tool is using YAML to define the templates, here is the snippet from <a href="https://github.com/danielireson/serverless-url-shortener">Serverless URL Shortener</a> example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">functions:
  store:
    handler: api/store.handle
    events:
      - http:
          path: /
          method: post
          cors: <span style="color:#00f">true</span>
</code></pre></div><p>The domain-specific language yields a shorter definition: this example has 45 lines of YAML + 123 lines of JavaScript functions.</p>
<p>However, the conciseness has a flip side: as soon as you veer outside of the fairly &ldquo;thin&rdquo; golden path—the cloud functions and an incomplete list of event sources—you have to fall back to more generic tools like CloudFormation. As soon as your landscape includes lower-level infrastructure work or some container-based components, you&rsquo;re stuck using multiple config languages and tools again.</p>
<p>Amazon&rsquo;s <a href="https://docs.aws.amazon.com/serverless-application-model/index.html">AWS Serverless Application Model</a> (SAM) looks very similar to the Serverless Framework but tailored to be AWS-specific.</p>
<p>Is that the end game? I don&rsquo;t think so.</p>
<h2 id="desired-properties-of-infrastructure-definition-tool">Desired Properties of Infrastructure Definition Tool</h2>
<p>So what have we learned while going through the existing landscape? The perfect infrastructure tools should:</p>
<ul>
<li>Provide <strong>reproducible</strong> results of deployments</li>
<li>Be <strong>scriptable</strong>, i.e., require no human intervention after the definition is complete</li>
<li>Define the <strong>desired state</strong> rather than exact steps to achieve it</li>
<li>Support <strong>multiple cloud providers</strong> and hybrid scenarios</li>
<li>Be <strong>universal</strong> in the sense of using the same tool to define any type of resource</li>
<li>Be <strong>succinct</strong> and <strong>concise</strong> to stay readable and manageable</li>
<li><del>Use YAML-based format</del></li>
</ul>
<p>Nah, I crossed out the last item. YAML seems to be the most popular language among this class of tools (and I haven&rsquo;t even touched Kubernetes yet!), but I&rsquo;m not convinced it works well for me. <a href="https://noyaml.com/">YAML has many flaws, and I just don&rsquo;t want to use it</a>.</p>
<p>Have you noticed that I haven&rsquo;t mentioned <strong>Infrastructure as code</strong> a single time yet? Well, here we go (from <a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">Wikipedia</a>):</p>
<blockquote>
<p>Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.</p>
</blockquote>
<p>Shouldn&rsquo;t it be called &ldquo;Infrastructure as definition files&rdquo;, or &ldquo;Infrastructure as YAML&rdquo;?</p>
<p>As a software developer, what I really want is &ldquo;Infrastructure as actual code, you know, the program thing&rdquo;. I want to use <strong>the same language</strong> that I already know. I want to stay in the same editor. I want to get IntelliSense <strong>auto-completion</strong> when I type. I want to see the <strong>compilation errors</strong> when what I typed is not syntactically correct. I want to reuse the <strong>developer skills</strong> that I already have. I want to come up with <strong>abstractions</strong> to generalize my code and create <strong>reusable components</strong>. I want to <strong>leverage the open-source community</strong> who would create much better components than I ever could. I want to <strong>combine the code and infrastructure</strong> in one code project.</p>
<p>If you are with me on that, keep reading. You get all of that with Pulumi.</p>
<h2 id="pulumi">Pulumi</h2>
<p><a href="https://pulumi.io/">Pulumi</a> is a tool to build cloud-based software using real programming languages. They support all major cloud providers, plus Kubernetes.</p>
<p>Pulumi programming model supports Go and Python too, but I&rsquo;m going to use TypeScript for the rest of the article.</p>
<p>While prototyping a URL shortener, I explain the fundamental way of working and illustrate the benefits and some trade-offs. If you want to follow along, <a href="https://pulumi.io/quickstart/install.html">install Pulumi</a>.</p>
<h2 id="how-pulumi-works">How Pulumi Works</h2>
<p>Let&rsquo;s start defining our URL shortener application in TypeScript. I installed <code>@pulumi/pulumi</code> and <code>@pulumi/aws</code> NPM modules so that I can start the program. The first resource to create is a DynamoDB table:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript">    <span style="color:#00f">import</span> * <span style="color:#00f">as</span> aws <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/aws&#34;</span>;

    <span style="color:#008000">// A DynamoDB table with a single primary key
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> counterTable = <span style="color:#00f">new</span> aws.dynamodb.Table(<span style="color:#a31515">&#34;urls&#34;</span>, {
        name: <span style="color:#a31515">&#34;urls&#34;</span>,
        attributes: [
            { name: <span style="color:#a31515">&#34;name&#34;</span>, <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;S&#34;</span> },
        ],
        hashKey: <span style="color:#a31515">&#34;name&#34;</span>,
        readCapacity: <span style="color:#2b91af">1</span>,
        writeCapacity: <span style="color:#2b91af">1</span>
    });
</code></pre></div><p>I use <code>pulumi</code> CLI to run this program to provision the actual resource in AWS:</p>
<pre><code class="language-console" data-lang="console">&gt; pulumi up

Previewing update (urlshortener):

     Type                   Name             Plan
 +   pulumi:pulumi:Stack    urlshortener     create
 +    aws:dynamodb:Table    urls             create

Resources:
    + 2 to create

Do you want to perform this update? yes
Updating (urlshortener):

     Type                   Name             Status
 +   pulumi:pulumi:Stack    urlshortener     created
 +    aws:dynamodb:Table    urls             created

Resources:
    + 2 created
</code></pre><p>The CLI first shows the preview of the changes to be made, and when I confirm, it creates the resource. It also creates a <strong>stack</strong>—a container for all the resources of the application.</p>
<p>This code might look like an imperative command to create a DynamoDB table, but it actually isn&rsquo;t. If I go ahead and change <code>readCapacity</code> to <code>2</code> and then re-run <code>pulumi up</code>, it produces a different outcome:</p>
<pre><code class="language-console" data-lang="console">&gt; pulumi up

Previewing update (urlshortener):

     Type                   Name             Plan
     pulumi:pulumi:Stack    urlshortener
 ~   aws:dynamodb:Table     urls             update  [diff: ~readCapacity]

Resources:
    ~ 1 to update
    1 unchanged
</code></pre><p>It detects the exact change that I made and suggests an update. The following picture illustrates how Pulumi works:</p>
<p><img src="how-pulumi-works.png" alt="How Pulumi works"></p>
<figcaption><h4>How Pulumi works</h4></figcaption>
<p><code>index.ts</code> in the red square is my program. Pulumi&rsquo;s language host understands TypeScript and translates the code to commands to the internal engine. As a result, the engine builds a tree of resources-to-be-provisioned, the desired state of the infrastructure.</p>
<p>The end state of the last deployment is persisted in the storage (can be in pulumi.com backend or a file on disk). The engine then compares the current state of the system with the desired state of the program and calculates the delta in terms of create-update-delete commands to the cloud provider.</p>
<h2 id="help-of-types">Help Of Types</h2>
<p>Now I can proceed to the code that defines a Lambda function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#008000">// Create a Role giving our Lambda access.
</span><span style="color:#008000"></span><span style="color:#00f">let</span> policy: <span style="color:#2b91af">aws.iam.PolicyDocument</span> = { <span style="color:#008000">/* Redacted for brevity */</span> };
<span style="color:#00f">let</span> role = <span style="color:#00f">new</span> aws.iam.Role(<span style="color:#a31515">&#34;lambda-role&#34;</span>, {
    assumeRolePolicy: <span style="color:#2b91af">JSON.stringify</span>(policy),
});
<span style="color:#00f">let</span> fullAccess = <span style="color:#00f">new</span> aws.iam.RolePolicyAttachment(<span style="color:#a31515">&#34;lambda-access&#34;</span>, {
    role: <span style="color:#2b91af">role</span>,
    policyArn: <span style="color:#2b91af">aws.iam.AWSLambdaFullAccess</span>,
});

<span style="color:#008000">// Create a Lambda function, using code from the `./app` folder.
</span><span style="color:#008000"></span><span style="color:#00f">let</span> lambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;lambda-get&#34;</span>, {
    runtime: <span style="color:#2b91af">aws.lambda.NodeJS8d10Runtime</span>,
    code: <span style="color:#2b91af">new</span> pulumi.asset.AssetArchive({
        <span style="color:#a31515">&#34;.&#34;</span>: <span style="color:#00f">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./app&#34;</span>),
    }),
    timeout: <span style="color:#2b91af">300</span>,
    handler: <span style="color:#a31515">&#34;read.handler&#34;</span>,
    role: <span style="color:#2b91af">role.arn</span>,
    environment: {
        variables: {
            <span style="color:#a31515">&#34;COUNTER_TABLE&#34;</span>: counterTable.name
        }
    },
}, { dependsOn: [fullAccess] });
</code></pre></div><p>You can see that the complexity kicked in and the code size is growing. However, now I start to gain real benefits from using a typed programming language:</p>
<ul>
<li>I&rsquo;m using objects in the definitions of other object&rsquo;s parameters. If I misspell their name, I don&rsquo;t get a runtime failure but an immediate error message from the editor.</li>
<li>If I don&rsquo;t know which options I need to provide, I can go to the type definition and look it up (or use IntelliSense).</li>
<li>If I forget to specify a mandatory option, I get a clear error.</li>
<li>If the type of the input parameter doesn&rsquo;t match the type of the object I&rsquo;m passing, I get an error again.</li>
<li>I can use language features like <code>JSON.stringify</code> right inside my program. In fact, I can reference and use any NPM module.</li>
</ul>
<p>You can see the code for API Gateway <a href="https://github.com/mikhailshilkov/fosdem2019/blob/master/samples/1-raw/index.ts#L60-L118">here</a>. It looks too verbose, doesn&rsquo;t it? Moreover, I&rsquo;m only half-way through with only one Lambda function defined.</p>
<h2 id="reusable-components">Reusable Components</h2>
<p>We can do better than that. Here is the improved definition of the same Lambda function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">import</span> { Lambda } <span style="color:#00f">from</span> <span style="color:#a31515">&#34;./lambda&#34;</span>;

<span style="color:#00f">const</span> func = <span style="color:#00f">new</span> Lambda(<span style="color:#a31515">&#34;lambda-get&#34;</span>, {
    path: <span style="color:#a31515">&#34;./app&#34;</span>,
    file: <span style="color:#a31515">&#34;read&#34;</span>,
    environment: {
       <span style="color:#a31515">&#34;COUNTER_TABLE&#34;</span>: counterTable.name
    },
});
</code></pre></div><p>Now, isn&rsquo;t that beautiful? Only the essential options remained, while all the machinery is gone. Well, it&rsquo;s not completely gone, it&rsquo;s been hidden behind an <em>abstraction</em>.</p>
<p>I defined a <strong>custom component</strong> called <code>Lambda</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">export</span> <span style="color:#00f">interface</span> LambdaOptions {
    <span style="color:#00f">readonly</span> path: <span style="color:#2b91af">string</span>;
    <span style="color:#00f">readonly</span> file: <span style="color:#2b91af">string</span>;

    <span style="color:#00f">readonly</span> environment?:  <span style="color:#2b91af">pulumi.Input</span>&lt;{
        [key: <span style="color:#2b91af">string</span>]: pulumi.Input&lt;string&gt;;
    }&gt;;
}

<span style="color:#00f">export</span> <span style="color:#00f">class</span> Lambda <span style="color:#00f">extends</span> pulumi.ComponentResource {
    <span style="color:#00f">public</span> <span style="color:#00f">readonly</span> lambda: <span style="color:#2b91af">aws.lambda.Function</span>;

    <span style="color:#00f">constructor</span>(name: <span style="color:#2b91af">string</span>,
        options: <span style="color:#2b91af">LambdaOptions</span>,
        opts?: <span style="color:#2b91af">pulumi.ResourceOptions</span>) {

        <span style="color:#00f">super</span>(<span style="color:#a31515">&#34;my:Lambda&#34;</span>, name, opts);

        <span style="color:#00f">const</span> role = <span style="color:#008000">//... Role as defined in the last snippet
</span><span style="color:#008000"></span>        <span style="color:#00f">const</span> fullAccess = <span style="color:#008000">//... RolePolicyAttachment as defined in the last snippet
</span><span style="color:#008000"></span>
        <span style="color:#00f">this</span>.lambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">-func`</span>, {
            runtime: <span style="color:#2b91af">aws.lambda.NodeJS8d10Runtime</span>,
            code: <span style="color:#2b91af">new</span> pulumi.asset.AssetArchive({
                <span style="color:#a31515">&#34;.&#34;</span>: <span style="color:#00f">new</span> pulumi.asset.FileArchive(options.path),
            }),
            timeout: <span style="color:#2b91af">300</span>,
            handler: <span style="color:#a31515">`</span><span style="color:#a31515">${</span>options.file<span style="color:#a31515">}</span><span style="color:#a31515">.handler`</span>,
            role: <span style="color:#2b91af">role.arn</span>,
            environment: {
                variables: <span style="color:#2b91af">options.environment</span>
            }
        }, { dependsOn: [fullAccess], parent: <span style="color:#2b91af">this</span> });
    }
}
</code></pre></div><p>The interface <code>LambdaOptions</code> defines options that are important for my abstraction. The class <code>Lambda</code> derives from <code>pulumi.ComponentResource</code> and creates all the child resources in its constructor.</p>
<p>A nice effect is that one can see the structure in <code>pulumi</code> preview:</p>
<pre><code class="language-console" data-lang="console">Previewing update (urlshortener):

     Type                                Name                  Plan
 +   pulumi:pulumi:Stack                 urlshortener          create
 +     my:Lambda                         lambda-get            create
 +       aws:iam:Role                    lambda-get-role       create
 +       aws:iam:RolePolicyAttachment    lambda-get-access     create
 +       aws:lambda:Function             lambda-get-func       create
 +     aws:dynamodb:Table                urls                  create
</code></pre><p>The <code>Endpoint</code> component simplifies the definition of API Gateway (see <a href="https://github.com/mikhailshilkov/fosdem2019/blob/master/samples/2-components/endpoint.ts">the source</a>):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> api = <span style="color:#00f">new</span> Endpoint(<span style="color:#a31515">&#34;urlapi&#34;</span>, {
    path: <span style="color:#a31515">&#34;/{proxy+}&#34;</span>,
    lambda: <span style="color:#2b91af">func.lambda</span>
});
</code></pre></div><p>The component hides the complexity from the clients—if the abstraction was selected correctly, that is. The component class can be reused in multiple places, in several projects, across teams, etc.</p>
<h2 id="standard-component-library">Standard Component Library</h2>
<p>In fact, Pulumi team came up with lots of high-level components that build abstractions on top of raw resources. The components from the <code>@pulumi/cloud-aws</code> package are particularly useful for serverless applications.</p>
<p>Here is the full URL shortener application with DynamoDB table, Lambdas, API Gateway, and S3-based static files:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> aws <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/cloud-aws&#34;</span>;

<span style="color:#008000">// Create a table `urls`, with `name` as primary key.
</span><span style="color:#008000"></span><span style="color:#00f">let</span> urlTable = <span style="color:#00f">new</span> aws.Table(<span style="color:#a31515">&#34;urls&#34;</span>, <span style="color:#a31515">&#34;name&#34;</span>);

<span style="color:#008000">// Create a web server.
</span><span style="color:#008000"></span><span style="color:#00f">let</span> endpoint = <span style="color:#00f">new</span> aws.API(<span style="color:#a31515">&#34;urlshortener&#34;</span>);

<span style="color:#008000">// Serve all files in the www directory to the root.
</span><span style="color:#008000"></span>endpoint.<span style="color:#00f">static</span>(<span style="color:#a31515">&#34;/&#34;</span>, <span style="color:#a31515">&#34;www&#34;</span>);

<span style="color:#008000">// GET /url/{name} redirects to the target URL based on a short-name.
</span><span style="color:#008000"></span>endpoint.<span style="color:#00f">get</span>(<span style="color:#a31515">&#34;/url/{name}&#34;</span>, <span style="color:#00f">async</span> (req, res) =&gt; {
    <span style="color:#00f">let</span> name = req.params[<span style="color:#a31515">&#34;name&#34;</span>];
    <span style="color:#00f">let</span> value = <span style="color:#00f">await</span> urlTable.<span style="color:#00f">get</span>({name});
    <span style="color:#00f">let</span> url = value &amp;&amp; value.url;

    <span style="color:#008000">// If we found an entry, 301 redirect to it; else, 404.
</span><span style="color:#008000"></span>    <span style="color:#00f">if</span> (url) {
        res.setHeader(<span style="color:#a31515">&#34;Location&#34;</span>, url);
        res.status(301);
        res.end(<span style="color:#a31515">&#34;&#34;</span>);
    }
    <span style="color:#00f">else</span> {
        res.status(404);
        res.end(<span style="color:#a31515">&#34;&#34;</span>);
    }
});

<span style="color:#008000">// POST /url registers a new URL with a given short-name.
</span><span style="color:#008000"></span>endpoint.post(<span style="color:#a31515">&#34;/url&#34;</span>, <span style="color:#00f">async</span> (req, res) =&gt; {
    <span style="color:#00f">let</span> url = req.query[<span style="color:#a31515">&#34;url&#34;</span>];
    <span style="color:#00f">let</span> name = req.query[<span style="color:#a31515">&#34;name&#34;</span>];
    <span style="color:#00f">await</span> urlTable.insert({ name, url });
    res.json({ shortenedURLName: <span style="color:#2b91af">name</span> });
});

<span style="color:#00f">export</span> <span style="color:#00f">let</span> endpointUrl = endpoint.publish().url;
</code></pre></div><p>The coolest thing here is that the actual <em>implementation code</em> of AWS Lambdas is <a href="https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions">intertwined</a> with the <em>definition of resources</em>. The code looks very similar to an Express application. AWS Lambdas are defined as TypeScript lambdas. All strongly typed and compile-time checked.</p>
<p>It&rsquo;s worth noting that at the moment such high-level components only exist in TypeScript. One could create their custom components in Python or Go, but there is no standard library available. Pulumi folks <a href="https://github.com/pulumi/pulumi/issues/2430">are actively trying to figure out a way to bridge this gap</a>.</p>
<h2 id="avoiding-vendor-lock-in">Avoiding Vendor Lock-in?</h2>
<p>If you look closely at the previous code block, you notice that only one line is AWS-specific: the <code>import</code> statement. The rest is just naming.</p>
<p>We can get rid of that one too: just change the import to <code>import * as cloud from &quot;@pulumi/cloud&quot;;</code> and replace <code>aws.</code> with <code>cloud.</code> everywhere. Now, we&rsquo;d have to go to the stack configuration file and specify the cloud provider there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">config:
  cloud:provider: aws
</code></pre></div><p>Which is enough to make the application work again!</p>
<p>Vendor lock-in seems to be a big concern among many people when it comes to cloud architectures heavily relying on managed cloud services, including serverless applications. While I don&rsquo;t necessarily share those concerns and am not sure if generic abstractions are the right way to go, Pulumi Cloud library can be one direction for the exploration.</p>
<p>The following picture illustrates the choice of the level of abstraction that Pulumi provides:</p>
<p><img src="pulumi-layers.png" alt="Pulumi abstraction layers"></p>
<figcaption><h4>Pulumi abstraction layers</h4></figcaption>
<p>Working on top of the cloud provider&rsquo;s API and internal resource provider, you can choose to work with raw components with maximum flexibility, or opt-in for higher-level abstractions. Mix-and-match in the same program is possible too.</p>
<h2 id="infrastructure-as-real-code">Infrastructure as Real Code</h2>
<p>Designing applications for the modern cloud means utilizing multiple cloud services which have to be configured to play nicely together. The Infrastructure as Code approach is almost a requirement to keep the management of such applications reliable in a team setting and over the extended period.</p>
<p>Application code and supporting infrastructure become more and more blended, so it&rsquo;s natural that software developers take the responsibility to define both. The next logical step is to use the same set of languages, tooling, and practices for both software and infrastructure.</p>
<p>Pulumi exposes cloud resources as APIs in several popular general-purpose programming languages. Developers can directly transfer their skills and experience to define, build, compose, and deploy modern cloud-native and serverless applications more efficiently than ever.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[Scalability]]></title>
            <link href="https://mikhail.io/tags/scalability/"/>
            <id>https://mikhail.io/tags/scalability/</id>
            
            <published>2019-01-21T00:00:00+00:00</published>
            <updated>2019-01-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless at Scale: Serving StackOverflow-like Traffic]]></title>
            <link href="https://mikhail.io/2019/serverless-at-scale-serving-stackoverflow-like-traffic/"/>
            <id>https://mikhail.io/2019/serverless-at-scale-serving-stackoverflow-like-traffic/</id>
            
            <published>2019-01-21T00:00:00+00:00</published>
            <updated>2019-01-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Scalability test for HTTP-triggered serverless functions across AWS, Azure and GCP</blockquote><p>Serverless compute is a very productive and quick way to get an application up and running. A developer writes a piece of code that solves a particular task and uploads it to the cloud. The provider handles code deployment and the ops burden of managing all the required infrastructure, so that the Function is always available, secure and performant.</p>
<p>Performance is a feature, and the ability to run the same application for 10 users or 10 million users is very appealing. Unfortunately, FaaS is not magical, so scalability limits do exist. That&rsquo;s why I spend time testing the existing FaaS services to highlight the cases where performance might not be perfect. For some background, you can read my previous articles:  <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/">From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</a> for queue-based workloads and <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">Serverless: Cold Start War</a> for exploring cold start latencies.</p>
<p>Today, I want to dig into the scalability of serverless HTTP-based functions. HTTP-based functions are a popular use case that most developers can relate to, and they are also heavily impacted by the ability to scale. When your app goes viral on social networks, scores the Hacker News front page, or gets featured on TV, the last thing you want is slow responses and timeouts.</p>
<p>I implemented a simple HTTP-triggered function and deployed it across the Big-3 cloud providers—Amazon, Microsoft, and Google. Next, I ran a load test issuing hundreds of requests per second to each function. In this article, I present the design and the results of these experiments.</p>
<p><em>DISCLAIMER: Performance testing is hard. I might be missing some crucial factors and parameters that influence the outcome. My interpretation might be wrong. The results might change over time. If you happen to know a way to improve my tests, please let me know, and I will re-run them and re-publish the results.</em></p>
<h2 id="stackoverflow-on-faas">StackOverflow on FaaS</h2>
<p>Every developer knows <a href="https://stackoverflow.com/">StackOverflow</a> and uses it pretty much every day. I&rsquo;ve made the goal to serve traffic comparable to what StackOverflow sees, solely from a serverless function.</p>
<p>StackOverflow is an excellent target for many reasons:</p>
<ul>
<li>Publish the actual request statistics from the site (see <a href="https://nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition/">the data from 2016</a>)</li>
<li>Very transparent about their tech stack (same link above)</li>
<li>Publish  <a href="https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/">the full database</a> and provide <a href="https://data.stackexchange.com/stackoverflow/queries">a tool to query the data online</a></li>
</ul>
<p>StackOverflow runs on .NET Core, SQL Server, Redis, Elastic, etc. Obviously, my goal is not to replicate the whole site. I just want to serve the comparable traffic to the outside world.</p>
<p>Here are some important metrics for my experiment:</p>
<ul>
<li>StackOverflow served 66 million pages per day, which is 760 pages/sec on average.</li>
<li>We’ll make the assumption that the vast majority of those pageviews are question pages, so I will ignore everything else.</li>
<li>We know they serve the whole page as one server-rendered HTML, so we’ll do something comparable.</li>
<li>Each page should be ~ 100kb size before compression.</li>
</ul>
<p>With this in mind, I came up with the following experiment design:</p>
<ul>
<li>Create a HTML template for the whole question page with question and answer markup replaced by placeholders.</li>
<li>Download the data of about 1000 questions and their respective answers from the <a href="https://data.stackexchange.com/stackoverflow/query/new">the data explorer</a>.</li>
<li>Save the HTML templates and JSON data in blob storage of each cloud provider.</li>
<li>Implement a serverless function that retrieves the question data, populates the template, and returns the HTML in response.</li>
</ul>
<p><img src="stackoverflow-test-setup.png" alt="Serving StackOverflow Traffic from a Serverless Function"></p>
<figcaption><h4>Serving StackOverflow Traffic from a Serverless Function</h4></figcaption>
<p>The HTML template is loaded at the first request and then cached in memory. The question/answers data file is loaded from the blob storage for every request. Template population is accomplished with string concatenation.</p>
<p>In my view, this setup is a simple but fair approximation of StackOverflow’s front-end. In addition, it is somewhat representative of many real-world web applications.</p>
<h2 id="metrics-setup">Metrics Setup</h2>
<p>I analyzed the scalability of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via Amazon API Gateway (<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/with-on-demand-https.html">docs</a>)</li>
<li>Azure Function with an HTTP trigger (<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook">docs</a>)</li>
<li>Google Cloud HTTP Function (<a href="https://cloud.google.com/functions/docs/writing/http">docs</a>)</li>
</ul>
<p>All functions were implemented in JavaScript (Node.js) and were running on the latest GA runtime.</p>
<p>Since built-in monitoring tools, such as CloudWatch, would only report the function execution duration, which does not include other potential delays in the HTTP pipeline, I instead measured end-to-end latency from the client perspective. This means that the latency of the network and HTTP gateway (e.g., API Gateway in the case of AWS) were included in the total duration.</p>
<p>Requests were sent from multiple VMs outside the target cloud provider&rsquo;s region but in geographical proximity. Network latency was present in the metrics, but I estimated it to be 20-30 milliseconds at most.</p>
<p><img src="measuring-response-time.png" alt="Measuring Response Time of a Serverless Function"></p>
<figcaption><h4>Measuring Response Time of a Serverless Function</h4></figcaption>
<p>Blob storage services of all cloud providers have enough throughput to serve one blob per HTTP request. However, the latencies differ among the clouds, so I included blob fetch duration measurements in the performance baseline.</p>
<p>Each measurement was then saved to persistent storage and analyzed afterward.</p>
<p>The charts below show <a href="https://en.wikipedia.org/wiki/Percentile">percentile</a> values. For instance, the 95th percentile (written as P95) value of 100ms means that 95% of the requests were faster than 100ms while 5% were slower than that. P50 is the median.</p>
<h2 id="load-pattern">Load Pattern</h2>
<p>I wanted to test the ability of serverless functions to scale up rapidly in response to the growth in request rate, so I came up with a dynamic load scenario.</p>
<p>The experiments started with a baseline 10% of the target load. The goal of the baseline was to make sure that the app was overall healthy, to evaluate the basic latency and the impact of blob storage on it.</p>
<p>At some point (around minute 0 of the charts), the load began to grow and reached 1000 RPS within 8 minutes. After the peak, the cooldown period started, and the load steadily decreased to zero in 8 more minutes.</p>
<p><img src="request-distribution.png" alt="Request Distribution during the Load Test"></p>
<figcaption><h4>Request Distribution during the Load Test</h4></figcaption>
<p>Even though the growth period on the left and the decline period on the right represented the same number of requests, the hypothesis was that the first half might be more challenging because of the need to provision new resources rapidly.</p>
<p>In total, 600,000 requests were served within 17 minutes with the total outbound traffic of 70 GB.</p>
<p>Finally, we’ve made it through all the mechanics, and now it&rsquo;s time to present the actual results.</p>
<h2 id="aws-lambda">AWS Lambda</h2>
<p>AWS Lambda was our first target for the experiment. I provisioned 512 MB size for Lambda instances, which is a medium-range value. I expected larger instances to be slightly faster, and smaller instances to be a bit slower, but the load is not very demanding to CPU, so the overall results should be comparable across the spectrum.</p>
<p>During the low-load baseline, the median response time was about 70 ms with a minimum of 50 ms. The median response time from the S3 bucket was 50 ms.</p>
<p>Here is the P50-P95 latency chart during the load test:</p>
<p><img src="aws-lambda-p50-p95.png" alt="AWS Lambda Response Time Distribution (P50-P95)"></p>
<figcaption><h4>AWS Lambda Response Time Distribution (P50-P95)</h4></figcaption>
<p>The percentiles were very consistent and flat. The median response time was still around 70 ms with no variance observed. P90 and P95 were quite stable too.</p>
<p>Only the 99th percentile displayed the difference between the ramp-up period on the left and the cooldown period on the right:</p>
<p><img src="aws-lambda-p99.png" alt="AWS Lambda Response Time Distribution (P99)"></p>
<figcaption><h4>AWS Lambda Response Time Distribution (P99)</h4></figcaption>
<p>AWS Lambda scales by creating multiple instances of the same function that handle the requests in parallel. Each Lambda instance is handling a single request at any given time, which is why the scale is measured in &ldquo;concurrent executions.&rdquo; When the current request is done being processed, the same instance can be reused for a subsequent request.</p>
<p>Instance identifier can be retrieved from <code>/proc/self/cgroup</code> of a lambda, so I recorded this value for each execution. The following chart shows the number of instances throughout the experiment:</p>
<p><img src="aws-lambda-concurrent-executions.png" alt="AWS Lambda Concurrent Executions"></p>
<figcaption><h4>AWS Lambda Concurrent Executions</h4></figcaption>
<p>There were about 80 concurrent executions at peak. That&rsquo;s quite a few, but still, almost an order of magnitude fewer instances compared to <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/#crunching-numbers">my queue processing experiment</a>. It felt that AWS was capable of scaling even further.</p>
<p>P99.9 showed slowness of the least lucky 0.1% requests. Most probably, it had lots of cold starts in it:</p>
<p><img src="aws-lambda-p999.png" alt="AWS Lambda Response Time Distribution (P99.9)"></p>
<figcaption><h4>AWS Lambda Response Time Distribution (P99.9)</h4></figcaption>
<p>Still, even those requests were mostly served within 2-3 seconds.</p>
<h2 id="google-cloud-functions">Google Cloud Functions</h2>
<p>Now, let&rsquo;s look at the results of Google Cloud Functions. Once again I provisioned 512 MB instance size (same as for Lambda).</p>
<p>During the low-load baseline, the median response time was about 150 ms with a minimum of 100 ms. Almost all of that time was spent fetching blobs from Cloud Storage: Its median latency was over 130 ms! I haven&rsquo;t spent too much time investigating the reason, but I assume that Google Cloud Storage has higher latency for small files than S3. Zach Bjornson published  <a href="http://blog.zachbjornson.com/2015/12/29/cloud-storage-performance.html">the comparison of storage latencies</a>. Although it&rsquo;s 3 years old, the conclusion was that &ldquo;GCS averaged more than three times higher latency&rdquo; when compared to Azure and AWS.</p>
<p>That&rsquo;s an important observation because the Function execution times were twice as big as those recorded on AWS. Keeping this difference in mind, here is the P50-P95 latency chart during the GCP load test:</p>
<p><img src="google-cloud-function-p50-p95.png" alt="Google Cloud Function Response Time Distribution (P50-P95)"></p>
<figcaption><h4>Google Cloud Function Response Time Distribution (P50-P95)</h4></figcaption>
<p>The median value was stable and flat at 150-180 ms. P90 and P95 had some spikes during the first 3 minutes. Google passed the test, but the lower percentiles were not perfect.</p>
<p>The 99th percentile was relatively solid though. It was higher on the left, but it stayed within 1 second most of the time:</p>
<p><img src="google-cloud-function-p99.png" alt="Google Cloud Function Response Time Distribution (P99)"></p>
<figcaption><h4>Google Cloud Function Response Time Distribution (P99)</h4></figcaption>
<p>The scaling model of Google Functions appeared to be very similar to the one of AWS Lambda. This means that 2x duration of the average execution required 2x more concurrent executions to run and 2x more instances to be provisioned:</p>
<p><img src="google-cloud-function-instances.png" alt="Google Cloud Function Concurrent Executions"></p>
<figcaption><h4>Google Cloud Function Concurrent Executions</h4></figcaption>
<p>Indeed, there were about 160 concurrent executions at peak. GCP had to work twice as hard because of the storage latency, which might explain some of the additional variations of response time.</p>
<p>Besides, Google seems to manage instance lifecycle differently. It provisioned a larger batch of instances during the first two minutes, which was in line with <a href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/#pause-the-world-workload">my previous findings</a>. It also kept instances for longer when the traffic went down (or at least, reused the existing instances more evenly).</p>
<p>For completeness, here are the P99.9 values:</p>
<p><img src="google-cloud-function-p999.png" alt="Google Cloud Function Response Time Distribution (P99.9)"></p>
<figcaption><h4>Google Cloud Function Response Time Distribution (P99.9)</h4></figcaption>
<p>They fluctuated between 1 and 5 seconds on the left and were incredibly stable on the right.</p>
<h2 id="azure">Azure</h2>
<p>Experiments with Azure Functions were run on <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
—the dynamically scaled and billed-per-execution runtime. Consumption Plan doesn&rsquo;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>During the low-load baseline, the median response time was about 95 ms with a minimum of 45 ms, which is close to AWS and considerably faster than GCP. This time, JSON file retrieval was not the main contributor to the end-to-end latency: The median response time of Azure Blob Storage was an amazing 8 ms.</p>
<p>However, it turns out that the scaling model of Azure Functions doesn&rsquo;t work well for my experiment. Very high latencies were observed during the load test on Azure:</p>
<p><img src="azure-function-js-p50-p95.png" alt="Azure Function (Node.js) Response Time Distribution (P50-P95)"></p>
<figcaption><h4>Azure Function (Node.js) Response Time Distribution (P50-P95)</h4></figcaption>
<p>The concurrency model of Azure Functions is different from the counterparts of AWS/GCP. Function App instance is closer to a VM than a single-task container. It runs multiple concurrent executions in parallel. A central coordinator called Scale Controller monitors the metrics from existing instances and determines how many instances to provision on top. Instance identifier can be retrieved from environment variables of a function, so I recorded this value for each execution.</p>
<p>The multiple-requests-at-one-instance model didn&rsquo;t help in terms of the total instances required to process the traffic:</p>
<p><img src="azure-function-js-instances.png" alt="Azure Function (Node.js) Instances"></p>
<figcaption><h4>Azure Function (Node.js) Instances</h4></figcaption>
<p>At peak, 90 instances were required, which is almost the same as the number of current executions of AWS Lambda. Given the I/O bound nature of my function, this was surprising to me.</p>
<p>Puzzled by the moderate results, I decided to run the same application as a .NET Azure Function and compare the performance. The same function ported to C# got much faster:</p>
<p><img src="azure-function-dotnet-p50-p95.png" alt="Azure Function (.NET) Response Time Distribution (P50-P95)"></p>
<figcaption><h4>Azure Function (.NET) Response Time Distribution (P50-P95)</h4></figcaption>
<p>P50 was extremely good: It stayed below 50 ms (leveraging the blazingly fast Blob Storage) for the whole period except for one point when it was 140 ms. P90 and P95 were stable except for three data points.</p>
<p>The chart of instance growth was very different from the JavaScript one too:</p>
<p><img src="azure-function-dotnet-instances.png" alt="Azure Function (.NET) Instances"></p>
<figcaption><h4>Azure Function (.NET) Instances</h4></figcaption>
<p>Basically, it spiked to 20 instances at the third minute, and that was enough for the rest of the test. I concluded that the .NET worker was more efficient compared to Node.js worker, at least for my scenario.</p>
<p>If I compare the percentile charts with the instance charts, it looks as if the latency spikes happen at the time when new instances get provisioned. For some reason, the performance suffers during the scale out. It&rsquo;s not just cold starts at the new instances: P90 and even P50 are affected. It might be a good topic for a separate investigation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>During the experiment, sample StackOverflow pages were built and served from AWS Lambda, Google Cloud Functions, and Azure Functions at the rate of up to 1000 pageviews per second. Each Function call served a single pageview and was a combination of I/O workload (reading blob storage) and CPU usage (for parsing JSON and rendering HTML).</p>
<p>All cloud providers were able to scale up and serve the traffic. However, the latency distributions were quite different.</p>
<p>AWS Lambda was solid: Median response time was always below 100 ms, 95th percentile was below 200 ms, and 99th percentile exceeded 500 ms just once.</p>
<p>Google Cloud Storage seemed to have the highest latency out of the three cloud providers. Google Cloud Functions had a bit of a slowdown during the first two minutes of the scale-out but otherwise were quite stable and responsive.</p>
<p>Azure Functions had difficulties during the scale-out period and the response time went up to several seconds. .NET worker appeared to be more performant compared to Node.js one, but both of them show undesirable spikes when new instances are provisioned.</p>
<p>Here is my practical advice to take home:</p>
<ul>
<li>Function-as-a-Service is a great model to build applications that can work for low-usage scenarios, high-load applications, and even spiky workloads.</li>
<li>Scalability limits do exist, so if you anticipate high growth in the application&rsquo;s usage, run a simple load test to see how it behaves.</li>
<li>Always test in combination with your non-serverless dependencies. I&rsquo;ve selected scalable-by-definition cloud blob storage, and yet I got some influence of its behavior on the results. If you use a database or a third-party service, it&rsquo;s quite likely they will hit the scalability limit much earlier than the serverless compute.</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2018s]]></title>
            <link href="https://mikhail.io/2018/"/>
            <id>https://mikhail.io/2018/</id>
            
            <published>2018-12-20T00:00:00+00:00</published>
            <updated>2018-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[A Fairy Tale of F# and Durable Functions]]></title>
            <link href="https://mikhail.io/2018/12/fairy-tale-of-fsharp-and-durable-functions/"/>
            <id>https://mikhail.io/2018/12/fairy-tale-of-fsharp-and-durable-functions/</id>
            
            <published>2018-12-20T00:00:00+00:00</published>
            <updated>2018-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>How F# and Azure Durable Functions make children happy (most developers are still kids at heart)</blockquote><p><em>The post is a part of
<a href="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/">F# Advent Calendar 2018</a>.
It&rsquo;s Christmas time!</em></p>
<p>This summer I was hired by the office of Santa Claus. Santa is not just a fairy tale
character on his own—he leads a large organization that supplies gifts and happiness to millions of
children around the globe. Like any large organization, Santa&rsquo;s office employs an impressive number of
IT systems.</p>
<p>As part of its IT modernization
effort, North Pole HQ restructured the whole supply chain of Christmas gifts. Many legacy components were moved from
a self-managed data center at the North Pole—although the cooling is quite cheap there—to
Azure cloud. Azure was an easy sell since Santa&rsquo;s techy elves use Office 365, SharePoint and
the .NET development stack.</p>
<p>One of the goals of the redesign was to leverage managed cloud services and serverless architecture
wherever possible. Santa has no spare elves to keep reinventing IT wheels.</p>
<h2 id="wish-fulfillment-service">Wish Fulfillment Service</h2>
<p>My assignment was to redesign the <strong>Wish Fulfillment</strong> service. The service receives
wish lists from clients (they call children &ldquo;clients&rdquo;):</p>
<figure >
    
        <img src="wish-list.png"
            alt="Christmas Card with a Wish List (c) my son Tim"
             />
        
    
    <figcaption>
        <h4>Christmas Card with a Wish List (c) my son Tim</h4>
    </figcaption>
    
</figure>
<p>Luckily, the list is already parsed by some other service, and also contains the metadata about
the kid&rsquo;s background (age, gender, and so on) and preferences.</p>
<p>For each item in the list, our service calls the <strong>Matching</strong> service, which uses machine learning,
Azure Cognitive services, and a bit of magic to determine the actual products (they call gifts &ldquo;products&rdquo;)
that best fit the client&rsquo;s expressed desire and profile. For instance, my son&rsquo;s wish for &ldquo;LEGO Draak&rdquo; matches
to &ldquo;LEGO NINJAGO Masters of Spinjitzu Firstbourne Red Dragon&rdquo;. You get the point.</p>
<p>There might be several matches for each desired item, and each result has an estimate of how
likely it is to fulfill the original request and make the child happy.</p>
<p>All the matching products are combined and sent over to the <strong>Gift Picking</strong> service. Gift Picking selects one
of the options based on its price, demand, confidence level, and the Naughty-or-Nice score of the client.</p>
<p>The last step of the workflow is to <strong>Reserve</strong> the selected gift in the warehouse and shipping system
called &ldquo;Santa&rsquo;s Archive of Products&rdquo;, also referred to as SAP.</p>
<p>Here is the whole flow in one picture:</p>
<figure >
    
        <img src="gift-fulfillment-service.png"
            alt="Gift Fulfillment Workflow"
             />
        
    
    <figcaption>
        <h4>Gift Fulfillment Workflow</h4>
    </figcaption>
    
</figure>
<p>How should we implement this service?</p>
<h2 id="original-design">Original Design</h2>
<p>The Wish Fulfillment service should run in the cloud and integrate with other services. It
should be able to process millions of requests in December and stay very cheap to run during the
rest of the year. We decided to leverage serverless architecture with
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview">Azure Functions</a> on the
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">Consumption Plan</a>. Serverless
Functions are:</p>
<ul>
<li>
<p><strong>Fully Managed</strong>: the cloud provider provisions resources, scales them based on the load, takes
care of uptime and reliability;</p>
</li>
<li>
<p><strong>Event-Driven</strong>: for each serverless Function you have to define a specific trigger—the
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li>
<p><strong>Changed per Execution</strong>: it costs nothing to run the application if there is no usage,
and the cost of busy applications is proportional to the actual resource utilization.</p>
</li>
</ul>
<p>Here is the diagram of the original design:</p>
<figure >
    
        <img src="azure-functions-diagram.png"
            alt="Workflow Design with Azure Functions and Storage Queues"
             />
        
    
    <figcaption>
        <h4>Workflow Design with Azure Functions and Storage Queues</h4>
    </figcaption>
    
</figure>
<p>We used Azure Storage Queues to keep the whole flow asynchronous and more resilient to failures
and load fluctuation.</p>
<p>This design would mostly work, but we found a couple of problems with it:</p>
<ul>
<li>
<p>The Functions were manually wired via storage queues and corresponding bindings. The workflow
was spread over infrastructure definition and thus was hard to grasp.</p>
</li>
<li>
<p>We had to pass all items of each wish list into a single invocation of Matching Function,
otherwise combining the matching results from multiple queue messages would be tricky.</p>
</li>
<li>
<p>Although not in scope for the initial release, there were plans to add manual elf
intervention for poorly matched items. This feature would require a change in the flow design:
it&rsquo;s not trivial to fit long-running processes into the pipeline.</p>
</li>
</ul>
<p>To improve on these points, we decided to try
<a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>—a library
that brings workflow orchestration to Azure Functions. It introduces several tools to define stateful,
potentially long-running operations, and handles a lot of the mechanics of reliable communication
and state management behind the scenes.</p>
<p>If you want to know more about what Durable Functions are and why they might be a good idea,
I invite you to read my article
<a href="https://mikhail.io/2018/12/making-sense-of-azure-durable-functions/">Making Sense of Azure Durable Functions</a>
(20 minutes read).</p>
<p>For the rest of this post, I will walk you through the implementation of the Wish Fulfillment workflow
with Azure Durable Functions.</p>
<h2 id="domain-model">Domain Model</h2>
<p>A good design starts with a decent domain model. Luckily, the project was built with F#—the language with
the richest domain modeling capabilities in the .NET ecosystem.</p>
<h3 id="types">Types</h3>
<p>Our service is invoked with a wish list as the input parameter, so let&rsquo;s start with the type <code>WishList</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">WishList</span> =
{
    Kid: Customer
    Wishes: <span style="color:#2b91af">string</span> <span style="color:#2b91af">list</span>
}
</code></pre></div><p>It contains information about the author of the list and recognized &ldquo;order&rdquo; items. <code>Customer</code> is a custom type;
for now, it&rsquo;s not important what&rsquo;s in it.</p>
<p>For each wish we want to produce a list of possible matches:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Match</span> =
{
    Product: Product
    Confidence: Probability
}
</code></pre></div><p>The product is a specific gift option from Santa&rsquo;s catalog, and the confidence is a number
from <code>0.0</code> to <code>1.0</code> of how strong the match is.</p>
<p>The end goal of our service is to produce a <code>Reservation</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Reservation</span> =
{
    Kid: Customer
    Product: Product
}
</code></pre></div><p>It represents the exact product selection for the specific kid.</p>
<h3 id="functions">Functions</h3>
<p>The Wish Fulfillment service needs to perform three actions, which can be
modeled with three strongly-typed asynchronous functions.</p>
<p><em>Note: I use lowercase &ldquo;function&rdquo; for F# functions and capitalize &ldquo;Function&rdquo; for Azure Functions
throughout the article to minimize confusion.</em></p>
<p>The <strong>first action</strong> finds matches for each wish:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// string -&gt; Async&lt;Match list&gt;
</span><span style="color:#008000"></span><span style="color:#00f">let</span> findMatchingGift (wish: <span style="color:#2b91af">string</span>) = async {
    <span style="color:#008000">// Call a custom machine learning model
</span><span style="color:#008000"></span>    <span style="color:#008000">// The real implementation uses the Customer profile to adjust decisions by age, etc.
</span><span style="color:#008000"></span>    <span style="color:#008000">// but we&#39;ll keep the model simple for now.
</span><span style="color:#008000"></span>}
</code></pre></div><p>The first line of all my function snippets shows the function type. In this case, it&rsquo;s a mapping
from the text of the child&rsquo;s wish (<code>string</code>) to a list of matches (<code>Match list</code>).</p>
<p>The <strong>second action</strong> takes the <em>combined</em> list of all matches of all wishes and picks one. Its
real implementation is Santa&rsquo;s secret sauce, but my model just picks the one with the highest
confidence level:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// Match list -&gt; Product
</span><span style="color:#008000"></span><span style="color:#00f">let</span> pickGift (candidates: Match <span style="color:#2b91af">list</span>) =
    candidates
    |&gt; List.sortByDescending (<span style="color:#00f">fun</span> x -&gt; x.Confidence)
    |&gt; List.head
    |&gt; (<span style="color:#00f">fun</span> x -&gt; x.Product)
</code></pre></div><p>Given the picked <code>gift</code>, the reservation is merely <code>{ Kid = wishlist.Kid; Product = gift }</code>,
not worthy of a separate action.</p>
<p>The <strong>third action</strong> registers a reservation in the SAP system:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// Reservation -&gt; Async&lt;unit&gt;
</span><span style="color:#008000"></span><span style="color:#00f">let</span> reserve (reservation: Reservation) = async {
    <span style="color:#008000">// Call Santa&#39;s Archive of Products
</span><span style="color:#008000"></span>}
</code></pre></div><h3 id="workflow">Workflow</h3>
<p>The fulfillment service combines the three actions into one workflow:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// WishList -&gt; Async&lt;Reservation&gt;
</span><span style="color:#008000"></span><span style="color:#00f">let</span> workflow (wishlist: WishList) = async {

    <span style="color:#008000">// 1. Find matches for each wish
</span><span style="color:#008000"></span>    <span style="color:#00f">let!</span> matches =
        wishlist.Wishes
        |&gt; List.map findMatchingGift
        |&gt; Async.Parallel

    <span style="color:#008000">// 2. Pick one product from the combined list of matches
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> gift = pickGift (List.concat matches)

    <span style="color:#008000">// 3. Register and return the reservation
</span><span style="color:#008000"></span>    <span style="color:#00f">let</span> reservation = { Kid = wishlist.Kid; Product = gift }
    <span style="color:#00f">do</span>! reserve reservation
    <span style="color:#00f">return</span> reservation
}
</code></pre></div><p>The workflow implementation is a nice and concise summary of the actual domain flow.</p>
<p>Note that the Matching service is called multiple times in parallel, and then
the results are easily combined by virtue of the <code>Async.Parallel</code> F# function.</p>
<p>So how do we translate the domain model to the actual implementation on top of
serverless Durable Functions?</p>
<h2 id="classic-durable-functions-api">Classic Durable Functions API</h2>
<p>C# was the first target language for Durable Functions; Javascript is now fully supported too.</p>
<p>F# wasn&rsquo;t initially declared as officially supported, but since F# runs on top of the same .NET runtime
as C#, it has always worked. I have a blog post about
<a href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/">Azure Durable Functions in F#</a> and
have added <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">F# samples</a>
to the official repository.</p>
<p>Here are two examples from that old F# code of mine (they have nothing to do with our
gift fulfillment domain):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#008000">// 1. Simple sequencing of activities
</span><span style="color:#008000"></span><span style="color:#00f">let</span> Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = task {
  <span style="color:#00f">let!</span> hello1 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Tokyo&#34;</span>)
  <span style="color:#00f">let!</span> hello2 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Seattle&#34;</span>)
  <span style="color:#00f">let!</span> hello3 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;London&#34;</span>)
  <span style="color:#00f">return</span> [hello1; hello2; hello3]
}

<span style="color:#008000">// 2. Parallel calls snippet
</span><span style="color:#008000"></span><span style="color:#00f">let</span> tasks = Array.map (<span style="color:#00f">fun</span> f -&gt; context.CallActivityAsync&lt;<span style="color:#2b91af">int64</span>&gt;(<span style="color:#a31515">&#34;E2_CopyFileToBlob&#34;</span>, f)) files
<span style="color:#00f">let!</span> results = Task.WhenAll tasks
</code></pre></div><p>This code works and does its job, but doesn&rsquo;t look like idiomatic F# code:</p>
<ul>
<li>No strong typing: Activity Functions are called by name and with types manually specified</li>
<li>Functions are not curried, so partial application is hard</li>
<li>The need to pass the <code>context</code> object around for any Durable operation</li>
</ul>
<p>Although not shown here, the other samples read input parameters, handle errors, and enforce
timeouts—all look too C#-y.</p>
<h2 id="better-durable-functions">Better Durable Functions</h2>
<p>Instead of following the sub-optimal route, we implemented the service with a more F#-idiomatic API.
I&rsquo;ll show the code first, and then I&rsquo;ll explain its foundation.</p>
<p>The implementation consists of three parts:</p>
<ul>
<li><strong>Activity</strong> Functions—one per action from the domain model</li>
<li><strong>Orchestrator</strong> Function defines the workflow</li>
<li><strong><a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings">Azure Functions bindings</a></strong>
to instruct how to run the application in the cloud</li>
</ul>
<h3 id="activity-functions">Activity Functions</h3>
<p>Each Activity Function defines one step of the workflow: Matching, Picking, and Reserving. We
simply reference the F# functions of those actions in one-line definitions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> findMatchingGiftActivity = Activity.defineAsync <span style="color:#a31515">&#34;FindMatchingGift&#34;</span> findMatchingGift
<span style="color:#00f">let</span> pickGiftActivity = Activity.define <span style="color:#a31515">&#34;PickGift&#34;</span> pickGift
<span style="color:#00f">let</span> reserveActivity = Activity.defineAsync <span style="color:#a31515">&#34;Reserve&#34;</span> reserve
</code></pre></div><p>Each activity is defined by a name and a function.</p>
<h3 id="orchestrator">Orchestrator</h3>
<p>The Orchestrator calls Activity Functions to produce the desired outcome of the service. The code
uses a custom computation expression:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> workflow wishlist = orchestrator {
    <span style="color:#00f">let!</span> matches =
        wishlist.Wishes
        |&gt; List.map (Activity.call findMatchingGiftActivity)
        |&gt; Activity.all

    <span style="color:#00f">let!</span> gift = Activity.call pickGiftActivity (List.concat matches)

    <span style="color:#00f">let</span> reservation = { Kid = wishlist.Kid; Product = gift }
    <span style="color:#00f">do</span>! Activity.call reserveActivity reservation
    <span style="color:#00f">return</span> reservation
}
</code></pre></div><p>Notice how closely it matches the workflow definition from our domain model:</p>
<figure >
    
        <img src="durable-orchestrator-vs-async.png"
            alt="Async function vs. Durable Orchestrator"
             />
        
    
    <figcaption>
        <h4>Async function vs. Durable Orchestrator</h4>
    </figcaption>
    
</figure>
<p>The only differences are:</p>
<ul>
<li><code>orchestrator</code> computation expression is used instead of <code>async</code> because multi-threading is
not allowed in Orchestrators</li>
<li><code>Activity.call</code> replaces of direct invocations of functions</li>
<li><code>Activity.all</code> substitutes <code>Async.Parallel</code></li>
</ul>
<h3 id="hosting-layer">Hosting layer</h3>
<p>An Azure Function trigger needs to be defined to host any piece of code as a cloud Function. This can
be done manually in <code>function.json</code>, or via trigger generation from .NET attributes. In my case
I added the following four definitions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;FunctionName(<span style="color:#a31515">&#34;FindMatchingGift&#34;</span>)&gt;]
<span style="color:#00f">let</span> FindMatchingGift([&lt;ActivityTrigger&gt;] wish) =
    Activity.run findMatchingGiftActivity wish

[&lt;FunctionName(<span style="color:#a31515">&#34;PickGift&#34;</span>)&gt;]
<span style="color:#00f">let</span> PickGift([&lt;ActivityTrigger&gt;] matches) =
    Activity.run pickGiftActivity matches

[&lt;FunctionName(<span style="color:#a31515">&#34;Reserve&#34;</span>)&gt;]
<span style="color:#00f">let</span> Reserve([&lt;ActivityTrigger&gt;] wish) =
    Activity.run reserveActivity wish

[&lt;FunctionName(<span style="color:#a31515">&#34;WishlistFulfillment&#34;</span>)&gt;]
<span style="color:#00f">let</span> Workflow ([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) =
    Orchestrator.run (workflow, context)
</code></pre></div><p>The definitions are very mechanical and, again, strongly typed (apart from Functions' names).</p>
<h3 id="ship-it">Ship It!</h3>
<p>These are all the bits required to get our Durable Wish Fulfillment service up and running.
From this point, we can leverage all the existing tooling of Azure Functions:</p>
<ul>
<li>Visual Studio and Visual Studio Code for development and debugging</li>
<li><a href="https://github.com/Azure/azure-functions-core-tools">Azure Functions Core Tools</a> to run
the application locally and deploy it to Azure</li>
<li>The latest version of the Core Tools has dedicated commands to
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management">manage instances of Durable Functions</a></li>
</ul>
<p>There is a learning curve in the process of adopting the serverless architecture. However, a small
project like ours is a great way to do the learning. It sets Santa&rsquo;s IT department on the road
to success, and children will get better gifts more reliably!</p>
<h2 id="durablefunctionsfsharp">DurableFunctions.FSharp</h2>
<p>The above code was implemented with the library
<a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">DurableFunctions.FSharp</a>. I created
this library as a thin F#-friendly wrapper around Durable Functions.</p>
<p>Frankly speaking, the whole purpose of this article is to introduce the library and make you curious
enough to give it a try. DurableFunctions.FSharp has several pieces in the toolbox:</p>
<ul>
<li>
<p><code>OrchestratorBuilder</code> and <code>orchestrator</code> computation expression which encapsulates proper
usage of <code>Task</code>-based API of <code>DurableOrchestrationContext</code></p>
</li>
<li>
<p><code>Activity</code> generic type to define activities as first-class values</p>
</li>
<li>
<p><code>Activity</code> module with helper functions to call activities</p>
</li>
<li>
<p>Adapters for Azure Functions definition for <code>Async</code> and <code>Orchestrator</code></p>
</li>
<li>
<p>API of the original Durable Extensions is still available, so you can fall back to them if needed</p>
</li>
</ul>
<p>In my opinion, F# is a great language to develop serverless Functions. The simplicity of working with functions,
immutability by default, strong type system, focus on data pipelines are all useful in the world of
event-driven cloud applications.</p>
<p>Azure Durable Functions brings higher-level abstractions to compose workflows out of simple building
blocks. The goal of DurableFunctions.FSharp is to make such composition natural and enjoyable for F#
developers.</p>
<p><a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp#getting-started">Getting Started</a>
is as easy as creating a new .NET Core project and referencing a NuGet package.</p>
<p>I&rsquo;d love to get as much feedback as possible! Leave comments below, create issues
on the <a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">GitHub repository</a>, or open a PR.
This would be super awesome!</p>
<p>Happy coding, and Merry Christmas!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/DevonBurriss">Devon Burriss</a>,
<a href="https://twitter.com/iwasdavid">Dave Lowe</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>
for reviewing the draft of this article and their valuable contributions and suggestions.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                             
                                <category scheme="https://mikhail.io/tags/azure-durable-functions" term="azure-durable-functions" label="Azure Durable Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Durable Functions]]></title>
            <link href="https://mikhail.io/tags/azure-durable-functions/"/>
            <id>https://mikhail.io/tags/azure-durable-functions/</id>
            
            <published>2018-12-20T00:00:00+00:00</published>
            <updated>2018-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Making Sense of Azure Durable Functions]]></title>
            <link href="https://mikhail.io/2018/12/making-sense-of-azure-durable-functions/"/>
            <id>https://mikhail.io/2018/12/making-sense-of-azure-durable-functions/</id>
            
            <published>2018-12-07T00:00:00+00:00</published>
            <updated>2018-12-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Why and How of Stateful Workflows on top of serverless functions</blockquote><p>Stateful Workflows on top of Stateless Serverless Cloud Functions—this is the essence
of the Azure Durable Functions library. That&rsquo;s a lot of fancy words in one sentence, and they
might be hard for the majority of readers to understand.</p>
<p>Please join me on the journey where I&rsquo;ll try to explain how those buzzwords fit
together. I will do this in 3 steps:</p>
<ul>
<li>Describe the context of modern cloud applications relying on serverless architecture;</li>
<li>Identify the limitations of basic approaches to composing applications out of the simple building blocks;</li>
<li>Explain the solutions that Durable Functions offer for those problems.</li>
</ul>
<h2 id="microservices">Microservices</h2>
<p>Traditionally, server-side applications were built in a style which is now referred to as
<strong>Monolith</strong>. If multiple people and teams were developing parts of the same application, they
mostly contributed to the same code base. If the code base were structured well, it would have
some distinct modules or components, and a single team would typically own each module:</p>
<p><img src="monolith.png" alt="Monolith"></p>
<figcaption><h4>Multiple components of a monolithic application</h4></figcaption>
<p>Usually, the modules would be packaged together at build time and then deployed as a single
unit, so a lot of communication between modules would stay inside the OS process.</p>
<p>Although the modules could stay loosely coupled over time, the coupling almost always occurred
on the level of the data store because all teams would use a single centralized database.</p>
<p>This model works great for small- to medium-size applications, but it turns out that teams
start getting in each other&rsquo;s way as the application grows since synchronization of contributions
takes more and more effort.</p>
<p>As a complex but viable alternative, the industry came up with a revised service-oriented
approach commonly called <strong>Microservices</strong>. The teams split the big application into &ldquo;vertical slices&rdquo;
structured around the distinct business capabilities:</p>
<p><img src="microservices.png" alt="Microservices"></p>
<figcaption><h4>Multiple components of a microservice-based application</h4></figcaption>
<p>Each team then owns a whole vertical—from public communication contracts, or even UIs, down
to the data storage. Explicitly shared databases are strongly discouraged. Services talk to each
other via documented and versioned public contracts.</p>
<p>If the borders for the split were selected well—and that&rsquo;s the most tricky part—the
contracts stay stable over time, and thin enough to avoid too much chattiness. This gives
each team enough autonomy to innovate at their best pace and to make independent technical decisions.</p>
<p>One of the drawbacks of microservices is the change in deployment model. The services are now
deployed to separate servers connected via a network:</p>
<p><img src="distributed-system.png" alt="Distributed Systems"></p>
<figcaption><h4>Challenges of communication between distributed components</h4></figcaption>
<p>Networks are fundamentally unreliable: they work just fine most of the time, but when they
fail, they fail in all kinds of unpredictable and least desirable manners. There are books
written on the topic of distributed systems architecture. TL;DR: it&rsquo;s hard.</p>
<p>A lot of the new adopters of microservices tend to ignore such complications. REST over HTTP(S) is the
dominant style of connecting microservices. Like any other synchronous communication
protocol, it makes the system brittle.</p>
<p>Consider what happens when one service becomes temporary unhealthy: maybe its database goes offline, or it&rsquo;s struggling to
keep up with the request load, or a new version of the service is being deployed. All the requests to the problematic service start
failing—or worse—become very slow. The dependent service waits for the response, and
thus blocks all incoming requests of its own. The error propagates upstream very quickly causing cascading
failures all over the place:</p>
<p><img src="cascading-failures.png" alt="Cascading Failures"></p>
<figcaption><h4>Error in one component causes cascading failures</h4></figcaption>
<p>The application is down. Everybody screams and starts the blame war.</p>
<h2 id="event-driven-applications">Event-Driven Applications</h2>
<p>While cascading failures of HTTP communication can be mitigated with patterns like a circuit breaker
and graceful degradation, a better solution is to switch to the asynchronous style of communication
as the default. Some kind of persistent queueing service is used as an intermediary.</p>
<p>The style of application architecture which is based on sending events between services
is known as <strong>Event-Driven</strong>. When a service does something useful, it publishes an event—a record
about the fact which happened to its business domain. Another service listens to the published events and
executes its own duty in response to those facts:</p>
<p><img src="event-driven.png" alt="Event-Driven Application"></p>
<figcaption><h4>Communication in event-driven applications</h4></figcaption>
<p>The service that produces events might not know about the consumers. New event subscribers can
be introduced over time. This works better in theory than in practice, but the services tend to
get coupled less.</p>
<p>More importantly, if one service is down, other services don&rsquo;t catch fire immediately. The
upstream services keep publishing the events, which build up in the queue but can be stored safely
for hours or days. The downstream services might not be doing anything useful for this particular
flow, but it can stay healthy otherwise.</p>
<p>However, another potential issue comes hand-in-hand with loose coupling: low cohesion.
As Martin Fowler notices in his essay
<a href="https://martinfowler.com/articles/201701-event-driven.html">What do you mean by &ldquo;Event-Driven&rdquo;</a>:</p>
<blockquote>
<p>It&rsquo;s very easy to make nicely decoupled systems with event notification, without realizing
that you&rsquo;re losing sight of the larger-scale flow.</p>
</blockquote>
<p>Given many components that publish and subscribe to a large number of event types, it&rsquo;s easy to stop
seeing the forest for the trees. Combinations of events usually constitute gradual workflows executed
in time. A workflow is more than the sum of its parts, and understanding of the high-level flow is
paramount to controlling the system behavior.</p>
<p>Hold this thought for a minute; we&rsquo;ll get back to it later. Now it&rsquo;s time to talk <em>cloud</em>.</p>
<h2 id="cloud">Cloud</h2>
<p>The birth of public cloud changed the way we architect applications. It made many things
much more straightforward: provisioning of new resources in minutes instead of months, scaling elastically
based on demand, and resiliency and disaster recovery at the global scale.</p>
<p>It made other things more complicated. Here is the picture of the global Azure network:</p>
<p><img src="azure-network.png" alt="Azure Network"></p>
<figcaption><h4>Azure locations with network connections</h4></figcaption>
<p>There are good reasons to deploy applications to more than one geographical location:
among others, to reduce network latency by staying close to the customer, and to achieve resilience through
geographical redundancy. Public Cloud is the ultimate distributed system. As you remember,
distributed systems are hard.</p>
<p>There&rsquo;s more to that. Each cloud provider has dozens and dozens of managed services, which is
the curse and the blessing. Specialized services are great to provide off-the-shelf solutions
to common complex problems. On the flip side, each service has distinct properties regarding
consistency, resiliency and fault tolerance.</p>
<p>In my opinion, at this point developers have to embrace the public cloud and apply the distributed
system design on top of it. If you agree, there is an excellent way to approach it.</p>
<h2 id="serverless">Serverless</h2>
<p>The slightly provocative term <strong>serverless</strong> is used to describe cloud services that do not
require provisioning of VMs, instances, workers, or any other fixed capacity to run
custom applications on top of them. Resources are allocated dynamically and transparently,
and the cost is based on their actual consumption, rather than on pre-purchased capacity.</p>
<p>Serverless is more about operational and economical properties of the system than about the
technology per se. Servers do exist, but they are someone else&rsquo;s concern. You don&rsquo;t manage
the uptime of serverless applications: the cloud provider does.</p>
<p>On top of that, you pay for what you use, similar to the consumption of other commodity resources
like electricity. Instead of buying a generator to power up your house, you just purchase energy
from the power company. You lose some control (e.g., no way to select the voltage), but this is fine
in most cases. The great benefit is no need to buy and maintain the hardware.</p>
<p>Serverless compute does the same: it supplies standard services on a pay-per-use basis.</p>
<p>If we talk more specifically about Function-as-a-Service offerings like Azure Functions, they
provide a standard model to run small pieces of code in the cloud.
You zip up the code or binaries and send it to Azure; Microsoft takes care of all the
hardware and software required to run it. The infrastructure automatically scales up or down based
on demand, and you pay per request, CPU time and memory that the application consumed.
No usage—no bill.</p>
<p>However, there&rsquo;s always a &ldquo;but&rdquo;. FaaS services come with an opinionated development model that
applications have to follow:</p>
<ul>
<li>
<p><strong>Event-Driven</strong>: for each serverless function you have to define a specific trigger—the
event type which causes it to run, be it an HTTP endpoint or a queue message;</p>
</li>
<li>
<p><strong>Short-Lived</strong>: functions can only run up to several minutes, and preferably for a few seconds
or less;</p>
</li>
<li>
<p><strong>Stateless</strong>: as you don&rsquo;t control where and when function instances are provisioned or
deprovisioned, there is no way to store data within the process between requests reliably;
external storage has to be utilized.</p>
</li>
</ul>
<p>Frankly speaking, the majority of existing applications don&rsquo;t really fit into this model.
If you are lucky to work on a new application (or a new module of it), you are in better shape.</p>
<p>A lot of the serverless applications may be designed to look somewhat similar to this example
from <a href="https://www.serverless360.com/blog/building-reactive-solution-with-azure-event-grid">the Serverless360 blog</a>:</p>
<p><img src="serviceful-example.png" alt="Serviceful Serverless Application"></p>
<figcaption><h4>Sample application utilizing "serviceful" serverless architecture</h4></figcaption>
<p>There are 9 managed Azure services working together in this app. Most of them have a unique purpose, but
the services are all glued together with Azure Functions. An image is uploaded to Blob Storage, an
Azure Function calls Vision API to recognize the license plate and send the result to Event Grid, another
Azure Function puts that event to Cosmos DB, and so on.</p>
<p>This style of cloud applications is sometimes referred to as <strong>Serviceful</strong> to emphasize the heavy usage
of managed services &ldquo;glued&rdquo; together by serverless functions.</p>
<p>Creating a comparable application without any managed services would be a much harder task,
even more so, if the application has to run at scale. Moreover, there&rsquo;s no way to keep the pay-as-you-go
pricing model in the self-service world.</p>
<p>The application pictured above is still pretty straightforward. The processes
in enterprise applications are often much more sophisticated.</p>
<p>Remember the quote from Martin Fowler about losing sight of the large-scale flow. That was
true for microservices, but it&rsquo;s even more true for the &ldquo;nanoservices&rdquo; of cloud functions.</p>
<p>I want to dive deeper and give you several examples of related problems.</p>
<h2 id="challenges-of-serverless-composition">Challenges of Serverless Composition</h2>
<p>For the rest of the article, I&rsquo;ll define an imaginary business application for booking trips to software
conferences. In order to go to a conference, I need to buy tickets to the conference itself,
purchase the flights, and book a room at a hotel.</p>
<p>In this scenario, it makes sense to create three Azure Functions, each one responsible for one step
of the booking process. As we prefer message passing, each Function emits an event which
the next function can listen for:</p>
<p><img src="conference-booking.png" alt="Conference Booking Application"></p>
<figcaption><h4>Conference booking application</h4></figcaption>
<p>This approach works, however, problems do exist.</p>
<h3 id="flexible-sequencing">Flexible Sequencing</h3>
<p>As we need to execute the whole booking process in sequence, the Azure Functions are wired
one after another by configuring the output of one function to match with the event source of
the downstream function.</p>
<p>In the picture above, the functions' sequence is hard-defined. If we were to swap the order of booking
the flights and reserving the hotel, that would require a code change—at least of the
input/output wiring definitions, but probably also the functions' parameter types.</p>
<p>In this case, are the functions <em>really</em> decoupled?</p>
<h3 id="error-handling">Error Handling</h3>
<p>What happens if the Book Flight function becomes unhealthy, perhaps due to the
outage of the third-party flight-booking service? Well, that&rsquo;s why we use asynchronous messaging:
after the function execution fails, the message returns to the queue and is picked
up again by another execution.</p>
<p>However, such retries happen almost immediately for most event sources. This might not
be what we want: an exponential back-off policy could be a smarter idea. At this point,
the retry logic becomes <strong>stateful</strong>: the next attempt should &ldquo;know&rdquo; the history of previous attempts
to make a decision about retry timing.</p>
<p>There are more advanced error-handling patterns too. If executions failures are not
intermittent, we may decide to cancel the whole process and run compensating actions
against the already completed steps.</p>
<p>An example of this is a fallback action: if the flight is not possible (e.g., no routes for this
origin-destination combination), the flow could choose to book a train instead:</p>
<p><img src="fallback-on-error.png" alt="Fallback On Error"></p>
<figcaption><h4>Fallback after 3 consecutive failures</h4></figcaption>
<p>This scenario is not trivial to implement with stateless functions. We could wait until a
message goes to the dead-letter queue and then route it from there, but this is brittle and
not expressive enough.</p>
<h3 id="parallel-actions">Parallel Actions</h3>
<p>Sometimes the business process doesn&rsquo;t have to be sequential. In our reservation scenario,
there might be no difference whether we book a flight before a hotel or vice versa. It could
be desirable to run those actions in parallel.</p>
<p>Parallel execution of actions is easy with the pub-sub capabilities of an event bus: both functions
should subscribe to the same event and act on it independently.</p>
<p>The problem comes when we need to reconcile the outcomes of parallel actions, e.g., calculate the
final price for expense reporting purposes:</p>
<p><img src="fanout-fanin.png" alt="Fan-out / Fan-in"></p>
<figcaption><h4>Fan-out / fan-in pattern</h4></figcaption>
<p>There is no way to implement the Report Expenses block as a single Azure Function: functions
can&rsquo;t be triggered by two events, let alone correlate two <em>related</em> events.</p>
<p>The solution would probably include two functions, one per event, and the shared storage
between them to pass information about the first completed booking to the one who
completes last. All this wiring has to be implemented in custom code. The complexity grows
if more than two functions need to run in parallel.</p>
<p>Also, don&rsquo;t forget the edge cases. What if one of the function fails? How do you make sure there is
no race condition when writing and reading to/from the shared storage?</p>
<h3 id="missing-orchestrator">Missing Orchestrator</h3>
<p>All these examples give us a hint that we need an additional tool to organize low-level
single-purpose independent functions into high-level workflows.</p>
<p>Such a tool can be called an <strong>Orchestrator</strong> because its sole mission is to delegate work
to stateless actions while maintaining the big picture and history of the flow.</p>
<p>Azure Durable Functions aims to provide such a tool.</p>
<h2 id="introducing-azure-durable-functions">Introducing Azure Durable Functions</h2>
<h3 id="azure-functions">Azure Functions</h3>
<p>Azure Functions is the serverless compute service from Microsoft. Functions are event-driven:
each function defines a <strong>trigger</strong>—the exact definition of the event source, for instance,
the name of a storage queue.</p>
<p>Azure Functions can be programmed in <a href="https://docs.microsoft.com/en-us/azure/azure-functions/supported-languages">several languages</a>.
A basic Function with a
<a href="https://docs.microsoft.com/azure/azure-functions/functions-bindings-storage-queue">Storage Queue trigger</a>
implemented in C# would look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;MyFirstFunction&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> QueueTrigger(
    [QueueTrigger(&#34;myqueue-items&#34;)] <span style="color:#2b91af">string</span> myQueueItem,
    ILogger log)
{
    log.LogInformation(<span style="color:#a31515">$&#34;C# function processed: {myQueueItem}&#34;</span>);
}
</code></pre></div><p>The <code>FunctionName</code> attribute exposes the C# static method as an Azure Function named <code>MyFirstFunction</code>.
The <code>QueueTrigger</code> attribute defines the name of the storage queue to listen to. The function body
logs the information about the incoming message.</p>
<h3 id="durable-functions">Durable Functions</h3>
<p><a href="https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview">Durable Functions</a>
is a library that brings workflow orchestration abstractions to
Azure Functions. It introduces a number of idioms and tools to define stateful,
potentially long-running operations, and manages a lot of mechanics of reliable communication
and state management behind the scenes.</p>
<p>The library records the history of all actions in Azure Storage services, enabling durability
and resilience to failures.</p>
<p>Durable Functions is
<a href="https://github.com/Azure/azure-functions-durable-extension">open source</a>,
Microsoft accepts external contributions, and the community is quite active.</p>
<p>Currently, you can write Durable Functions in 3 programming languages: C#, F#, and
Javascript (Node.js). All my examples are going to be in C#. For Javascript,
check <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/quickstart-js-vscode">this quickstart</a>
and <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/javascript">these samples</a>.
For F# see <a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/fsharp">the samples</a>,
<a href="https://github.com/mikhailshilkov/DurableFunctions.FSharp">the F#-specific library</a> and my article
<a href="https://mikhail.io/2018/12/fairy-tale-of-fsharp-and-durable-functions/">A Fairy Tale of F# and Durable Functions</a>.</p>
<p>Workflow building functionality is achieved by the introduction of two additional types
of triggers: Activity Functions and Orchestrator Functions.</p>
<h3 id="activity-functions">Activity Functions</h3>
<p>Activity Functions are simple stateless single-purpose building blocks
that do just one task and have no awareness of the bigger workflow.
A new trigger type,
<code>ActivityTrigger</code>, was introduced to expose functions as workflow steps, as
I explain below.</p>
<p>Here is a simple Activity Function implemented in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;BookConference&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> ConfTicket BookConference([ActivityTrigger] <span style="color:#2b91af">string</span> conference)
{
    <span style="color:#2b91af">var</span> ticket = BookingService.Book(conference);
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> ConfTicket { Code = ticket };
}
</code></pre></div><p>It has a common <code>FunctionName</code> attribute to expose the C# static method as an
Azure Function named <code>BookConference</code>. The name is important because it is used to
invoke the activity from orchestrators.</p>
<p>The <code>ActivityTrigger</code> attribute defines the trigger type and points to the input
parameter <code>conference</code> which the activity expects to get for each invocation.</p>
<p>The function can return a result of any serializable type; my sample function
returns a simple property bag called <code>ConfTicket</code>.</p>
<p>Activity Functions can do pretty much anything: call other services, load and
save data from/to databases, and use any .NET libraries.</p>
<h3 id="orchestrator-functions">Orchestrator Functions</h3>
<p>The Orchestrator Function is a unique concept introduced by Durable Functions. Its sole
purpose is to manage the flow of execution and data among several activity functions.</p>
<p>Its most basic form chains multiple independent activities into a single
sequential workflow.</p>
<p>Let&rsquo;s start with an example which books a conference ticket, a flight itinerary, and
a hotel room one-by-one:</p>
<p><img src="sequential-workflow.png" alt="Sequential Workflow"></p>
<figcaption><h4>3 steps of a workflow executed in sequence</h4></figcaption>
<p>The implementation of this workflow is defined by another C# Azure Function, this time with
<code>OrchestrationTrigger</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;SequentialWorkflow&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    <span style="color:#2b91af">var</span> conference = <span style="color:#00f">await</span> context.CallActivityAsync&lt;ConfTicket&gt;(<span style="color:#a31515">&#34;BookConference&#34;</span>, <span style="color:#a31515">&#34;ServerlessDays&#34;</span>);
    <span style="color:#2b91af">var</span> flight = <span style="color:#00f">await</span> context.CallActivityAsync&lt;FlightTickets&gt;(<span style="color:#a31515">&#34;BookFlight&#34;</span>, conference.Dates);
    <span style="color:#00f">await</span> context.CallActivityAsync(<span style="color:#a31515">&#34;BookHotel&#34;</span>, flight.Dates);
}
</code></pre></div><p>Again, attributes are used to describe the function for the Azure runtime.</p>
<p>The only input parameter has type <code>DurableOrchestrationContext</code>. This context is the tool that
enables the orchestration operations.</p>
<p>In particular, the <code>CallActivityAsync</code> method is used three times to invoke three activities one after the other.
The method body looks very typical for any C# code working with a <code>Task</code>-based API. However,
the behavior is entirely different. Let&rsquo;s have a look at the implementation details.</p>
<h2 id="behind-the-scenes">Behind the Scenes</h2>
<p>Let&rsquo;s walk through the lifecycle of one execution of the sequential workflow above.</p>
<p>When the orchestrator starts running, the first <code>CallActivityAsync</code> invocation is made to book the
conference ticket. What actually happens here is that a queue message is sent from the orchestrator
to the activity function.</p>
<p>The corresponding activity function gets triggered by the queue message. It does its job (books the
ticket) and returns the result. The activity function serializes the result and sends it as a queue
message back to the orchestrator:</p>
<p><img src="durable-messaging.png" alt="Durable Functions: Message Passing"></p>
<figcaption><h4>Messaging between the orchestrator and the activity</h4></figcaption>
<p>When the message arrives, the orchestrator gets triggered again and can proceed to the second
activity. The cycle repeats—a message gets sent to Book Flight activity, it gets triggered, does its
job, and sends a message back to the orchestrator.
The same message flow happens for the third call.</p>
<h3 id="stop-resume-behavior">Stop-resume behavior</h3>
<p>As discussed earlier, message passing is intended to decouple the sender and receiver in time.
For every message in the scenario above, no immediate response is expected.</p>
<p>On the C# level, when the <code>await</code> operator is executed, the code doesn&rsquo;t block the execution of the whole
orchestrator. Instead, it just quits: the orchestrator stops being active and its current step completes.</p>
<p>Whenever a return message arrives from an activity, the orchestrator code restarts. It always starts
with the first line. Yes, this means that the same line is executed multiple times: up to the number of
messages to the orchestrator.</p>
<p>However, the orchestrator stores the history of its past executions in Azure Storage, so the effect of the second pass
of the first line is different: instead of sending a message to the activity it already knows
the result of that activity, so <code>await</code> returns this result back and assigns it to the <code>conference</code> variable.</p>
<p>Because of these &ldquo;replays&rdquo;, the orchestrator&rsquo;s implementation has to be deterministic: don&rsquo;t use
<code>DateTime.Now</code>, random numbers or multi-thread operations; more details
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-checkpointing-and-replay#orchestrator-code-constraints">here</a>.</p>
<h3 id="event-sourcing">Event Sourcing</h3>
<p>Azure Functions are stateless, while workflows require a state to keep track of their progress. Every time a new
action towards the workflow&rsquo;s execution happens, the framework automatically records an event in table storage.</p>
<p>Whenever an orchestrator restarts the execution because a new message arrives from its activity,
it loads the complete history of this particular execution from storage. Durable Context uses
this history to make decisions whether to call the activity or return the previously stored result.</p>
<p>The pattern of storing the complete history of state changes as an append-only event store is
known as Event Sourcing. Event store provides several benefits:</p>
<ul>
<li><strong>Durability</strong>—if a host running an orchestration fails, the history is retained in
persistent storage and is loaded by the new host where the orchestration restarts;</li>
<li><strong>Scalability</strong>—append-only writes are fast and easy to spread over multiple storage servers;</li>
<li><strong>Observability</strong>—no history is ever lost, so it&rsquo;s straightforward to inspect and
analyze even after the workflow is complete.</li>
</ul>
<p>Here is an illustration of the notable events that get recorded during our sequential workflow:</p>
<p><img src="event-sourcing.png" alt="Durable Functions: Event Sourcing"></p>
<figcaption><h4>Log of events in the course of orchestrator progression</h4></figcaption>
<h3 id="billing">Billing</h3>
<p>Azure Functions on the serverless consumption-based plan are billed per execution + per duration of
execution.</p>
<p>The stop-replay behavior of durable orchestrators causes the single workflow &ldquo;instance&rdquo; to execute
the same orchestrator function multiple times. This also means paying for several short
executions.</p>
<p>However, the total bill usually ends up being much lower compared to the potential cost of blocking
synchronous calls to activities. The price of 5 executions of 100 ms each is significantly lower
than the cost of 1 execution of 30 seconds.</p>
<p>By the way, the first million executions per month are
<a href="https://azure.microsoft.com/en-us/pricing/details/functions/">at no charge</a>,
so many scenarios incur no cost at all from Azure Functions service.</p>
<p>Another cost component to keep in mind is Azure Storage. Queues and Tables that are used behind the
scenes are charged to the end customer. In my experience, this charge remains close to zero for
low- to medium-load applications.</p>
<p>Beware of unintentional eternal loops or indefinite recursive fan-outs in your orchestrators. Those
can get expensive if you leave them out of control.</p>
<h2 id="error-handling-and-retries">Error-handling and retries</h2>
<p>What happens when an error occurs somewhere in the middle of the workflow? For instance, a third-party
flight booking service might not be able to process the request:</p>
<p><img src="error-handling.png" alt="Error Handling"></p>
<figcaption><h4>One activity is unhealthy</h4></figcaption>
<p>This situation is expected by Durable Functions. Instead of silently failing, the activity function
sends a message containing the information about the error back to the orchestrator.</p>
<p>The orchestrator deserializes the error details and, at the time of replay, throws a .NET exception
from the corresponding call. The developer is free to put a <code>try .. catch</code> block around the call
and handle the exception:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;SequentialWorkflow&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task Sequential([OrchestrationTrigger] DurableOrchestrationContext context)
{
    <span style="color:#2b91af">var</span> conf = <span style="color:#00f">await</span> context.CallActivityAsync&lt;ConfTicket&gt;(<span style="color:#a31515">&#34;BookConference&#34;</span>, <span style="color:#a31515">&#34;ServerlessDays&#34;</span>);
    <span style="color:#00f">try</span>
    {
        <span style="color:#2b91af">var</span> itinerary = MakeItinerary(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">await</span> context.CallActivityAsync(<span style="color:#a31515">&#34;BookFlight&#34;</span>, itinerary);
    }
    <span style="color:#00f">catch</span> (FunctionFailedException)
    {
        <span style="color:#2b91af">var</span> alternativeItinerary = MakeAnotherItinerary(<span style="color:#008000">/* ... */</span>);
        <span style="color:#00f">await</span> context.CallActivityAsync(<span style="color:#a31515">&#34;BookFlight&#34;</span>, alternativeItinerary);
    }
    <span style="color:#00f">await</span> context.CallActivityAsync(<span style="color:#a31515">&#34;BookHotel&#34;</span>, flight.Dates);
}
</code></pre></div><p>The code above falls back to a &ldquo;backup plan&rdquo; of booking another itinerary. Another typical pattern
would be to run a compensating activity to cancel the effects of any previous actions (un-book the
conference in our case) and leave the system in a clean state.</p>
<p>Quite often, the error might be transient, so it might make sense to retry the failed operation
after a pause. It&rsquo;s a such a common scenario that Durable Functions provides a dedicated API:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> options = <span style="color:#00f">new</span> RetryOptions(
    firstRetryInterval: TimeSpan.FromMinutes(1),
    maxNumberOfAttempts: 5);
options.BackoffCoefficient = 2.0;

<span style="color:#00f">await</span> context.CallActivityWithRetryAsync(<span style="color:#a31515">&#34;BookFlight&#34;</span>, options, itinerary);
</code></pre></div><p>The above code instructs the library to</p>
<ul>
<li>Retry up to 5 times</li>
<li>Wait for 1 minute before the first retry</li>
<li>Increase delays before every subsequent retry by the factor of 2 (1 min, 2 min, 4 min, etc.)</li>
</ul>
<p>The significant point is that, once again, the orchestrator does not block while
awaiting retries. After a failed call, a message is scheduled for the moment in the future
to re-run the orchestrator and retry the call.</p>
<h2 id="sub-orchestrators">Sub-orchestrators</h2>
<p>Business processes may consist of numerous steps. To keep the code of orchestrators manageable,
Durable Functions allows nested orchestrators. A &ldquo;parent&rdquo; orchestrator can call out to child
orchestrators via the <code>context.CallSubOrchestratorAsync</code> method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;CombinedOrchestrator&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task CombinedOrchestrator([OrchestrationTrigger] DurableOrchestrationContext context)
{
    <span style="color:#00f">await</span> context.CallSubOrchestratorAsync(<span style="color:#a31515">&#34;BookTrip&#34;</span>, serverlessDaysAmsterdam);
    <span style="color:#00f">await</span> context.CallSubOrchestratorAsync(<span style="color:#a31515">&#34;BookTrip&#34;</span>, serverlessDaysHamburg);
}
</code></pre></div><p>The code above books two conferences, one after the other.</p>
<h2 id="fan-out--fan-in">Fan-out / Fan-in</h2>
<p>What if we want to run multiple activities in parallel?</p>
<p>For instance, in the example above, we could wish to book two conferences, but the
booking order might not matter. Still, when both bookings are completed, we want to combine the results
to produce an expense report for the finance department:</p>
<p><img src="parallel-calls.png" alt="Parallel Calls"></p>
<figcaption><h4>Parallel calls followed by a final step</h4></figcaption>
<p>In this scenario, the <code>BookTrip</code> orchestrator accepts an input parameter with the name of the
conference and returns the expense information. <code>ReportExpenses</code> needs to receive both
expenses combined.</p>
<p>This goal can be easily achieved by scheduling two tasks (i.e., sending two messages) without
awaiting them separately. We use the familiar <code>Task.WhenAll</code> method to await both and combine
the results:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;ParallelWorkflow&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task Parallel([OrchestrationTrigger] DurableOrchestrationContext context)
{
    <span style="color:#2b91af">var</span> amsterdam = context.CallSubOrchestratorAsync(<span style="color:#a31515">&#34;BookTrip&#34;</span>, serverlessDaysAmsterdam);
    <span style="color:#2b91af">var</span> hamburg   = context.CallSubOrchestratorAsync(<span style="color:#a31515">&#34;BookTrip&#34;</span>, serverlessDaysHamburg);

    <span style="color:#2b91af">var</span> expenses = <span style="color:#00f">await</span> Task.WhenAll(amsterdam, hamburg);

    <span style="color:#00f">await</span> context.CallActivityAsync(<span style="color:#a31515">&#34;ReportExpenses&#34;</span>, expenses);
}
</code></pre></div><p>Remember that awaiting the <code>WhenAll</code> method doesn&rsquo;t synchronously block the orchestrator. It quits
the first time and then restarts two times on reply messages received from activities.
The first restart quits again, and only the second restart makes it past the <code>await</code>.</p>
<p><code>Task.WhenAll</code> returns an array of results (one result per each input task), which is then
passed to the reporting activity.</p>
<p>Another example of parallelization could be a workflow sending e-mails to hundreds of
recipients. Such fan-out wouldn&rsquo;t be hard with normal queue-triggered functions: simply send
hundreds of messages. However, combining the results, if required for the next step
of the workflow, is quite challenging.</p>
<p>It&rsquo;s straightforward with a durable orchestrator:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> emailSendingTasks =
    recepients
    .Select(to =&gt; context.CallActivityAsync&lt;<span style="color:#2b91af">bool</span>&gt;(<span style="color:#a31515">&#34;SendEmail&#34;</span>, to))
    .ToArray();

<span style="color:#2b91af">var</span> results = <span style="color:#00f">await</span> Task.WhenAll(emailSendingTasks);

<span style="color:#00f">if</span> (results.All(r =&gt; r)) { <span style="color:#008000">/* ... */</span> }
</code></pre></div><p>Making hundreds of roundtrips to activities and back could cause numerous replays
of the orchestrator. As an optimization, if multiple activity functions complete around the same
time, the orchestrator may internally process several messages as a batch and restart
the orchestrator function only once per batch.</p>
<h2 id="other-concepts">Other Concepts</h2>
<p>There are many more patterns enabled by Durable Functions. Here is a quick list to give you some perspective:</p>
<ul>
<li>Waiting for the <em>first</em> completed task in a collection (rather than <em>all</em> of them) using the <code>Task.WhenAny</code>
method. Useful for scenarios like timeouts or competing actions.</li>
<li>Pausing the workflow for a given period or until a deadline.</li>
<li>Waiting for external events, e.g., bringing human interaction into the workflow.</li>
<li>Running recurring workflows, when the flow repeats until a certain condition is met.</li>
</ul>
<p>Further explanation and code samples are in
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">the docs</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I firmly believe that serverless applications utilizing a broad range of managed cloud services
are highly beneficial to many companies, due to both rapid development process and
the properly aligned billing model.</p>
<p>Serverless tech is still young; more high-level architectural patterns need to emerge
to enable expressive and composable implementations of large business systems.</p>
<p>Azure Durable Functions suggests some of the possible answers. It combines the clarity and readability
of sequential RPC-style code with the power and resilience of event-driven architecture.</p>
<p><a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-overview">The documentation</a>
for Durable Functions is excellent, with plenty of examples and how-to guides.
Learn it, try it for your real-life scenarios, and let me know your opinion—I&rsquo;m
excited about the serverless future!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Many thanks to <a href="https://twitter.com/kashimizMSFT">Katy Shimizu</a>, <a href="https://twitter.com/cgillum">Chris Gillum</a>,
<a href="https://twitter.com/efleming18">Eric Fleming</a>, <a href="https://twitter.com/KevinJonesD">KJ Jones</a>,
<a href="https://twitter.com/William_DotNet">William Liebenberg</a>, <a href="https://twitter.com/ATosato86">Andrea Tosato</a>
for reviewing the draft of this article and their valuable contributions and suggestions. The community
around Azure Functions and Durable Functions is superb!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/workflows" term="workflows" label="Workflows" />
                             
                                <category scheme="https://mikhail.io/tags/azure-durable-functions" term="azure-durable-functions" label="Azure Durable Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing]]></title>
            <link href="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/"/>
            <id>https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/</id>
            
            <published>2018-11-19T00:00:00+00:00</published>
            <updated>2018-11-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Comparison of queue processing scalability for FaaS across AWS, Azure and GCP</blockquote><p>Whenever I see a &ldquo;Getting Started with Function-as-a-Service&rdquo; tutorial, it usually shows off
a synchronous HTTP-triggered scenario. In my projects, though, I use a lot of asynchronous
functions triggered by a queue or an event stream.</p>
<p>Quite often, the number of messages passing through a queue isn&rsquo;t uniform over time. I might
drop batches of work now and then. My app may get piles of queue items arriving from upstream
systems that were down or under maintenance for an extended period. The system might see some
rush-hour peaks every day or only a few busy days per month.</p>
<p>This is where serverless tech shines: You pay per execution, and then the promise is that the
provider takes care of scaling up or down for you. Today, I want to put this scalability under
test.</p>
<p>The goal of this article is to explore queue-triggered serverless functions and hopefully distill
some practical advice regarding asynchronous functions for real projects. I will be evaluating
the problem:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different types of workloads</li>
<li>For different performance tiers</li>
</ul>
<p>Let&rsquo;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some crucial factors and parameters
that influence the outcome. My interpretation might be wrong. The results might change over time.
If you happen to know a way to improve my tests, please let me know, and I will re-run them and
re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>In this article I analyze the execution results of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via SQS queues</li>
<li>Azure Function triggered via Storage queues</li>
<li>Google Cloud Function triggered via Cloud Pub/Sub</li>
</ul>
<p>All functions are implemented in Javascript and are running on GA runtime.</p>
<p>At the beginning of each test, I threw 100,000 messages into a queue that was previously idle.
Enqueuing never took longer than one minute (I sent the messages from multiple clients in
parallel).</p>
<p>I disabled any batch processing, so each message was consumed by a separate function invocation.</p>
<p>I then analyzed the logs (AWS CloudWatch, Azure Application Insights, and GCP Stackdriver
Logging) to generate charts of execution distribution over time.</p>
<h2 id="how-scaling-actually-works">How Scaling Actually Works</h2>
<p>To understand the experiment better, let&rsquo;s look at a very simplistic but still useful model of
how cloud providers scale serverless applications.</p>
<p>All providers handle the increased load by
<a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">scaling out</a>, i.e.,
by creating multiple instances of the same application that execute the chunks of work in
parallel.</p>
<p>In theory, a cloud provider could spin up an instance for each message in the queue as soon as
the messages arrive. The backlog processing time would then stay very close to zero.</p>
<p>In practice, allocating instances is not cheap. The Cloud provider has to boot up the function
runtime, hit a <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">cold start</a>, and waste
expensive resources on a job that potentially will take just a few milliseconds.</p>
<p>So the providers are trying to find a sweet spot between handling the work as soon as possible
and using resources efficiently. The outcomes differ, which is the point of my article.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda defines scale out with a notion of Concurrent Executions. Each instance of your AWS
Lambda is handling a single execution at any given time. In our case, it&rsquo;s processing a single
SQS message.</p>
<p>It&rsquo;s helpful to think of a function instance as a container working on a single task. If execution
pauses or waits for an external I/O operation, the instance is on hold.</p>
<p>The model of concurrent executions is universal to all trigger types supported by Lambdas. An
instance doesn&rsquo;t work with event sources directly; it just receives an event to work on.</p>
<p>There is a central element in the system, let&rsquo;s call it &ldquo;Orchestrator&rdquo;. The Orchestrator is the
component talking to an SQS queue and getting the messages from it. It&rsquo;s then the job of the
Orchestrator and related infrastructure to provision the required number of instances for working
on concurrent executions:</p>
<figure >
    
        <img src="aws-lambda-queue-scaling.png"
            alt="Model of AWS Lambda Scale-Out"
             />
        
    
    <figcaption>
        <h4>Model of AWS Lambda Scale-Out</h4>
    </figcaption>
    
</figure>
<p>As to scaling behavior, here is what the official
<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/scaling.html">AWS docs</a> say:</p>
<blockquote>
<p>AWS Lambda automatically scales up &hellip; until the number of concurrent function executions
reaches 1000 &hellip; Amazon Simple Queue Service supports an initial burst of 5 concurrent function
invocations and increases concurrency by 60 concurrent invocations per minute.</p>
</blockquote>
<h3 id="gcp">GCP</h3>
<p>The model of Google Cloud Functions is very similar to what AWS does. It runs a single
simultaneous execution per instance and routes the messages centrally.</p>
<p>I wasn&rsquo;t able to find any scaling specifics except the definition of
<a href="https://cloud.google.com/functions/quotas">Function Quotas</a>.</p>
<h3 id="azure">Azure</h3>
<p>Experiments with Azure Functions were run on
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
—the dynamically scaled and billed-per-execution runtime. The concurrency model of Azure Functions
is different from the counterparts of AWS/GCP.</p>
<p>Function App instance is closer to a VM than a single-task container. It runs multiple concurrent
executions in parallel. Equally importantly, it pulls messages from the queue on its own instead of
getting them pushed from a central Orchestrator.</p>
<p>There is still a central coordinator called Scale Controller, but its role is a bit more subtle. It
connects to the same data source (the queue) and needs to determine how many instances to provision
based on the metrics from that queue:</p>
<figure >
    
        <img src="azure-function-queue-scaling.png"
            alt="Model of Azure Function Scale-Out"
             />
        
    
    <figcaption>
        <h4>Model of Azure Function Scale-Out</h4>
    </figcaption>
    
</figure>
<p>This model has pros and cons. If one execution is idle, waiting for some I/O operation such as an
HTTP request to finish, the instance might become busy processing other messages, thus being more
efficient. Running multiple executions is useful in terms of shared resource utilization, e.g.,
keeping database connection pools and reusing HTTP connections.</p>
<p>On the flip side, the Scale Controller now needs to be smarter: to know not only the queue backlog
but also how instances are doing and at what pace they are processing the messages. It&rsquo;s probably
achievable based on queue telemetry though.</p>
<p>Let&rsquo;s start applying this knowledge in practical experiments.</p>
<h2 id="pause-the-world-workload">Pause-the-World Workload</h2>
<p>My first serverless function is aimed to simulate I/O-bound workloads without using external
dependencies to keep the experiment clean. Therefore, the implementation is extremely
straightforward: pause for 500 ms and return.</p>
<p>It could be loading data from a scalable third-party API. It could be running a database query.
Instead, it just runs <code>setTimeout</code>.</p>
<p>I sent 100k messages to queues of all three cloud providers and observed the result.</p>
<h3 id="aws-1">AWS</h3>
<p>AWS Lambda allows multiple instance sizes to be provisioned. Since the workload is neither CPU-
nor memory-intensive, I was using the smallest memory allocation of 128 MB.</p>
<p>Here comes the first chart of many, so let&rsquo;s learn to read it. The horizontal axis shows time in
minutes since all the messages were sent to the queue.</p>
<p>The line going from top-left to bottom-right shows the decreasing queue backlog. Accordingly, the
left vertical axis denotes the number of items still-to-be-handled.</p>
<p>The bars show the number of concurrent executions crunching the messages at a given time. Every
execution logs the instance ID so that I could derive the instance count from the logs. The right
vertical axis shows the instance number.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-iobound-scaling.png"
            alt="AWS Lambda processing 100k SQS messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda processing 100k SQS messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>It took AWS Lambda 5.5 minutes to process the whole batch of 100k messages. For comparison, the
same batch processed sequentially would take about 14 hours.</p>
<p>Notice how linear the growth of instance count is. If I apply the official scaling formula:</p>
<pre><code>Instance Count = 5 + Minutes * 60 = 5 + 5.5 * 60  = 335
</code></pre><p>We get a very close result! Promises kept.</p>
<h3 id="gcp-1">GCP</h3>
<p>Same function, same chart, same instance size of 128 MB of RAM—but this time for Google Cloud Functions:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-iobound-scaling.png"
            alt="Google Cloud Function processing 100k Pub/Sub messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function processing 100k Pub/Sub messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Coincidentally, the total amount of instances, in the end, was very close to AWS. The scaling
pattern looks entirely different though: Within the very first minute, there was a burst of
scaling close to 300 instances, and then the growth got very modest.</p>
<p>Thanks to this initial jump, GCP managed to finish processing almost one minute earlier than AWS.</p>
<h3 id="azure-1">Azure</h3>
<p>Azure Function doesn&rsquo;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>The shape of the chart for Azure Functions is very similar, but the instance number growth is
significantly different:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="azure-function-queue-iobound-scaling.png"
            alt="Azure Function processing 100k queue messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>Azure Function processing 100k queue messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The total processing time was a bit faster than AWS and somewhat slower than GCP. Azure Function
instances process several messages in parallel, so it takes much less of them to do the same amount
of work.</p>
<p>Instance number growth seems far more linear than bursty.</p>
<h3 id="what-we-learned">What we learned</h3>
<p>Based on this simple test, it&rsquo;s hard to say if one cloud provider handles scale-out better than
the others.</p>
<p>It looks like all serverless platforms under stress are making decisions at the resolution of 5-15
seconds, so the backlog processing delays are likely to be measured in minutes. It sounds quite far
from the theoretical &ldquo;close to zero&rdquo; target but is most likely good enough for the majority of
applications.</p>
<h2 id="crunching-numbers">Crunching Numbers</h2>
<p>That was an easy job though. Let&rsquo;s give cloud providers a hard time by executing CPU-heavy workloads
and see if they survive!</p>
<p>This time, each message handler calculates a <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hash with a cost of 10. One such calculation takes about 200 ms on my laptop.</p>
<h3 id="aws-2">AWS</h3>
<p>Once again, I sent 100k messages to an SQS queue and recorded the processing speed and instance count.</p>
<p>Since the workload is CPU-bound, and AWS allocates CPU shares proportionally to the allocated
memory, the instance size might have a significant influence on the result.</p>
<p>I started with the smallest memory allocation of 128 MB:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-cpubound-scaling.png"
            alt="AWS Lambda (128 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda (128 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>This time it took almost 10 minutes to complete the experiment.</p>
<p>The scaling shape is pretty much the same as last time, still correctly described by the formula
<code>60 * Minutes + 5</code>. However, because AWS allocates a small fraction of a full CPU to each 128 MB
execution, one message takes around 1,700 ms to complete. Thus, the total work increased
approximately by the factor of 3 (47 hours if done sequentially).</p>
<p>At the peak, 612 concurrent executions were running, nearly double the amount in our initial
experiment. So, the total processing time increased only by the factor of 2—up to 10 minutes.</p>
<p>Let&rsquo;s see if larger Lambda instances would improve the outcome. Here is the chart for 512 MB of
allocated memory:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-cpubound-scaling-512.png"
            alt="AWS Lambda (512 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda (512 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>And yes it does. The average execution duration is down to 400 ms: 4 times less, as expected.
The scaling shape still holds, so the entire batch was done in less than four minutes.</p>
<h3 id="gcp-2">GCP</h3>
<p>I executed the same experiment on Google Cloud Functions. I started with 128 MB, and it looks
impressive:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling.png"
            alt="Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The average execution duration is very close to Amazon&rsquo;s: 1,600 ms. However, GCP scaled more
aggressively—to a staggering 1,169 parallel executions! Scaling also has a different shape:
It&rsquo;s not linear but grows in steep jumps. As a result, it took less than six minutes on the
lowest CPU profile—very close to AWS&rsquo;s time on a 4x more powerful CPU.</p>
<p>What will GCP achieve on a faster CPU? Let&rsquo;s provision 512 MB. It must absolutely crush the
test. Umm, wait, look at that:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling-512.png"
            alt="Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>It actually&hellip; got slower. Yes, the average execution time is 4x lower: 400 ms, but the scaling
got much less aggressive too, which canceled the speedup.</p>
<p>I confirmed it with the largest instance size of 2,048 MB:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling-2048.png"
            alt="Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>CPU is fast: 160 ms average execution time, but the total time to process 100k messages went up
to eight minutes. Beyond the initial spike at the first minute, it failed to scale up any further
and stayed at about 110 concurrent executions.</p>
<p>It seems that GCP is not that keen to scale out larger instances. It&rsquo;s probably easier to find
many small instances available on the pool rather than a similar number of giant instances.</p>
<h3 id="azure-2">Azure</h3>
<p>A single invocation takes about 400 ms to complete on Azure Function. Here is the burndown chart:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="azure-function-queue-cpubound-scaling.png"
            alt="Azure Function processing 100k queue messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Azure Function processing 100k queue messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Azure spent 21 minutes to process the whole backlog. The scaling was linear, similarly to AWS,
but with a much slower pace regarding instance size growth, about <code>2.5 * Minutes</code>.</p>
<p>As a reminder, each instance could process multiple queue messages in parallel, but each such
execution would be competing for the same CPU resource, which doesn&rsquo;t help for the purely
CPU-bound workload.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<p>Time for some conclusions and pieces of advice to apply in real serverless applications.</p>
<h3 id="serverless-is-great-for-async-data-processing">Serverless is great for async data processing</h3>
<p>If you are already using cloud services, such as managed queues and topics, serverless functions
are the easiest way to consume them.</p>
<p>Moreover, the scalability is there too. When was the last time you ran 1,200 copies of your
application?</p>
<h3 id="serverless-is-not-infinitely-scalable">Serverless is not infinitely scalable</h3>
<p>There are limits. Your functions won&rsquo;t scale perfectly to accommodate your spike—a
provider-specific algorithm will determine the scaling pattern.</p>
<p>If you have large spikes in queue workloads, which is quite likely for medium- to high-load
scenarios, you can and should expect delays up to several minutes before the backlog is fully
digested.</p>
<p>All cloud providers have quotas and limits that define an upper boundary of scalability.</p>
<h3 id="cloud-providers-have-different-implementations">Cloud providers have different implementations</h3>
<p><strong>AWS Lambda</strong> seems to have a very consistent and well-documented linear scale growth for
SQS-triggered Lambda functions. It will happily scale to 1,000 instances, or whatever other
limit you hit first.</p>
<p><strong>Google Cloud Functions</strong> has the most aggressive scale-out strategy for the smallest instance
sizes. It can be a cost-efficient and scalable way to run your queue-based workloads. Larger
instances seem to scale in a more limited way, so a further investigation is required if you
need those.</p>
<p><strong>Azure Functions</strong> share instances for multiple concurrent executions, which works better
for I/O-bound workloads than for CPU-bound ones. Depending on the exact scenario that you
have, it might help to play with instance-level settings.</p>
<h3 id="dont-forget-batching">Don&rsquo;t forget batching</h3>
<p>For the tests, I was handling queue messages in the 1-by-1 fashion. In practice, it helps
if you can batch several messages together and execute a single action for all of them in one go.</p>
<p>If the destination for your data supports batched operations, the throughput will usually
increase immensely.
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2017/09/19/processing-100000-events-per-second-on-azure-functions/">Processing 100,000 Events Per Second on Azure Functions</a>
is an excellent case to prove the point.</p>
<h3 id="you-might-get-too-much-scale">You might get too much scale</h3>
<p>A month ago, Troy Hunt published a great post
<a href="https://www.troyhunt.com/breaking-azure-functions-with-too-many-connections/">Breaking Azure Functions with Too Many Connections</a>.
His scenario looks very familiar: He uses queue-triggered Azure Functions to notify
subscribers about data breaches. One day, he dropped 126 million items into the queue, and Azure
scaled out, which overloaded Mozilla&rsquo;s servers and caused them to go all-timeouts.</p>
<p>Another consideration is that non-serverless dependencies limit the scalability of your serverless
application. If you call a legacy HTTP endpoint, a SQL database, or a third-party web service—be
sure to test how they react when your serverless function scales out to hundreds of concurrent
executions.</p>
<p>Stay tuned for more serverless performance goodness!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions V2 Is Released, How Performant Is It?]]></title>
            <link href="https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/"/>
            <id>https://mikhail.io/2018/10/azure-functions-v2-released-how-performant-is-it/</id>
            
            <published>2018-10-10T00:00:00+00:00</published>
            <updated>2018-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Comparison of performance benchmarks for Azure Functions V1 and V2</blockquote><p>Azure Functions major version 2.0 was released into GA a few days back during Microsoft Ignite. The runtime is now
based on .NET Core and thus is cross-platform and more interoperable. It has a nice extensibility story too.</p>
<p>In theory, .NET Core runtime is more lean and performant.
But last time <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">I checked back in April</a>,
the preview version of Azure Functions V2 had some serious issues with cold start durations.</p>
<p>I decided to give the new and shiny version another try and ran several benchmarks. All tests were conducted on
Consumption plan.</p>
<p><strong>TL;DR</strong>: it&rsquo;s not perfect just yet.</p>
<h2 id="cold-starts">Cold Starts</h2>
<p>Cold starts happen when a new instance handles its first request, see my other posts:
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">one</a>,
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">two</a>,
<a href="https://mikhail.io/2018/08/serverless-cold-start-war/">three</a>.</p>
<h3 id="hello-world">Hello World</h3>
<p>The following chart gives a comparison of V1 vs V2 cold starts for the two most popular runtimes:
.NET and Javascript. The dark bar shows the most probable range of values, while the light ones
are possible but less frequent:</p>
<p><img src="cold-starts-dotnet-js.png" alt="Cold Starts V1 vs V2: .NET and Javascript"></p>
<p>Apparently, V2 is slower to start for both runtimes. V2 on .NET is slower by 10% on average and seems
to have higher variation. V2 on Javascript is massively slower: 2 times on average, and the slowest startup
time goes above 10 seconds.</p>
<h3 id="dependencies-on-board">Dependencies On Board</h3>
<p>The values for the previous chart were calculated for Hello-World type of functions with no extra dependencies.</p>
<p>The chart below shows two more Javascript functions, this time with a decent number of dependencies:</p>
<ul>
<li>Referencing 3 NPM packages - 5MB zipped</li>
<li>Referencing 38 NPM packages - 35 MB zipped</li>
</ul>
<p><img src="cold-starts-js-dependencies.png" alt="Cold Starts V1 vs V2: Javascript with NPM dependencies"></p>
<p>V2 clearly loses on both samples, but V2-V1 difference seems to be consistently within 2.5-3
seconds for any amount of dependencies.</p>
<p>All the functions were deployed with the Run-from-Package method which promises faster startup times.</p>
<h3 id="java">Java</h3>
<p>Functions V2 come with a preview of a new runtime: Java / JVM. It utilizes the same extensibility model
as Javascript, and thus it seems to be a first-class citizen now.</p>
<p>Cold starts are not first-class though:</p>
<p><img src="cold-starts-java.png" alt="Cold Starts Java"></p>
<p>If you are a Java developer, be prepared for 20-25 seconds of initial startup time. That will probably
be resolved when the Java runtime becomes generally available:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">That matches some of our internal data. We are looking into it.</p>&mdash; Paul Batum (@paulbatum) <a href="https://twitter.com/paulbatum/status/1048391445386735616?ref_src=twsrc%5Etfw">October 6, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<h2 id="queue-processor">Queue Processor</h2>
<p>Cold starts are most problematic for synchronous triggers like HTTP requests. They are less relevant
for queue-based workloads, where scale out is of higher importance.</p>
<p>Last year I ran some tests around the ability of Functions to keep up with variable queue load:
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">one</a>,
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">two</a>.</p>
<p>Today I ran two simple tests to compare the scalability of V1 vs. V2 runtimes.</p>
<h3 id="pause-and-go">Pause-and-Go</h3>
<p>In my first tests, a lightweight Javascript Function processed messages from an Azure Storage Queue. For
each message, it just pauses for 500 msec and then completes. This is supposed to simulate I/O-bound
Functions.</p>
<p>I&rsquo;ve sent 100,000 messages to the queue and measured how fast they went away. Batch size (degree of parallelism
on each instance) was set to 16.</p>
<p><img src="queue-scaling-io-based.png" alt="Processing Queue Messages with Lightweight I/O Workload"></p>
<p>Two lines show the queue backlogs of two runtimes, while the bars indicate the number of instances working
in parallel at a given minute.</p>
<p>We see that V2 was a bit faster to complete, probably due to more instances provisioned to it at any moment.
The difference is not big though and might be statistically insignificant.</p>
<h3 id="cpu-at-work">CPU at Work</h3>
<p>Functions in my second experiment are CPU-bound. Each message invokes calculation of a 10-stage Bcrypt
hash. On a very quiet moment, 1 such function call takes about 300-400 ms to complete, consuming 100% CPU
load on a single core.</p>
<p>Both Functions are precompiled .NET and both are using <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.NET</a>.</p>
<p>Batch size (degree of parallelism on each instance) was set to 2 to avoid too much fighting for the same CPU. Yet,
the average call duration is about 1.5 seconds (3x slower than possible).</p>
<p><img src="queue-scaling-cpu-bound.png" alt="Processing Queue Messages with CPU-bound Workload"></p>
<p>The first thing to notice: it&rsquo;s the same number of messages with comparable &ldquo;sequential&rdquo; execution time, but
the total time to complete the job increased 3-fold. That&rsquo;s because the workload is much more demanding to
the resources of application instances, and they struggle to parallelize work more aggressively.</p>
<p>V1 and V2 are again close to each other. One more time, V2 got more instances allocated to it most of the time.
And yet, it seemed to be <em>consistently</em> slower and lost about 2.5 minutes on 25 minutes interval (~10%).</p>
<h2 id="http-scalability">HTTP Scalability</h2>
<p>I ran two similar Functions — I/O-bound &ldquo;Pause&rdquo; (~100 ms) and CPU-bound Bcrypt (9 stages, ~150ms) — under a stress test.
But this time they were triggered by HTTP requests. Then I compared the results for V1 and V2.</p>
<h3 id="pause-and-go-1">Pause-and-Go</h3>
<p>The grey bars on the following charts represent the rate of requests sent and processed within a given minute.</p>
<p>The lines are percentiles of response time: green lines for V2 and orange lines for V1.</p>
<p><img src="http-scaling-io-based.png" alt="Processing HTTP Requests with Lightweight I/O Workload"></p>
<p>Yes, you saw it right, my Azure Functions were processing 100,000 messages per minute at peak. That&rsquo;s a lot of
messages.</p>
<p>Apart from the initial spike at minutes 2 and 3, both versions performed pretty close to each other.</p>
<p>50th percentile is flat close to the theoretic minimum of 100 ms, while the 95th percentile fluctuates a bit, but still
mostly stays quite low.</p>
<p>Note that the response time is measured from the client perspective, not by looking at the statistics provided by Azure.</p>
<h3 id="cpu-fanout">CPU Fanout</h3>
<p>How did CPU-heavy workload perform?</p>
<p>To skip ahead, I must say that the response time increased much more significantly, so my sample clients were
not able to generate request rates of 100k per minute. They &ldquo;only&rdquo; did about 48k per minute at peak, which still
seems massive to me.</p>
<p>I&rsquo;ve run the same test twice: one for Bcrypt implemented in .NET, and one for Javascript.</p>
<p><img src="http-scaling-cpu-bound-dotnet.png" alt="Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>V2 had a real struggle during the first minute, where response time got terribly slow up to 9 seconds.</p>
<p>Looking at the bold-green 50th percentile, we can see that it&rsquo;s consistently higher than the orange one throughout
the load growth period of the first 10 minutes. V2 seemed to have a harder time to adjust.</p>
<p>This might be explainable by slower growth of instance count:</p>
<p><img src="http-scaling-cpu-bound-dotnet-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with .NET CPU-bound Workload"></p>
<p>This difference could be totally random, so let&rsquo;s look at a similar test with Javascript worker. Here is the percentile chart:</p>
<p><img src="http-scaling-cpu-bound-js.png" alt="Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>The original slowness of the first 3 minutes is still there, but after that time V2 and V1 are on-par.</p>
<p>On-par doesn&rsquo;t sound that great though if you look at the significant edge in the number of allocated instances, in
favor of V2 this time:</p>
<p><img src="http-scaling-cpu-bound-js-instance-growth.png" alt="Instance Count Growth while Processing HTTP Requests with Javascript CPU-bound Workload"></p>
<p>Massive 147 instances were crunching Bcrypt hashes in Javascript V2, and that made it a bit faster to respond than V1.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As always, be reluctant to make definite conclusions based on simplistic benchmarks. But I see some trends which might
be true as of today:</p>
<ul>
<li>Performance of .NET Functions is comparable across two versions of Functions runtimes;</li>
<li>V1 still has a clear edge in the cold start time of Javascript Functions;</li>
<li>V2 is the only option for Java developers, but be prepared to very slow cold starts;</li>
<li>Scale-out characteristics seem to be independent of the runtime version, although there are blurry signs of
V2 being a bit slower to ramp up or slightly more resource hungry.</li>
</ul>
<p>I hope this helps in your serverless journey!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Serverless: Cold Start War]]></title>
            <link href="https://mikhail.io/2018/08/serverless-cold-start-war/"/>
            <id>https://mikhail.io/2018/08/serverless-cold-start-war/</id>
            
            <published>2018-08-30T00:00:00+00:00</published>
            <updated>2018-08-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Comparison of cold start statistics for FaaS across AWS, Azure and GCP</blockquote><p><em>The newer and much more detailed cold start comparison is now available: <a href="/serverless/coldstarts/">Cold Starts in Serverless Functions</a></em></p>
<p>Serverless cloud services are hot. Except when they are not :)</p>
<p>AWS Lambda, Azure Functions, Google Cloud Functions are all similar in their attempt
to enable rapid development of cloud-native serverless applications.</p>
<p>Auto-provisioning and auto-scalability are the killer features of those Function-as-a-Service
cloud offerings. No management required, cloud providers will deliver infrastructure for the user
based on the actual incoming load.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &ldquo;cold start&rdquo;. Basically,
applications that haven&rsquo;t been used for a while take longer to startup and to handle the
first request.</p>
<p>Cloud providers keep a bunch of generic unspecialized workers in stock. Whenever a serverless
application needs to scale up, be it from 0 to 1 instances, or from N to N+1 likewise, the runtime
will pick one of the spare workers and will configure it to serve the named application:</p>
<p><img src="coldstart.png" alt="Cold Start"></p>
<p>This procedure takes time, so the latency of the application event handling increases. To avoid
doing this for every event, the specialized worker will be kept intact for some period of time.
When another event comes in, this worker will stand available to process it as soon as possible.
This is a &ldquo;warm start&rdquo;:</p>
<p><img src="warmstart.png" alt="Warm Start"></p>
<p>The problem of cold start latency was described multiple times, here are the notable links:</p>
<ul>
<li><a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a></li>
<li><a href="https://hackernoon.com/cold-starts-in-aws-lambda-f9e3432adbf0">Everything you need to know about cold starts in AWS Lambda</a></li>
<li><a href="https://serverless.com/blog/keep-your-lambdas-warm/">Keeping Functions Warm</a></li>
<li><a href="https://theburningmonk.com/2018/01/im-afraid-youre-thinking-about-aws-lambda-cold-starts-all-wrong/">I&rsquo;m afraid you&rsquo;re thinking about AWS Lambda cold starts all wrong</a></li>
</ul>
<p>The goal of my article today is to explore how cold starts compare:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different languages and runtimes</li>
<li>For smaller vs larger applications (including dependencies)</li>
<li>How often cold starts happen</li>
<li>What can be done to optimize the cold starts</li>
</ul>
<p>Let&rsquo;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some important factors and parameters that
influence the outcome. My interpretation might be wrong. The results might change over time. If you happen
to know a way to improve my tests, please let me know and I will re-run them and re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions because that&rsquo;s where cold start matters the most.</p>
<p>All the functions were returning a simple JSON reporting their current instance ID, language etc.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by a cloud provider. Instead, I measured end-to-end duration from
the client perspective. This means that durations of HTTP gateway (e.g. API Gateway in case of AWS) are included
into the total duration. However, all calls were made from within the same region, so network latency should
have minimal impact:</p>
<p><img src="test-setup.png" alt="Test Setup"></p>
<p>Important note: I ran all my tests on GA (generally available) versions of services/languages, so e.g.
Azure tests were done with version 1 of Functions runtime (.NET Framework), and GCP tests were only made for
Javascript runtime.</p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The answer differs between cloud providers.</p>
<p>To help you read the charts in this section, I&rsquo;ve marked cold starts with blue color dots, and warm starts
with orange color dots.</p>
<h3 id="azure">Azure</h3>
<p>Here is the chart for Azure. It shows the values of normalized request durations across
different languages and runtime versions (Y-axis) depending on the time since the previous
request in minutes (X-axis):</p>
<p><img src="azure-coldstart-threshold.png" alt="Azure Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h3 id="aws">AWS</h3>
<p>AWS is more tricky. Here is the same kind of chart, relative durations vs time since the last request,
measured for AWS Lambda:</p>
<p><img src="aws-coldstart-threshold.png" alt="AWS Cold Start vs Warm Start"></p>
<p>There&rsquo;s no clear threshold here&hellip; For this sample, no cold starts happened within 28 minutes after the previous
invocation. Afterward, the frequency of cold starts slowly rises. But even after 1 hour of inactivity, there&rsquo;s still a
good chance that your instance is alive and ready to take requests.</p>
<p>This doesn&rsquo;t match the official information that AWS Lambdas stay alive for just 5 minutes after the last
invocation. I reached out to Chris Munns, and he confirmed:</p>
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">
So what you are seeing is very much possible as the team plays with certain knobs/levers for execution environment lifecycle.
let me know if you have concerns about it, but it should be just fine</p>&mdash; chrismunns (@chrismunns)
<a href="https://twitter.com/chrismunns/status/1021452964630851585?ref_src=twsrc%5Etfw">July 23, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>A couple learning points here:</p>
<ul>
<li>AWS is working on improving cold start experience (and probably Azure/GCP do too)</li>
<li>My results might not be reliably reproducible in your application since it&rsquo;s affected by recent adjustments</li>
</ul>
<h3 id="gcp">GCP</h3>
<p>Google Cloud Functions left me completely puzzled. Here is the same chart for GCP cold starts (again,
orange dots are warm and blue ones are cold):</p>
<p><img src="gcp-coldstart-threshold.png" alt="GCP Cold Start vs Warm Start"></p>
<p>This looks totally random to me. A cold start can happen in 3 minutes after the previous request, or an instance
can be kept alive for the whole hour. The probability of a cold start doesn&rsquo;t seem to depend on the interval,
at least just by looking at this chart.</p>
<p>Any ideas about what&rsquo;s going on are welcome!</p>
<h3 id="parallel-requests">Parallel requests</h3>
<p>Cold starts happen not only when the first instance of an application is provisioned. The same issue will happen whenever
all the provisioned instances are busy handling incoming events, and yet another event comes in (at scale out).</p>
<p>As far as I&rsquo;m aware, this behavior is common to all 3 providers, so I haven&rsquo;t prepared any comparison charts
for N+1 cold starts. Yet, be aware of them!</p>
<h2 id="reading-candle-charts">Reading Candle Charts</h2>
<p>In the following sections, you will see charts that represent statistical distribution of cold start time as
measured during my experiments. I repeated experiments multiple times and then grouped the metric values, e.g.
by the cloud provider or by language.</p>
<p>Each group will be represented by a &ldquo;candle&rdquo; on the chart. This is how you should read each candle:</p>
<p><img src="sample-coldstart-chart.png" alt="How to Read Cold Start Charts"></p>
<h2 id="memory-allocation">Memory Allocation</h2>
<p>AWS Lambda and Google Cloud Functions have a setting to define the memory size that gets allocated to a single
instance of a function. A user can select a value from 128MB to 2GB and above at creation time.</p>
<p>More importantly, the virtual CPU cycles get allocated proportionally to this provisioned memory size. This means
that an instance of 512 MB will have twice as much CPU speed as an instance of 256MB.</p>
<p>Does this affect the cold start time?</p>
<p>I&rsquo;ve run a series of tests to compare cold start latency across the board of memory/CPU sizes. The results are
somewhat mixed.</p>
<p>AWS Lambda Javascript doesn&rsquo;t seem to have significant differences. This probably means that not so much CPU load
is required to start a Node.js &ldquo;Hello World&rdquo; application:</p>
<p><img src="aws-coldstart-js-by-memory.png" alt="AWS Javascript Cold Start by Memory"></p>
<p>AWS Lambda .NET Core runtime does depend on memory size though. Cold start time drops dramatically with every increase
in allocated memory and CPU:</p>
<p><img src="aws-coldstart-csharp-by-memory.png" alt="AWS C# Cold Start by Memory"></p>
<p>GCP Cloud Functions expose a similar effect even for Javascript runtime:</p>
<p><img src="gcp-coldstart-js-by-memory.png" alt="GCP Javascript Cold Start by Memory"></p>
<p>In contrast to Amazon and Google, Microsoft doesn&rsquo;t ask to select a memory limit. Azure will charge Functions based
on the actual memory usage. More importantly, it will always dedicate a full vCore for a given Function execution.</p>
<p>It&rsquo;s not exactly apples-to-apples, but I chose to fix the memory allocations of AWS Lambda and GCF to 1024 MB.
This feels the closest to Azure&rsquo;s vCore capacity, although I haven&rsquo;t tried a formal CPU performance comparison.</p>
<p>Given that, let&rsquo;s see how the 3 cloud providers compare in cold start time.</p>
<h2 id="javascript-baseline">Javascript Baseline</h2>
<p>Node.js is the only runtime supported in production by Google Cloud Functions right now. Javascript is also
probably by far the most popular language for serverless applications across the board.</p>
<p>Thus, it makes sense to compare the 3 cloud providers on how they perform in Javascript. The
base test measures the cold starts of &ldquo;Hello World&rdquo; type of functions. Functions have no
dependencies, so deployment package is really small.</p>
<p>Here are the numbers for cold starts:</p>
<p><img src="coldstart-js-baseline.png" alt="Cold Start for Basic Javascript Functions"></p>
<p>AWS is clearly doing the best job here. GCP takes the second place, and Azure is the slowest. The rivals are
sort of close though, seemingly playing in the same league so the exact disposition might change over time.</p>
<h2 id="how-do-languages-compare">How Do Languages Compare?</h2>
<p>I&rsquo;ve written Hello World HTTP function in all supported languages of the cloud platforms:</p>
<ul>
<li>AWS: Javascript, Python, Java, Go and C# (.NET Core)</li>
<li>Azure: Javascript and C# (precompiled .NET assembly)</li>
<li>GCP: Javascript</li>
</ul>
<p>Azure kind of supports much more languages, including Python and Java, but they are still considered
experimental / preview, so the cold starts are not fully optimized. See
<a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">my previous article</a> for exact numbers.</p>
<p>Same applies to Python on GCP.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest:</p>
<p><img src="coldstart-per-language.png" alt="Cold Start per Language per Cloud and Language"></p>
<p>AWS provides the richest selection of runtimes, and 4 out of 5 are faster than the other two cloud providers.
C# / .NET seems to be the least optimized (Amazon, why is that?).</p>
<h2 id="does-size-matter">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&rsquo;ve measured cold starts for functions with extra dependencies:</p>
<ul>
<li>Javascript referencing 3 NPM packages - 5MB zipped</li>
<li>Javascript referencing 38 NPM packages - 35 MB zipped</li>
<li>C# function referencing 5 NuGet packages - 2 MB zipped</li>
<li>Java function referencing 5 Maven packages - 15 MB zipped</li>
</ul>
<p>Here are the results:</p>
<p><img src="coldstart-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise, you will pay in seconds for every cold start.</p>
<p>However, the increase in cold start seems quite low, especially for precompiled languages.</p>
<p>A very cool feature of GCP Cloud Functions is that you don&rsquo;t have to include NPM packages into
the deployment archive. You just add <code>package.json</code> file and the runtime will restore them for you.
This makes the deployment artifact ridiculously small, but doesn&rsquo;t seem to slow down the cold
starts either. Obviously, Google pre-restores the packages in advance, before the actual request
comes in.</p>
<h2 id="avoiding-cold-starts">Avoiding Cold Starts</h2>
<p>The overall impression is that cold start delays aren&rsquo;t that high, so most applications can tolerate
them just fine.</p>
<p>If that&rsquo;s not the case, some tricks can be implemented to keep function instances warm.
The approach is universal for all 3 providers: once in X minutes, make an artificial call to
the function to prevent it from expiring.</p>
<p>Implementation details will differ since the expiration policies are different, as we explored
above.</p>
<p>For applications with higher load profile, you might want to fire several parallel &ldquo;warming&rdquo;
requests in order to make sure that enough instances are kept in warm stock.</p>
<p>For further reading, have a look at my
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a>
and <a href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/">AWS Lambda Warmer as Pulumi Component</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Different languages and runtimes have roughly comparable cold start time within the same platform</li>
<li>Minimize the number of dependencies, only bring what&rsquo;s needed</li>
<li>AWS keeps cold starts below 1 second most of the time, which is pretty amazing</li>
<li>All cloud providers are aware of the problem and are actively optimizing the cold start experience</li>
<li>It&rsquo;s likely that in middle term these optimizations will make cold starts a non-issue for the
vast majority of applications</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper into other aspects?
Please leave a comment below or ping me on <a href="https://twitter.com/MikhailShilkov">twitter</a>, and let&rsquo;s
sort it all out.</p>
<p>Stay tuned for more serverless perf goodness!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/google-cloud-functions" term="google-cloud-functions" label="Google Cloud Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[AWS Lambda Warmer as Pulumi Component]]></title>
            <link href="https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/"/>
            <id>https://mikhail.io/2018/08/aws-lambda-warmer-as-pulumi-component/</id>
            
            <published>2018-08-02T00:00:00+00:00</published>
            <updated>2018-08-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Preventing cold stats of AWS Lambda during longer periods of inactivity, implemented as a reusable Pulumo component</blockquote><p>Out of curiosity, I&rsquo;m currently investigating cold starts of Function-as-a-Service platforms of major cloud providers. Basically,
if a function is not called for several minutes, the cloud instance behind it might be recycled, and then the next request will
take longer because a new instance will need to be provisioned.</p>
<p>Recently, Jeremy Daly <a href="https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/">posted</a> a nice
article about the proper way to keep AWS Lambda instances &ldquo;warm&rdquo; to (mostly) prevent cold starts with minimal overhead.
Chris Munns <a href="https://twitter.com/chrismunns/status/1017777028274294784">endorsed</a> the article, so we know it&rsquo;s the right way.</p>
<p>The amount of actions to be taken is quite significant:</p>
<ul>
<li>Define a CloudWatch event which would fire every 5 minutes</li>
<li>Bind this event as another trigger for your Lambda</li>
<li>Inside the Lambda, detect whether current invocation is triggered by our CloudWatch event</li>
<li>If so, short-circuit the execution and return immediately; otherwise, run the normal workload</li>
<li>(Bonus point) If you want to keep multiple instances alive, do some extra dancing with calling itself N times in parallel,
provided by an extra permission to do so.</li>
</ul>
<h2 id="pursuing-reusability">Pursuing Reusability</h2>
<p>To simplify this for his readers, Jeremy was so kind to</p>
<ul>
<li>Create an NPM package which you can install and then call from a function-to-be-warmed</li>
<li>Provide SAM and Serverless Framework templates to automate Cloud Watch integration</li>
</ul>
<p>Those are still two distinct steps: writing the code (JS + NPM) and provisioning the cloud resources (YAML + CLI). There are some
drawbacks to that:</p>
<ul>
<li>You need to change two parts, which don&rsquo;t look like each other</li>
<li>They have to work in sync, e.g. Cloud Watch event must provide the right payload for the handler</li>
<li>There&rsquo;s still some boilerplate for every new Lambda</li>
</ul>
<h2 id="pulumi-components">Pulumi Components</h2>
<p>Pulumi takes a different approach. You can blend the application code and infrastructure management code
into one cohesive cloud application.</p>
<p>Related resources can be combined together into reusable components, which hide repetitive stuff behind code abstractions.</p>
<p>One way to define an AWS Lambda with Typescript in Pulumi is the following:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> handler = (event: <span style="color:#2b91af">any</span>, context: <span style="color:#2b91af">any</span>, callback: (error: <span style="color:#2b91af">any</span>, result: <span style="color:#2b91af">any</span>) =&gt; <span style="color:#00f">void</span>) =&gt; {
    <span style="color:#00f">const</span> response = {
        statusCode: <span style="color:#2b91af">200</span>,
        body: <span style="color:#a31515">&#34;Cheers, how are things?&#34;</span>
      };
    
    callback(<span style="color:#00f">null</span>, response);
};

<span style="color:#00f">const</span> lambda = <span style="color:#00f">new</span> aws.serverless.Function(<span style="color:#a31515">&#34;my-function&#34;</span>, { <span style="color:#008000">/* options */</span> }, handler);
</code></pre></div><p>The processing code <code>handler</code> is just passed to infrastructure code as a parameter.</p>
<p>So, if I wanted to make reusable API for an &ldquo;always warm&rdquo; function, how would it look like?</p>
<p>From the client code perspective, I just want to be able to do the same thing:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> lambda = <span style="color:#00f">new</span> mylibrary.WarmLambda(<span style="color:#a31515">&#34;my-warm-function&#34;</span>, { <span style="color:#008000">/* options */</span> }, handler);
</code></pre></div><p>CloudWatch? Event subscription? Short-circuiting? They are implementation details!</p>
<h2 id="warm-lambda">Warm Lambda</h2>
<p>Here is how to implement such component. The declaration starts with a Typescript class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">export</span> <span style="color:#00f">class</span> WarmLambda <span style="color:#00f">extends</span> pulumi.ComponentResource {
    <span style="color:#00f">public</span> lambda: <span style="color:#2b91af">aws.lambda.Function</span>;

    <span style="color:#008000">// Implementation goes here...
</span><span style="color:#008000"></span>}
</code></pre></div><p>We expose the raw Lambda Function object, so that it could be used for further bindings and retrieving outputs.</p>
<p>The constructor accepts the same parameters as <code>aws.serverless.Function</code> provided by Pulumi:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">constructor</span>(name: <span style="color:#2b91af">string</span>,
        options: <span style="color:#2b91af">aws.serverless.FunctionOptions</span>,
        handler: <span style="color:#2b91af">aws.serverless.Handler</span>,
        opts?: <span style="color:#2b91af">pulumi.ResourceOptions</span>) {

    <span style="color:#008000">// Subresources are created here...
</span><span style="color:#008000"></span>}
</code></pre></div><p>We start resource provisioning by creating the CloudWatch rule to be triggered every 5 minutes:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> eventRule = <span style="color:#00f">new</span> aws.cloudwatch.EventRule(<span style="color:#a31515">`</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">-warming-rule`</span>, 
    { scheduleExpression: <span style="color:#a31515">&#34;rate(5 minutes)&#34;</span> },
    { parent: <span style="color:#2b91af">this</span>, ...opts }
);
</code></pre></div><p>Then goes the cool trick. We substitute the user-provided handler with our own &ldquo;outer&rdquo; handler. This handler closes
over <code>eventRule</code>, so it can use the rule to identify the warm-up event coming from CloudWatch. If such is identified,
the handler short-circuits to the callback. Otherwise, it passes the event over to the original handler:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> outerHandler = (event: <span style="color:#2b91af">any</span>, context: <span style="color:#2b91af">aws.serverless.Context</span>, callback: (error: <span style="color:#2b91af">any</span>, result: <span style="color:#2b91af">any</span>) =&gt; <span style="color:#00f">void</span>) =&gt;
{
    <span style="color:#00f">if</span> (event.resources &amp;&amp; event.resources[0] &amp;&amp; event.resources[0].includes(eventRule.name.<span style="color:#00f">get</span>())) {
        console.log(<span style="color:#a31515">&#39;Warming...&#39;</span>);
        callback(<span style="color:#00f">null</span>, <span style="color:#a31515">&#34;warmed!&#34;</span>);
    } <span style="color:#00f">else</span> {
        console.log(<span style="color:#a31515">&#39;Running the real handler...&#39;</span>);
        handler(event, context, callback);
    }
};
</code></pre></div><p>That&rsquo;s a great example of synergy enabled by doing both application code and application infrastructure in a
single program. I&rsquo;m free to mix and match objects from both worlds.</p>
<p>It&rsquo;s time to bind both <code>eventRule</code> and <code>outerHandler</code> to a new serverless function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">const</span> func = <span style="color:#00f">new</span> aws.serverless.Function(
    <span style="color:#a31515">`</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">-warmed`</span>, 
    options, 
    outerHandler, 
    { parent: <span style="color:#2b91af">this</span>, ...opts });
<span style="color:#00f">this</span>.lambda = func.lambda;            
</code></pre></div><p>Finally, I create an event subscription from CloudWatch schedule to Lambda:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#00f">this</span>.subscription = <span style="color:#00f">new</span> serverless.cloudwatch.CloudwatchEventSubscription(
    <span style="color:#a31515">`</span><span style="color:#a31515">${</span>name<span style="color:#a31515">}</span><span style="color:#a31515">-warming-subscription`</span>, 
    eventRule,
    <span style="color:#00f">this</span>.lambda,
    { },
    { parent: <span style="color:#2b91af">this</span>, ...opts });
</code></pre></div><p>And that&rsquo;s all we need for now! See the full code
<a href="https://github.com/mikhailshilkov/pulumi-serverless-examples/blob/master/WarmedLambda-TypeScript/warmLambda.ts">here</a>.</p>
<p>Here is the output of <code>pulumi update</code> command for my sample &ldquo;warm&rdquo; lambda application:</p>
<pre><code>     Type                                                      Name                            Plan
 +   pulumi:pulumi:Stack                                       WarmLambda-WarmLambda-dev       create
 +    samples:WarmLambda                                       i-am-warm                       create
 +      aws-serverless:cloudwatch:CloudwatchEventSubscription  i-am-warm-warming-subscription  create
 +        aws:lambda:Permission                                i-am-warm-warming-subscription  create
 +        aws:cloudwatch:EventTarget                           i-am-warm-warming-subscription  create
 +      aws:cloudwatch:EventRule                               i-am-warm-warming-rule          create
 +      aws:serverless:Function                                i-am-warm-warmed                create
 +         aws:lambda:Function                                 i-am-warm-warmed                create
</code></pre><p>7 Pulumi components and 4 AWS cloud resources are provisioned by one <code>new WarmLambda()</code> line.</p>
<h2 id="multi-instance-warming">Multi-Instance Warming</h2>
<p>Jeremy&rsquo;s library supports warming several instances of Lambda by issuing parallel self-calls.</p>
<p>Reproducing the same with Pulumi component should be fairly straightforward:</p>
<ul>
<li>Add an extra constructor option to accept the number of instances to keep warm</li>
<li>Add a permission to call Lambda from itself</li>
<li>Fire N calls when warming event is triggered</li>
<li>Short-circuit those calls in each instance</li>
</ul>
<p>Note that only the first item would be visible to the client code. That&rsquo;s the power of componentization
and code reuse.</p>
<p>I didn&rsquo;t need multi-instance warming, so I&rsquo;ll leave the implementation as exercise for the reader.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Obligatory note: most probably, you don&rsquo;t need to add warming to your AWS Lambdas.</p>
<p>But whatever advanced scenario you might have, it&rsquo;s likely that it is easier to express the scenario
in terms of general-purpose reusable component, rather than a set of guidelines or templates.</p>
<p>Happy hacking!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Getting Started with AWS Lambda in Pulumi]]></title>
            <link href="https://mikhail.io/2018/07/getting-started-with-aws-lambda-in-pulumi/"/>
            <id>https://mikhail.io/2018/07/getting-started-with-aws-lambda-in-pulumi/</id>
            
            <published>2018-07-12T00:00:00+00:00</published>
            <updated>2018-07-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Provisioning AWS Lambda and API Gateway with Pulumi, examples in 5 programming languages</blockquote><p>For a small research project of mine, I needed to create HTTP triggered
AWS Lambda&rsquo;s in all supported programming languages.</p>
<p>I&rsquo;m not a power AWS user, so I get easily confused about the configuration
of things like IAM roles or API Gateway. Moreover, I wanted my environment to
be reproducible, so manual AWS Console wasn&rsquo;t a good option.</p>
<p>I decided it was a good job for Pulumi. They pay a lot of attention to
serverless and especially AWS Lambda, and I love the power of
configuration as code.</p>
<p>I created a Pulumi program which provisions Lambda&rsquo;s running on Javascript,
.NET, Python, Java and Go. Pulumi program itself is written in Javascript.</p>
<p>I&rsquo;m describing the resulting code below in case folks need to do the same thing.
The code itself is on <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github</a>.</p>
<h2 id="javascript">Javascript</h2>
<p>Probably, the vast majority of Pulumi + AWS Lambda users will be using
Javascript as programming language for their serverless functions.</p>
<p>No wonder that this scenario is the easiest to start with. There is a
high-level package <code>@pulumi/cloud-aws</code> which hides all the AWS machinery from
a developer.</p>
<p>The simplest function will consist of just several lines:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> cloud = require(<span style="color:#a31515">&#34;@pulumi/cloud-aws&#34;</span>);

<span style="color:#00f">const</span> api = <span style="color:#00f">new</span> cloud.API(<span style="color:#a31515">&#34;aws-hellolambda-js&#34;</span>);
api.get(<span style="color:#a31515">&#34;/js&#34;</span>, (req, res) =&gt; {
    res.status(200).json(<span style="color:#a31515">&#34;Hi from Javascript lambda&#34;</span>);
});

exports.endpointJs = api.publish().url;
</code></pre></div><p>Configure your Pulumi stack, run <code>pulumi update</code> and a Lambda
is up, running and accessible via HTTP.</p>
<h2 id="net-core">.NET Core</h2>
<p>.NET is my default development environment and AWS Lambda supports .NET Core
as execution runtime.</p>
<p>Pulumi program is still Javascript, so it can&rsquo;t mix C# code in. Thus, the setup
looks like this:</p>
<ul>
<li>There is a .NET Core 2.0 application written in C# and utilizing
<code>Amazon.Lambda.*</code> NuGet packages</li>
<li>I build and publish this application with <code>dotnet</code> CLI</li>
<li>Pulumi then utilizes the published binaries to create deployment artifacts</li>
</ul>
<p>C# function looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Functions</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">async</span> Task&lt;APIGatewayProxyResponse&gt; GetAsync(APIGatewayProxyRequest request, ILambdaContext context)
    {
        <span style="color:#00f">return</span> <span style="color:#00f">new</span> APIGatewayProxyResponse
        {
            StatusCode = (<span style="color:#2b91af">int</span>)HttpStatusCode.OK,
            Body = <span style="color:#a31515">&#34;\&#34;Hi from C# Lambda\&#34;&#34;</span>,
            Headers = <span style="color:#00f">new</span> Dictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">string</span>&gt; { { <span style="color:#a31515">&#34;Content-Type&#34;</span>, <span style="color:#a31515">&#34;application/json&#34;</span> } }
        };
    }
}
</code></pre></div><p>For non-Javascript lambdas I utilize <code>@pulumi/aws</code> package. It&rsquo;s of lower level
than <code>@pulumi/cloud-aws</code>, so I had to setup IAM first:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> aws = require(<span style="color:#a31515">&#34;@pulumi/aws&#34;</span>);

<span style="color:#00f">const</span> policy = {
    <span style="color:#a31515">&#34;Version&#34;</span>: <span style="color:#a31515">&#34;2012-10-17&#34;</span>,
    <span style="color:#a31515">&#34;Statement&#34;</span>: [
        {
            <span style="color:#a31515">&#34;Action&#34;</span>: <span style="color:#a31515">&#34;sts:AssumeRole&#34;</span>,
            <span style="color:#a31515">&#34;Principal&#34;</span>: {
                <span style="color:#a31515">&#34;Service&#34;</span>: <span style="color:#a31515">&#34;lambda.amazonaws.com&#34;</span>,
            },
            <span style="color:#a31515">&#34;Effect&#34;</span>: <span style="color:#a31515">&#34;Allow&#34;</span>,
            <span style="color:#a31515">&#34;Sid&#34;</span>: <span style="color:#a31515">&#34;&#34;</span>,
        },
    ],
};
<span style="color:#00f">const</span> role = <span style="color:#00f">new</span> aws.iam.Role(<span style="color:#a31515">&#34;precompiled-lambda-role&#34;</span>, {
    assumeRolePolicy: JSON.stringify(policy),
});
</code></pre></div><p>And then I did a raw definition of AWS Lambda:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> pulumi = require(<span style="color:#a31515">&#34;@pulumi/pulumi&#34;</span>);

<span style="color:#00f">const</span> csharpLambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;aws-hellolambda-csharp&#34;</span>, {
    runtime: aws.lambda.DotnetCore2d0Runtime,
    code: <span style="color:#00f">new</span> pulumi.asset.AssetArchive({
        <span style="color:#a31515">&#34;.&#34;</span>: <span style="color:#00f">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./csharp/bin/Debug/netcoreapp2.0/publish&#34;</span>),
    }),
    timeout: 5,
    handler: <span style="color:#a31515">&#34;app::app.Functions::GetAsync&#34;</span>,
    role: role.arn
});
</code></pre></div><p>Note the path to <code>publish</code> folder, which should match the path created by
<code>dotnet publish</code>, and the handler name matching C# class/method.</p>
<p>Finally, I used <code>@pulumi/aws-serverless</code> to define API Gateway endpoint for
the lambda:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> serverless = require(<span style="color:#a31515">&#34;@pulumi/aws-serverless&#34;</span>);

<span style="color:#00f">const</span> precompiledApi = <span style="color:#00f">new</span> serverless.apigateway.API(<span style="color:#a31515">&#34;aws-hellolambda-precompiledapi&#34;</span>, {
    routes: [
        { method: <span style="color:#a31515">&#34;GET&#34;</span>, path: <span style="color:#a31515">&#34;/csharp&#34;</span>, handler: csharpLambda },
    ],
});
</code></pre></div><p>That&rsquo;s definitely more ceremony compared to Javascript version. But hey, it&rsquo;s
code, so if you find yourself repeating the same code, go ahead and make a
higher order component out of it, incapsulating the repetitive logic.</p>
<h2 id="python">Python</h2>
<p>Pulumi supports Python as scripting language, but I&rsquo;m sticking to Javascript
for uniform experience.</p>
<p>In this case, the flow is similar to .NET but simpler: no compilation step
is required. Just define a <code>handler.py</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00f">def</span> handler(event, context): 
    <span style="color:#00f">return</span> {
        <span style="color:#a31515">&#39;statusCode&#39;</span>: 200,
        <span style="color:#a31515">&#39;headers&#39;</span>: {<span style="color:#a31515">&#39;Content-Type&#39;</span>: <span style="color:#a31515">&#39;application/json&#39;</span>},
        <span style="color:#a31515">&#39;body&#39;</span>: <span style="color:#a31515">&#39;&#34;Hi from Python lambda&#34;&#39;</span>
    }
</code></pre></div><p>and package it into zip in AWS lambda definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> pythonLambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;aws-hellolambda-python&#34;</span>, {
    runtime: aws.lambda.Python3d6Runtime,
    code: <span style="color:#00f">new</span> pulumi.asset.AssetArchive({
        <span style="color:#a31515">&#34;.&#34;</span>: <span style="color:#00f">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./python&#34;</span>),
    }),
    timeout: 5,
    handler: <span style="color:#a31515">&#34;handler.handler&#34;</span>,
    role: role.arn
});
</code></pre></div><p>I&rsquo;m reusing the <code>role</code> definition from above. The API definition will also
be the same as for .NET.</p>
<h2 id="go">Go</h2>
<p>Golang is a compiled language, so the approach is similar to .NET: write code,
build, reference the built artifact from Pulumi.</p>
<p>My Go function looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#00f">func</span> Handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, <span style="color:#2b91af">error</span>) {

 <span style="color:#00f">return</span> events.APIGatewayProxyResponse{
  Body:       <span style="color:#a31515">&#34;\&#34;Hi from Golang lambda\&#34;&#34;</span>,
  StatusCode: 200,
 }, <span style="color:#00f">nil</span>

}
</code></pre></div><p>Because I&rsquo;m on Windows but AWS Lambda runs on Linux, I had to use
<a href="https://github.com/aws/aws-lambda-go"><code>build-lambda-zip</code></a>
tool to make the package compatible. Here is the PowerShell build script:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-powershell" data-lang="powershell">$env:GOOS = <span style="color:#a31515">&#34;linux&#34;</span>
$env:GOARCH = <span style="color:#a31515">&#34;amd64&#34;</span>
go build -o main main.go
~\Go\bin\build-lambda-zip.exe -o main.zip main
</code></pre></div><p>and Pulumi function definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> golangLambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;aws-hellolambda-golang&#34;</span>, {
    runtime: aws.lambda.Go1dxRuntime,
    code: <span style="color:#00f">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;./go/main.zip&#34;</span>),
    timeout: 5,
    handler: <span style="color:#a31515">&#34;main&#34;</span>,
    role: role.arn
});
</code></pre></div><h2 id="java">Java</h2>
<p>Java class implements an interface from AWS SDK:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Hello</span> <span style="color:#00f">implements</span> RequestStreamHandler {

    <span style="color:#00f">public</span> <span style="color:#2b91af">void</span> handleRequest(InputStream inputStream, OutputStream outputStream, Context context) <span style="color:#00f">throws</span> IOException {

        JSONObject responseJson = <span style="color:#00f">new</span> JSONObject();

        responseJson.put(<span style="color:#a31515">&#34;isBase64Encoded&#34;</span>, <span style="color:#00f">false</span>);
        responseJson.put(<span style="color:#a31515">&#34;statusCode&#34;</span>, <span style="color:#a31515">&#34;200&#34;</span>);
        responseJson.put(<span style="color:#a31515">&#34;body&#34;</span>, <span style="color:#a31515">&#34;\&#34;Hi from Java lambda\&#34;&#34;</span>);  

        OutputStreamWriter writer = <span style="color:#00f">new</span> OutputStreamWriter(outputStream, <span style="color:#a31515">&#34;UTF-8&#34;</span>);
        writer.write(responseJson.toJSONString());  
        writer.close();
    }
}
</code></pre></div><p>I compiled this code with Maven (<code>mvn package</code>), which produced a <code>jar</code> file. AWS Lambda accepts
<code>jar</code> directly, but Pulumi&rsquo;s <code>FileArchive</code> is unfortunately crashing on trying
to read it.</p>
<p>As a workaround, I had to define a <code>zip</code> file with <code>jar</code> placed inside <code>lib</code>
folder:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">const</span> javaLambda = <span style="color:#00f">new</span> aws.lambda.Function(<span style="color:#a31515">&#34;aws-coldstart-java&#34;</span>, {
    code: <span style="color:#00f">new</span> pulumi.asset.AssetArchive({
        <span style="color:#a31515">&#34;lib/lambda-java-example-1.0-SNAPSHOT.jar&#34;</span>: <span style="color:#00f">new</span> pulumi.asset.FileAsset(<span style="color:#a31515">&#34;./java/target/lambda-java-example-1.0-SNAPSHOT.jar&#34;</span>),
    }),
    runtime: aws.lambda.Java8Runtime,
    timeout: 5,
    handler: <span style="color:#a31515">&#34;example.Hello&#34;</span>,
    role: role.arn
});
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>The complete code for 5 lambda functions in 5 different programming languages
can be found in <a href="https://github.com/mikhailshilkov/pulumi-aws-serverless-examples">my github repository</a>.</p>
<p>Running <code>pulumi update</code> provisions 25 AWS resources in a matter of 1 minute,
so I can start playing with my test lambdas in no time.</p>
<p>And the best part: when I don&rsquo;t need them anymore, I run <code>pulumi destroy</code> and
my AWS Console is clean again!</p>
<p>Happy serverless moments!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aws" term="aws" label="AWS" />
                             
                                <category scheme="https://mikhail.io/tags/aws-lambda" term="aws-lambda" label="AWS Lambda" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Functional Programming]]></title>
            <link href="https://mikhail.io/tags/functional-programming/"/>
            <id>https://mikhail.io/tags/functional-programming/</id>
            
            <published>2018-07-05T00:00:00+00:00</published>
            <updated>2018-07-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[LINQ]]></title>
            <link href="https://mikhail.io/tags/linq/"/>
            <id>https://mikhail.io/tags/linq/</id>
            
            <published>2018-07-05T00:00:00+00:00</published>
            <updated>2018-07-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Monads]]></title>
            <link href="https://mikhail.io/tags/monads/"/>
            <id>https://mikhail.io/tags/monads/</id>
            
            <published>2018-07-05T00:00:00+00:00</published>
            <updated>2018-07-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Monads explained in C# (again)]]></title>
            <link href="https://mikhail.io/2018/07/monads-explained-in-csharp-again/"/>
            <id>https://mikhail.io/2018/07/monads-explained-in-csharp-again/</id>
            
            <published>2018-07-05T00:00:00+00:00</published>
            <updated>2018-07-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Yet another Monad tutorial, this time for C# OOP developers</blockquote><p>I love functional programming for the simplicity that it brings.</p>
<p>But at the same time, I realize that learning functional programming is a challenging
process. FP comes with a baggage of unfamiliar vocabulary that can be daunting for
somebody coming from an object-oriented language like C#.</p>
<figure >
    
        <img src="functional-programming-word-cloud.png"
            alt="Some of functional lingo"
             />
        
    
    <figcaption>
        <h4>Some of functional lingo</h4>
    </figcaption>
    
</figure>
<p>&ldquo;Monad&rdquo; is probably the most infamous term from the list above. Monads have reputation of being
something very abstract and very confusing.</p>
<h2 id="the-fallacy-of-monad-tutorials">The Fallacy of Monad Tutorials</h2>
<p>Numerous attempts were made to explain monads in simple definitions; and monad tutorials have become a
genre of its own. And yet, times and times again, they fail to enlighten the readers.</p>
<p>The shortest explanation of monads looks like this:</p>
<p><img src="monoid-endofunctors.png" alt="A Monad is just a monoid in the category of endofunctors"></p>
<p>It&rsquo;s both mathematically correct and totally useless to anybody learning functional programming. To
understand this statement, one has to know the terms &ldquo;monoid&rdquo;, &ldquo;category&rdquo; and &ldquo;endofunctors&rdquo; and be able
to mentally compose them into something meaningful.</p>
<p>The same problem is apparent in most monad tutorials. They assume some pre-existing knowledge in
heads of their readers, and if that assumption fails, the tutorial doesn&rsquo;t click.</p>
<p>Focusing too much on mechanics of monads instead of explaining why they are important is another
common problem.</p>
<p>Douglas Crockford grasped this fallacy very well:</p>
<blockquote>
<p>The monadic curse is that once someone learns what monads are and how to use them, they lose the ability to explain them to other people</p>
</blockquote>
<p>The problem here is likely the following. Every person who understands monads had their own path to
this knowledge. It hasn&rsquo;t come all at once, instead there was a series of steps, each giving an insight,
until the last final step made the puzzle complete.</p>
<p>But they don&rsquo;t remember the whole path anymore. They go online and blog about that very last step as
the key to understanding, joining the club of flawed explanations.</p>
<p>There is an actual <a href="http://tomasp.net/academic/papers/monads/monads-programming.pdf">academic paper from Tomas Petricek</a>
that studies monad tutorials.</p>
<p>I&rsquo;ve read that paper and a dozen of monad tutorials online. And of course, now I came up with my own.</p>
<p>I&rsquo;m probably doomed to fail too, at least for some readers. Yet, I know that many people found the
<a href="https://mikhail.io/2016/01/monads-explained-in-csharp/">previous version</a> of this article useful.</p>
<p>I based my explanation on examples from C# - the object-oriented language familiar to .NET developers.</p>
<h2 id="story-of-composition">Story of Composition</h2>
<p>The base element of each functional program is Function. In typed languages each function
is just a mapping between the type of its input parameter and output parameter.
Such type can be annotated as <code>func: TypeA -&gt; TypeB</code>.</p>
<p>C# is object-oriented language, so we use methods to declare functions. There are two ways
to define a method comparable to <code>func</code> function above. I can use static method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Mapper</span>
{
    <span style="color:#00f">static</span> ClassB func(ClassA a) { ... }
}
</code></pre></div><p>&hellip; or instance method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">ClassA</span>
{
    <span style="color:#008000">// Instance method
</span><span style="color:#008000"></span>    ClassB func() { ... }
}
</code></pre></div><p>Static form looks closer to the function annotation, but both ways are actually equivalent
for the purpose of our discussion. I will use instance methods in my examples, however all of
them could be written as static extension methods too.</p>
<p>How do we compose more complex workflows, programs and applications out of such simple
building blocks? A lot of patterns both in OOP and FP worlds revolve around this question.
And monads are one of the answers.</p>
<p>My sample code is going to be about conferences and speakers. The method implementations
aren&rsquo;t really important, just watch the types carefully. There are 4 classes (types) and
3 methods (functions):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Speaker</span>
{
    Talk NextTalk() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Talk</span>
{
    Conference GetConference() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Conference</span>
{
    City GetCity() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">City</span> { ... }
</code></pre></div><p>These methods are currently very easy to compose into a workflow:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> City NextTalkCity(Speaker speaker)
{
    Talk talk = speaker.NextTalk();
    Conference conf = talk.GetConference();
    City city = conf.GetCity();
    <span style="color:#00f">return</span> city;
}
</code></pre></div><p>Because the return type of the previous step always matches the input type of the next step, we can
write it even shorter:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> City NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .NextTalk()
        .GetConference()
        .GetCity();
}
</code></pre></div><p>This code looks quite readable. It&rsquo;s concise and it flows from top to bottom, from left to right,
similar to how we are used to read any text. There is not much noise too.</p>
<p>That&rsquo;s not what real codebases look like though, because there are multiple complications
along the happy composition path. Let&rsquo;s look at some of them.</p>
<h2 id="nulls">NULLs</h2>
<p>Any class instance in C# can be <code>null</code>. In the example above I might get runtime errors if
one of the methods ever returns <code>null</code> back.</p>
<p>Typed functional programming always tries to be explicit about types, so I&rsquo;ll re-write the signatures
of my methods to annotate the return types as nullables:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Speaker</span>
{
    Nullable&lt;Talk&gt; NextTalk() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Talk</span>
{
    Nullable&lt;Conference&gt; GetConference() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Conference</span>
{
    Nullable&lt;City&gt; GetCity() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">City</span> { ... }
</code></pre></div><p>This is actually invalid syntax in current C# version, because <code>Nullable&lt;T&gt;</code> and its short form
<code>T?</code> are not applicable to reference types. This <a href="https://blogs.msdn.microsoft.com/dotnet/2017/11/15/nullable-reference-types-in-csharp/">might change in C# 8</a>
though, so bear with me.</p>
<p>Now, when composing our workflow, we need to take care of <code>null</code> results:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> Nullable&lt;City&gt; NextTalkCity(Speaker speaker)
{
    Nullable&lt;Talk&gt; talk = speaker.NextTalk();
    <span style="color:#00f">if</span> (talk == <span style="color:#00f">null</span>) <span style="color:#00f">return</span> <span style="color:#00f">null</span>;

    Nullable&lt;Conference&gt; conf = talk.GetConference();
    <span style="color:#00f">if</span> (conf == <span style="color:#00f">null</span>) <span style="color:#00f">return</span> <span style="color:#00f">null</span>;

    Nullable&lt;City&gt; city = conf.GetCity();
    <span style="color:#00f">return</span> city;
}
</code></pre></div><p>It&rsquo;s still the same method, but it got more noise now. Even though I used short-circuit returns
and one-liners, it still got harder to read.</p>
<p>To fight that problem, smart language designers came up with the Null Propagation Operator:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> Nullable&lt;City&gt; NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        ?.NextTalk()
        ?.GetConference()
        ?.GetCity();
}
</code></pre></div><p>Now we are almost back to our original workflow code: it&rsquo;s clean and concise, we just got
3 extra <code>?</code> symbols around.</p>
<p>Let&rsquo;s take another leap.</p>
<h2 id="collections">Collections</h2>
<p>Quite often a function returns a collection of items, not just a single item. To some extent,
that&rsquo;s a generalization of <code>null</code> case: with <code>Nullable&lt;T&gt;</code> we might get 0 or 1 results back,
while with a collection we can get <code>0</code> to any <code>n</code> results.</p>
<p>Our sample API could look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Speaker</span>
{
    List&lt;Talk&gt; GetTalks() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Talk</span>
{
    List&lt;Conference&gt; GetConferences() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Conference</span>
{
    List&lt;City&gt; GetCities() { ... }
}
</code></pre></div><p>I used <code>List&lt;T&gt;</code> but it could be any class or plain <code>IEnumerable&lt;T&gt;</code> interface.</p>
<p>How would we combine the methods into one workflow? Traditional version would look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> List&lt;City&gt; AllCitiesToVisit(Speaker speaker)
{
    <span style="color:#2b91af">var</span> result = <span style="color:#00f">new</span> List&lt;City&gt;();

    <span style="color:#00f">foreach</span> (Talk talk <span style="color:#00f">in</span> speaker.GetTalks())
        <span style="color:#00f">foreach</span> (Conference conf <span style="color:#00f">in</span> talk.GetConferences())
            <span style="color:#00f">foreach</span> (City city <span style="color:#00f">in</span> conf.GetCities())
                result.Add(city);

    <span style="color:#00f">return</span> result;
}
</code></pre></div><p>It reads ok-ish still. But the combination of nested loops and mutation with some conditionals sprinkled
on them can get unreadable pretty soon. The exact workflow might be lost in the mechanics.</p>
<p>As an alternative, C# language designers invented LINQ extension methods. We can write code like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> List&lt;City&gt; AllCitiesToVisit(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .GetTalks()
        .SelectMany(talk =&gt; talk.GetConferences())
        .SelectMany(conf =&gt; conf.GetCities())
        .ToList();
}
</code></pre></div><p>Let me do one further trick and format the same code in an unusual way:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> List&lt;City&gt; AllCitiesToVisit(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .GetTalks()           .SelectMany(x =&gt; x
        .GetConferences()    ).SelectMany(x =&gt; x
        .GetCities()         ).ToList();
}
</code></pre></div><p>Now you can see the same original code on the left, combined with just a bit of technical repeatable
clutter on the right. Hold on, I&rsquo;ll show you where I&rsquo;m going.</p>
<p>Let&rsquo;s discuss another possible complication.</p>
<h2 id="asynchronous-calls">Asynchronous Calls</h2>
<p>What if our methods need to access some remote database or service to produce the results? This
should be shown in type signature, and C# has <code>Task&lt;T&gt;</code> for that:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Speaker</span>
{
    Task&lt;Talk&gt; NextTalk() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Talk</span>
{
    Task&lt;Conference&gt; GetConference() { ... }
}

<span style="color:#00f">class</span> <span style="color:#2b91af">Conference</span>
{
    Task&lt;City&gt; GetCity() { ... }
}
</code></pre></div><p>This change breaks our nice workflow composition again.</p>
<p>We&rsquo;ll get back to async-await later, but the original way to combine <code>Task</code>-based
methods was to use <code>ContinueWith</code> and <code>Unwrap</code> API:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> Task&lt;City&gt; NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .NextTalk()
        .ContinueWith(talk =&gt; talk.Result.GetConference())
        .Unwrap()
        .ContinueWith(conf =&gt; conf.Result.GetCity())
        .Unwrap();
}
</code></pre></div><p>Hard to read, but let me apply my formatting trick again:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> Task&lt;City&gt; NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .NextTalk()         .ContinueWith(x =&gt; x.Result
        .GetConference()   ).Unwrap().ContinueWith(x =&gt; x.Result
        .GetCity()         ).Unwrap();
}
</code></pre></div><p>You can see that, once again, it&rsquo;s our nice readable workflow on the left + some mechanical repeatable
junction code on the right.</p>
<h2 id="pattern">Pattern</h2>
<p>Can you see a pattern yet?</p>
<p>I&rsquo;ll repeat the <code>Nullable</code>-, <code>List</code>- and <code>Task</code>-based workflows again:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> Nullable&lt;City&gt; NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker               ?
        .NextTalk()           ?
        .GetConference()      ?
        .GetCity();
}

<span style="color:#00f">static</span> List&lt;City&gt; AllCitiesToVisit(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .GetTalks()            .SelectMany(x =&gt; x
        .GetConferences()     ).SelectMany(x =&gt; x
        .GetCities()          ).ToList();
}

<span style="color:#00f">static</span> Task&lt;City&gt; NextTalkCity(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .NextTalk()            .ContinueWith(x =&gt; x.Result
        .GetConference()      ).Unwrap().ContinueWith(x =&gt; x.Result
        .GetCity()            ).Unwrap();
}
</code></pre></div><p>In all 3 cases there was a complication which prevented us from sequencing method
calls fluently. In all 3 cases we found the gluing code to get back to fluent composition.</p>
<p>Let&rsquo;s try to generalize this approach. Given some generic container type
<code>WorkflowThatReturns&lt;T&gt;</code>, we have a method to combine an instance of such workflow with
a function which accepts the result of that workflow and returns another workflow back:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">WorkflowThatReturns</span>&lt;T&gt;
{
    WorkflowThatReturns&lt;U&gt; AddStep(Func&lt;T, WorkflowThatReturns&lt;U&gt;&gt; step);
}
</code></pre></div><p>In case this is hard to grasp, have a look at the picture of what is going on:</p>
<p><img src="monad-bind.png" alt="Monad Bind Internals"></p>
<ol>
<li>
<p>An instance of type <code>T</code> sits in a generic container.</p>
</li>
<li>
<p>We call <code>AddStep</code> with a function, which maps <code>T</code> to <code>U</code> sitting inside yet another
container.</p>
</li>
<li>
<p>We get an instance of <code>U</code> but inside two containers.</p>
</li>
<li>
<p>Two containers are automatically unwrapped into a single container to get back to the
original shape.</p>
</li>
<li>
<p>Now we are ready to add another step!</p>
</li>
</ol>
<p>In the following code, <code>NextTalk</code> returns the first instance inside the container:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">WorkflowThatReturns&lt;City&gt; Workflow(Speaker speaker)
{
    <span style="color:#00f">return</span>
        speaker
        .NextTalk()
        .AddStep(x =&gt; x.GetConference())
        .AddStep(x =&gt; x.GetCity());
}
</code></pre></div><p>Subsequently, <code>AddStep</code> is called two times to transfer to <code>Conference</code> and then
<code>City</code> inside the same container:</p>
<p><img src="monad-two-binds.png" alt="Monad Bind Chaining"></p>
<h2 id="finally-monads">Finally, Monads</h2>
<p>The name of this pattern is <strong>Monad</strong>.</p>
<p>In C# terms, a Monad is a generic class with two operations: constructor and bind.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Monad</span>&lt;T&gt; {
    Monad(T instance);
    Monad&lt;U&gt; Bind(Func&lt;T, Monad&lt;U&gt;&gt; f);
}
</code></pre></div><p>Constructor is used to put an object into container, <code>Bind</code> is used to replace one
contained object with another contained object.</p>
<p>It&rsquo;s important that <code>Bind</code>&rsquo;s argument returns <code>Monad&lt;U&gt;</code> and not just <code>U</code>. We can think
of <code>Bind</code> as a combination of <code>Map</code> and <code>Unwrap</code> as defined per following signature:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">class</span> <span style="color:#2b91af">Monad</span>&lt;T&gt; {
    Monad(T instance);
    Monad&lt;U&gt; Map(Function&lt;T, U&gt; f);
    <span style="color:#00f">static</span> Monad&lt;U&gt; Unwrap(Monad&lt;Monad&lt;U&gt;&gt; nested);
}
</code></pre></div><p>Even though I spent quite some time with examples, I expect you to be slightly confused
at this point. That&rsquo;s ok.</p>
<p>Keep going and let&rsquo;s have a look at several sample implementations of Monad pattern.</p>
<h2 id="maybe-option"><a name="maybe"></a>
Maybe (Option)</h2>
<p>My first motivational example was with <code>Nullable&lt;T&gt;</code> and <code>?.</code>. The full pattern
containing either 0 or 1 instance of some type is called <code>Maybe</code> (it maybe has a value,
or maybe not).</p>
<p><code>Maybe</code> is another approach to dealing with &lsquo;no value&rsquo; value, alternative to the
concept of <code>null</code>.</p>
<p>Functional-first language F# typically doesn&rsquo;t allow <code>null</code> for its types. Instead, F# has
a maybe implementation built into the language:
it&rsquo;s called <code>option</code> type.</p>
<p>Here is a sample implementation in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Maybe</span>&lt;T&gt; <span style="color:#00f">where</span> T : <span style="color:#00f">class</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> T <span style="color:#00f">value</span>;

    <span style="color:#00f">public</span> Maybe(T someValue)
    {
        <span style="color:#00f">if</span> (someValue == <span style="color:#00f">null</span>)
            <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(nameof(someValue));
        <span style="color:#00f">this</span>.<span style="color:#00f">value</span> = someValue;
    }

    <span style="color:#00f">private</span> Maybe()
    {
    }

    <span style="color:#00f">public</span> Maybe&lt;U&gt; Bind&lt;U&gt;(Func&lt;T, Maybe&lt;U&gt;&gt; func) <span style="color:#00f">where</span> U : <span style="color:#00f">class</span>
    {
        <span style="color:#00f">return</span> <span style="color:#00f">value</span> != <span style="color:#00f">null</span> ? func(<span style="color:#00f">value</span>) : Maybe&lt;U&gt;.None();
    }

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Maybe&lt;T&gt; None() =&gt; <span style="color:#00f">new</span> Maybe&lt;T&gt;();
}
</code></pre></div><p>When <code>null</code> is not allowed, any API contract gets more explicit: either you
return type <code>T</code> and it&rsquo;s always going to be filled, or you return <code>Maybe&lt;T&gt;</code>.
The client will see that <code>Maybe</code> type is used, so it will be forced to handle
the case of absent value.</p>
<p>Given an imaginary repository contract (which does something with customers and
orders):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IMaybeAwareRepository
{
    Maybe&lt;Customer&gt; GetCustomer(<span style="color:#2b91af">int</span> id);
    Maybe&lt;Address&gt; GetAddress(<span style="color:#2b91af">int</span> id);
    Maybe&lt;Order&gt; GetOrder(<span style="color:#2b91af">int</span> id);
}
</code></pre></div><p>The client can be written with <code>Bind</code> method composition, without branching,
in fluent style:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">Maybe&lt;Shipper&gt; shipperOfLastOrderOnCurrentAddress =
    repo.GetCustomer(customerId)
        .Bind(c =&gt; c.Address)
        .Bind(a =&gt; repo.GetAddress(a.Id))
        .Bind(a =&gt; a.LastOrder)
        .Bind(lo =&gt; repo.GetOrder(lo.Id))
        .Bind(o =&gt; o.Shipper);
</code></pre></div><p>As we saw above, this syntax looks very much like a LINQ query with a bunch
of <code>SelectMany</code> statements. One of the common
implementations of <code>Maybe</code> implements <code>IEnumerable</code> interface to enable
a more C#-idiomatic binding composition. Actually:</p>
<h2 id="enumerable--selectmany-is-a-monad">Enumerable + SelectMany is a Monad</h2>
<p><code>IEnumerable</code> is an interface for enumerable containers.</p>
<p>Enumerable containers can be created - thus the constructor monadic operation.</p>
<p>The <code>Bind</code> operation is defined by the standard LINQ extension method, here
is its signature:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> IEnumerable&lt;U&gt; SelectMany&lt;T, U&gt;(
    <span style="color:#00f">this</span> IEnumerable&lt;T&gt; first,
    Func&lt;T, IEnumerable&lt;U&gt;&gt; selector)
</code></pre></div><p>Direct implementation is quite straightforward:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Enumerable</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> IEnumerable&lt;U&gt; SelectMany(
        <span style="color:#00f">this</span> IEnumerable&lt;T&gt; values,
        Func&lt;T, IEnumerable&lt;U&gt;&gt; func)
    {
        <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> item <span style="color:#00f">in</span> values)
            <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> subItem <span style="color:#00f">in</span> func(item))
                <span style="color:#00f">yield</span> <span style="color:#00f">return</span> subItem;
    }
}
</code></pre></div><p>And here is an example of composition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">IEnumerable&lt;Shipper&gt; shippers =
    customers
        .SelectMany(c =&gt; c.Addresses)
        .SelectMany(a =&gt; a.Orders)
        .SelectMany(o =&gt; o.Shippers);
</code></pre></div><p>The query has no idea about how the collections are stored (encapsulated in
containers). We use functions <code>T -&gt; IEnumerable&lt;U&gt;</code> to produce new enumerables
(<code>Bind</code> operation).</p>
<h2 id="task-future">Task (Future)</h2>
<p>In C# <code>Task&lt;T&gt;</code> type is used to denote asynchronous computation which will eventually
return an instance of <code>T</code>. The other names for similar concepts in other languages
are <code>Promise</code> and <code>Future</code>.</p>
<p>While the typical usage of <code>Task</code> in C# is different from the Monad pattern we
discussed, I can still come up with a <code>Future</code> class with the familiar structure:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Future</span>&lt;T&gt;
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> Task&lt;T&gt; instance;

    <span style="color:#00f">public</span> Future(T instance)
    {
        <span style="color:#00f">this</span>.instance = Task.FromResult(instance);
    }

    <span style="color:#00f">private</span> Future(Task&lt;T&gt; instance)
    {
        <span style="color:#00f">this</span>.instance = instance;
    }

    <span style="color:#00f">public</span> Future&lt;U&gt; Bind&lt;U&gt;(Func&lt;T, Future&lt;U&gt;&gt; func)
    {
        <span style="color:#2b91af">var</span> a = <span style="color:#00f">this</span>.instance.ContinueWith(t =&gt; func(t.Result).instance).Unwrap();
        <span style="color:#00f">return</span> <span style="color:#00f">new</span> Future&lt;U&gt;(a);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">void</span> OnComplete(Action&lt;T&gt; action)
    {
        <span style="color:#00f">this</span>.instance.ContinueWith(t =&gt; action(t.Result));
    }
}
</code></pre></div><p>Effectively, it&rsquo;s just a wrapper around the <code>Task</code> which doesn&rsquo;t add too much value,
but it&rsquo;s a useful illustration because now we can do:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">repository
    .LoadSpeaker()
    .Bind(speaker =&gt; speaker.NextTalk())
    .Bind(talk =&gt; talk.GetConference())
    .Bind(conference =&gt; conference.GetCity())
    .OnComplete(city =&gt; reservations.BookFlight(city));
</code></pre></div><p>We are back to the familiar structure. Time for some more complications.</p>
<h2 id="non-sequential-workflows">Non-Sequential Workflows</h2>
<p>Up until now, all the composed workflows had very liniar, sequential
structure: the output of a previous step was always the input for the next step.
That piece of data could be discarded after the first use because it was never needed
for later steps:</p>
<p><img src="linear-workflow.png" alt="Linear Workflow"></p>
<p>Quite often though, this might not be the case. A workflow step might need data
from two or more previous steps combined.</p>
<p>In the example above, <code>BookFlight</code> method might actually need both <code>Speaker</code> and
<code>City</code> objects:</p>
<p><img src="non-linear-workflow.png" alt="Non Linear Workflow"></p>
<p>In this case, we would have to use closure to save <code>speaker</code> object until we get
a <code>talk</code> too:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">repository
    .LoadSpeaker()
    .OnComplete(speaker =&gt;
        speaker
            .NextTalk()
            .Bind(talk =&gt; talk.GetConference())
            .Bind(conference =&gt; conference.GetCity())
            .OnComplete(city =&gt; reservations.BookFlight(speaker, city))
        );
</code></pre></div><p>Obviously, this gets ugly very soon.</p>
<p>To solve this structural problem, C# language got its <code>async</code>-<code>await</code> feature,
which is now being reused in more languages including Javascript.</p>
<p>If we move back to using <code>Task</code> instead of our custom <code>Future</code>, we are able to
write</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> speaker = <span style="color:#00f">await</span> repository.LoadSpeaker();
<span style="color:#2b91af">var</span> talk = <span style="color:#00f">await</span> speaker.NextTalk();
<span style="color:#2b91af">var</span> conference = <span style="color:#00f">await</span> talk.GetConference();
<span style="color:#2b91af">var</span> city = <span style="color:#00f">await</span> conference.GetCity();
<span style="color:#00f">await</span> reservations.BookFlight(speaker, city);
</code></pre></div><p>Even though we lost the fluent syntax, at least the block has just one level,
which makes it easier to navigate.</p>
<h2 id="monads-in-functional-languages">Monads in Functional Languages</h2>
<p>So far we learned that</p>
<ul>
<li>Monad is a workflow composition pattern</li>
<li>This pattern is used in functional programming</li>
<li>Special syntax helps simplify the usage</li>
</ul>
<p>It should come at no surprise that functional languages support monads on syntactic
level.</p>
<p>F# is a functional-first language running on .NET framework. F# had its own way of
doing workflows comparable to <code>async</code>-<code>await</code> before C# got it. In F#, the above
code would look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> sendReservation () = async {
    <span style="color:#00f">let!</span> speaker = repository.LoadSpeaker()
    <span style="color:#00f">let!</span> talk = speaker.nextTalk()
    <span style="color:#00f">let!</span> conf = talk.getConference()
    <span style="color:#00f">let!</span> city = conf.getCity()
    <span style="color:#00f">do</span>! bookFlight(speaker, city)
}
</code></pre></div><p>Apart from syntax (<code>!</code> instead of <code>await</code>), the major difference to C# is that
<code>async</code> is just one possible monad type to be used this way. There are many
other monads in F# standard library (they are called Computation Expressions).</p>
<p>The best part is that any developer can create their own monads, and then use
all the power of language features.</p>
<p>Say, we want a hand-made <code>Maybe</code> computation expressoin in F#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> nextTalkCity (speaker: Speaker) = maybe {
    <span style="color:#00f">let!</span> talk = speaker.nextTalk()
    <span style="color:#00f">let!</span> conf = talk.getConference()
    <span style="color:#00f">let!</span> city = conf.getCity(talk)
    <span style="color:#00f">return</span> city
}
</code></pre></div><p>To make this code runnable, we need to define Maybe computation expression
builder:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">MaybeBuilder</span>() =

    <span style="color:#00f">member</span> this.Bind(x, f) =
        <span style="color:#00f">match</span> x <span style="color:#00f">with</span>
        | None -&gt; None
        | Some a -&gt; f a

    <span style="color:#00f">member</span> this.Return(x) =
        Some x

<span style="color:#00f">let</span> maybe = <span style="color:#00f">new</span> MaybeBuilder()
</code></pre></div><p>I won&rsquo;t explain the details of what happens here, but you can see that the code is
quite trivial. Note the presence of <code>Bind</code> operation (and <code>Return</code> operation being
the monad constructor).</p>
<p>The feature is widely used by third-party F# libraries. Here is an actor definition
in Akka.NET F# API:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> loop () = actor {
    <span style="color:#00f">let!</span> message = mailbox.Receive()
    <span style="color:#00f">match</span> message <span style="color:#00f">with</span>
    | Greet(name) -&gt; printfn <span style="color:#a31515">&#34;Hello %s&#34;</span> name
    | Hi -&gt; printfn <span style="color:#a31515">&#34;Hello from F#!&#34;</span>
    <span style="color:#00f">return</span>! loop ()
}
</code></pre></div><h2 id="monad-laws">Monad Laws</h2>
<p>There are a couple laws that constructor and <code>Bind</code> need to adhere to, so
that they produce a proper monad.</p>
<p>A typical monad tutorial will make a lot of emphasis on the laws, but I find them
less important to explain to a beginner. Nonetheless, here they are for the sake
of completeness.</p>
<p><strong>Left Identity law</strong> says that Monad constructor is a neutral operation: you can safely
run it before <code>Bind</code>, and it won&rsquo;t change the result of the function call:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Given
</span><span style="color:#008000"></span>T <span style="color:#00f">value</span>;
Func&lt;T, Monad&lt;U&gt;&gt; f;

<span style="color:#008000">// Then (== means both parts are equivalent)
</span><span style="color:#008000"></span><span style="color:#00f">new</span> Monad&lt;T&gt;(<span style="color:#00f">value</span>).Bind(f) == f(<span style="color:#00f">value</span>)
</code></pre></div><p><strong>Right Identity law</strong> says that given a monadic value, wrapping its contained data
into another monad of same type and then <code>Bind</code>ing it, doesn&rsquo;t change the original value:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Given
</span><span style="color:#008000"></span>Monad&lt;T&gt; monadicValue;

<span style="color:#008000">// Then (== means both parts are equivalent)
</span><span style="color:#008000"></span>monadicValue.Bind(x =&gt; <span style="color:#00f">new</span> Monad&lt;T&gt;(x)) == monadicValue
</code></pre></div><p><strong>Associativity law</strong> means that the order in which <code>Bind</code> operations
are composed does not matter:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Given
</span><span style="color:#008000"></span>Monad&lt;T&gt; m;
Func&lt;T, Monad&lt;U&gt;&gt; f;
Func&lt;U, Monad&lt;V&gt;&gt; g;

<span style="color:#008000">// Then (== means both parts are equivalent)
</span><span style="color:#008000"></span>m.Bind(f).Bind(g) == m.Bind(a =&gt; f(a).Bind(g))
</code></pre></div><p>The laws may look complicated, but in fact they are very natural
expectations that any developer has when working with monads, so don&rsquo;t
spend too much mental effort on memorizing them.</p>
<h2 id="conclusion">Conclusion</h2>
<p>You should not be afraid of the &ldquo;M-word&rdquo; just because you are a C# programmer.</p>
<p>C# does not have a notion of monads as predefined language constructs, but
that doesn&rsquo;t mean we can&rsquo;t borrow some ideas from the functional world. Having
said that, it&rsquo;s also true that C# is lacking some powerful ways to combine
and generalize monads that are available in functional programming
languages.</p>
<p>Go learn some more Functional Programming!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/monads" term="monads" label="Monads" />
                             
                                <category scheme="https://mikhail.io/tags/linq" term="linq" label="LINQ" />
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Programmable Cloud: Provisioning Azure App Service with Pulumi]]></title>
            <link href="https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi/"/>
            <id>https://mikhail.io/2018/06/programmable-cloud-provisioning-azure-app-service-with-pulumi/</id>
            
            <published>2018-06-22T00:00:00+00:00</published>
            <updated>2018-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Exploring Infrastructure-as-Code approach suggested by Pulumi with examples around provisioning Azure App Service</blockquote><p>Modern Cloud providers offer a wide variety of services of different types
and levels. A modern cloud application would leverage multiple services
in order to be efficient in terms of developer experience, price, operations
etc.</p>
<p>For instance, a very simple Web Application deployed to Azure PaaS services
could use</p>
<ul>
<li>App Service - to host the application</li>
<li>App Service Plan - to define the instance size, price, scaling and other
hosting parameters</li>
<li>Azure SQL Database - to store relational data</li>
<li>Application Insights - to collect telemetry and logs</li>
<li>Storage Account - to store the binaries and leverage Run-as-Zip feature</li>
</ul>
<p>Provisioning such environment becomes a task on its own:</p>
<ul>
<li>How do we create the initial setup?</li>
<li>How do we make changes?</li>
<li>What if we need multiple environments?</li>
<li>How do we apply settings?</li>
<li>How do we recycle resources which aren&rsquo;t needed anymore?</li>
</ul>
<p>Well, there are several options.</p>
<h2 id="manually-in-azure-portal">Manually in Azure Portal</h2>
<p>We all start doing this in Azure Portal. User Interface is great for
discovering new services and features, and it&rsquo;s a quick way to make a single
change.</p>
<figure >
    
        <img src="azureportal.png"
            alt="Creating an App Service in Azure Portal"
             />
        
    
    <figcaption>
        <h4>Creating an App Service in Azure Portal</h4>
    </figcaption>
    
</figure>
<p>Clicking buttons manually doesn&rsquo;t scale though. After the initial setup is
complete, maintaining the environment over time poses significant challenges:</p>
<ul>
<li>Every change requires going back to the portal, finding the right resource
and doing the right change</li>
<li>People make mistakes, so if you have multiple environments, they are likely
to be different in subtle ways</li>
<li>Naming gets messy over time</li>
<li>There is no easily accessible history of environment changes</li>
<li>Cleaning up is hard: usually some leftovers will remain unnoticed</li>
<li>Skills are required from everybody involved in provisioning</li>
</ul>
<p>So, how do we streamline this process?</p>
<h2 id="azure-powershell-cli-and-management-sdks">Azure PowerShell, CLI and Management SDKs</h2>
<p>Azure comes with a powerful set of tools to manage resources with code.</p>
<p>You can use PowerShell, CLI scripts or custom code like C# to do with code
whatever is possible to do via portal.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> webApp = azure.WebApps.Define(appName)
    .WithRegion(Region.WestEurope)
    .WithNewResourceGroup(rgName)
    .WithNewFreeAppServicePlan()
    .Create();
</code></pre></div><p><em>Fluent C# code creating an App Service</em></p>
<p>However, those commands are usually expressed in imperative style of
CRUD operations. You can run the commands once, but it&rsquo;s hard to modify
existing resources from an arbitrary state to the desired end state.</p>
<h2 id="azure-resource-manager-templates">Azure Resource Manager Templates</h2>
<p>All services in Azure are managed by Azure Resource Manager (ARM). ARM
has a special JSON-based format for templates.</p>
<p>Once a template is defined,
it&rsquo;s relatively straightforward to be deployed to Azure environment. So, if
resources are defined in JSON, they will be created automatically via
PowerShell or CLI commands.</p>
<p>It is also possible to deploy templates in incremental mode, when the tool
will compare existing environment with desired configuration and will deploy
the difference.</p>
<p>Templates can be parametrized, which enables multi-environment deployments.</p>
<p>There&rsquo;s a problem with templates though: they are JSON files. They get
very large very fast, they are hard to reuse, it&rsquo;s easy to make a typo.</p>
<figure >
    
        <img src="armtemplate.png"
            alt="A fragment of auto-generated ARM Template for App Service, note the line numbers"
             />
        
    
    <figcaption>
        <h4>A fragment of auto-generated ARM Template for App Service, note the line numbers</h4>
    </figcaption>
    
</figure>
<p>Terraform is another templating tool to provision cloud resources but it uses
YAML instead of JSON. I don&rsquo;t have much experience with it, but the problems
seem to be very similar.</p>
<p>Can we combine the power of SDKs and the power of JSON-/YAML-based desired state
configuration tools?</p>
<h2 id="pulumi">Pulumi</h2>
<p>One potential solution has just arrived.
A startup called Pulumi <a href="http://joeduffyblog.com/2018/06/18/hello-pulumi/">just went out of private beta to open source</a>.</p>
<p><img src="pulumi.jpg" alt="Pulumi"></p>
<p>Pulumi wants to be much more than a better version of ARM templates, aiming
to become the tool to build cloud-first distributed systems. But for today I&rsquo;ll
focus on lower level of resource provisioning task.</p>
<p>With Pulumi cloud infrastructure is defined in code using full-blown general
purpose programming languages.</p>
<p>The workflow goes like this:</p>
<ul>
<li>Define a Stack, which is a container for a group of related resources</li>
<li>Write a program in one of supported languages (I&rsquo;ll use TypeScript) which
references <code>pulumi</code> libraries and constructs all the resources as objects</li>
<li>Establish connection with your Azure account</li>
<li>Call <code>pulumi</code> CLI to create, update or destroy Azure resources based on
the program</li>
<li>Pulumi will first show the preview of changes, and then apply them as
requested</li>
</ul>
<h2 id="pulumi-program">Pulumi Program</h2>
<p>I&rsquo;m using TypeScript to define my Azure resources in Pulumi. So, the program
is a normal Node.js application with <code>index.ts</code> file, package references in
<code>package.json</code> and one extra file <code>Pulumi.yaml</code> to define the program:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">name: azure-appservice
runtime: nodejs
</code></pre></div><p>Our <code>index.js</code> is as simple as a bunch of <code>import</code> statements followed by
creating TypeScript objects per desired resource. The simplest program can
look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> pulumi <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/pulumi&#34;</span>;
<span style="color:#00f">import</span> * <span style="color:#00f">as</span> azure <span style="color:#00f">from</span> <span style="color:#a31515">&#34;@pulumi/azure&#34;</span>;

<span style="color:#00f">const</span> resourceGroup = <span style="color:#00f">new</span> azure.core.ResourceGroup(<span style="color:#a31515">&#34;myrg&#34;</span>, {
    location: <span style="color:#a31515">&#34;West Europe&#34;</span>
});
</code></pre></div><p>When executed by <code>pulumi update</code> command, this program will create a new
Resource Group in your Azure subscription.</p>
<h2 id="chaining-resources">Chaining Resources</h2>
<p>When multiple resources are created, the properties of one resource will
depend on properties of the others. E.g. I&rsquo;ve defined the Resource Group
above, and now I want to create an App Service Plan under this Group:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> resourceGroupArgs = {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    location: <span style="color:#2b91af">resourceGroup.location</span>
};

<span style="color:#00f">const</span> appServicePlan = <span style="color:#00f">new</span> azure.appservice.Plan(<span style="color:#a31515">&#34;myplan&#34;</span>, {
    ...resourceGroupArgs,

    kind: <span style="color:#a31515">&#34;App&#34;</span>,

    sku: {
        tier: <span style="color:#a31515">&#34;Basic&#34;</span>,
        size: <span style="color:#a31515">&#34;B1&#34;</span>,
    },
});
</code></pre></div><p>I&rsquo;ve assigned <code>resourceGroupName</code> and <code>location</code> of App Service Plan to
values from the Resource Group. It looks like a simple assignment of
strings but in fact it&rsquo;s more complicated.</p>
<p>Property <code>resourceGroup.name</code> has the type of <code>pulumi.Output&lt;string&gt;</code>.
Constructor argument <code>resourceGroupName</code> of <code>Plan</code> has the type of
<code>pulumi.Input&lt;string&gt;</code>.</p>
<p>We assigned <code>&quot;myrg&quot;</code> value to Resource Group name, but during the actual
deployment it will change. Pulumi will append a unique identifier to the name,
so the actually provisioned group will be named e.g. <code>&quot;myrg65fb103e&quot;</code>.</p>
<p>This value will materialize inside <code>Output</code> type only at deployment time,
and then it will get propagated to <code>Input</code> by Pulumi.</p>
<p>There is also a nice way to return the end values of <code>Output</code>&rsquo;s from Pulumi
program. Let&rsquo;s say we define an App Service:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;mywebsite&#34;</span>, {
    ...resourceGroupArgs,

    appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>
});
</code></pre></div><p>First, notice how we used TypeScript spread operator to reuse
properties from <code>resourceGroupArgs</code>.</p>
<p>Second, <code>Output</code>-<code>Input</code> assignment got used again to propagate App Service
Plan ID.</p>
<p>Lastly, we can now export App Service host name from our program, e.g.
for the user to be able to go to the web site immediately after deployment:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts">exports.hostname = app.defaultSiteHostname;
</code></pre></div><p><code>Output</code> can also be transformed with <code>apply</code> function. Here is the code to
format output URL:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts">exports.endpoint = app.defaultSiteHostname.apply(n =&gt; <span style="color:#a31515">`https://</span><span style="color:#a31515">${</span>n<span style="color:#a31515">}</span><span style="color:#a31515">`</span>);
</code></pre></div><p>Running <code>pulumi update</code> from CLI will then print the endpoint for us:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">---outputs:---
endpoint: <span style="color:#a31515">&#34;https://mywebsiteb76260b5.azurewebsites.net&#34;</span>
</code></pre></div><p>Multiple outputs can be combined with <code>pulumi.all</code>, e.g. given SQL Server
and Database, we could make a connection string:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> connectionString =
    pulumi.all([sqlServer, database]).apply(([server, db]) =&gt;
        <span style="color:#a31515">`Server=tcp:</span><span style="color:#a31515">${</span>server<span style="color:#a31515">}</span><span style="color:#a31515">.database.windows.net;initial catalog=</span><span style="color:#a31515">${</span>db<span style="color:#a31515">}</span><span style="color:#a31515">;user ID=</span><span style="color:#a31515">${</span>username<span style="color:#a31515">}</span><span style="color:#a31515">;password=</span><span style="color:#a31515">${</span>pwd<span style="color:#a31515">}</span><span style="color:#a31515">;Min Pool Size=0;Max Pool Size=30;Persist Security Info=true;`</span>)
</code></pre></div><h2 id="using-the-power-of-npm">Using the Power of NPM</h2>
<p>Since our program is just a TypeScript application, we are free to use any
3rd party package which exists out there in NPM.</p>
<p>For instance, we can install Azure Storage SDK. Just</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">npm install azure-storage@2.9.0-preview
</code></pre></div><p>and then we can write a function to produce SAS token for a Blob in Azure
Storage:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> * <span style="color:#00f">as</span> azurestorage <span style="color:#00f">from</span> <span style="color:#a31515">&#34;azure-storage&#34;</span>;

<span style="color:#008000">// Given an Azure blob, create a SAS URL that can read it.
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">function</span> signedBlobReadUrl(
    blob: <span style="color:#2b91af">azure.storage.Blob</span> | azure.storage.ZipBlob,
    account: <span style="color:#2b91af">azure.storage.Account</span>,
    container: <span style="color:#2b91af">azure.storage.Container</span>,
): pulumi.Output&lt;string&gt; {
    <span style="color:#00f">const</span> signatureExpiration = <span style="color:#00f">new</span> Date(2100, 1);

    <span style="color:#00f">return</span> pulumi.all([
        account.primaryConnectionString,
        container.name,
        blob.name,
    ]).apply(([connectionString, containerName, blobName]) =&gt; {
        <span style="color:#00f">let</span> blobService = <span style="color:#00f">new</span> azurestorage.BlobService(connectionString);
        <span style="color:#00f">let</span> signature = blobService.generateSharedAccessSignature(
            containerName,
            blobName,
            {
                AccessPolicy: {
                    Expiry: <span style="color:#2b91af">signatureExpiration</span>,
                    Permissions: <span style="color:#2b91af">azurestorage.BlobUtilities.SharedAccessPermissions.READ</span>,
                },
            }
        );

        <span style="color:#00f">return</span> blobService.getUrl(containerName, blobName, signature);
    });
}
</code></pre></div><p>I took this function from <a href="https://github.com/pulumi/examples/tree/master/azure-ts-functions">Azure Functions</a>
example, and it will probably move to Pulumi libraries at some point, but until
then you are free to leverage the package ecosystem.</p>
<h2 id="deploying-application-files">Deploying Application Files</h2>
<p>So far we provisioned Azure App Service, but we can also deploy the application
files as part of the same workflow.</p>
<p>The code below is using <a href="https://github.com/Azure/app-service-announcements/issues/84">Run from Zip</a>
feature of App Service:</p>
<ol>
<li>
<p>Define Storage Account and Container</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> storageAccount = <span style="color:#00f">new</span> azure.storage.Account(<span style="color:#a31515">&#34;mystorage&#34;</span>, {
    ...resourceGroupArgs,

    accountKind: <span style="color:#a31515">&#34;StorageV2&#34;</span>,
    accountTier: <span style="color:#a31515">&#34;Standard&#34;</span>,
    accountReplicationType: <span style="color:#a31515">&#34;LRS&#34;</span>,
});

<span style="color:#00f">const</span> storageContainer = <span style="color:#00f">new</span> azure.storage.Container(<span style="color:#a31515">&#34;mycontainer&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
    containerAccessType: <span style="color:#a31515">&#34;private&#34;</span>,
});
</code></pre></div></li>
<li>
<p>Create a folder with application files, e.g. <code>wwwroot</code>. It may contain
some test HTML, ASP.NET application, or anything supported by App Service.</p>
</li>
<li>
<p>Produce a zip file from that folder in Pulumi program:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> blob = <span style="color:#00f">new</span> azure.storage.ZipBlob(<span style="color:#a31515">&#34;myzip&#34;</span>, {
    resourceGroupName: <span style="color:#2b91af">resourceGroup.name</span>,
    storageAccountName: <span style="color:#2b91af">storageAccount.name</span>,
    storageContainerName: <span style="color:#2b91af">storageContainer.name</span>,
    <span style="color:#00f">type</span>: <span style="color:#a31515">&#34;block&#34;</span>,

    content: <span style="color:#2b91af">new</span> pulumi.asset.FileArchive(<span style="color:#a31515">&#34;wwwroot&#34;</span>)
});
</code></pre></div></li>
<li>
<p>Produce SAS Blob URL and assign it to App Service Run-as-Zip setting:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">const</span> codeBlobUrl = signedBlobReadUrl(blob, storageAccount, storageContainer);

<span style="color:#00f">const</span> app = <span style="color:#00f">new</span> azure.appservice.AppService(<span style="color:#a31515">&#34;mywebsite&#34;</span>, {
    ...resourceGroupArgs,

    appServicePlanId: <span style="color:#2b91af">appServicePlan.id</span>,

    appSettings: {
        <span style="color:#a31515">&#34;WEBSITE_RUN_FROM_ZIP&#34;</span>: codeBlobUrl
    }
});
</code></pre></div></li>
</ol>
<p>Run the program, and your Application will start as soon as <code>pulumi update</code>
is complete.</p>
<h2 id="determinism">Determinism</h2>
<p>Pulumi programs should strive to be deterministic.
That means you should avoid using things like current date/time or random numbers.</p>
<p>The reason is incremental updates. Every time you run <code>pulumi update</code>, it
will execute the program from scratch. If your resources depend on random
values, they will not match the existing resources and thus the false
delta will be detected and deployed.</p>
<p>In the SAS generation example above we used a fixed date in the future
instead of doing today + 1 year kind of calculation.</p>
<p>Should Pulumi provide some workaround for this?</p>
<h2 id="conclusion">Conclusion</h2>
<p>My code was kindly merged to
<a href="https://github.com/pulumi/examples/tree/master/azure-ts-appservice">Pulumi examples</a>,
go there for the complete runnable program that provisions App Service with
Azure SQL Database and Application Insights.</p>
<p>I really see high potential in Cloud-as-Code approach suggested by Pulumi.
Today we just scratched the surface of the possibilities. We were working
with cloud services on raw level: provisioning specific services with
given parameters.</p>
<p>Pulumi&rsquo;s vision includes providing higher-level components to blur the line
between infrastructure and code, and to enable everybody to create such
components on their own.</p>
<p>Exciting future ahead!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/pulumi" term="pulumi" label="Pulumi" />
                             
                                <category scheme="https://mikhail.io/tags/azure-app-service" term="azure-app-service" label="Azure App Service" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Cold Starts Beyond First Request in Azure Functions]]></title>
            <link href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/"/>
            <id>https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/</id>
            
            <published>2018-05-18T00:00:00+00:00</published>
            <updated>2018-05-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Can we avoid cold starts by keeping Functions warm, and will cold starts occur on scale out? Let&rsquo;s try!</blockquote><p>In my <a href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/">previous article</a>
I&rsquo;ve explored the topic of Cold Starts in Azure Functions. Particularly, I&rsquo;ve measured the
cold start delays per language and runtime version.</p>
<p>I received some follow-up questions that I&rsquo;d like to explore in today&rsquo;s post:</p>
<ul>
<li>Can we avoid cold starts except the very first one by keeping the instance warm?</li>
<li>Given one warm instance, if two requests come at the same time, will one request hit
a cold start because existing instance is busy with the other?</li>
<li>In general, does a cold start happen at scale-out when a new extra instance is provisioned?</li>
</ul>
<p>Again, we are only talking Consumption Plan here.</p>
<h2 id="theory">Theory</h2>
<p>Azure Functions are running on instances provided by Azure App Service. Each instance is
able to process several requests concurrently, which is different comparing to AWS Lambda.</p>
<p>Thus, the following <em>could</em> be true:</p>
<ul>
<li>If we issue at least 1 request every 20 minutes, the first instance should stay warm for
long time</li>
<li>Simultaneous requests don&rsquo;t cause cold start unless the existing instance gets too busy</li>
<li>When runtime decides to scale out and spin up a new instance, it could do so in the background,
still forwarding incoming requests to the existing warm instance(s). Once the new instance
is ready, it could be added to the pool without causing cold starts</li>
<li>If so, cold starts are mitigated beyond the very first execution</li>
</ul>
<p>Let&rsquo;s put this theory under test!</p>
<h2 id="keeping-always-warm">Keeping Always Warm</h2>
<p>I&rsquo;ve tested a Function App which consists of two Functions:</p>
<ul>
<li>HTTP Function under test</li>
<li>Timer Function which runs every 10 minutes and does nothing but logging 1 line of text</li>
</ul>
<p>I then measured the cold start statistics similar to all the tests from my previous article.</p>
<p>During 2 days I was issuing infrequent requests to the same app, most of them would normally
lead to a cold start. Interestingly, even though I was regularly firing the timer, Azure
switched instances to serve my application 2 times during the test period:</p>
<p><img src="cold-starts-keep-warm.png" alt="Infrequent Requests to Azure Functions with &ldquo;Keep It Warm&rdquo; Timer"></p>
<p>I can see that most responses are fast, so timer &ldquo;warmer&rdquo; definitely helps.</p>
<p>The first request(s) to a new instance are slower than subsequent ones. Still, they are faster
than normal full cold start time, so it could be related to HTTP stack loading.</p>
<p>Anyway, keeping Functions warm seems a viable strategy.</p>
<h2 id="parallel-requests">Parallel Requests</h2>
<p>What happens when there is a warm instance, but it&rsquo;s already busy with processing another
request? Will the parallel request be delayed, or will it be processed by the same
warm instance?</p>
<p>I tested with a very lightweight function, which nevertheless takes some time to complete:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;HttpResponseMessage&gt; Delay500([HttpTrigger] HttpRequestMessage req)
{
    <span style="color:#00f">await</span> Task.Delay(500);
    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK, <span style="color:#a31515">&#34;Done&#34;</span>);
}
</code></pre></div><p>I believe it&rsquo;s an OK approximation for an IO-bound function.</p>
<p>The test client then issued 2 to 10 parallel requests to this function and measured the
response time for all requests.</p>
<p>It&rsquo;s not the easiest chart to understand in full, but note the following:</p>
<ul>
<li>
<p>Each group of bars are for requests sent at the same time. Then there goes a pause about
20 seconds before the next group of requests gets sent</p>
</li>
<li>
<p>The bars are colored by the instance which processed that request: same instance - same
color</p>
</li>
</ul>
<p><img src="cold-starts-during-simultaneous-requests.png" alt="Azure Functions Response Time to Batches of Simultaneous Requests"></p>
<p>Here are some observations from this experiment:</p>
<ul>
<li>
<p>Out of 64 requests, there were 11 cold starts</p>
</li>
<li>
<p>Same instance <em>can</em> process multiple simultaneous requests, e.g. one instance processed
7 out of 10 requests in the last batch</p>
</li>
<li>
<p>Nonetheless, Azure is eager to spin up new instances for multiple requests. In total
12 instances were created, which is even more than max amount of requests in any single
batch</p>
</li>
<li>
<p>Some of those instances were actually never reused (gray-ish bars in batched x2 and x3,
brown bar in x10)</p>
</li>
<li>
<p>The first request to each new instance pays the full cold start price. Runtime doesn&rsquo;t
provision them in background while reusing existing instances for received requests</p>
</li>
<li>
<p>If an instance handled more than one request at a time, response time invariably suffers,
even though the function is super lightweight (<code>Task.Delay</code>)</p>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Getting back to the experiment goals, there are several things that we learned.</p>
<p>For low-traffic apps with sporadic requests it makes sense to setup a &ldquo;warmer&rdquo; timer
function firing every 10 minutes or so to prevent the only instance from being recycled.</p>
<p>However, scale-out cold starts are real and I don&rsquo;t see any way to prevent them from
happening.</p>
<p>When multiple requests come in at the same time, we might expect some of them to hit
a new instance and get slowed down. The exact algorithm of instance reuse is not
entirely clear.</p>
<p>Same instance is capable of processing multiple requests in parallel, so there are
possibilities for optimization in terms of routing to warm instances during the
provisioning of cold ones.</p>
<p>If such optimizations happen, I&rsquo;ll be glad to re-run my tests and report any noticeable
improvements.</p>
<p>Stay tuned for more serverless perf goodness!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions: Cold Starts in Numbers]]></title>
            <link href="https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/"/>
            <id>https://mikhail.io/2018/04/azure-functions-cold-starts-in-numbers/</id>
            
            <published>2018-04-24T00:00:00+00:00</published>
            <updated>2018-04-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Auto-provisioning and auto-scalability are the killer features of Function-as-a-Service
cloud offerings, and Azure Functions in particular.</p>
<p>One drawback of such dynamic provisioning is a phenomenon called &ldquo;Cold Start&rdquo;. Basically,
applications that haven&rsquo;t been used for a while take longer to startup and to handle the
first request.</p>
<p>The problem is nicely described in
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2018/02/07/understanding-serverless-cold-start/">Understanding Serverless Cold Start</a>,
so I won&rsquo;t repeat it here. I&rsquo;ll just copy a picture from that article:</p>
<p><img src="coldstart.jpg" alt="Cold Start"></p>
<p>Based on the 4 actions which happen during a cold start, we may guess that the following factors
might affect the cold start duration:</p>
<ul>
<li>Language / execution runtime</li>
<li>Azure Functions runtime version</li>
<li>Application size including dependencies</li>
</ul>
<p>I ran several sample functions and tried to analyze the impact of these factors on cold start time.</p>
<h2 id="methodology">Methodology</h2>
<p>All tests were run against HTTP Functions, because that&rsquo;s where cold start matters the most.</p>
<p>All the functions were just returning &ldquo;Hello, World&rdquo; taking the &ldquo;World&rdquo; value from the query string.
Some functions were also loading extra dependencies, see below.</p>
<p>I did not rely on execution time reported by Azure. Instead, I measured end-to-end duration from
client perspective. All calls were made from within the same Azure region, so network latency should
have minimal impact:</p>
<p><img src="test-setup.png" alt="Test Setup"></p>
<h2 id="when-does-cold-start-happen">When Does Cold Start Happen?</h2>
<p>Obviously, cold start happens when the very first request comes in. After that request is processed,
the instance is kept alive in case subsequent requests arrive. But for how long?</p>
<p>The following chart gives the answer. It shows values of normalized request durations across
different languages and runtime versions (Y axis) depending on the time since the previous
request in minutes (X axis):</p>
<p><img src="coldstart-threshold.png" alt="Cold Start Threshold"></p>
<p>Clearly, an idle instance lives for 20 minutes and then gets recycled. All requests after 20 minutes
threshold hit another cold start.</p>
<h2 id="how-do-languages-compare">How Do Languages Compare?</h2>
<p>I&rsquo;ll start with version 1 of Functions runtime, which is the production-ready GA version as of today.</p>
<p>I&rsquo;ve written Hello World HTTP function in all GA languages: C#, F# and Javascript, and I added Python
for comparison. C#/F# were executed both in the form of script, and as a precompiled .NET assembly.</p>
<p>The following chart shows some intuition about the cold start duration per language. The languages
are ordered based on mean response time, from lowest to highest. 65% of request
durations are inside the vertical bar (1-sigma interval) and 95% are inside the vertical line (2-sigma):</p>
<p><img src="coldstarts-v1.png" alt="Cold Start V1 per Language"></p>
<p>Somewhat surprisingly, precompiled .NET is exactly on par with Javascript. Javascript &ldquo;Hello World&rdquo;
is really lightweight, so I expected it to win, but I was wrong.</p>
<p>C# Script is slower but somewhat comparable. F# Script presented a really negative surprise though: it&rsquo;s much
slower. It&rsquo;s even slower than experimental Python support where no performance optimization would
be expected at all!</p>
<h2 id="functions-runtime-v1-vs-v2">Functions Runtime: V1 vs V2</h2>
<p>Version 2 of Functions runtime is currently in preview and not suitable for production load. That
probably means they haven&rsquo;t done too much performance optimization, especially from cold start
standpoint.</p>
<p>Can we see this on the chart? We sure can:</p>
<p><img src="coldstarts-v2.png" alt="Cold Start V1 vs V2"></p>
<p>V2 is massively slower. The fastest cold starts are around 6 seconds, but the slowest can come
up to 40-50 seconds.</p>
<p>Javascript is again on-par with precompiled .NET.</p>
<p>Java is noticeably slower, even though the
deployment package is just 33kB, so I assume I didn&rsquo;t overblow it.</p>
<h2 id="does-size-matter">Does Size Matter?</h2>
<p>OK, enough of Hello World. A real-life function might be more heavy, mainly because it would
depend on other third-party libraries.</p>
<p>To simulate such scenario, I&rsquo;ve measured cold starts for a .NET function with references to
Entity Framework, Automapper, Polly and Serilog.</p>
<p>For Javascript I did the same, but referenced Bluebird, lodash and AWS SDK.</p>
<p>Here are the results:</p>
<p><img src="coldstarts-dependencies.png" alt="Cold Start Dependencies"></p>
<p>As expected, the dependencies slow the loading down. You should keep your Functions lean,
otherwise you will pay in seconds for every cold start.</p>
<p>An important note for Javascript developers: the above numbers are for Functions deployed
after <a href="https://github.com/Azure/azure-functions-pack"><code>Funcpack</code></a> preprocessor. The package
contained the single <code>js</code> file with Webpack-ed dependency tree. Without that, the mean
cold start time of the same function is 20 seconds!</p>
<h2 id="conclusions">Conclusions</h2>
<p>Here are some lessons learned from all the experiments above:</p>
<ul>
<li>Be prepared for 1-3 seconds cold starts even for the smallest Functions</li>
<li>Stay on V1 of runtime until V2 goes GA unless you don&rsquo;t care about perf</li>
<li>.NET precompiled and Javascript Functions have roughly same cold start time</li>
<li>Minimize the amount of dependencies, only bring what&rsquo;s needed</li>
</ul>
<p>Do you see anything weird or unexpected in my results? Do you need me to dig deeper on other aspects?
Please leave a comment below or ping me on twitter, and let&rsquo;s sort it all out.</p>
<p>There is a follow-up post available:
<a href="https://mikhail.io/2018/05/azure-functions-cold-starts-beyond-first-load/">Cold Starts Beyond First Request in Azure Functions</a></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/serverless" term="serverless" label="Serverless" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/cold-starts" term="cold-starts" label="Cold Starts" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Awesome F# Exchange 2018]]></title>
            <link href="https://mikhail.io/2018/04/fsharp-exchange-2018/"/>
            <id>https://mikhail.io/2018/04/fsharp-exchange-2018/</id>
            
            <published>2018-04-07T00:00:00+00:00</published>
            <updated>2018-04-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I&rsquo;m writing this post in the train to London Stensted, on my way back from F# Exchange 2018
conference.</p>
<p>F# Exchange is a yearly conference taking place in London, and 2018 edition was the first one
for me personally. I also had an honour to speak there about creating Azure Functions with
F#.</p>
<h2 id="impression">Impression</h2>
<p>F# is still relatively niche language, so the conference is not overcrowded, but that gives
it a special feeling of family gathering. There were 162 participants this year, and I have an
impression that every one of them is extremely friendly, enthusiastic and just plain awesome.</p>
<p>The conference itself had 2 tracks of 45-minute talks and 60-minute keynotes. Most talks were
of high quality, and the topics ranging from compiler internals to fun applications like
music generation, car racing and map drawing.</p>
<p>Both Don Syme, the creator of F#, and Philip Carter, F# program manager, were there and gave
keynotes, but they were careful enough not to draw too much attention on Microsoft and let
the community speak loud.</p>
<h2 id="corridor-track">Corridor Track</h2>
<p>But the talks were just a part of the story. For me, the conference started in the evening
before the first day at the speakers drinks party, and only finished at 1 a.m. after the
second day (the pubs in London are lovely).</p>
<p>I spoke to so many great people, I learnt a lot, and had fun too. I&rsquo;ve never seen so many
F# folks at the same place, and I guess there must be something about F# which attracts
the right kind of people to it.</p>
<p>And of course it&rsquo;s so much fun to meet face-to-face all those twitter, slack, github and
Channel 9 persona&rsquo;s and to see that they are actually real people :)</p>
<h2 id="my-talk">My Talk</h2>
<p>The talk I gave was called &ldquo;Azure F#unctions&rdquo;. It was not a hard-core F# talk, but people
seemed to be genuinely interested in the topic.</p>
<p>A decent amount of attendees are already familiar with Azure Functions, and many either run
them in production or plan to do so.</p>
<p>The reference version conflict problem is very well known and raises a lots of questions
or concerns. This even leads to workarounds like transpiling F# Functions to Javascript
with Fable. Yikes.</p>
<p>Durable Functions seem to be sparkling a lot of initial interest. I&rsquo;ll be definitely
spending more time to play with them, and maybe to make F# story more smooth.</p>
<p>Functions were mentioned in Philip&rsquo;s keynote as one of the important areas for F#
application, which is cool. We should spend some extra effort to make the documentation
and onboarding story as smooth as possible.</p>
<h2 id="call-to-action">Call to Action</h2>
<p>Skills Matter is the company behind the conference. Carla, Nicole and others did a great
job preparing the event; everything went smooth, informal and fun.</p>
<p>The videos are already online at <a href="https://skillsmatter.com/conferences/9419-f-sharp-exchange-2018#skillscasts">Skillscasts</a>
(requires free signup).</p>
<p><a href="https://skillsmatter.com/conferences/10869-f-sharp-exchange-2019">F# Exchange 2019</a>
super early bird tickets are for sale now and until Monday April 9, go
get one and join F# Exchange in London next year!</p>
<p>I&rsquo;m already missing you all.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/talk" term="talk" label="Talk" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Talk]]></title>
            <link href="https://mikhail.io/tags/talk/"/>
            <id>https://mikhail.io/tags/talk/</id>
            
            <published>2018-04-07T00:00:00+00:00</published>
            <updated>2018-04-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Durable Functions in F#]]></title>
            <link href="https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/"/>
            <id>https://mikhail.io/2018/02/azure-durable-functions-in-fsharp/</id>
            
            <published>2018-02-19T00:00:00+00:00</published>
            <updated>2018-02-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Azure Functions are designed for stateless, fast-to-execute,
simple actions. Typically, they are triggered by an HTTP call or a queue message,
then they read something from the storage or database and return the result
to the caller or send it to another queue. All within several seconds at most.</p>
<p>However, there exists a preview of <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview">Durable Functions</a>,
an extension that lets you write stateful functions for long-running workflows.
Here is a picture of one possible workflow from the docs:</p>
<p><img src="fan-out-fan-in.png" alt="Fan-out Fan-in Workflow"></p>
<p>Such workflows might take arbitrary time to complete. Instead of blocking and
waiting for all that period, Durable Functions use the combination of
Storage Queues and Tables to do all the work asynchronously.</p>
<p>The code still <em>feels</em> like one continuous thing because it&rsquo;s programmed
as a single orchestrator function. So, it&rsquo;s easier for a human to reason
about the functionality without the complexities of low-level communication.</p>
<p>I won&rsquo;t describe Durable Functions any further, just go read
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview">documentation</a>,
it&rsquo;s nice and clean.</p>
<h2 id="language-support">Language Support</h2>
<p>As of February 2018, Durable Functions are still in preview. That also means
that language support is limited:</p>
<blockquote>
<p>Currently C# is the only supported language for Durable Functions. This
includes orchestrator functions and activity functions. In the future,
we will add support for all languages that Azure Functions supports.</p>
</blockquote>
<p>I was a bit disappointed that F# is not an option. But actually, since
Durable Functions support precompiled .NET assembly model, pretty much
anything doable in C# can be done in F# too.</p>
<p>The goal of this post is to show that you can write Durable Functions in F#.
I used precompiled .NET Standard 2.0 F# Function App running on 2.0 preview
runtime.</p>
<h2 id="orchestration-functions">Orchestration Functions</h2>
<p>The stateful workflows are Azure Functions with a special <code>OrchestrationTrigger</code>.
Since they are asynchronous, C# code is always based on <code>Task</code> and <code>async</code>-<code>await</code>.
Here is a simple example of orchestrator in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;List&lt;<span style="color:#2b91af">string</span>&gt;&gt; Run([OrchestrationTrigger] DurableOrchestrationContext context)
{
    <span style="color:#2b91af">var</span> outputs = <span style="color:#00f">new</span> List&lt;<span style="color:#2b91af">string</span>&gt;();

    outputs.Add(<span style="color:#00f">await</span> context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Tokyo&#34;</span>));
    outputs.Add(<span style="color:#00f">await</span> context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Seattle&#34;</span>));
    outputs.Add(<span style="color:#00f">await</span> context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;London&#34;</span>));

    <span style="color:#008000">// returns [&#34;Hello Tokyo!&#34;, &#34;Hello Seattle!&#34;, &#34;Hello London!&#34;]
</span><span style="color:#008000"></span>    <span style="color:#00f">return</span> outputs;
}
</code></pre></div><p>F# has its own preferred way of doing asynchronous code based on <code>async</code>
computation expression. The direct refactoring could look something like</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = async {
  <span style="color:#00f">let!</span> hello1 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Tokyo&#34;</span>)   |&gt; Async.AwaitTask
  <span style="color:#00f">let!</span> hello2 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Seattle&#34;</span>) |&gt; Async.AwaitTask
  <span style="color:#00f">let!</span> hello3 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;London&#34;</span>)  |&gt; Async.AwaitTask
  <span style="color:#00f">return</span> [hello1; hello2; hello3]
} |&gt; Async.StartAsTask
</code></pre></div><p>That would work for a normal HTTP trigger, but it blows up for the Orchestrator
trigger because multi-threading operations are not allowed:</p>
<blockquote>
<p>Orchestrator code must never initiate any async operation except by
using the DurableOrchestrationContext API. The Durable Task Framework
executes orchestrator code on a single thread and cannot interact with
any other threads that could be scheduled by other async APIs.</p>
</blockquote>
<p>To solve this issue, we need to keep working with <code>Task</code> directly. This is
not very handy with standard F# libraries. So, I pulled an extra NuGet
package <code>TaskBuilder.fs</code> which provides a <code>task</code> computation expression.</p>
<p>The above function now looks very simple:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> Run([&lt;OrchestrationTrigger&gt;] context: DurableOrchestrationContext) = task {
  <span style="color:#00f">let!</span> hello1 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Tokyo&#34;</span>)
  <span style="color:#00f">let!</span> hello2 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;Seattle&#34;</span>)
  <span style="color:#00f">let!</span> hello3 = context.CallActivityAsync&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>, <span style="color:#a31515">&#34;London&#34;</span>)
  <span style="color:#00f">return</span> [hello1; hello2; hello3]
}
</code></pre></div><p>And the best part is that it works just fine.</p>
<p><code>SayHello</code> function is Activity trigger based, and no special effort is required
to implement it in F#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;FunctionName(<span style="color:#a31515">&#34;E1_SayHello&#34;</span>)&gt;]
<span style="color:#00f">let</span> SayHello([&lt;ActivityTrigger&gt;] name) =
  sprintf <span style="color:#a31515">&#34;Hello %s!&#34;</span> name
</code></pre></div><h2 id="more-examples">More Examples</h2>
<p>Durable Functions repository comes with
<a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/precompiled">a set of 4 samples</a>
implemented in C#. I took all of those samples and ported them over to F#.</p>
<p>You&rsquo;ve already seen the first <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/HelloSequence.fs"><code>Hello Sequence</code> sample</a>
above: the orchestrator calls the activity function 3 times and combines the
results. As simple as it looks, the function will actually run 3 times for each
execution, saving state before each subsequent call.</p>
<p>The second <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/BackupSiteContent.fs"><code>Backup Site Content</code> sample</a>
is using this persistence mechanism to run
a potentially slow workflow of copying all files from a given directory to
a backup location. It shows how multiple activities can be executed in
parallel:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> tasks = Array.map (<span style="color:#00f">fun</span> f -&gt; backupContext.CallActivityAsync&lt;<span style="color:#2b91af">int64</span>&gt;(<span style="color:#a31515">&#34;E2_CopyFileToBlob&#34;</span>, f)) files
<span style="color:#00f">let!</span> results = Task.WhenAll tasks
</code></pre></div><p>The third <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/Counter.fs"><code>Counter</code> example</a>
demos a potentially infinite actor-like workflow, where state can exist and
evolve for indefinite period of time. The key API calls are based on
<code>OrchestrationContext</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> counterState = counterContext.GetInput&lt;int&gt;()
<span style="color:#00f">let!</span> command = counterContext.WaitForExternalEvent&lt;<span style="color:#2b91af">string</span>&gt;(<span style="color:#a31515">&#34;operation&#34;</span>)
</code></pre></div><p>The final elaborate <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/12-durable/PhoneVerification.fs"><code>Phone Verification</code> workflow</a>
has several twists, like output binding for activity (<code>ICollector</code> is required
instead of C#&rsquo;s <code>out</code> parameter), third-party integration (Twilio to send SMSs),
recursive sub-function to loop through several attempts and context-based
timers for reliable timeout implementation.</p>
<p>So, if you happen to be an F# fan, you can still give Durable Functions a try.
Be sure to leave your feedback, so that the library could get even better
before going GA.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/durable-functions" term="durable-functions" label="Durable Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Durable Functions]]></title>
            <link href="https://mikhail.io/tags/durable-functions/"/>
            <id>https://mikhail.io/tags/durable-functions/</id>
            
            <published>2018-02-19T00:00:00+00:00</published>
            <updated>2018-02-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Event Hubs]]></title>
            <link href="https://mikhail.io/tags/azure-event-hubs/"/>
            <id>https://mikhail.io/tags/azure-event-hubs/</id>
            
            <published>2018-02-06T00:00:00+00:00</published>
            <updated>2018-02-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure SQL Database]]></title>
            <link href="https://mikhail.io/tags/azure-sql-database/"/>
            <id>https://mikhail.io/tags/azure-sql-database/</id>
            
            <published>2018-02-06T00:00:00+00:00</published>
            <updated>2018-02-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Load Testing Azure SQL Database by Copying Traffic from Production SQL Server]]></title>
            <link href="https://mikhail.io/2018/02/load-testing-azure-sql-database-by-copying-traffic-from-production-sql-server/"/>
            <id>https://mikhail.io/2018/02/load-testing-azure-sql-database-by-copying-traffic-from-production-sql-server/</id>
            
            <published>2018-02-06T00:00:00+00:00</published>
            <updated>2018-02-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Azure SQL Database is a managed service that provides low-maintenance SQL Server instances in the cloud.
You don&rsquo;t have to run and update VMs, or even take backups and setup failover clusters.
Microsoft will do administration for you, you just pay an hourly fee.</p>
<p>So, let&rsquo;s say you decide this value proposition is a good reason to migrate
away from your existing self-hosted SQL Server database running in production and replace
it with Azure SQL Database.</p>
<p>You do the functional testing and eventually everything works like charm. The next set of questions
is going to be related to Database performance level:</p>
<ul>
<li>Which tier / how many DTU&rsquo;s should I provision?</li>
<li>How much will it cost?</li>
<li>Will it be able to handle my current production load?</li>
</ul>
<h2 id="dtus">DTUs</h2>
<p>Even if you collect all the specs of the hardware behind your existing SQL Server, you can&rsquo;t
directly use that knowledge to choose the right Azure SQL Database size.</p>
<p>The sizes are measured in Database Transaction Units (DTUs). These are abstract units
of measure which don&rsquo;t necessarily mean much on their own. Within a given tier
(Standard / Premium), doubling the DTU amount will double the max throughput.</p>
<p>That doesn&rsquo;t really help to plan for workload migrations.</p>
<p>There are some ways to estimate the DTU requirements by measuring metrics like CPU
and IOPS on your existing server. Have a look at <a href="http://dtucalculator.azurewebsites.net/">DTU Calculator</a>:
it consists of a data collector and an online converter from metric values to DTUs.</p>
<p>While useful as a first approximation, I&rsquo;m reluctant to provision Azure SQL Database
size solely based on such estimates.</p>
<p>My answer to the problem is: Measure It!</p>
<h2 id="synthetic-tests">Synthetic Tests</h2>
<p>Go get a backup of your existing production database and Export / Import it into
Azure SQL Database. Pick the size based on your gut feel, run a load test, evaluate
the results, adjust the size, repeat.</p>
<p>If you know your workload really well, you can create a synthetic test:</p>
<ul>
<li>Create a script or scenario which resembles the real production load</li>
<li>Run it for a given period of time</li>
<li>Measure the DTU&rsquo;s consumed</li>
</ul>
<p>Unfortunately, I&rsquo;m yet to see a non-trivial database where I could manually create
such script and be reasonably sure that it reflects the reality. Most of the time
the load is consumer-driven, changes over time and heavily depends on exact query
parameter values.</p>
<p>Which brings me to the need of replaying <em>the actual production workload</em> on Azure
SQL Database.</p>
<h2 id="trace-and-replay">Trace and Replay</h2>
<p>SQL Server comes with a marvelous suite of tools refined over years of its existence.
It includes the tools to capture and replay the queries, so I started with those.</p>
<p>SQL Server Profiler has a trace template called <code>TSQL_Replay</code>:</p>
<blockquote>
<p>This template records information required to replay the trace. Use this template
to perform iterative turning, such as benchmark testing.</p>
</blockquote>
<p>This sounded like what I needed, so I ran the profiler with this template to save
a short trace.</p>
<p>Afterwards, it is possible to use the same SQL Server Profiler to replay the trace against
another target database. So the process looks like this:</p>
<p><img src="sql-trace-replay.png" alt="Replaying Traffic with SQL Server Profiler"></p>
<p>Unfortunately, this didn&rsquo;t go very well:</p>
<ul>
<li>
<p>Azure SQL Database is not supported by the tooling. The replay kind of runs, but
it throws lots of errors like reading from non-existent system tables, trying to
switch between databases and so on</p>
</li>
<li>
<p>Related or not to the previous item, but replay went terribly slow. It
seemed to slow down exponentially over time</p>
</li>
<li>
<p>The trace file itself was of huge size. Because the template tries to record pretty
much everything, tracing 5 minutes on production produced 10 GB of XML</p>
</li>
<li>
<p>Replay was not real-time: you first record, then you replay. This might not be a big
issue for many databases, but some of our queries have time parameter, and results would
change if I replay the trace 1 hour later</p>
</li>
</ul>
<p>Just to give you a rough idea, our production database-under-study is handling about
1000 RPC calls per second (mostly stored procedures).</p>
<h2 id="custom-trace--replay">Custom Trace &amp; Replay</h2>
<p>Since off-the-shelf solution didn&rsquo;t work for me, I decided to come up with my own
custom tool chain. Here is the idea:</p>
<p><img src="sql-trace-replay-event-hubs-functions.png" alt="Replaying Traffic with SQL Server Profiler"></p>
<p>There are two custom steps that I implemented:</p>
<ol>
<li>
<p>Run a console app which would host a custom trace server. The trace server
receives SQL commands and sends them to Azure Event Hubs in batches</p>
</li>
<li>
<p>Create an Azure Function application triggered by the Event Hub. Each function
call gets one SQL command to execute and runs it against Azure SQL database that
we are trying to load-test</p>
</li>
</ol>
<p>This setup worked remarkably well for me: I got the real-time replay of SQL commands
from production SQL Server to Azure SQL Database.</p>
<p>The rest of the article describes my setup so that you could reproduce it for your
workload.</p>
<h2 id="azure-sql-database">Azure SQL Database</h2>
<p>Ideally, you want your copy of the database to be as fresh as possible, so that the
query plans and results match.</p>
<p>Some ideas to accomplish this are given in
<a href="https://docs.microsoft.com/en-us/azure/sql-database/sql-database-cloud-migrate">SQL Server database migration to SQL Database in the cloud</a>.</p>
<p>Premium RS tier is great for testing, because it is much cheaper than Premium
tier, while it provides the same level of performance.</p>
<h2 id="event-hubs">Event Hubs</h2>
<p>I used Azure Event Hubs as messaging middleware between Trace Server and Replay
Function App.</p>
<p>I started with Azure Storage Queues, but the server wasn&rsquo;t able to send messages
fast enough, mostly due to lack of batching.</p>
<p>Event Hubs match naturally my choice of Azure Functions: Functions have a built-in
trigger with dynamic scaling out of the box.</p>
<p>So, I just created a new Event Hub via the portal, with 32 partitions allocated.</p>
<h2 id="trace-definition-file">Trace Definition File</h2>
<p>In order to run a custom Trace Server, you still need a trace definition file.
The built-in template <code>TSQL_Replay</code> mentioned above could work, but
it&rsquo;s subscribed to way too many events and columns.</p>
<p>Instead, I produced my own trace template with minimal selection.
To do that, open SQL Server Profiler, then navigate to <code>File -&gt; Templates -&gt; New Template</code>,
give it a name and then on <code>Events Selection</code> tab exclude everything except
exactly the commands that you want to replay.</p>
<p>We use stored procedures for pretty much everything, so my selection looked
just like this:</p>
<p><img src="sql-profiler-template.png" alt="SQL Profiler Template"></p>
<p>For the first few runs, I advise you to restrict the trace even further. Click
<code>Column Filters</code> button, select <code>TextData</code> there and set <strong>Like</strong> filter
to a single stored procedure, e.g. matching the pattern <code>%spProductList%</code>.</p>
<p>This way you can debug your whole replay chain without immediately overloading
any part of it with huge stream of commands.</p>
<p>Once done, save the <code>tdf</code> file to disk. An example of such trace definition file
can be found in <a href="https://github.com/mikhailshilkov/sql-trace-replay/tree/master/TDF">my github</a>.</p>
<h2 id="trace-server">Trace Server</h2>
<p>My trace server is a simple C# console application.</p>
<p>Create a new console app and reference a NuGet package <code>Microsoft.SqlServer.SqlManagementObjects</code>.
Mine is of version <code>140.17218.0</code> (latest as of today).</p>
<p>Unfortunately, this NuGet package is not fully self-contained. In order to run
a profiling session, you have to install SQL Server Profiler tool on the machine
where you want to run the trace server.</p>
<p>Chances are that you already have it there, but be sure to update to the matching
version: mine works with <code>17.4 / 14.0.17213.0</code> but refused to work with older
versions.</p>
<p>Now we can implement our trace server as a console application. The main
method looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">static</span> <span style="color:#00f">void</span> Main(<span style="color:#2b91af">string</span>[] args) <span style="color:#008000">// args: &lt;db server name&gt; &lt;db name&gt; &lt;trace file&gt;
</span><span style="color:#008000"></span>{
    <span style="color:#008000">// 1. Run trace server
</span><span style="color:#008000"></span>    <span style="color:#2b91af">var</span> connectionInfo = <span style="color:#00f">new</span> SqlConnectionInfo(args[0])
    {
        DatabaseName = args[1],
        UseIntegratedSecurity = <span style="color:#00f">true</span>
    };
    <span style="color:#2b91af">var</span> trace = <span style="color:#00f">new</span> TraceServer();
    trace.InitializeAsReader(connectionInfo, args[2]);

    <span style="color:#008000">// 2. Continuously read traces and send them to event hubs
</span><span style="color:#008000"></span>    <span style="color:#2b91af">var</span> tokenSource = <span style="color:#00f">new</span> CancellationTokenSource();
    <span style="color:#2b91af">var</span> readerTask = Task.Factory.StartNew(() =&gt; ReadTrace(trace, tokenSource.Token), tokenSource.Token);
    <span style="color:#2b91af">var</span> senderTask = Task.Factory.StartNew(() =&gt; SendToEventHubs(tokenSource.Token), tokenSource.Token);

    <span style="color:#008000">// 3. Stop the trace
</span><span style="color:#008000"></span>    Console.WriteLine(<span style="color:#a31515">&#34;Press any key to stop...&#34;</span>);
    Console.ReadKey();
    tokenSource.Cancel();
    Task.WaitAll(readerTask, senderTask);
}
</code></pre></div><p>The first block initializes SQL connection using command line arguments and integrated
security, and then starts the Trace Server.</p>
<p>Because of the large volume, I made trace reader and event sender to work on separate
threads. They talk to each other via a concurrent queue:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">readonly</span> ConcurrentQueue&lt;<span style="color:#2b91af">string</span>&gt; eventQueue = <span style="color:#00f">new</span> ConcurrentQueue&lt;<span style="color:#2b91af">string</span>&gt;();
</code></pre></div><p>Finally, when operator presses any key, the cancellation is requested and the reader and
sender get shut down.</p>
<p>Trace Reader task is a loop crunching though trace data and sending the SQL statements
(with some exclusions) to the concurrent in-memory queue:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> ReadTrace(TraceServer trace, CancellationToken token)
{ 
    <span style="color:#00f">while</span> (trace.Read() &amp;&amp; !token.IsCancellationRequested)
    {
        <span style="color:#2b91af">var</span> eventClass = trace[<span style="color:#a31515">&#34;EventClass&#34;</span>].ToString();
        <span style="color:#00f">if</span> (<span style="color:#2b91af">string</span>.Compare(eventClass, <span style="color:#a31515">&#34;RPC:Completed&#34;</span>) == 0)
        {
            <span style="color:#2b91af">var</span> textData = trace[<span style="color:#a31515">&#34;TextData&#34;</span>].ToString();
            <span style="color:#00f">if</span> (!textData.Contains(<span style="color:#a31515">&#34;sp_reset_connection&#34;</span>)
                &amp;&amp; !textData.Contains(<span style="color:#a31515">&#34;sp_trace&#34;</span>)
                &amp;&amp; !textData.Contains(<span style="color:#a31515">&#34;sqlagent&#34;</span>))
            {
                eventQueue.Enqueue(textData);
            }
        }
    }

    trace.Stop();
    trace.Close();
}
</code></pre></div><p>Event Sender is dequeueing SQL commands from in-memory queue to collect batches of
events. As soon as a batch fills up, it gets dispatched to Event Hub:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> SendToEventHubs(CancellationToken token)
{
    <span style="color:#2b91af">var</span> client = EventHubClient.CreateFromConnectionString(EventHubsConnectionString);
    <span style="color:#2b91af">var</span> batch = client.CreateBatch();
    <span style="color:#00f">while</span> (!token.IsCancellationRequested)
    {
        <span style="color:#00f">if</span> (!eventQueue.TryDequeue(<span style="color:#00f">out</span> <span style="color:#2b91af">string</span> sql))
        {
            Thread.Sleep(10);
            <span style="color:#00f">continue</span>;
        }

        <span style="color:#2b91af">var</span> eventData = <span style="color:#00f">new</span> EventData(Encoding.UTF8.GetBytes(sql));
        <span style="color:#00f">if</span> (!batch.TryAdd(eventData) &amp;&amp; batch.Count &gt; 0)
        {
            client.SendAsync(batch.ToEnumerable())
                .ContinueWith(OnAsyncMethodFailed, token, TaskContinuationOptions.OnlyOnFaulted, TaskScheduler.Default);
            batch = client.CreateBatch();
            batch.TryAdd(eventData);
        }
    }
}
</code></pre></div><p>If your trace doesn&rsquo;t produce so many messages, you will probably want to periodically send out the batches
even before they get full, just to keep that process closer to real time.</p>
<p>Note that sender does not await <code>SendAsync</code> call. Instead, we only subscribe to failures via <code>OnAsyncMethodFailed</code>
callback to print it to console:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> OnMyAsyncMethodFailed(Task task)
{
    Console.WriteLine(task.Exception?.ToString() ?? <span style="color:#a31515">&#34;null error&#34;</span>);
}
</code></pre></div><p>And that concludes the implementation of the Trace Server. SQL commands now go to Event Hub, to be picked
up by Trace Replay.</p>
<h2 id="trace-replay-function-app">Trace Replay Function App</h2>
<p>To replay those traces against the target Azure SQL Database, I could make another console
application which would contain <code>EventProcessorHost</code> to receive and process SQL commands.</p>
<p>However, under high load a single machine might not be able to keep up with executing all
those commands in real time.</p>
<p>Instead, I decided to distribute such Replay App over multiple machines. To deploy a
DDoS network, if you will :)</p>
<p>And I don&rsquo;t have to build, find, configure and synchronize all those servers myself, since we
are living in the world of serverless.</p>
<p><a href="https://azure.microsoft.com/en-us/services/functions/">Azure Functions</a> are the perfect tool for this job. Once you start the trace server,
Function App will start scaling up based on the amount of events in Event Hub, and will
expand until it catches up with the workload.</p>
<p>But as long as you don&rsquo;t run the trace server, it won&rsquo;t consume any servers and won&rsquo;t cost you
a dime.</p>
<p>Here is the implementation of Trace Replay Azure Function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Replay</span>
{
    [FunctionName(&#34;Replay&#34;)]
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> Run(
        [EventHubTrigger(&#34;sqltrace&#34;, Connection = &#34;EventHubsConn&#34;)] <span style="color:#2b91af">string</span> sql,
        TraceWriter log)
    {
        <span style="color:#2b91af">var</span> commandName = sql
            .Split(<span style="color:#00f">null</span>)
            .SkipWhile(r =&gt; r != <span style="color:#a31515">&#34;exec&#34;</span> &amp;&amp; r != <span style="color:#a31515">&#34;sp_executesql&#34;</span>)
            .FirstOrDefault(r =&gt; !r.Contains(<span style="color:#a31515">&#34;exec&#34;</span>)) ?? <span style="color:#a31515">&#34;&lt;empty&gt;&#34;</span>;

        <span style="color:#2b91af">var</span> stopwatch = <span style="color:#00f">new</span> Stopwatch();
        stopwatch.Start();

        <span style="color:#00f">try</span>
        {
            <span style="color:#00f">using</span> (<span style="color:#2b91af">var</span> sqlConnection = <span style="color:#00f">new</span> SqlConnection(AzureSqlConnectionString))
            <span style="color:#00f">using</span> (<span style="color:#2b91af">var</span> cmd = <span style="color:#00f">new</span> SqlCommand())
            {
                sqlConnection.Open();

                cmd.CommandText = sql;
                cmd.CommandType = CommandType.Text;

                cmd.Connection = sqlConnection;

                <span style="color:#2b91af">int</span> count = 0;
                <span style="color:#00f">using</span> (<span style="color:#2b91af">var</span> reader = cmd.ExecuteReader())
                {
                    <span style="color:#00f">while</span> (reader.Read())
                    {
                        count++;
                    }
                }

                log.Info(<span style="color:#a31515">$&#34;Processed {commandName} in {stopwatch.ElapsedMilliseconds} ms with {count} rows&#34;</span>);
            }
        }
        <span style="color:#00f">catch</span> (Exception ex)
        {
            log.Error(<span style="color:#a31515">$&#34;Error in {commandName} in {stopwatch.ElapsedMilliseconds} {ex.Message}&#34;</span>);
            <span style="color:#00f">throw</span>;
        }
    }
}
</code></pre></div><p>It&rsquo;s super simple: the function gets a SQL statement, executes it with <code>SqlCommand</code> class and
logs the result with timing and returned row count. And that&rsquo;s everything required to start
bombarding my Azure SQL Database.</p>
<h2 id="evaluating-results">Evaluating Results</h2>
<p>The purpose of this whole exercise was to evaluate whether a provisioned DTU level
is enough to stand the load comparable to existing production.</p>
<p>So, after I ran the test, I could browse through the DTU usage chart in Azure portal to
get overall usage statistics.</p>
<p>I&rsquo;ve also spent quite some time analyzing the usage breakdown as reported by <code>sp_BlitzCache</code>
from <a href="https://github.com/BrentOzarULTD/SQL-Server-First-Responder-Kit">Responder Kit</a>.
Please note that it&rsquo;s not officially supported for Azure SQL Database, but it seems to work
reasonably well.</p>
<p>Be sure to re-run your experiments multiple times, at different days and time intervals.</p>
<p>The full code sample can be found in <a href="https://github.com/mikhailshilkov/sql-trace-replay/tree/master/TDF">my github</a>.</p>
<p>I hope Azure SQL Database will perform to your expectations and within your budget. But
hope is not a good strategy, so go ahead and try it out!</p>
<p>Happy DDoS-ing!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-sql-database" term="azure-sql-database" label="Azure SQL Database" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                             
                                <category scheme="https://mikhail.io/tags/azure-event-hubs" term="azure-event-hubs" label="Azure Event Hubs" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[HATEOAS]]></title>
            <link href="https://mikhail.io/tags/hateoas/"/>
            <id>https://mikhail.io/tags/hateoas/</id>
            
            <published>2018-01-23T00:00:00+00:00</published>
            <updated>2018-01-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Property-Based Testing]]></title>
            <link href="https://mikhail.io/tags/property-based-testing/"/>
            <id>https://mikhail.io/tags/property-based-testing/</id>
            
            <published>2018-01-23T00:00:00+00:00</published>
            <updated>2018-01-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Tic-Tac-Toe with F#, Azure Functions, HATEOAS and Property-Based Testing]]></title>
            <link href="https://mikhail.io/2018/01/tictactoe-with-fsharp-azurefunctions-hateoas-and-property-based-testing/"/>
            <id>https://mikhail.io/2018/01/tictactoe-with-fsharp-azurefunctions-hateoas-and-property-based-testing/</id>
            
            <published>2018-01-23T00:00:00+00:00</published>
            <updated>2018-01-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>A toy application built with F# and Azure Functions: a simple end-to-end implementation from domain design to property-based tests.</blockquote><p>This post describes a toy application that I&rsquo;ve built with F# and Azure Functions
in about 1 day of work. It shows a simple end-to-end implementation with some
useful techniques applied, and can be used as a reference point for anyone interested in
one of the topics mentioned in the title.</p>
<p>The requirements for my application are quite simple:</p>
<ul>
<li>Implement the game of Tic-Tac-Toe for a human player to play against the computer</li>
<li>The field is 3x3, the player to have three-in-a-row wins</li>
<li>After the game, the score is calculated based on the number of moves combined
with the duration of the game</li>
<li>The history of players' scores is persisted and presented as the leaderboard</li>
</ul>
<p>Below I go through the code step by step. Feel free to jump to the part which interests
you the most:
<a href="#DomainModelling">Domain Modelling</a>,
<a href="#AzureFunctions">Azure Functions</a>,
<a href="#HATEOAS">HATEOAS</a>,
<a href="#PropertyBasedTesting">Property-Based Testing</a>.</p>
<p>The game is online, so you can play it <a href="https://tictactoefs.azurewebsites.net/home">here</a>.</p>
<p>The full source code can be found in <a href="https://github.com/mikhailshilkov/tictactoe">my github</a>.</p>
<h2 id="modeling-the-game-with-types"><a name="DomainModelling"></a>
Modeling the Game with Types</h2>
<p>I start with a domain model. The model is composed of immutable F# types (records and discriminated
unions) and pure functions.</p>
<p>We have two players, so we need a type for them:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Player</span> = X | O
</code></pre></div><p>In addition, there is a useful function to return the other player based on the given one.
Simple pattern-matching will do:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Player =
  <span style="color:#00f">let</span> other = <span style="color:#00f">function</span> | X -&gt; O | O -&gt; X
</code></pre></div><p>The domain code is the most important part of the application, so I want it to be covered
by unit tests. Of course, the above function doesn&rsquo;t really warrant testing, but it&rsquo;s a nice
and simple way to try out <a href="https://fsharpforfunandprofit.com/posts/property-based-testing/">Property-Based Testing</a>.
That is, instead of defining specific tests, we define properties which hold for any valid input.</p>
<p>For <code>other</code> function, I came up with two properties:</p>
<ul>
<li>Other player is not equal to original player</li>
<li>Other player of other player is the player itself</li>
</ul>
<p>Here is the code with <a href="https://github.com/haf/expecto">Expecto</a> and <a href="https://github.com/fscheck/FsCheck">FsCheck</a>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">testProperty <span style="color:#a31515">&#34;Other player is not equal to player&#34;</span> &lt;| <span style="color:#00f">fun</span> x -&gt;
  Expect.notEqual x (Player.other x) <span style="color:#a31515">&#34;Other should be different from original&#34;</span>

testProperty <span style="color:#a31515">&#34;Other player of other player is the player itself&#34;</span> &lt;| <span style="color:#00f">fun</span> x -&gt;
  Expect.equal x (Player.other (Player.other x)) <span style="color:#a31515">&#34;Other other should be equal to original&#34;</span>
</code></pre></div><p>Let&rsquo;s move on to modelling the game. I decided to define a union type to be used for
horizontal and vertical positions of the cells:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Position</span> = One | Two | Three
</code></pre></div><p>I could use the normal integers instead, but I don&rsquo;t want to be worried about validating
the ranges all the time.</p>
<p>My first record type models the move, or action done by a player: it has <code>X</code> and <code>Y</code>
positions of the chosen cell, plus the player information:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Move</span> = {
  X: Position
  Y: Position
  By: Player
}
</code></pre></div><p>The following type <code>RunningGame</code> has just two properties, but its shape defines the
design of the whole application:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">RunningGame</span> = {
  MovesDone: Move <span style="color:#2b91af">list</span>
  PossibleMoves: Move <span style="color:#2b91af">list</span>
}
</code></pre></div><p>This type models any state of the game which is not finished yet.</p>
<p><code>MovesDone</code> represents the ordered log of all moves, so we have the complete history
of actions at any time. Event Sourcing in small.</p>
<p>Equally importantly, there is a list of all possible moves at this point of the game.
I could get away without this property: in the end, it can always be derived from
the history of done moves and 3x3 size of the field.</p>
<p>However, having the list of possible moves simplifies the design of all the decision
maker (client) code:</p>
<ul>
<li>Clients don&rsquo;t have to search for remaining cells based on move log</li>
<li>Validation of a move received from clients gets trivial: just check that it&rsquo;s in
the list of possible moves</li>
<li>Bot implementation gets easy: it just needs to pick one of the valid moves. The
most trivial bot is a one-liner: it picks a random move from the collection</li>
<li>Tests take advantage of this in a similar way, see <a href="#PropertyBasedTesting">Game Tests</a> below</li>
<li>We build a nice bridge into HATEOAS-style API, where links provided in the response
correspond to possible moves, see <a href="#HATEOAS">REST API</a> below</li>
</ul>
<p>Now, we can model a game which is already finished:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">GameOutcome</span> = Won <span style="color:#00f">of</span> Player | Tie

<span style="color:#00f">type</span> <span style="color:#2b91af">FinishedGame</span> = {
  MovesDone: Move <span style="color:#2b91af">list</span>
  Outcome: GameOutcome
}
</code></pre></div><p>Each finished game has a list of moves and the outcome: either one player won, or there
was a tie.</p>
<p>Each state of a game can be described by the union of the previous two states:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">GameState</span> =
  | Finished <span style="color:#00f">of</span> FinishedGame
  | InProgress <span style="color:#00f">of</span> RunningGame
</code></pre></div><h2 id="modeling-game-flow">Modeling Game Flow</h2>
<p>Now, when all the types are in place, we can model the game flow. The flow is a sequence
of transitions between game states, implemented with pure functions.</p>
<p>First, each game starts at the same state, which is an empty field, and X turn. Here
is the value which represents this initial state:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Game =
  <span style="color:#00f">let</span> initialState =
    <span style="color:#00f">let</span> positions = [One; Two; Three]
    <span style="color:#00f">let</span> cells = seq {
      <span style="color:#00f">for</span> x <span style="color:#00f">in</span> positions <span style="color:#00f">do</span>
         <span style="color:#00f">for</span> y <span style="color:#00f">in</span> positions <span style="color:#00f">do</span>
            <span style="color:#00f">yield</span> { X = x; Y = y; By = X }
      }
    { MovesDone = []; PossibleMoves = List.ofSeq cells }
</code></pre></div><p>After each move is made, we need a function to evaluate move outcome: whether current
game is finished or is still in progress. I defined a function <code>evaluate</code> for that:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> private evaluate (history: Move <span style="color:#2b91af">list</span>): GameOutcome option = ...
</code></pre></div><p>I don&rsquo;t show the full body here, since it&rsquo;s quite boring in evaluating rows, columns
and diagonals for three-in-a-row. See the <a href="https://github.com/mikhailshilkov/tictactoe/blob/master/TicTacToe/Game.fs#L42-L56">full code</a>
if you want to.</p>
<p>The following function is even more important: that&rsquo;s the main domain function called
<code>makeMove</code>. Its type is <code>RunningGame -&gt; Move -&gt; GameState</code> which perfectly communicates
its intent: given a running game and a move, it returns the game state after the move.
Note that</p>
<ul>
<li>You can&rsquo;t pass a finished game as an argument, because making a move on finished game
doesn&rsquo;t make sense</li>
<li>The result <em>can</em> be a finished game</li>
</ul>
<p>Here is the function implementation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> makeMove (game: RunningGame) (move: Move): GameState =
  <span style="color:#00f">let</span> movesDone = move :: game.MovesDone
  <span style="color:#00f">match</span> evaluate movesDone <span style="color:#00f">with</span>
  | Some result -&gt; Finished { MovesDone = movesDone; Outcome = result }
  | None -&gt;
    <span style="color:#00f">let</span> possibleMoves =
      List.except [move] game.PossibleMoves
      |&gt; List.map (<span style="color:#00f">fun</span> m -&gt; { m <span style="color:#00f">with</span> By = Player.other m.By })
    InProgress { MovesDone = movesDone; PossibleMoves = possibleMoves }
</code></pre></div><p>It works like this:</p>
<ul>
<li>Prepend the new move to moves done</li>
<li>Evaluate the game result of these combined moves</li>
<li>If the result is known, return a <code>Finished</code> game with calculated outcome</li>
<li>If the result is not clear yet, return <code>InProgress</code> game with possible
moves same as before, but excluding the move and assigned to the other player</li>
</ul>
<p>Tic-Tic-Toe is two-player game, so I defined another function which runs
a turn of 2 moves by 2 players, given the decision making functions of both
players (so it&rsquo;s a higher-order function):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> makeRound player1 player2 gameState =
  <span style="color:#00f">let</span> newGameState = player1 gameState |&gt; makeMove gameState
  <span style="color:#00f">match</span> newGameState <span style="color:#00f">with</span>
  | Finished _ -&gt; newGameState
  | InProgress p -&gt; player2 p |&gt; makeMove p
</code></pre></div><p>Looks almost like monadic <code>bind</code> operation&hellip;</p>
<h2 id="property-based-testing"><a name="PropertyBasedTesting"></a>
Property-Based Testing</h2>
<p>I&rsquo;ve already shown two simplistic properties for <code>other</code> function.</p>
<p>It&rsquo;s a bit more challenging to come up with invariant properties when it
comes to testing the game itself. After some brainstorming, I&rsquo;ve made the
following list:</p>
<ul>
<li>The game is never finished after 4 moves</li>
<li>The game is always finished after 9 moves</li>
<li>X and 0 have to make moves in turns</li>
<li>Player wins by filling one column</li>
<li>Player wins by filling one row</li>
<li>Player wins by filling diagonal</li>
<li>Evaluate a known tie</li>
</ul>
<p>Each property-based test should accept some input from the testing framework.
It should then evaluate the test against this input and assert the invariants.
If the property holds for any possible valid input, the test is green.</p>
<p>I decided to structure my property tests in the following way:</p>
<ul>
<li>Each test accepts a list of non-negative integers</li>
<li>Each integer is interpreted as an index of a possible move to select at turn <code>i</code>.
That means that the test receives a sequence which uniquely identifies the moves
to be made</li>
<li>We can restrict this sequence to a scenario under test, e.g. make it less
than 4 moves, or exactly 9 moves, or pick moves from a limited subset of all
possible moves</li>
<li>We apply the moves to calculate the end result</li>
<li>We assert that the result confirms the property under test</li>
</ul>
<p>Now, it&rsquo;s the responsibility of property based testing framework to generate
all kinds of input lists to try to break our property. If it succeeds, it will
print the exact input which causes the test to fail.</p>
<p>Here is how one such test is implemented.</p>
<p>A helper function plays a sequence of indexes as moves:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> playSequence moves =
  <span style="color:#00f">let</span> playOne s i =
    <span style="color:#00f">match</span> s <span style="color:#00f">with</span>
    | InProgress p -&gt; Game.makeMove p (p.PossibleMoves.[i % p.PossibleMoves.Length])
    | _ -&gt; s
  List.fold playOne (InProgress Game.initialState) moves
</code></pre></div><p>Then the property &ldquo;The game is always finished after 9 moves&rdquo; is simply:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">testProp <span style="color:#a31515">&#34;The game is always finished after 9 moves&#34;</span> &lt;| <span style="color:#00f">fun</span> (Gen.ListOf9 xs) -&gt;
  <span style="color:#00f">let</span> result = playSequence xs
  Expect.isTrue (Game.isFinished result) <span style="color:#a31515">&#34;Game should be finished&#34;</span>
</code></pre></div><p>Note the restriction <code>Gen.ListOf9 xs</code> that we put on the input sequence. It&rsquo;s a
generator that I <a href="https://github.com/mikhailshilkov/tictactoe/blob/master/TicTacToe.Tests/Gen.fs#L31-L32">defined</a>,
so that the list always contains exactly 9 elements.</p>
<p>Other property tests follow a similar pattern, you can see them
<a href="https://github.com/mikhailshilkov/tictactoe/blob/master/TicTacToe.Tests/GameTests.fs">here</a>.</p>
<h2 id="rest-api"><a name="HATEOAS"></a>
REST API</h2>
<p>Now, when I&rsquo;m done with Game domain model, I want to define API that our HTTP
service will expose to the clients. I define my API in REST model.</p>
<p>The main resource is <code>/game</code> resource. To start a new game, a client has to send
<code>POST</code> command:</p>
<pre><code>POST /game
Content-Type: application/json

{ &quot;name&quot;: &quot;Mikhail&quot; }
</code></pre><p>And the response body is JSON which denotes a new game created:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;id&#34;: <span style="color:#a31515">&#34;5d7b2261&#34;</span>,
    &#34;busyCells&#34;: [],
    &#34;links&#34;: [
        {
            &#34;rel&#34;: <span style="color:#a31515">&#34;x1y1&#34;</span>,
            &#34;href&#34;: <span style="color:#a31515">&#34;/game/5d7b2261/move/0&#34;</span>
        },
        {
            &#34;rel&#34;: <span style="color:#a31515">&#34;x1y2&#34;</span>,
            &#34;href&#34;: <span style="color:#a31515">&#34;/game/5d7b2261/move/1&#34;</span>
        },
        <span style="">//</span> <span style="">...</span> 7 <span style="">more</span> <span style="">links</span> <span style="">follow</span>
    ]
}
</code></pre></div><p>The response contains a game ID and the list of occupied cells, empty for now,
because no moves have been made.</p>
<p>More importantly, it contains a list of links, each one of which has a <code>rel</code>
field representing a cell, and a link. The client should <code>POST</code> to this link
if it wants to make a move on the corresponding cell:</p>
<pre><code>POST /game/5d7b2261/move/1
</code></pre><p>And the response is:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;id&#34;: <span style="color:#a31515">&#34;5d7b2261&#34;</span>,
    &#34;result&#34;: <span style="color:#00f">null</span>,
    &#34;busyCells&#34;: [
        {
            &#34;name&#34;: <span style="color:#a31515">&#34;x2y3&#34;</span>,
            &#34;value&#34;: <span style="color:#a31515">&#34;O&#34;</span>
        },
        {
            &#34;name&#34;: <span style="color:#a31515">&#34;x1y2&#34;</span>,
            &#34;value&#34;: <span style="color:#a31515">&#34;X&#34;</span>
        }
    ],
    &#34;links&#34;: [
        {
            &#34;rel&#34;: <span style="color:#a31515">&#34;x1y1&#34;</span>,
            &#34;href&#34;: <span style="color:#a31515">&#34;/game/5d7b2261/move/0&#34;</span>
        },
        {
            &#34;rel&#34;: <span style="color:#a31515">&#34;x1y3&#34;</span>,
            &#34;href&#34;: <span style="color:#a31515">&#34;/game/5d7b2261/move/1&#34;</span>
        },
        <span style="">//</span> <span style="">...</span> 5 <span style="">more</span> <span style="">links</span> <span style="">follow</span>
    ]
}
</code></pre></div><p>It has the same structure as before, but now two cells are occupied: one <code>X</code> and one <code>O</code>. The
list of links now has only 7 links, based on the count of free cells.</p>
<p>The client keeps navigating the links until it gets non-empty <code>result</code> field:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    &#34;id&#34;: <span style="color:#a31515">&#34;5d7b2261&#34;</span>,
    &#34;result&#34;: <span style="color:#a31515">&#34;You Win!&#34;</span>,
    &#34;busyCells&#34;: [
        {
            &#34;name&#34;: <span style="color:#a31515">&#34;x3y1&#34;</span>,
            &#34;value&#34;: <span style="color:#a31515">&#34;X&#34;</span>
        },
        {
            &#34;name&#34;: <span style="color:#a31515">&#34;x3y2&#34;</span>,
            &#34;value&#34;: <span style="color:#a31515">&#34;O&#34;</span>
        },
        <span style="">//</span> <span style="">...</span> <span style="">more</span> <span style="">cells</span> <span style="">here</span>
    ],
    &#34;links&#34;: [],
    &#34;score&#34;: 401
}
</code></pre></div><p>This denotes the end of the game. There&rsquo;s no more links to navigate, so the client knows it
has to stop playing.</p>
<p>This API is designed in HATEOAS-style (Hypermedia as the Engine of Application State). The
clients only need to know the initial URL, while all the other URLs are received from the
previous responses. It resembles the way a human navigates websites.</p>
<h2 id="azure-functions"><a name="AzureFunctions"></a>
Azure Functions</h2>
<p>I implemented the above API with Azure Functions. I used .NET Standard based v2 runtime
with precompiled F# functions.</p>
<p>The initial <code>POST /game</code> request is handled by <code>Start</code> function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">GameRequest</span> = { Name: <span style="color:#2b91af">string</span> }

<span style="color:#00f">type</span> <span style="color:#2b91af">Cell</span> = { Name: <span style="color:#2b91af">string</span>; Value: <span style="color:#2b91af">string</span> }
<span style="color:#00f">type</span> <span style="color:#2b91af">Link</span> = { Rel:  <span style="color:#2b91af">string</span>; Href:  <span style="color:#2b91af">string</span> }

<span style="color:#00f">type</span> <span style="color:#2b91af">GameDTO</span> = {
  Id: <span style="color:#2b91af">string</span>
  Result: <span style="color:#2b91af">string</span>
  BusyCells: Cell <span style="color:#2b91af">list</span>
  Links: Link <span style="color:#2b91af">list</span>
  Score: int
}

[&lt;FunctionName(<span style="color:#a31515">&#34;Start&#34;</span>)&gt;]
<span style="color:#00f">let</span> start([&lt;HttpTrigger(AuthorizationLevel.Anonymous, <span style="color:#a31515">&#34;POST&#34;</span>, Route = <span style="color:#a31515">&#34;game&#34;</span>)&gt;] req: GameRequest,
          [&lt;Table(<span style="color:#a31515">&#34;TicTacToe&#34;</span>)&gt;] store: ICollector&lt;GameEntity&gt;) =
  <span style="color:#00f">let</span> gameid = Guid.NewGuid().ToString()
  <span style="color:#00f">let</span> state = InProgress Game.initialState
  <span style="color:#00f">let</span> serializedState = JsonConvert.SerializeObject state
  store.Add(GameEntity(PartitionKey = <span style="color:#a31515">&#34;default&#34;</span>, RowKey = gameid, Name = req.Name, State = serializedState))
  ObjectResult(Api.serialize gameid state 0)
</code></pre></div><p>The outline of this function:</p>
<ul>
<li>It&rsquo;s triggered by HTTP POST request, as configured for <code>req</code> parameter</li>
<li>The request body is parsed to <code>GameRequest</code> type containing player name</li>
<li>It generates a new game ID</li>
<li>It creates initial game state of empty field</li>
<li>It serializes the state and saves it to Table Storage with <code>store</code> output binding</li>
<li>It returns HTTP body with
<a href="https://github.com/mikhailshilkov/tictactoe/blob/master/TicTacToe.Functions/Api.fs#L30-L51">serialized</a> game response of type <code>GameDTO</code></li>
</ul>
<p>The second Function <code>Play</code> handles the moves:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;FunctionName(<span style="color:#a31515">&#34;Play&#34;</span>)&gt;]
<span style="color:#00f">let</span> play([&lt;HttpTrigger(AuthorizationLevel.Anonymous, <span style="color:#a31515">&#34;POST&#34;</span>, Route = <span style="color:#a31515">&#34;game/{gameid}/move/{index}&#34;</span>)&gt;]
         req: HttpRequest, gameid: <span style="color:#2b91af">string</span>, index: int,
         [&lt;Table(<span style="color:#a31515">&#34;TicTacToe&#34;</span>, <span style="color:#a31515">&#34;default&#34;</span>, <span style="color:#a31515">&#34;{gameid}&#34;</span>)&gt;] entity: GameEntity) =
  <span style="color:#00f">let</span> state = JsonConvert.DeserializeObject&lt;GameState&gt; entity.State
  <span style="color:#00f">match</span> state <span style="color:#00f">with</span>
  | Finished _ -&gt; BadRequestResult() :&gt; IActionResult
  | InProgress p <span style="color:#00f">when</span> index &lt; 0 || index &gt;= p.PossibleMoves.Length -&gt; BadRequestResult() :&gt; IActionResult
  | InProgress p -&gt;
    <span style="color:#00f">let</span> result = Game.makeRound (<span style="color:#00f">fun</span> _ -&gt; p.PossibleMoves.[index]) Bot.pickMove p
    entity.State &lt;- JsonConvert.SerializeObject result
    entity.Score &lt;- Scoring.calculateScore (DateTime.UtcNow - entity.StartedAt).TotalMilliseconds result
    ObjectResult(Api.serialize gameid result entity.Score) :&gt; IActionResult
</code></pre></div><p>The outline is very similar:</p>
<ul>
<li>It&rsquo;s triggered by a <code>POST</code> request with a URL template containing game ID and move index</li>
<li>It has an in/out Table Storage binding which reads the serialized state saved after previous
game and move requests</li>
<li>It validates the state: if the game is already finished, or if the move index is not in the valid range,
<code>Bad Request</code> HTTP status is returned</li>
<li>If the move is valid, it runs the round, including bot play</li>
<li>It also calculates the score, which is going to be non-zero only for finished games</li>
<li>The game state and the score are saved to Table Storage entity (that&rsquo;s the only mutation in the whole
application)</li>
</ul>
<h2 id="bot">Bot</h2>
<p>Azure Function above used a <code>Bot.pickMove</code> function which I haven&rsquo;t described yet.</p>
<p>This function has the type <code>RunningGame -&gt; Move</code>, exactly what is expected by <code>makeRound</code> game
function. Its goal is to pick the <code>O</code> move for any given game-in-progress.</p>
<p>Obviously, 3x3 Tic-Tac-Toe is a very simple game and it&rsquo;s quite easy to make a perfectly
playing bot. This wasn&rsquo;t the goal though: it&rsquo;s more fun for a human to win.</p>
<p>So, actually, the only property test that I ended up implementing is the following:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">testProp <span style="color:#a31515">&#34;Bot is able to play O at any possible position&#34;</span> &lt;| <span style="color:#00f">fun</span> (Gen.ListOfNonNegative xs) -&gt;
  <span style="color:#00f">let</span> human i p _ = p.PossibleMoves.[i % p.PossibleMoves.Length]
  <span style="color:#00f">let</span> round s i =
    <span style="color:#00f">match</span> s <span style="color:#00f">with</span>
    | InProgress p -&gt; Game.makeRound (human i p) Bot.pickMove p
    | _ -&gt; s
  List.fold round (InProgress Game.initialState) xs |&gt; ignore
</code></pre></div><p>It makes sure that for any possible sequence of human moves, bot is actually able to make
<em>any</em> move of its own. Bot just shouldn&rsquo;t crash :)</p>
<p>My very first implementation of the bot was just picking a random move. Such bot is fine,
but it&rsquo;s too boring to play against.</p>
<p>So, my current bot implementation has 3 rules:</p>
<ul>
<li>If there is a move that immediately wins the game, do that move</li>
<li>If possible, don&rsquo;t pick a move which leads to immediate loss after the next human move</li>
<li>Otherwise, pick a random move</li>
</ul>
<p>I implemented the bot using the approach described in my
<a href="https://mikhail.io/2016/07/building-a-poker-bot-functional-fold-as-decision-tree-pattern/">Functional Fold as Decision Tree Pattern</a>
post:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> pickMove (game: RunningGame) =
  [winNow O; notLoseNow; pickRandom]
  |&gt; Seq.ofList
  |&gt; Seq.choose (<span style="color:#00f">fun</span> x -&gt; x game)
  |&gt; Seq.head
</code></pre></div><p>So, there is a prioritized list of decision functions. The first one returning <code>Some</code>
decision will be promoted to final decision.</p>
<p>And here are those functions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> winNow player (game: RunningGame) =
  <span style="color:#00f">let</span> isWin = <span style="color:#00f">function</span> | Finished { Outcome = Won x } <span style="color:#00f">when</span> x = player -&gt; <span style="color:#00f">true</span> | _ -&gt; <span style="color:#00f">false</span>
  game.PossibleMoves
  |&gt; List.tryFind (<span style="color:#00f">fun</span> move -&gt; Game.makeMove game move |&gt; isWin)

<span style="color:#00f">let</span> notLoseNow (game: RunningGame) =
  <span style="color:#00f">let</span> canLose = <span style="color:#00f">function</span>
    | InProgress p -&gt; <span style="color:#00f">match</span> winNow X p <span style="color:#00f">with</span> | Some _ -&gt; <span style="color:#00f">true</span> | None -&gt; <span style="color:#00f">false</span>
    | _ -&gt; <span style="color:#00f">false</span>
  <span style="color:#00f">let</span> notLosingMoves =
    game.PossibleMoves
    |&gt; List.filter (<span style="color:#00f">fun</span> move -&gt; Game.makeMove game move |&gt; canLose |&gt; <span style="color:#00f">not</span>)
  <span style="color:#00f">if</span> List.isEmpty notLosingMoves &amp;&amp; notLosingMoves.Length &lt; game.PossibleMoves.Length <span style="color:#00f">then</span> None
  <span style="color:#00f">else</span> Some (notLosingMoves.[random.Next notLosingMoves.Length])

<span style="color:#00f">let</span> pickRandom (game: RunningGame) =
  Some (game.PossibleMoves.[random.Next game.PossibleMoves.Length])
</code></pre></div><p>Such rule-based setup is easy to extend, and also to test when it becomes needed.</p>
<h2 id="score-calculation">Score Calculation</h2>
<p>Last twist to the application is the scoring system. If a human player wins or gets a tie,
they are assigned a numeric score, which can be then compared to the historic leaderboard
of all players.</p>
<p>The score is calculated based on two principles: the less moves you make, and the faster
you play, the higher the score is. Move count is more important than timing.</p>
<p>These principles are nicely expressed as property tests:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">testProp <span style="color:#a31515">&#34;The score of faster game is not lower than slower game&#34;</span>
  &lt;| <span style="color:#00f">fun</span> (Gen.Positive duration1) (Gen.Positive duration2) game -&gt;
  <span style="color:#00f">let</span> (slower, faster) = maxmin id duration1 duration2
  <span style="color:#00f">let</span> scoreFaster = Scoring.calculateScore faster game
  <span style="color:#00f">let</span> scoreSlower = Scoring.calculateScore slower game
  Expect.isGreaterThanOrEqual scoreFaster scoreSlower <span style="color:#a31515">&#34;Bigger duration has lower score (or same)&#34;</span>

testProp <span style="color:#a31515">&#34;The score of won game in less moves is greater than game with more moves&#34;</span>
  &lt;| <span style="color:#00f">fun</span> (Gen.Positive duration1) (Gen.Positive duration2) game1 game2 -&gt;
  <span style="color:#00f">let</span> (slower, faster) = maxmin id duration1 duration2
  <span style="color:#00f">let</span> (moreMoves, lessMoves) = maxmin List.length game1 game2
  <span style="color:#00f">let</span> score1 = Scoring.calculateScore slower (Finished { Outcome = Won X; MovesDone = lessMoves })
  <span style="color:#00f">let</span> score2 = Scoring.calculateScore faster (Finished { Outcome = Won X; MovesDone = moreMoves })
  <span style="color:#00f">if</span> moreMoves.Length = lessMoves.Length <span style="color:#00f">then</span>
    Expect.isGreaterThanOrEqual score1 score2 <span style="color:#a31515">&#34;Bigger duration has lower score (or same)&#34;</span>
  <span style="color:#00f">else</span>
    Expect.isGreaterThan score1 score2 <span style="color:#a31515">&#34;More moves have lower score&#34;</span>
</code></pre></div><p>Note that tests are parameterized for durations and game states. We don&rsquo;t have to come up with specific
scenarios: the framework should take care of those.</p>
<p>One of the possible implementations for scoring is:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> calculateScore duration (state: GameState) =
  <span style="color:#00f">let</span> durationScore = (100.0 * (1.0 - duration / (duration + 10000.0))) |&gt; int
  <span style="color:#00f">match</span> state <span style="color:#00f">with</span>
  | Finished { Outcome = Won X; MovesDone = ms } -&gt; (11 - ms.Length) * 100 + durationScore
  | Finished { Outcome = Tie } -&gt; durationScore
  | _ -&gt; 0
</code></pre></div><p>Now, the leaderboard piece. You&rsquo;ve already seen the bits of this functionality in Azure Functions:
they store the game state into Azure Table Storage.</p>
<p>There is another Azure Function which handles <code>GET</code> requests to <code>/leaderboard</code> resource. It
loads all the past games from Table Storage, and then passes them to leaderboard calculation
function below:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> calculateLeaderboard top ns =
  ns
  |&gt; Seq.filter (<span style="color:#00f">fun</span> entity -&gt; snd entity &gt; 0 &amp;&amp; <span style="color:#00f">not</span> (String.IsNullOrEmpty (fst entity)))
  |&gt; Seq.sortByDescending snd
  |&gt; Seq.truncate top
  |&gt; Seq.mapi (<span style="color:#00f">fun</span> index entity -&gt; { Index = index + 1; Name = fst entity; Score = snd entity })
</code></pre></div><h2 id="wrapping-up">Wrapping Up</h2>
<p>Ok, the application is simple, but the blog post ended up being quite long. Thank you if you
made it so far.</p>
<p>I touched base on several important concepts and tools, which can be useful apart or in
combination.</p>
<p>Please leave a comment if such kind of articles is useful, and which part you found most
inspirational or boring.</p>
<p>The full source code can be found in <a href="https://github.com/mikhailshilkov/tictactoe">my github</a>.</p>
<p>Happy coding!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/hateoas" term="hateoas" label="HATEOAS" />
                             
                                <category scheme="https://mikhail.io/tags/property-based-testing" term="property-based-testing" label="Property-Based Testing" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2017s]]></title>
            <link href="https://mikhail.io/2017/"/>
            <id>https://mikhail.io/2017/</id>
            
            <published>2017-12-13T00:00:00+00:00</published>
            <updated>2017-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions Get More Scalable and Elastic]]></title>
            <link href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/"/>
            <id>https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/</id>
            
            <published>2017-12-13T00:00:00+00:00</published>
            <updated>2017-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Back in August this year, I&rsquo;ve posted
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">Azure Functions: Are They Really Infinitely Scalable and Elastic?</a>
with two experiments about Azure Function App auto scaling. I ran a simple
CPU-bound function based on Bcrypt hashing, and measured how well Azure
was running my Function under load.</p>
<p>The results were rather pessimistic. Functions were scaling up to many
instances, but there were significant delays in doing so, so the processing
slowed down up to 40 minutes.</p>
<p>Azure Functions team notified me that they rolled out an updated version of
the service, which should significantly improve my results.</p>
<p>So I ran the exact same tests again, and got the new results. I will show
these results below.</p>
<p><em>TL;DR. Scaling responsiveness improved significantly. The max delay reduced
from 40 to 6 minutes. There are some improvements still to be desired:
sub-minute latency is not yet reachable for similar scenarios.</em></p>
<h2 id="setup">Setup</h2>
<p>See the Function code and the description of the two experiments in
<a href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/">my previous article</a>.</p>
<h2 id="experiment-1-steady-load">Experiment 1: Steady Load</h2>
<p>In &ldquo;Steady Load&rdquo; scenario 100,000 messages were sent to the queue at
constant pace, evenly spread over 2 hours.</p>
<p>Here are the <strong>old</strong> metrics of Queue Backlog and Instance Count over time:</p>
<figure >
    
        <img src="FunctionAppScaling.png"
            alt="Old charts are shown in gray background"
             />
        
    
    <figcaption>
        <h4>Old charts are shown in gray background</h4>
    </figcaption>
    
</figure>
<p>You can see a huge delay of almost one hour before the function caught up
to speed of incoming messages and half-an-hour more before the backlog
got cleared.</p>
<p>The <strong>new</strong> results on the same chart after the runtime update:</p>
<p><img src="FunctionAppScalingNew.png" alt="Function App Scaling (New)"></p>
<p>This looks much better. The maximum backlog is 7 times lower; there&rsquo;s almost
no initial delay before the auto scaling kicks in; and overall instance
allocation is much more stable.</p>
<hr>
<p>One more chart is from the same experiment, but it shows slightly different
metrics. The <strong>old</strong> results of Delay (Age) in seconds and Processing Rate
in messages per minute:</p>
<p><img src="FunctionAppDelay.png" alt="Function App Delay"></p>
<p>The <strong>new</strong> chart after the runtime update:</p>
<p><img src="FunctionAppDelayNew.png" alt="Function App Delay"></p>
<p>Again, much less delay overall, and processing rate more-or-less stabilizes
after the first 15 minutes.</p>
<h2 id="experiment-2-spiky-load">Experiment 2: Spiky Load</h2>
<p>The second experiment spanned over 5 hours. The messages were sent mostly
at low-ish fixed rate, except for 5 periods of sudden spikes. The
green line on the charts below shows these spikes very well.</p>
<p>At the first run 4 months ago, Functions runtime had troubles keeping up
to speed even between those bursts of messages.</p>
<p>Here is the chart of the <strong>old</strong> spiky load processing:</p>
<p><img src="SpikyLoadProcessing.png" alt="Spicky Load Processing (Old)"></p>
<p>You can see that the backlog after each spike goes down really slow. The
blue line of processing rate doesn&rsquo;t match the green line almost nowhere,
which reveals the struggle to adapt.</p>
<p>The <strong>new</strong> results of the same chart after the runtime update are quite
different:</p>
<p><img src="SpikyLoadProcessingNew.png" alt="Spicky Load Processing (New)"></p>
<p>Notice how the backlog is empty and the blue processing rate matches exactly
the incoming rate during all time except after traffic bursts. The queue goes
up during each spike, but the processing rate immediately accelerates too,
and the crisis is gone within 15 minutes.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Azure Functions team is clearly working on improvements. While the results in
August were puzzling or even embarrassing, the December benchmark makes much
more sense.</p>
<p>Looks like Azure Functions are now suitable for CPU-intensive data processing
scenarios with flexible load, targeting the maximum delay at about several
minutes.</p>
<p>Obviously, the results are not perfect just yet. Here&rsquo;s what still can be
done better:</p>
<ul>
<li>
<p><strong>Scale faster initially</strong>. In the first experiment, the biggest delay
appeared right after the start, when the backlog was growing linearly for
10 minutes. &ldquo;0 to 100&rdquo; might not be a very realistic scenario, but probably
that&rsquo;s how many folks will test Functions against their workloads.</p>
</li>
<li>
<p><strong>Do not scale down that fast after backlog goes to 0</strong>. Every time the
queue backlog goes to 0, the runtime kills the biggest part of instances
almost immediately. During my runs, this caused the queue to grow again without
a good reason from user&rsquo;s perspective.</p>
</li>
<li>
<p><strong>Do not allow the backlog to grow without message spikes</strong>. Related to
the previous item, but slightly different focus. When the load is stable,
I would expect the runtime to keep my queue as close to empty as possible.
I guess Azure tries to minimize the resources that it consumes behind
the scenes, but this should be balanced in favor of user experience.</p>
</li>
<li>
<p><strong>Make scaling algorithms more open</strong>. It&rsquo;s a black box right now. I
would love to see some documentation, if not code, to be published about
what exactly to expect from Consumption Plan auto scaling.</p>
</li>
</ul>
<p>I&rsquo;ll be running more scaling experiments with other types of workloads in the
nearest future, so&hellip; more benchmarks are coming.</p>
<p>Happy scaling!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Advent Calendar]]></title>
            <link href="https://mikhail.io/tags/advent-calendar/"/>
            <id>https://mikhail.io/tags/advent-calendar/</id>
            
            <published>2017-12-03T00:00:00+00:00</published>
            <updated>2017-12-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Precompiled Azure Functions in F#]]></title>
            <link href="https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/"/>
            <id>https://mikhail.io/2017/12/precompiled-azure-functions-in-fsharp/</id>
            
            <published>2017-12-03T00:00:00+00:00</published>
            <updated>2017-12-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>This post is giving a start to
<a href="https://sergeytihon.com/2017/10/22/f-advent-calendar-in-english-2017/">F# Advent Calendar in English 2017</a>.
Please follow the calendar for all the great posts to come.</em></p>
<p>Azure Functions is a &ldquo;serverless&rdquo; cloud offering from Microsoft. It
allows you to run your custom code as response to events in the cloud.
Functions are very easy to
start with; and you only pay per execution - with free allowance sufficient
for any proof-of-concept, hobby project or even low-usage production loads.
And when you need more, Azure will scale your project up automatically.</p>
<p>F# is one of the officially supported languages for Azure Functions.
Originally, F# support started with F# Script files (authored directly
in Azure portal or copied from local editor), so you can find many articles
online to get started, e.g.
<a href="http://brandewinder.com/2017/02/11/fsharp-azure-function-from-the-ground-up-part-1/">Creating an Azure Function in F# from the ground up</a> and
<a href="http://brandewinder.com/2017/03/06/fsharp-azure-function-from-the-ground-up-part-2/">Part 2</a>
by Mathias Brandewinder.</p>
<p>However, I find script-based model a bit limited. In today&rsquo;s article I
will focus on creating Azure Functions as precompiled .NET libraries.
Along the way, I&rsquo;ll use cross-platform tools like .NET Core and VS Code,
and I&rsquo;ll show how to integrate Functions with some popular tools
like Suave and Paket.</p>
<h2 id="create-a-project">Create a Project</h2>
<p>You can follow this walkthrough on Windows or Mac, just make sure that
you have <code>.NET Core 2</code> and <code>Node.js 8.x</code> with <code>npm</code> installed. My editor of
choice is Visual Studio Code with Ionide plugin.</p>
<p>I&rsquo;ll show you how to create a new F# Function App from scratch. If you want to
jump to runnable project, you can get it from
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/6-precompiled-timer">my github</a>.</p>
<p>We start with creating a new F# library project for .NET Standard 2. Run
in your command line:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">dotnet new classlib --language F# --name HelloFunctions
</code></pre></div><p>This command creates a folder with two files: <code>HelloFunctions.fsproj</code> project
file and <code>Library.fs</code> source code file.</p>
<p>Now, add a reference to Azure Functions NuGet package:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">dotnet add package Microsoft.NET.Sdk.Functions
</code></pre></div><h2 id="define-a-function">Define a Function</h2>
<p>Open <code>Library.fs</code> code file and change it to the following code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">namespace</span> HelloFunctions

<span style="color:#00f">open</span> System
<span style="color:#00f">open</span> Microsoft.Azure.WebJobs
<span style="color:#00f">open</span> Microsoft.Azure.WebJobs.Host

<span style="color:#00f">module</span> Say =
  <span style="color:#00f">let</span> private daysUntil (d: DateTime) =
    (d - DateTime.Now).TotalDays |&gt; int

  <span style="color:#00f">let</span> hello (timer: TimerInfo, log: TraceWriter) =
    <span style="color:#00f">let</span> christmas = <span style="color:#00f">new</span> DateTime(2017, 12, 25)

    daysUntil christmas
    |&gt; sprintf <span style="color:#a31515">&#34;%d days until Christmas&#34;</span>
    |&gt; log.Info
</code></pre></div><p>We defined a function <code>hello</code> which should be triggered by Functions runtime
based on time intervals. Every time the function is called, we log how many
days we still need to wait before Christmas 2017.</p>
<p>To convert this simple F# function to an Azure Function, create a folder called
<code>Hello</code> (or choose any other name) next to the project file and add
<code>function.json</code> file in there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;bindings&#34;: [
    {
      &#34;name&#34;: <span style="color:#a31515">&#34;timer&#34;</span>,
      &#34;type&#34;: <span style="color:#a31515">&#34;timerTrigger&#34;</span>,
      &#34;schedule&#34;: <span style="color:#a31515">&#34;0 * * * * *&#34;</span>
    }
  ],
  &#34;scriptFile&#34;: <span style="color:#a31515">&#34;../bin/HelloFunctions.dll&#34;</span>,
  &#34;entryPoint&#34;: <span style="color:#a31515">&#34;HelloFunctions.Say.hello&#34;</span>
}
</code></pre></div><p>We defined that:</p>
<ul>
<li>Our function is triggered by timer</li>
<li>It runs every minute at 0 seconds</li>
<li>The entry point is our <code>hello</code> function in the compiled assembly</li>
</ul>
<h2 id="prepare-local-runtime">Prepare Local Runtime</h2>
<p>There are a couple more configuration files needed to be able to
run the Function App locally. <code>host.json</code> defines hosting parameters; empty
file will do for now:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
}
</code></pre></div><p>Most triggers need to connect to a Storage Account. For examples, timer
trigger uses it to hold leases to define which running instance will
actually execute the action every minute. Copy a connection string to your
Storage Account (local Storage emulator is fine too) and put it into
<code>local.settings.json</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;IsEncrypted&#34;: <span style="color:#00f">false</span>,
  &#34;Values&#34;: {
    &#34;AzureWebJobsStorage&#34;: <span style="color:#a31515">&#34;...your connection string...&#34;</span>
  }
}
</code></pre></div><p>Note that this file is only used for local development and is not published
to Azure by default.</p>
<p>Finally, we need to modify <code>fsproj</code> file to make the build tool copy those
files into <code>bin</code> folder. Add the following section in there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml">&lt;ItemGroup&gt;
  &lt;Content Include=<span style="color:#a31515">&#34;Hello\function.json&#34;</span>&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
  &lt;/Content&gt;
  &lt;Content Include=<span style="color:#a31515">&#34;host.json&#34;</span>&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
  &lt;/Content&gt;
  &lt;Content Include=<span style="color:#a31515">&#34;local.settings.json&#34;</span>&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
  &lt;/Content&gt;
&lt;/ItemGroup&gt;
</code></pre></div><h2 id="run-app-locally">Run App Locally</h2>
<p>The first step is to build and publish our Function App with <code>dotnet</code>
commands:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">dotnet build
dotnet publish
</code></pre></div><p>The first line produces the dll file and the second line copies it
and all of its dependencies to <code>publish</code> folder.</p>
<p>The nice thing about Azure Functions is that you can easily run them
locally on a development machine. Execute the following command to
install the runtime and all the required libraries:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">npm install -g azure-functions-core-tools@core
</code></pre></div><p>This will add a <code>func</code> CLI to your system which is the tool to
use for all Function related operations.</p>
<p>Navigate to <code>bin\Debug\netstandard2.0\publish</code> folder and run <code>func start</code>
from there. You should see that your app is now running, and your timer
function is scheduled for execution:</p>
<p><img src="./funcstart.png" alt="Function App Start"></p>
<p>Once the next minute comes, the timer will trigger and you will see
messages in the log:</p>
<p><img src="./funcran.png" alt="Timer Trigger Working"></p>
<h2 id="integrate-into-vs-code">Integrate into VS Code</h2>
<p>You are free to use full Visual Studio or any editor to develop Function
Apps in F#. I&rsquo;ve been mostly using VS Code for this purpose, and I believe
it&rsquo;s quite popular among F# community.</p>
<p>If you use VS Code, be sure to setup the tasks that you can use from within
the editor. I usually have at least 3 tasks: &ldquo;build&rdquo; (<code>dotnet build</code>),
&ldquo;publish&rdquo; (<code>dotnet publish</code>) and &ldquo;run&rdquo; (<code>func start --script-root bin\\debug\\netstandard2.0\\publish</code>),
with shortcuts configured to all of them.</p>
<p>You can find an example of <code>tasks.json</code> file
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/blob/master/6-precompiled-timer/.vscode/tasks.json">here</a>.</p>
<p>Also, check out <a href="https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions">Azure Functions Extension</a>.</p>
<h2 id="deploy-to-azure">Deploy to Azure</h2>
<p>You can deploy the exact same application binaries to Azure. Start by
creating an empty Function App in the portal, or via Azure CLI (<code>func</code> CLI
does not support that).</p>
<p>Then run the following command to deploy your precompiled function to
this app:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">func azure functionapp publish &lt;FunctionAppName&gt;
</code></pre></div><p>At the first run, it will verify your Azure credentials.</p>
<p>In real-life production scenarios your workflow is probably going to be
similar to this:</p>
<ul>
<li>Change Function App code</li>
<li>Run it locally to test the change</li>
<li>Push the code changes to the source control repository</li>
<li>Have your CI/CD pipeline build it, run the tests and then push
the binaries to Azure Functions environment</li>
</ul>
<h2 id="http-trigger">HTTP Trigger</h2>
<p>Timer-triggered functions are useful, but that&rsquo;s just one limited use case.
Several other event types can trigger Azure Functions, and for all of them
you can create precompiled functions and run them locally.</p>
<p>The most ubiquotous trigger for any serverless app is probably HTTP. So,
for the rest of the article I will focus on several approaches to
implement HTTP functions. Nonetheless, the same techique can be applied to
other triggers too.</p>
<p>F# code for the simplest HTTP Function can look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">namespace</span> PrecompiledApp

<span style="color:#00f">open</span> Microsoft.AspNetCore.Mvc
<span style="color:#00f">open</span> Microsoft.AspNetCore.Http
<span style="color:#00f">open</span> Microsoft.Azure.WebJobs.Host

<span style="color:#00f">module</span> PrecompiledHttp =

  <span style="color:#00f">let</span> run(req: HttpRequest, log: TraceWriter) =
    log.Info(<span style="color:#a31515">&#34;F# HTTP trigger function processed a request.&#34;</span>)
    ContentResult(Content = <span style="color:#a31515">&#34;HO HO HO Merry Christmas&#34;</span>, ContentType = <span style="color:#a31515">&#34;text/html&#34;</span>)
</code></pre></div><p>You can find a full example of HTTP Function App
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/5-precompiled">here</a>.</p>
<p>This code is using ASP.NET Core classes for request and response. It&rsquo;s still
just an F# function, so we need to bind it to a trigger in <code>function.json</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;bindings&#34;: [
    {
      &#34;type&#34;: <span style="color:#a31515">&#34;httpTrigger&#34;</span>,
      &#34;methods&#34;: [<span style="color:#a31515">&#34;get&#34;</span>],
      &#34;authLevel&#34;: <span style="color:#a31515">&#34;anonymous&#34;</span>,
      &#34;name&#34;: <span style="color:#a31515">&#34;req&#34;</span>,
      &#34;route&#34;: <span style="color:#a31515">&#34;hellosanta&#34;</span>
    }
  ],
  &#34;scriptFile&#34;: <span style="color:#a31515">&#34;../bin/PrecompiledApp.dll&#34;</span>,
  &#34;entryPoint&#34;: <span style="color:#a31515">&#34;PrecompiledApp.PrecompiledHttp.run&#34;</span>
}
</code></pre></div><p>If you run the app, the function will be hosted at localhost</p>
<p><img src="./httpstart.png" alt="HTTP Trigger Working"></p>
<p>And a request to <code>http://localhost:7071/api/hellosanta</code> will get responded
with our &ldquo;HO HO HO&rdquo; message.</p>
<p>This function is of &ldquo;Hello World&rdquo; level, but the fact that it&rsquo;s inside a
normal F# library gives you lots of power.</p>
<p>Let&rsquo;s see at some examples of how to use it.</p>
<h2 id="suave-function">Suave Function</h2>
<p>What can we do to enhance developer experience? We can use our
favourite F# libraries.</p>
<p><a href="http://suave.io/">Suave</a> is one of the most popular libraries to
implement Web API&rsquo;s with. And we can use it in Azure Functions too!</p>
<p>Let&rsquo;s first make a small twist to HTTP trigger definition in <code>function.json</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#a31515">&#34;bindings&#34;</span><span style="">:</span> [
  {
    &#34;type&#34;: <span style="color:#a31515">&#34;httpTrigger&#34;</span>,
    &#34;methods&#34;: [<span style="color:#a31515">&#34;get&#34;</span>],
    &#34;authLevel&#34;: <span style="color:#a31515">&#34;anonymous&#34;</span>,
    &#34;name&#34;: <span style="color:#a31515">&#34;req&#34;</span>,
    &#34;route&#34;: <span style="color:#a31515">&#34;{*anything}&#34;</span>
  }
]<span style="">,</span>
</code></pre></div><p>Binding now defines a wildcard route to redirect all requests
to this function. That&rsquo;s because we want Suave to take care of routing
for us.</p>
<p>The definition of such routing will look familiar to all Suave users:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> App =
  <span style="color:#00f">open</span> Suave
  <span style="color:#00f">open</span> Suave.Successful
  <span style="color:#00f">open</span> Suave.Operators
  <span style="color:#00f">open</span> Suave.Filters

  <span style="color:#00f">let</span> app =
    GET &gt;=&gt; choose
      [ path <span style="color:#a31515">&#34;/api/what&#34;</span> &gt;=&gt; OK <span style="color:#a31515">&#34;Every time we love, every time we give, it&#39;s Christmas.&#34;</span>
        path <span style="color:#a31515">&#34;/api/when&#34;</span> &gt;=&gt; OK <span style="color:#a31515">&#34;Christmas isn&#39;t a season. It&#39;s a feeling.&#34;</span>
        path <span style="color:#a31515">&#34;/api/how&#34;</span> &gt;=&gt; OK <span style="color:#a31515">&#34;For it is in giving that we receive.&#34;</span> ]
</code></pre></div><p>Azure Function is just a one-liner wiring Suave app into the pipeline:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Http =
  <span style="color:#00f">open</span> Suave.Azure.Functions.Context

  <span style="color:#00f">let</span> run req =
    req |&gt; runWebPart App.app  |&gt; Async.StartAsTask
</code></pre></div><p>The heavy lifting is done by <code>runWebPart</code> function, which is a utility
function defined in the same application. You can see the full code
of this wiring in <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/7-suave">my repo</a>.</p>
<p>Run the application and request the URL <code>http://localhost:7071/api/what</code>
to see the function in action.</p>
<p>This example is very simple, but you can do lots of powerful stuff with Suave!
Most probably, you shouldn&rsquo;t go over the root and try to fit whole
mulpti-resource REST API into a single Azure Function. But it might still
make sense to keep related HTTP calls together, and Suave can help to keep
it cleaner.</p>
<h2 id="managing-dependencies-with-paket">Managing Dependencies with Paket</h2>
<p>Once your Function App becomes bigger and you start using multiple F#
projects, it makes sense to switch to <a href="https://fsprojects.github.io/Paket/">Paket</a>
package manager.</p>
<p>It is totally possible to use Paket with Azure Functions. There isn&rsquo;t much
specific to Azure Functions, really. Here is an example of <code>paket.dependecies</code>
file</p>
<pre><code>source https://www.nuget.org/api/v2

framework: &gt;= netstandard2.0
nuget FSharp.Core
nuget Microsoft.NET.Sdk.Functions
nuget Microsoft.AspNetCore.Mvc.Core
</code></pre><p>that I used in <a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/8-paket">example</a>
which demonstrates Paket + Functions combination.</p>
<h2 id="attribute-based-functions">Attribute-Based Functions</h2>
<p>Up until now, we were writing <code>function.json</code> files manually for each
function. This is not very tedious, but it is error prone. Microsoft offers an alternative
programming model where these files are auto-generated by Functions SDK.</p>
<p>This programming model is based on attributes, which are similar to WebJobs
SDK attributes. With this approach, there&rsquo;s no <code>function.json</code> file in
the project. Instead, the function declaration is decorated with attributes:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">[&lt;FunctionName(<span style="color:#a31515">&#34;AttributeBased&#34;</span>)&gt;]
<span style="color:#00f">let</span> run([&lt;HttpTrigger&gt;] req: HttpRequest, log: TraceWriter)
</code></pre></div><p>The same development flow still works. Once you run <code>dotnet build</code>, a new
<code>function.json</code> file will be generated and placed into <code>bin</code> folder. Functions
runtime will be able to use it to run the function as usual.</p>
<p>Note that the generated file looks a bit different from the manual
equivalent:</p>
<ol>
<li>
<p>It manifests itself with</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#a31515">&#34;generatedBy&#34;</span><span style="">:</span> <span style="color:#a31515">&#34;Microsoft.NET.Sdk.Functions.Generator-1.0.6&#34;</span><span style="">,</span>
<span style="color:#a31515">&#34;configurationSource&#34;</span><span style="">:</span> <span style="color:#a31515">&#34;attributes&#34;</span><span style="">,</span>
</code></pre></div></li>
<li>
<p>In case you use input and output bindings, you won&rsquo;t be able to see them
in the generated file. Only trigger will be visible in <code>json</code>. Don&rsquo;t worry,
input and output bindings will still work.</p>
</li>
</ol>
<p>You can find an example of HTTP function with attributes
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples/tree/master/9-attributes">here</a>.</p>
<p>There are pro&rsquo;s and con&rsquo;s in this model. Obviously, not having to write
JSON files manually is beneficial. Some people find the binding attributes
really ugly though, especially when you have 3 or 4 bindings and each has
multiple parameters.</p>
<p>My preference is to use attributes, but don&rsquo;t mix attribute decoration
with real code. I.e. keep the Function&rsquo;s body to a simple 1-liner, and
delegate the call to a properly defined F# function with the actual
domain logic.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>Lots of F# users value the language for how quickly one can be productive
with it: based on concise syntax, powerful libraries and tools like FSI.</p>
<p>In my opinion, Azure Functions fit nicely into the picture. It takes just
several minutes before you can run your first Function App on developer
machine, and then seamlessly transfer it into the cloud.</p>
<p>I&rsquo;ve prepared a github repository where you can find more
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples">Examples of Azure Functions implemented in F#</a>.</p>
<p>Merry Serverless Functional Christmas!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/advent-calendar" term="advent-calendar" label="Advent Calendar" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure F#unctions Talk at FSharping Meetup in Prague]]></title>
            <link href="https://mikhail.io/2017/11/azure-functions-fsharp-talk/"/>
            <id>https://mikhail.io/2017/11/azure-functions-fsharp-talk/</id>
            
            <published>2017-11-10T00:00:00+00:00</published>
            <updated>2017-11-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On November 8th 2017 I gave a talk about developing Azure Functions
in F# at
<a href="https://www.meetup.com/FSharping/events/244137693/">FSharping</a>
meetup in Prague.</p>
<p>I really enjoyed giving this talk: the audience was
great and asked awesome questions. One more prove that F# community is
so welcoming and energizing!</p>
<p>All the demos of that session can be found in my
<a href="https://github.com/mikhailshilkov/azure-functions-fsharp-examples">github repository</a>.</p>
<p>The slides were only a small portion of my talk, but you can see them
below anyways.</p>
<p>Link to full-screen HTML slides:
<a href="https://mikhail.io/talks/fsharping-azure-functions/">Azure F#unctions</a></p>
<p>Slides on SlideShare:</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/oQIZywbdCRXdQA"
width="595" height="485"
frameborder="0" marginwidth="0" marginheight="0" scrolling="no"
style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<p>Thanks for attending my talk! Feel free to post any feedback in the comments.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/talk" term="talk" label="Talk" />
                             
                                <category scheme="https://mikhail.io/tags/meetup" term="meetup" label="Meetup" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/slides" term="slides" label="Slides" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Meetup]]></title>
            <link href="https://mikhail.io/tags/meetup/"/>
            <id>https://mikhail.io/tags/meetup/</id>
            
            <published>2017-11-10T00:00:00+00:00</published>
            <updated>2017-11-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Slides]]></title>
            <link href="https://mikhail.io/tags/slides/"/>
            <id>https://mikhail.io/tags/slides/</id>
            
            <published>2017-11-10T00:00:00+00:00</published>
            <updated>2017-11-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Event Grid]]></title>
            <link href="https://mikhail.io/tags/azure-event-grid/"/>
            <id>https://mikhail.io/tags/azure-event-grid/</id>
            
            <published>2017-10-05T00:00:00+00:00</published>
            <updated>2017-10-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Function Triggered by Azure Event Grid]]></title>
            <link href="https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid/"/>
            <id>https://mikhail.io/2017/10/azure-function-triggered-by-azure-event-grid/</id>
            
            <published>2017-10-05T00:00:00+00:00</published>
            <updated>2017-10-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>Update: I missed the elephant in the room. There actually exists a specialized
trigger for Event Grid binding. In the portal, just select <code>Experimental</code>
in <code>Scenario</code> drop down while creating the function. In precompiled
functions, reference <code>Microsoft.Azure.WebJobs.Extensions.EventGrid</code> NuGet
package.</em></p>
<p><em>The rest of the article describes my original approach to trigger an
Azure Function from <a href="https://azure.microsoft.com/en-us/services/event-grid/">Azure Event Grid</a>
with generic Web Hook trigger.</em></p>
<p>Here are the steps to follow:</p>
<h2 id="create-a-function-with-webhook-trigger">Create a Function with Webhook Trigger</h2>
<p>I&rsquo;m not aware of a specialized trigger type for Event Grid, so
I decided to use Generic Webhook trigger (which is essentially an
HTTP trigger).</p>
<p>I used the Azure Portal to generate a function, so here is the
<code>function.json</code> that I got:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;bindings&#34;: [
    {
      &#34;type&#34;: <span style="color:#a31515">&#34;httpTrigger&#34;</span>,
      &#34;direction&#34;: <span style="color:#a31515">&#34;in&#34;</span>,
      &#34;webHookType&#34;: <span style="color:#a31515">&#34;genericJson&#34;</span>,
      &#34;name&#34;: <span style="color:#a31515">&#34;req&#34;</span>
    },
    {
      &#34;type&#34;: <span style="color:#a31515">&#34;http&#34;</span>,
      &#34;direction&#34;: <span style="color:#a31515">&#34;out&#34;</span>,
      &#34;name&#34;: <span style="color:#a31515">&#34;res&#34;</span>
    }
  ],
  &#34;disabled&#34;: <span style="color:#00f">false</span>
}
</code></pre></div><p>For precompiled functions, just decorate it with <code>HttpTriggerAttribute</code> with
POST method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> Task&lt;HttpResponseMessage&gt; Run(
    [HttpTrigger(AuthorizationLevel.Function, &#34;post&#34;)] HttpRequestMessage req)
</code></pre></div><h2 id="parse-the-payload">Parse the Payload</h2>
<p>Events from Event Grid will arrive in a specific predefined JSON format.
Here is an example of events to expect:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[{
  &#34;id&#34;: <span style="color:#a31515">&#34;0001&#34;</span>,
  &#34;eventType&#34;: <span style="color:#a31515">&#34;MyHelloWorld&#34;</span>,
  &#34;subject&#34;: <span style="color:#a31515">&#34;Hello World!&#34;</span>,
  &#34;eventTime&#34;: <span style="color:#a31515">&#34;2017-10-05T08:53:07&#34;</span>,
  &#34;data&#34;: {
    &#34;hello&#34;: <span style="color:#a31515">&#34;world&#34;</span>
  },
  &#34;topic&#34;: <span style="color:#a31515">&#34;/SUBSCRIPTIONS/GUID/RESOURCEGROUPS/NAME/PROVIDERS/MICROSOFT.EVENTGRID/TOPICS/MY-EVENTGRID-TOPIC1&#34;</span>
}]
</code></pre></div><p>To be able to parse those data more easily, I defined a C# class to deserialize
JSON to:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">GridEvent</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Id { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> EventType { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Subject { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> DateTime EventTime { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> Dictionary&lt;<span style="color:#2b91af">string</span>, <span style="color:#2b91af">string</span>&gt; Data { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Topic { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>Now, the function can read the events (note, that they are sent in arrays)
from the body of POST request:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;HttpResponseMessage&gt; Run(HttpRequestMessage req, TraceWriter log)
{
    <span style="color:#2b91af">string</span> jsonContent = <span style="color:#00f">await</span> req.Content.ReadAsStringAsync();
    <span style="color:#2b91af">var</span> events = JsonConvert.DeserializeObject&lt;GridEvent[]&gt;(jsonContent);

    <span style="color:#008000">// do something with events
</span><span style="color:#008000"></span>
    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK);
}
</code></pre></div><h2 id="validate-the-endpoint">Validate the Endpoint</h2>
<p>To prevent you from sending events to endpoints that you don&rsquo;t own, Event
Grid requires each subsriber to validate itself. For this purpose, Event
Grid will send events of the special type <code>SubscriptionValidation</code>.</p>
<p>The validation request will contain a code, which we need to echo back in
200-OK HTTP response.</p>
<p>Here is a small piece of code to do just that:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">if</span> (req.Headers.GetValues(<span style="color:#a31515">&#34;Aeg-Event-Type&#34;</span>).FirstOrDefault() == <span style="color:#a31515">&#34;SubscriptionValidation&#34;</span>)
{
    <span style="color:#2b91af">var</span> code = events[0].Data[<span style="color:#a31515">&#34;validationCode&#34;</span>];
    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK,
        <span style="color:#00f">new</span> { validationResponse = code });
}
</code></pre></div><p>The function is ready!</p>
<h2 id="create-a-custom-event-grid-topic">Create a Custom Event Grid Topic</h2>
<p>To test it out, go to the portal and create a custom Event Grid topic.
Then click on Add Event Subscription button, give it a name and copy paste
the function URL (including key) to Subscriber Endpoint field:</p>
<p><img src="function-url.png" alt="Azure Function URL"></p>
<p><img src="event-subscription.png" alt="Event Grid Subscription"></p>
<p>Creating a subscription will immediately trigger a validation request to
your function, so you should see one invocation in the logs.</p>
<h2 id="send-custom-events">Send Custom Events</h2>
<p>Now, go to your favorite HTTP client (curl, Postman, etc) and send a sample
event to check how the whole setup works:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-http" data-lang="http">POST /api/events <span style="color:#00f">HTTP</span>/1.1
Host: &lt;your-eventgrid-topic&gt;.westus2-1.eventgrid.azure.net
aeg-sas-key: &lt;key&gt;
Content-Type: application/json

[{
  &#34;id&#34;: <span style="color:#a31515">&#34;001&#34;</span>,
  &#34;eventType&#34;: <span style="color:#a31515">&#34;MyHelloWorld&#34;</span>,
  &#34;subject&#34;: <span style="color:#a31515">&#34;Hello World!&#34;</span>,
  &#34;eventTime&#34;: <span style="color:#a31515">&#34;2017-10-05T08:53:07&#34;</span>,
  &#34;data&#34;: {
    &#34;hello&#34;: <span style="color:#a31515">&#34;world&#34;</span>
  }
}]
</code></pre></div><p>Obviously, adjust the endpoint and key based on the data from the portal.</p>
<p>You should get a 200-OK back and then see your event in Azure Function
invocation logs.</p>
<p>Have fun!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/azure-event-grid" term="azure-event-grid" label="Azure Event Grid" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Architecture]]></title>
            <link href="https://mikhail.io/tags/architecture/"/>
            <id>https://mikhail.io/tags/architecture/</id>
            
            <published>2017-09-25T00:00:00+00:00</published>
            <updated>2017-09-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Data Processing]]></title>
            <link href="https://mikhail.io/tags/data-processing/"/>
            <id>https://mikhail.io/tags/data-processing/</id>
            
            <published>2017-09-25T00:00:00+00:00</published>
            <updated>2017-09-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Stream Processing]]></title>
            <link href="https://mikhail.io/tags/stream-processing/"/>
            <id>https://mikhail.io/tags/stream-processing/</id>
            
            <published>2017-09-25T00:00:00+00:00</published>
            <updated>2017-09-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Wanted: Effectively-Once Processing in Azure]]></title>
            <link href="https://mikhail.io/2017/09/wanted-effectively-once-processing-in-azure/"/>
            <id>https://mikhail.io/2017/09/wanted-effectively-once-processing-in-azure/</id>
            
            <published>2017-09-25T00:00:00+00:00</published>
            <updated>2017-09-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Are there any known patterns / tools / frameworks to provide scalable, stateful, effectively-once, end-to-end processing of messages, to be hosted in Azure?</blockquote><p><em>This experimental post is a question. The question is too broad for StackOverflow, so I&rsquo;m posting it here. Please engage in the comments section, or forward the link to subject experts.</em></p>
<p>TL;DR: Are there any known patterns / tools / frameworks to provide scalable, stateful, effectively-once, end-to-end
processing of messages, to be hosted in Azure, preferably on PaaS-level of service?</p>
<h2 id="motivational-example">Motivational Example</h2>
<p>Let&rsquo;s say we are making a TODO app. There is a constant flow of requests
to create a TODO in the system. Each request contains just two fields:
a title and a project ID which TODO should belong to. Here is the definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">TodoRequest</span> = {
  ProjectId: int
  Title: <span style="color:#2b91af">string</span>
}
</code></pre></div><p>Now, we want to process the request and assign each TODO an identifier,
which should be an auto-incremented integer. Numeration is unique per project,
so each TODO must have its own combination of <code>ProjectId</code> and <code>Id</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Todo</span> = {
  ProjectId: int
  Id: int
  Title: <span style="color:#2b91af">string</span>
}
</code></pre></div><p>Now, instead of relying on some database sequences, I want to describe this
transformation as a function. The function has the type <code>(TodoRequest, int) -&gt; (Todo, int)</code>, i.e. it transforms a tuple of a request and current per-project
state (last generated ID) to a tuple of a TODO and post-processing state:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> create (request: TodoRequest, state: int) =
  <span style="color:#00f">let</span> nextId = state + 1
  <span style="color:#00f">let</span> todo = {
    ProjectId = request.ProjectId
    Id = nextId
    Title = request.Title
  }
  todo, nextId
</code></pre></div><p>This is an extremely simple function, and I can use it to great success to
process local, non-durable data.</p>
<p>But if I need to make a reliable distributed application out of it, I need
to take care of lots of things:</p>
<ol>
<li>
<p>No request should be lost. I need to persist all the requests into
a durable storage in case of processor crash.</p>
</li>
<li>
<p>Similarly, I need to persist TODO&rsquo;s too. Presumably, some downstream
logic will use the persisted data later on in TODO&rsquo;s lifecycle.</p>
</li>
<li>
<p>The state (the counter) must be durable too. In case of crash of processing
function, I want to be able to restart processing after recovery.</p>
</li>
<li>
<p>Processing of the requests should be sequential per project ID. Otherwise
I might get a clash of ID&rsquo;s in case two requests belonging to the same
project are processed concurrently.</p>
</li>
<li>
<p>I still want requests to different projects to be processed in parallel,
to make sure the system scales up with the growth of project count.</p>
</li>
<li>
<p>There must be no holes or duplicates in TODO numbering per project, even
in face of system failures. In worst case, I agree to tolerate a duplicated
entry in the output log, but it must be exactly the same entry (i.e. two
entries with same project id, id and title).</p>
</li>
<li>
<p>The system should tolerate a permanent failure of any single hardware
dependency and automatically fail-over within reasonable time.</p>
</li>
</ol>
<p>It&rsquo;s not feasible to meet all of those requirements without relying on some
battle-tested distributed services or frameworks.</p>
<p>Which options do I know of?</p>
<h2 id="transactions">Transactions</h2>
<p>Traditionally, this kind of requirements were solved by using transactions
in something like SQL Server. If I store requests, TODO&rsquo;s and current ID per
project in the same relational database, I can make each processing step a
single atomic transaction.</p>
<p>This addresses all the concerns, as long as we can stay inside the single
database. That&rsquo;s probably a viable option for the TODO app, but less of so
if I convert my toy example to some real applications like IoT data
processing.</p>
<p>Can we do the same for distributed systems at scale?</p>
<h2 id="azure-event-hubs">Azure Event Hubs</h2>
<p>Since I touched IoT space, the logical choice would be to store our entries
in Azure Event Hubs. That works for many criteria, but I don&rsquo;t see any available
approach to make such processing consistent in the face of failures.</p>
<p>When processing is done, we need to store 3 pieces: generated TODO event,
current processing offset and current ID. Event goes to another event hub,
processing offset is stored in Blob Storage and ID can be saved to something
like Table Storage.</p>
<p>But there&rsquo;s no way to store those 3 pieces atomically. Whichever order we
choose, we are bound to get anomalies in some specific failure modes.</p>
<h2 id="azure-functions">Azure Functions</h2>
<p>Azure Functions don&rsquo;t solve those problems. But I want to mention this
Function-as-a-Service offering because they provide an ideal programming
model for my use case.</p>
<p>I need to take just one step from my domain function to Azure Function:
to define bindings for e.g. Event Hubs and Table Storage.</p>
<p>However, reliability guarantees will stay poor. I won&rsquo;t get neither sequential
processing per Event Hub partition key, nor atomic state commit.</p>
<h2 id="azure-service-fabric">Azure Service Fabric</h2>
<p>Service Fabric sounds like a good candidate service for reliable processing.
Unfortunately, I don&rsquo;t have much experience with it to judge.</p>
<p>Please leave a comment if you do.</p>
<h2 id="jvm-world">JVM World</h2>
<p>There are products in JVM world which claim to solve my problem perfectly.</p>
<p>Apache Kafka was the inspiration for Event Hubs log-based messaging. The recent
Kafka release provides effectively-once processing semantics as long as
data stay inside Kafka. Kafka does that with atomic publishing to multiple
topics, and state storage based on compacted topics.</p>
<p>Apache Flink has similar guarantees for its stream processing APIs.</p>
<p>Great, but how do I get such awesomeness in .NET code, and without installing
expensive ZooKeeper-managed clusters?</p>
<h2 id="call-for-feedback">Call for Feedback</h2>
<p>Do you know a solution, product or service?</p>
<p>Have you developed effectively-once processing on .NET / Azure stack?</p>
<p>Are you in touch with somebody who works on such framework?</p>
<p>Please leave a comment, or ping me on Twitter.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/architecture" term="architecture" label="Architecture" />
                             
                                <category scheme="https://mikhail.io/tags/data-processing" term="data-processing" label="Data Processing" />
                             
                                <category scheme="https://mikhail.io/tags/stream-processing" term="stream-processing" label="Stream Processing" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions: Are They Really Infinitely Scalable and Elastic?]]></title>
            <link href="https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/"/>
            <id>https://mikhail.io/2017/08/azure-functions-are-they-really-infinitely-scalable-and-elastic/</id>
            
            <published>2017-08-31T00:00:00+00:00</published>
            <updated>2017-08-31T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>Updated results are available at
<a href="https://mikhail.io/2017/12/azure-functions-get-more-scalable-and-elastic/">Azure Functions Get More Scalable and Elastic</a>.</em></p>
<p>Automatic elastic scaling is a built-in feature of Serverless computing
paradigm. One doesn&rsquo;t have to provision servers anymore, they just need to
write code that will be provisioned on as many servers as needed based on the
actual load. That&rsquo;s the theory.</p>
<p>In particular, Azure Functions can be hosted on the Consumption plan:</p>
<blockquote>
<p>The Consumption plan automatically allocates compute power when your
code is running, scales out as necessary to handle load, and then scales
down when code is not running.</p>
</blockquote>
<p>In this post I will run a simple stress test to get a feel of how such
automatic allocation works in practice and what kind of characteristics
we can rely on.</p>
<h2 id="setup">Setup</h2>
<p>Here are the parameters that I chose for my test of today:</p>
<ul>
<li>Azure Function written in C# and hosted on Consumption plan</li>
<li>Triggered by Azure Storage Queue binding</li>
<li>Workload is strictly CPU-bound, no I/O is executed</li>
</ul>
<p>Specifically, each queue item represents one password that I need to hash.
Each function call performs 12-round <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hashing. Bcrypt is a slow algorithm recommended for
password hashing, because it makes potential hash collision attacks really
hard and costly.</p>
<p>My function is based on <a href="https://github.com/BcryptNet/bcrypt.net">Bcrypt.Net</a>
implementation, and it&rsquo;s extremely simple:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> Run([QueueTrigger(<span style="color:#a31515">&#34;bcrypt-password&#34;</span>)] <span style="color:#2b91af">string</span> password)
{
    BCrypt.Net.BCrypt.HashPassword(password, 12);
}
</code></pre></div><p>It turns out that a single execution of this function takes approximately
1 second on an instance of Consumption plan, and consumes 100% CPU during
that second.</p>
<p>Now, the challenge is simple. I send 100,000 passwords
to the queue and see how long it will take to hash them, and also how the
autoscaling will behave. I will run it two times, with different pace of
sending messages to the queue.</p>
<p>That sounds like a perfect job for a Function App on Consumption plan:</p>
<ul>
<li>Needs to scale based on load</li>
<li>CPU intensive - easy to see how busy each server is</li>
<li>Queue-based - easy to see the incoming vs outgoing rate</li>
</ul>
<p>Let&rsquo;s see how it went.</p>
<h2 id="experiment-1-steady-load">Experiment 1: Steady Load</h2>
<p>In my first run, I was sending messages at constant rate. 100,000 messages
were sent within 2 hours, without spikes or drops in the pace.</p>
<p>Sounds like an easy job for autoscaling facilities. But here is the actual
chart of data processing:</p>
<p><img src="FunctionAppScaling.png" alt="Function App Scaling"></p>
<p>The horizontal axis is time in minutes since the first message came in.</p>
<p>The orange line shows the queue backlog - the amount of messages sitting in
the queue at a given moment.</p>
<p>The blue area represents the amount of instances (virtual servers) allocated
to the function by Azure runtime (see the numbers at the right side).</p>
<p>We can divide the whole process into 3 logical segments, approximately
40 minutes each:</p>
<p><strong>Laging behind</strong>. Runtime starts with 0 instances, and immediately switches
to 1 when the first message comes in. However it&rsquo;s reluctant to add any more
servers for the next 20 (!) minutes. The scaling heuristic is probably based
on the past history for this queue/function, and it wasn&rsquo;t busy at all during
the hours before.</p>
<p>After 20 minutes, the runtime starts adding more instances: it goes up to 2,
then jumps to 4, then reaches 5 at minute 40. The CPU is constantly at
100% and the queue backlog grows linearly.</p>
<p><strong>Rapid scale up</strong>. After minute 40, it looks like the runtime realizes
that it needs more power. Much more power! The growth speeds up real quick
and by minute 54 the backlog stops growing, even though the messages are still
coming in. But there are now 21 instances working, which is enough to
finally match and beat the rate of incoming messages.</p>
<p>The runtime doesn&rsquo;t stop growing though. CPU&rsquo;s are still at 100%, and the backlog
is still very high, so the scaling goes up and up. The amount of instances
reaches astonishing 55, at which point all the backlog is processed and
there are no messages in the queue.</p>
<p><strong>Searching for balance</strong>. When queue is almost empty and CPU drops below
100% for the first time, the runtime decides to scale down. It does that quickly
and aggressively, switching from 55 to 21 instances in just 2 minutes.</p>
<p>From there it keeps slowly reducing the number of instances until the backlog
starts growing again. The runtime allows the backlog to grow a bit, but
then figures out a balanced number of servers (17) to keep the backlog flat
at around 2,000 messages.</p>
<p>It stays at 17 until the producer stops sending new messages. The backlog
goes to 0, and the amount of instances gradually drops to 0 within 10 minutes.</p>
<p>The second chart from the same experiment looks very similar, but it shows
different metrics:</p>
<p><img src="FunctionAppDelay.png" alt="Function App Delay"></p>
<p>The gray line is the delay in minutes since the currently processed message
got enqueued (message &ldquo;age&rdquo;, in-queue latency). The blue line is the
total processing rate, measured in messages per minute.</p>
<p>Due to perfect scalability and stability of my function, both charts are almost
exactly the same. I&rsquo;ve put it here so that you could see that the slowest
message spent more than 40 minutes sitting inside the queue.</p>
<h2 id="experiment-2-spiky-load">Experiment 2: Spiky Load</h2>
<p>With the second run, I tried to emulate a spiky load profile. I was sending
my 100,000 messages throughout 6 hours at lower pace than during the first
run. But sometimes the producer switched to fast mode and sent a bigger bunch
of messages in just several minutes. Here is the actual chart of incoming
message rate:</p>
<p><img src="SpikyLoad.png" alt="Spiky Load"></p>
<p>It&rsquo;s easy to imagine some service which has a usage pattern like that, when
spikes of the events happen from time to time, or in rush hours.</p>
<p>This is how the Function App managed to process the messages:</p>
<p><img src="SpikyLoadProcessing.png" alt="Spiky Load Processing Result"></p>
<p>The green line still shows the amount of incoming messages per minute. The
blue line denotes how many messages were actually processed at that minute.
And the orange bars are queue backlogs - the amount of messages pending.</p>
<p>Here are several observations:</p>
<ul>
<li>
<p>Obviously, processing latency is way too far from real time. There is
constantly quite a significant backlog in the queue, and processing delay
reaches 20 minutes at peak.</p>
</li>
<li>
<p>It took the runtime 2 hours to clean the backlog for the first time. Even
without any spikes during the first hour, the autoscaling algorithm needs
time to get up to speed.</p>
</li>
<li>
<p>Function App runtime is able to scale up quite fast (look at the reaction
on the fourth spike), but it&rsquo;s not really willing to do that most of the time.</p>
</li>
<li>
<p>The growth of the backlog after minute 280 is purely caused by wrong
decision of runtime. While the load is completely steady, the runtime
decided to shut down most workers after 20 minutes of empty backlog, and could
not recover for the next hour.</p>
</li>
</ul>
<h2 id="conclusions">Conclusions</h2>
<p>I tried to get a feeling about the ability of Azure Functions to scale
on demand, adapting to the workload. The function under test was purely CPU-bound,
and for that I can give two main conclusions:</p>
<ul>
<li>
<p>Function Apps are able to scale to high amount of instances running at the
same time, and to eventually process large parallel jobs (at least up to 55
instances).</p>
</li>
<li>
<p>Significant processing delays are to be expected for heavy loads. Function
App runtime has quite some inertia, and the resulting processing latency can
easily go up to tens of minutes.</p>
</li>
</ul>
<p>If you know how these results can be improved, or why they are less than
optimal, please leave a comment or contact me directly.</p>
<p>I look forward to conducting more tests in the future!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Authoring a Custom Binding for Azure Functions]]></title>
            <link href="https://mikhail.io/2017/07/authoring-custom-binding-azure-functions/"/>
            <id>https://mikhail.io/2017/07/authoring-custom-binding-azure-functions/</id>
            
            <published>2017-07-26T00:00:00+00:00</published>
            <updated>2017-07-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The process of creating a custom binding for Azure Functions.</blockquote><p>In my <a href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/">previous post</a>
I described how I used Durable Functions extensions
in Azure Function App. Durable Functions are using several binding types
that are not part of the standard suite: <code>OrchestrationClient</code>,
<code>OrchestrationTrigger</code>, <code>ActivityTrigger</code>. These custom bindings
<a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">are installed</a>
by copying the corresponding assemblies to a special Extensions folder.</p>
<p>Although Bring-Your-Own-Binding (BYOB) feature hasn&rsquo;t been released yet, I
decided to follow the path of Durable Functions and create my own
custom binding.</p>
<h2 id="configuration-binding">Configuration Binding</h2>
<p>I&rsquo;ve picked a really simple use case for my first experiments with custom
bindings: reading configuration values.</p>
<p>Azure Functions store their configuration values in App Settings (local
runtime uses <code>local.settings.json</code> file for that).</p>
<p>That means, when you need a configuration value inside your C# code,
you normally do</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">string</span> setting = ConfigurationManager.AppSettings[<span style="color:#a31515">&#34;MySetting&#34;</span>];
</code></pre></div><p>Alternatively, <code>Environment.GetEnvironmentVariable()</code> method can be used.</p>
<p>When I <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">needed to collect</a>
service bus subscription metrics, I wrote this kind of bulky code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> resourceToScale = ConfigurationManager.AppSettings[<span style="color:#a31515">&#34;ResourceToScale&#34;</span>];

<span style="color:#2b91af">var</span> connectionString = ConfigurationManager.AppSettings[<span style="color:#a31515">&#34;ServiceBusConnection&#34;</span>];
<span style="color:#2b91af">var</span> topic = ConfigurationManager.AppSettings[<span style="color:#a31515">&#34;Topic&#34;</span>];
<span style="color:#2b91af">var</span> subscription = ConfigurationManager.AppSettings[<span style="color:#a31515">&#34;Subscription&#34;</span>];
</code></pre></div><p>The code is no rocket science, but it&rsquo;s tedious to write, so instead I came
up with this idea to define Functions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> MyFunction(
    [TimerTrigger(&#34;0 */1 * * * *&#34;)] TimerInfo timer,
    [Configuration(Key = &#34;ResourceToScale&#34;)] <span style="color:#2b91af">string</span> resource,
    [Configuration] ServiceBusSubscriptionConfig config)
</code></pre></div><p>Note two usages of <code>Configuration</code> attribute. The first one defines the
specific configuration key, and binds its value to a string parameter. The
other one binds <em>multiple</em> configuration values to a POCO parameter. I defined
the config class as</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ServiceBusSubscriptionConfig</span>
{
    <span style="color:#00f">public</span> ServiceBusSubscriptionConfig(<span style="color:#2b91af">string</span> serviceBusConnection, <span style="color:#2b91af">string</span> topic, <span style="color:#2b91af">string</span> subscription)
    {
        ServiceBusConnection = serviceBusConnection;
        Topic = topic;
        Subscription = subscription;
    }

    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> ServiceBusConnection { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Topic { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Subscription { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>The immutable class is a bit verbose, but I still prefer it over get-set
container in this scenario.</p>
<p>The binding behavior is convention-based in this case: the binding engine
should load configuration values based on the names of class properties.</p>
<h2 id="motivation">Motivation</h2>
<p>So, why do I need such binding?</p>
<p>As I said, it&rsquo;s a simple use case to play with BYOB feature, and overall,
<strong>understand</strong> the internals of Function Apps a bit better.</p>
<p>But apart from that, I removed 4 lines of garbage from the function body
(at the cost of two extra parameters). <strong>Less noise</strong> means more readable code,
especially when I put this code on a webpage.</p>
<p>As a bonus, the <strong>testability</strong> of the function immediately increased. It&rsquo;s so
much easier for the test just to accept the configuration as input parameter,
instead of fine-tuning the configuration files inside test projects, or
hiding <code>ConfigurationManager</code> usage behind a mockable facade.</p>
<p>Such approach does seem to be the strength of Azure Functions code in
general. It&rsquo;s often possible to reduce imperative IO-related code to
attribute-decorated function parameters.</p>
<h2 id="implementing-a-custom-binding">Implementing a Custom Binding</h2>
<p>The actual implementation process of a custom non-trigger binding is quite
simple:</p>
<p><strong>Create a class library</strong> with the word &ldquo;Extension&rdquo; in its name. Import
<code>Microsoft.Azure.WebJobs</code> and <code>Microsoft.Azure.WebJobs.Extensions</code> NuGet
packages (at the time of writing I used <code>2.1.0-beta1</code> version).</p>
<p><strong>Define</strong> a class for binding attribute:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[AttributeUsage(AttributeTargets.Parameter)]
[Binding]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ConfigurationAttribute</span> : Attribute
{
    [AutoResolve]
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Key { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>The attribute is marked as <code>Binding</code> and the <code>Key</code> property is marked as
resolvable from <code>function.json</code>.</p>
<p><strong>Implement</strong> <code>IExtensionConfigProvider</code> which will tell the function runtime
how to use your binding correctly.</p>
<p>The interface has just one method to implement:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ConfigurationExtensionConfigProvider</span> : IExtensionConfigProvider
{
    <span style="color:#00f">public</span> <span style="color:#00f">void</span> Initialize(ExtensionConfigContext context)
    {
        <span style="color:#008000">// ... see below
</span><span style="color:#008000"></span>    }
}

</code></pre></div><p>The first step of the implementation is to define a rule for our new
<code>ConfigurationAttribute</code> and tell this rule how to get a string value out
of any attribute instance:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> rule = context.AddBindingRule&lt;ConfigurationAttribute&gt;();
rule.BindToInput&lt;<span style="color:#2b91af">string</span>&gt;(a =&gt; ConfigurationManager.AppSettings[a.Key]);
</code></pre></div><p>That&rsquo;s really all that needs to happen to bind <code>string</code> parameters.</p>
<p>To make our binding work with any POCO, we need a more elaborate construct:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">rule.BindToInput&lt;Env&gt;(_ =&gt; <span style="color:#00f">new</span> Env());
<span style="color:#2b91af">var</span> cm = context.Config.GetService&lt;IConverterManager&gt;();
cm.AddConverter&lt;Env, OpenType, ConfigurationAttribute&gt;(<span style="color:#00f">typeof</span>(PocoConverter&lt;&gt;));
</code></pre></div><p>I instruct the rule to bind to my custom class <code>Env</code>, and then I say that
this class <code>Env</code> is convertable to any type (denoted by special <code>OpenType</code>
type argument) with a generic converter called <code>PocoConverter</code>.</p>
<p>The <code>Env</code> class is a bit dummy (it exists just because I need <em>some</em> class):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Env</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> GetValue(<span style="color:#2b91af">string</span> key) =&gt; ConfigurationManager.AppSettings[key];
}
</code></pre></div><p>And <code>PocoConverter</code> is a piece of reflection, that loops through property
names and reads configuration values out of them. Then it calls a constructor
which matches the property count:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> <span style="color:#00f">class</span> <span style="color:#2b91af">PocoConverter</span>&lt;T&gt; : IConverter&lt;Env, T&gt;
{
    <span style="color:#00f">public</span> T Convert(Env env)
    {
        <span style="color:#2b91af">var</span> values = <span style="color:#00f">typeof</span>(T)
            .GetProperties()
            .Select(p =&gt; p.Name)
            .Select(env.GetValue)
            .Cast&lt;<span style="color:#2b91af">object</span>&gt;()
            .ToArray();

        <span style="color:#2b91af">var</span> constructor = <span style="color:#00f">typeof</span>(T).GetConstructor(values.Select(v =&gt; v.GetType()).ToArray());
        <span style="color:#00f">if</span> (constructor == <span style="color:#00f">null</span>)
        {
            <span style="color:#00f">throw</span> <span style="color:#00f">new</span> Exception(<span style="color:#a31515">&#34;We tried to bind to your C# class, but it looks like there&#39;s no constructor which accepts all property values&#34;</span>);
        }

        <span style="color:#00f">return</span> (T)constructor.Invoke(values);
    }
}
</code></pre></div><p>This piece of code is not particularly robust, but it is good enough to
illustrate the concept.</p>
<p>And that&rsquo;s it, the binding it ready! You can find the complete example in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/tree/master/custom-binding-azure-functions">my github repo</a>.</p>
<h2 id="deploying-custom-bindings">Deploying Custom Bindings</h2>
<p>Since BYOB feature is in early preview, there is no tooling for automated
deployment, and we need to do everything manually. But the process is not
too sophisticated:</p>
<ol>
<li>
<p>Create a folder for custom bindings, e.g. <code>D:\BindingExtensions</code>.</p>
</li>
<li>
<p>Set <code>AzureWebJobs_ExtensionsPath</code> parameter in your app settings
to that folder&rsquo;s path. For local development add a line to <code>local.settings.json</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#a31515">&#34;AzureWebJobs_ExtensionsPath&#34;</span><span style="">:</span> <span style="color:#a31515">&#34;D:\\BindingExtensions&#34;</span><span style="">,</span>
</code></pre></div></li>
<li>
<p>Create a subfolder for your extension, e.g.
<code>D:\BindingExtensions\ConfigurationExtension</code>.</p>
</li>
<li>
<p>Copy the contents of <code>bin\Debug\</code> of your extension&rsquo;s class library
to that folder.</p>
</li>
<li>
<p>Reference your extension library from your Function App.</p>
</li>
</ol>
<p>You are good to go! Decorate your function parameters with the new attribute.</p>
<p>Run the function app locally to try it out. In the console output you should
be able to see something like</p>
<pre><code>Loaded custom extension: ConfigurationExtensionConfigProvider from
'D:\BindingExtensions\ConfigurationExtension\MyExtensions.dll'
</code></pre><p>You will be able to debug your extension if needed.</p>
<h2 id="useful-links">Useful Links</h2>
<p>Use the following links to find out more about custom bindings, see more
examples and walkthroughs, and get fresh updates:</p>
<ul>
<li><a href="https://github.com/Azure/azure-webjobs-sdk/wiki/Extensibility">Extensibility in Azure WebJobs SDK</a></li>
<li><a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/SampleExtension">Sample Extension for Azure Functions</a>,
<a href="https://github.com/Azure/WebJobsExtensionSamples/blob/master/FunctionApp/ReaderFunction.cs">Sample Usage in Precompiled App</a> and
<a href="https://github.com/Azure/WebJobsExtensionSamples/tree/master/ScriptRuntimeSample/Reader">Sample Usage in Script Runtime</a></li>
<li><a href="https://github.com/Azure/azure-functions-durable-extension/tree/master/src/WebJobs.Extensions.DurableTask">Custom Bindings of Durable Functions</a></li>
<li><a href="https://azure.github.io/azure-functions-durable-extension/articles/installation.html">Installation Guide for Durable Functions</a></li>
</ul>
<p>Have a good binding!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Service Bus]]></title>
            <link href="https://mikhail.io/tags/azure-service-bus/"/>
            <id>https://mikhail.io/tags/azure-service-bus/</id>
            
            <published>2017-07-24T00:00:00+00:00</published>
            <updated>2017-07-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Custom Autoscaling with Durable Functions]]></title>
            <link href="https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/"/>
            <id>https://mikhail.io/2017/07/custom-autoscaling-with-durable-functions/</id>
            
            <published>2017-07-24T00:00:00+00:00</published>
            <updated>2017-07-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Leverage Azure Durable Functions to scale-out and scale-in App Service based on a custom metric</blockquote><p>In my previous post
<a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">Custom Autoscaling of Azure App Service with a Function App</a>
I&rsquo;ve created a Function App which watches a Service Bus Subscription
backlog and adjusts the scale of App Service based on the observed load.</p>
<p>It works fine but there are two minor issues that I would like to address
in this article:</p>
<ul>
<li>
<p><strong>Scaling Logic</strong> function from that workflow needs to preserve state
between calls. I used Table Storage bindings for that, which proved to
be a bit verbose and low level: I needed to manage conversion to entity and
JSON serialization myself;</p>
</li>
<li>
<p>There is no feedback from <strong>Scaler</strong> function (which executes the change)
back to <strong>Scaling Logic</strong> function. Thus, if scaling operation is slow or
fails, there is no easy way to notify the logic about that.</p>
</li>
</ul>
<p>Let&rsquo;s see how these issues can be solved with Azure Durable Functions.</p>
<h2 id="meet-durable-functions">Meet Durable Functions</h2>
<p>Microsoft has recently announced the preview of
<a href="https://azure.github.io/azure-functions-durable-extension/">Durable Functions</a>:</p>
<blockquote>
<p>Durable Functions is an Azure Functions extension for building long-running,
stateful function orchestrations in code using C# in a serverless environment.</p>
</blockquote>
<p>The library is built on top of <a href="https://github.com/Azure/durabletask">Durable Task Framework</a>
and introduces several patterns for Function coordination and stateful
processing. Please go read the <a href="https://azure.github.io/azure-functions-durable-extension/">documentation</a>,
it&rsquo;s great and has some very useful examples.</p>
<p>I decided to give Durable Functions a try for my autoscaling workflow. Feel
free to refer to <a href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/">the first part</a>
to understand my goals and the previous implementation.</p>
<h2 id="architecture">Architecture</h2>
<p>The flow of metric collection, scaling logic and scaling action stays the
same. The state and cross-function communication aspects are now delegated
to Durable Functions, so the diagram becomes somewhat simpler:</p>
<p><img src="AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The blue sign on <strong>Scaling Logic</strong> function denotes its statefulness.</p>
<p>Let&rsquo;s walk through the functions implementation to see how the workflow
plays out.</p>
<p>This time I&rsquo;ll start with <strong>Scaler</strong> function and then flow from right to left
to make the explanation more clear.</p>
<h2 id="scaler">Scaler</h2>
<p><strong>Scaler</strong> function applies the scaling decisions to the Azure resource, App
Service Plan in my case. I&rsquo;ve extracted App Service related code to a helper,
to keep the function minimal and clean. You can see the full code in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p><strong>Scaler</strong> function is triggered by Durable Function&rsquo;s <code>ActivityTrigger</code>. That
basically means that it&rsquo;s ready to be called from other functions. Here is
the code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(nameof(Scaler))]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">int</span> Scaler([ActivityTrigger] DurableActivityContext context)
{
    <span style="color:#2b91af">var</span> action = context.GetInput&lt;ScaleAction&gt;();

    <span style="color:#2b91af">var</span> newCapacity = ScalingHelper.ChangeAppServiceInstanceCount(
        action.ResourceName,
        action.Type == ScaleActionType.Down ? -1 : +1);

    <span style="color:#00f">return</span> newCapacity;
}
</code></pre></div><p>In order to receive an input value, I utilize <code>context.GetInput()</code> method.
I believe that the team is working on support of custom classes
(<code>ScaleAction</code> in my case) directly as function parameters.</p>
<p>The function then executes the scale change and returns back the new capacity
of App Service Plan. Note that this is new: we were not able to return
values in the previous implementation.</p>
<h2 id="scaling-logic">Scaling Logic</h2>
<p><strong>Scaling Logic</strong> is using <a href="https://azure.github.io/azure-functions-durable-extension/articles/samples/counter.html">Stateful Actor pattern</a>.
One instance of such actor is created for each scalable resource (I only use
1 now). Here is the implementation (again, simplified for readability):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(nameof(ScalingLogic))]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;ScalingState&gt; ScalingLogic(
    [OrchestrationTrigger] DurableOrchestrationContext context, 
    TraceWriter log)
{
    <span style="color:#2b91af">var</span> state = context.GetInput&lt;ScalingState&gt;();

    <span style="color:#2b91af">var</span> metric = <span style="color:#00f">await</span> context.WaitForExternalEvent&lt;Metric&gt;(nameof(Metric));

    UpdateHistory(state.History, metric.Value);
    ScaleAction action = CalculateScalingAction(state);

    <span style="color:#00f">if</span> (action != <span style="color:#00f">null</span>)
    {
        <span style="color:#2b91af">var</span> result = <span style="color:#00f">await</span> context.CallFunctionAsync&lt;<span style="color:#2b91af">int</span>&gt;(nameof(Scaler), action);
        log.Info(<span style="color:#a31515">$&#34;Scaling logic: Scaled to {result} instances.&#34;</span>);
        state.LastScalingActionTime = context.CurrentUtcDateTime;
    }

    context.ContinueAsNew(state);
    <span style="color:#00f">return</span> state;
}
</code></pre></div><p>Here is how it works:</p>
<ul>
<li>
<p>Function is bound to <code>OrchestrationTrigger</code>, yet another trigger type from
Durable Functions;</p>
</li>
<li>
<p>It loads durable state from the received <code>context</code>;</p>
</li>
<li>
<p>It then waits for an external event called Metric (to be sent by <strong>Collector</strong>
function, see the next section);</p>
</li>
<li>
<p>When an event is received, the function updates its state and calculates
if a scaling action is warranted;</p>
</li>
<li>
<p>If yes, it calls <strong>Scaler</strong> function and sends the scale action. It expects
an integer result, denoting the new amount of instances;</p>
</li>
<li>
<p>It then calls <code>ContinueAsNew</code> method to start a new iteration of the actor
loop, providing the updated state.</p>
</li>
</ul>
<p>One important note: the orchestrated function
<a href="https://azure.github.io/azure-functions-durable-extension/articles/topics/checkpointing-and-replay.html">has to be deterministic</a>.
That means, for example, that <code>DateTime.Now</code> is not allowed to be used.
I use <code>context.CurrentUtcDateTime</code> instead for time-related calculations.</p>
<p>The implementation of this function solves both problems that I mentioned
in the introduction. We do not manage state storage and serialization manually,
and we now have the ability to get feedback from <strong>Scaler</strong> function.</p>
<h2 id="metrics-collector">Metrics Collector</h2>
<p>I&rsquo;ve extracted Service Bus related code to a helper to keep the code sample
minimal and clean. You can see the full code in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/durable-functions/DurableScaling.cs">my github repo</a>.</p>
<p>Here is the remaining implementation of <strong>Metric Collector</strong>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(nameof(MetricCollector))]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task MetricCollector(
    [TimerTrigger(&#34;0 */1 * * * *&#34;)] TimerInfo myTimer,
    [OrchestrationClient] DurableOrchestrationClient client,
    TraceWriter log)
{
    <span style="color:#2b91af">var</span> resource = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ResourceToScale&#34;</span>);

    <span style="color:#2b91af">var</span> status = <span style="color:#00f">await</span> client.GetStatusAsync(resource);
    <span style="color:#00f">if</span> (status == <span style="color:#00f">null</span>)
    {
        <span style="color:#00f">await</span> client.StartNewAsync(nameof(ScalingLogic), resource, <span style="color:#00f">new</span> ScalingState());
    }
    <span style="color:#00f">else</span>
    {
        <span style="color:#2b91af">var</span> metric = ServiceBusHelper.GetSubscriptionMetric(resource);
        log.Info(<span style="color:#a31515">$&#34;Collector: Current metric value is {metric.Value.Value} at {DateTime.Now}&#34;</span>);
        <span style="color:#00f">await</span> client.RaiseEventAsync(resource, nameof(Metric), metric);
    }
}
</code></pre></div><p>It&rsquo;s still a timer-triggered &ldquo;normal&rdquo; (non-durable) function, but now it
also has an additional binding to <code>OrchestrationClient</code>. This client is used
to communicate metric data to the <strong>Scaling Logic</strong>.</p>
<p>With the current implementation, <strong>Metric Collector</strong> also has a second
responsibility: actor instance management. At every iteration, it queries
for the current status of corresponding actor. If that is <code>null</code>, Collector
creates a new instance with initial empty state.</p>
<p>To my liking, this aspect is a bit unfortunate, but it seems to be required
with the current implementation of Durable Functions framework. See
<a href="https://github.com/Azure/azure-functions-durable-extension/issues/21">my related question on github</a>.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I adjusted the initial flow of autoscaling functions to use Durable Functions
library. It made the state management look more straightforward, and also
allowed direct communication between two functions in strongly-typed
request-response manner.</p>
<p>The resulting code is relatively clear and resembles the typical structure
of async-await code that C# developers are used to.</p>
<p>There are some downsides that I found about Durable Functions too:</p>
<ul>
<li>
<p>This is a very early preview, so there are some implementation issues.
A couple times I managed to put my functions into a state where they were stuck
and no calls could be made anymore. The only way I could get out of there is by
clearing some blobs in the Storage Account;</p>
</li>
<li>
<p>The actor instance management story feels raw. The function, which needs to
send events to actors, has to manage their lifecycle and instance IDs. I would
need to add some more checks to make the code production ready, e.g. to
restart actors if they end up in faulty state;</p>
</li>
<li>
<p>There are some concurrency issues in function-to-function communication
to be resolved;</p>
</li>
<li>
<p>Some discipline is required to keep Durable functions side-effect free
and deterministic. The multiple executions caused by awaits and replays are
counter-intuitive (at least for novice devs), and thus error-prone.</p>
</li>
</ul>
<p>Having said that, I believe Durable Functions can be a very useful abstraction
to simplify some of the more advanced scenarios and workflows. I look
forward to further iterations of the library, and I will keep trying it out
for more scenarios.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                             
                                <category scheme="https://mikhail.io/tags/azure-app-service" term="azure-app-service" label="Azure App Service" />
                             
                                <category scheme="https://mikhail.io/tags/azure-service-bus" term="azure-service-bus" label="Azure Service Bus" />
                             
                                <category scheme="https://mikhail.io/tags/durable-functions" term="durable-functions" label="Durable Functions" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Custom Autoscaling of Azure App Service with a Function App]]></title>
            <link href="https://mikhail.io/2017/07/custom-auto-scaling-in-azure/"/>
            <id>https://mikhail.io/2017/07/custom-auto-scaling-in-azure/</id>
            
            <published>2017-07-17T00:00:00+00:00</published>
            <updated>2017-07-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>How to scale-out and scale-in App Service based on a custom metric</blockquote><p>The power of cloud computing comes from its elasticity and ability to adapt to changing
load. Most Azure services can be scaled up or down manually: by human interaction in the
portal, or by running a command or a script.</p>
<p>Some services in Azure also support Autoscaling, i.e. they may change the resource
allocation dynamically, based on predefined rules and current operational metrics.</p>
<p>Azure App Service is one example of such service: it supports
<a href="https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/insights-how-to-scale#scaling-based-on-a-pre-set-metric">Scaling based on a pre-set metric</a>.
This is a powerful option that enables website or webjobs to react on varying load,
e.g. based on CPU utilization.</p>
<p>At the same time, the flexibility of the built-in autoscaling is somewhat
limited:</p>
<ul>
<li>
<p>Only a handful of metrics is supported: for instance, Service Bus Queues
are supported as metric source, while Service Bus Subscriptions are not;</p>
</li>
<li>
<p>It&rsquo;s not possible to combine several metrics in one rule: e.g. scale down only if
several queues are empty at the same time, not just one of them;</p>
</li>
<li>
<p>Thresholds are the same for any number of instances: I can&rsquo;t define
a scale down rule threshold to be 60% for 8 instances but 30% for 2 instances;</p>
</li>
<li>
<p>The minimum time of reaction is limited to 5 minutes.</p>
</li>
</ul>
<p>Other services, like SQL Database and Cosmos DB, don&rsquo;t have the built-in autoscaling
functionality at all.</p>
<p>This post starts the series of articles about custom implementation
of autoscaling. The implementation will be based on Azure Functions as building
blocks of scaling workflows.</p>
<h2 id="goal">Goal</h2>
<p>To keep the task very specific for now, I want the following from my first
custom autoscaling implementation:</p>
<ul>
<li>
<p>Be able to scale the amount of instances up and down in a given App Service
Plan;</p>
</li>
<li>
<p>Do so based on the given Service Bus Subscription backlog (amount of messages
pending to be processed);</p>
</li>
<li>
<p>Scale up, if the average backlog during any 10 minutes is above a threshold;</p>
</li>
<li>
<p>Scale down, if the maximum backlog during any 10 minutes is below another
(lower) threshold;</p>
</li>
<li>
<p>After scaling up or down, take a cooldown period of 10 minutes;</p>
</li>
<li>
<p>Have a log of scaling decisions and numbers behind;</p>
</li>
<li>
<p>Scaling rules should be extensible to allow more complex calculation later
on.</p>
</li>
</ul>
<h2 id="architecture">Architecture</h2>
<p>I decided that the scaling rules should be written in a general-purpose programming language
(C# for this post), instead of just picking from a limited list of configurations.</p>
<p>I chose Azure Functions as the mechanism to host and run this logic in Azure cloud.</p>
<p>Here is a diagram of Functions that I ended up creating:</p>
<p><img src="AutoscalingArchitecture.png" alt="Autoscaling Architecture"></p>
<p>The components of my autoscaling app are:</p>
<ul>
<li>
<p><strong>Metric Collector</strong> function is based on Timer trigger: it fires every minute and collects
the subscription backlog metric from a given Service Bus Subscription;</p>
</li>
<li>
<p>Collector then sends this metric to the <strong>Metrics</strong> storage queue;</p>
</li>
<li>
<p><strong>Scaling Logic</strong> function pulls the metric from the queue. It maintains the
metric values for 10 minutes, calculates average/maximum value, and if they hit
thresholds - issues a command to scale App Service Plan up or down;</p>
</li>
<li>
<p>The command is sent to <strong>Actions</strong> storage queue;</p>
</li>
<li>
<p><strong>Scaler</strong> function receives the commands from the queue and executes
the re-scaling action on App Service Plan using Azure Management SDK.</p>
</li>
</ul>
<p>The implementation of this workflow is discussed below. I am using Visual Studio 2017 Version 15.3
Preview 4.0 to author pre-compiled Azure Functions with nice built-in tooling.</p>
<h2 id="metric-collector">Metric Collector</h2>
<p>First, let&rsquo;s define <code>MetricValue</code> class, which simply holds time and value:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">MetricValue</span>
{
    <span style="color:#00f">public</span> MetricValue(DateTime time, <span style="color:#2b91af">int</span> <span style="color:#00f">value</span>)
    {
        <span style="color:#00f">this</span>.Time = time;
        <span style="color:#00f">this</span>.Value = <span style="color:#00f">value</span>;
    }

    <span style="color:#00f">public</span> DateTime Time { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Value { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>and <code>Metric</code> class which extends the value with resource name (e.g. App Service
Plan name) and measured parameter name:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Metric</span>
{
    <span style="color:#00f">public</span> Metric(<span style="color:#2b91af">string</span> resourceName, <span style="color:#2b91af">string</span> name, MetricValue <span style="color:#00f">value</span>)
    {
        <span style="color:#00f">this</span>.ResourceName = resourceName;
        <span style="color:#00f">this</span>.Name = name;
        <span style="color:#00f">this</span>.Value = <span style="color:#00f">value</span>;
    }

    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> ResourceName { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Name { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> MetricValue Value { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>The function definition has two associated bindings: timer trigger (runs every
minute) and return binding to the storage queue:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;MetricCollector&#34;)]
[return: Queue(&#34;Metrics&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> Metric MetricCollector([TimerTrigger(<span style="color:#a31515">&#34;0 */1 * * * *&#34;</span>)] TimerInfo myTimer, TraceWriter log)
{
    <span style="color:#2b91af">var</span> connectionString = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ServiceBusConnection&#34;</span>);
    <span style="color:#2b91af">var</span> topic = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;Topic&#34;</span>);
    <span style="color:#2b91af">var</span> subscription = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;Subscription&#34;</span>);

    <span style="color:#2b91af">var</span> nsmgr = NamespaceManager.CreateFromConnectionString(connectionString);
    <span style="color:#2b91af">var</span> subscriptionClient = nsmgr.GetSubscription(topic, subscription);
    <span style="color:#2b91af">var</span> backlog = subscriptionClient.MessageCountDetails.ActiveMessageCount;

    log.Info(<span style="color:#a31515">$&#34;Collector: Current metric value is {backlog}&#34;</span>);

    <span style="color:#2b91af">var</span> resource = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ResourceToScale&#34;</span>);
    <span style="color:#2b91af">var</span> <span style="color:#00f">value</span> = <span style="color:#00f">new</span> MetricValue(DateTime.Now, (<span style="color:#2b91af">int</span>)backlog);
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> Metric(resource, <span style="color:#a31515">$&#34;{topic}-{subscription}-backlog&#34;</span>, <span style="color:#00f">value</span>);
}
</code></pre></div><p>The function executes the following steps:</p>
<ul>
<li>Reads configuration value for Service Bus parameters;</li>
<li>Connects to Service Bus and retrieves <code>ActiveMessageCount</code> for the given
subscription;</li>
<li>Logs the value for tracing and debugging;</li>
<li>Returns the metric value mentioning which resource it&rsquo;s intended for.</li>
</ul>
<h2 id="scaling-logic">Scaling Logic</h2>
<p>The core of autoscaling implementation resides in <code>ScalingLogic</code> function.</p>
<p>The function defines 4 (oh my!) bindings:</p>
<ul>
<li>Queue trigger to react on messages from the collector;</li>
<li>Output queue binding to send commands with action to execute;</li>
<li>Combination of input and output bindings to the same row in Table Storage to
keep the state in between function calls.</li>
</ul>
<p>The bindings are illustrated on the following picture:</p>
<p><img src="ScalingLogicBindings.png" alt="Binding of Scaling Logic Function"></p>
<p>And here is the corresponding Function signature:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;ScalingLogic&#34;)]
[return: Queue(&#34;Actions&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> ScaleAction ScalingLogic(
    [QueueTrigger(&#34;Metrics&#34;)] Metric metric, 
    [Table(&#34;Scaling&#34;, &#34;{ResourceName}&#34;, &#34;{Name}&#34;)] ScalingStateEntity stateEntity, 
    [Table(&#34;Scaling&#34;, &#34;{ResourceName}&#34;, &#34;{Name}&#34;)] <span style="color:#00f">out</span> ScalingStateEntity newStateEntity,
    TraceWriter log)
</code></pre></div><p>Table storage is partitioned per scalable resource, and state is stored per metric;
thus multiple resources and metrics are supported out of the box.</p>
<p>The function implementation is relatively complex, so I&rsquo;ll describe it in parts.</p>
<p><code>ScaleAction</code> is a simple message class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">enum</span> ScaleActionType
{
    Up,
    Down
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ScaleAction</span>
{
    <span style="color:#00f">public</span> ScaleAction(<span style="color:#2b91af">string</span> resourceName, ScaleActionType type)
    {
        <span style="color:#00f">this</span>.ResourceName = resourceName;
        <span style="color:#00f">this</span>.Type = type;
    }

    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> ResourceName { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> ScaleActionType Type { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>Table Storage only allows primitive types for its columns, like strings.
So I had to create a separate Table Storage entity class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ScalingStateEntity</span> : TableEntity
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> SerializedState { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>which stores serialized state, from the state class itself:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ScalingState</span>
{
    <span style="color:#00f">public</span> List&lt;MetricValue&gt; History { <span style="color:#00f">get</span>; } = <span style="color:#00f">new</span> List&lt;MetricValue&gt;();

    <span style="color:#00f">public</span> DateTime LastScalingActionTime { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; } = DateTime.MinValue;
}
</code></pre></div><p>Now let&rsquo;s look at the function body. It consists of four blocks.</p>
<p>The first block retrieves the previous values of the metric and logs it too:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// 1. Deserialize state
</span><span style="color:#008000"></span><span style="color:#2b91af">var</span> state = stateEntity?.SerializedState != <span style="color:#00f">null</span> 
    ? JsonConvert.DeserializeObject&lt;ScalingState&gt;(stateEntity.SerializedState) 
    : <span style="color:#00f">new</span> ScalingState();
<span style="color:#2b91af">var</span> history = state.History;
log.Info(<span style="color:#a31515">$&#34;Scaling logic: Received {metric.Name}, previous state is {string.Join(&#34;</span>, <span style="color:#a31515">&#34;, history)}&#34;</span>);
</code></pre></div><p>The second block adds the current metric value and removes all metrics which are
not in the target period of 10 minutes anymore:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// 2. Add current metric value, remove old values
</span><span style="color:#008000"></span>history.Add(metric.Value);
history.RemoveAll(e =&gt; e.Time &lt; metric.Value.Time.Substract(period));
</code></pre></div><p>Now, the actual logic finally kicks in and produces the scaling action if average
or maximum value is above or below respective thresholds. For my implementation I also
chose to apply this rule after 5th data point. Cooldown period is also respected:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// 3. Compare the aggregates to thresholds, produce scaling action if needed
</span><span style="color:#008000"></span>ScaleAction action = <span style="color:#00f">null</span>;
<span style="color:#00f">if</span> (history.Count &gt;= 5
    &amp;&amp; DateTime.Now - state.LastScalingActionTime &gt; cooldownPeriod)
{
    <span style="color:#2b91af">var</span> average = (<span style="color:#2b91af">int</span>)history.Average(e =&gt; e.Value);
    <span style="color:#2b91af">var</span> maximum = (<span style="color:#2b91af">int</span>)history.Max(e =&gt; e.Value);
    <span style="color:#00f">if</span> (average &gt; thresholdUp)
    {
        log.Info(<span style="color:#a31515">$&#34;Scaling logic: Value {average} is too high, scaling {metric.ResourceName} up...&#34;</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span style="color:#00f">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Up);
    }
    <span style="color:#00f">else</span> <span style="color:#00f">if</span> (maximum &lt; thresholdDown)
    {
        log.Info(<span style="color:#a31515">$&#34;Scaling logic: Value {maximum} is low, scaling {metric.ResourceName} down...&#34;</span>);
        state.LastScalingActionTime = DateTime.Now;
        action = <span style="color:#00f">new</span> ScaleAction(metric.ResourceName, ScaleActionType.Down);
    }
}
</code></pre></div><p>Finally, the state is serialized back to table entity and action is returned:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// 4. Serialize the state back and return the action
</span><span style="color:#008000"></span>newStateEntity = stateEntity != <span style="color:#00f">null</span> 
    ? stateEntity 
    : <span style="color:#00f">new</span> ScalingStateEntity { PartitionKey = metric.ResourceName, RowKey = metric.Name };
newStateEntity.SerializedState = JsonConvert.SerializeObject(state);
<span style="color:#00f">return</span> action;
</code></pre></div><p>Note, that if no scaling action is warranted, the function simply returns <code>null</code> and no message
gets sent to the output queue.</p>
<h2 id="scaler">Scaler</h2>
<p>The last function of the workflow is called <code>Scaler</code>: it listens for scaling commands and executes them.
I am using Azure Management Fluent SDK to scale the App Service Plan capacity:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[FunctionName(&#34;Scaler&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> Scaler([QueueTrigger(<span style="color:#a31515">&#34;Actions&#34;</span>)] ScaleAction action, TraceWriter log)
{
    <span style="color:#2b91af">var</span> secrets = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ServicePrincipal&#34;</span>).Split(<span style="color:#a31515">&#39;,&#39;</span>);
    <span style="color:#2b91af">var</span> credentials = SdkContext.AzureCredentialsFactory
        .FromServicePrincipal(secrets[0], secrets[1], secrets[2], AzureEnvironment.AzureGlobalCloud);
    <span style="color:#2b91af">var</span> azure = Azure.Configure()
        .Authenticate(credentials)
        .WithDefaultSubscription();

    <span style="color:#2b91af">var</span> plan = azure.AppServices
        .AppServicePlans
        .List()
        .First(p =&gt; p.Name.Contains(action.ResourceName));

    <span style="color:#2b91af">var</span> newCapacity = action.Type == ScaleActionType.Down ? plan.Capacity - 1 : plan.Capacity + 1;
    log.Info(<span style="color:#a31515">$&#34;Scaler: Switching {action.ResourceName} from {plan.Capacity} {action.Type} to {newCapacity}&#34;</span>);

    plan.Update()
        .WithCapacity(newCapacity)
        .Apply();
}
</code></pre></div><p>The functionality is pretty straightforward. Here are some links where you can read more about
<a href="https://docs.microsoft.com/en-us/dotnet/azure/dotnet-sdk-azure-authenticate?view=azure-dotnet#a-namemgmt-authaazure-management-libraries-for-net-authentication">Authentication in Azure Management libraries</a>
and <a href="https://github.com/Azure-Samples/app-service-dotnet-scale-web-apps">Managing Web App with Fluent SDK</a>.</p>
<h2 id="conclusion-and-further-steps">Conclusion and Further Steps</h2>
<p>This was quite a lot of code for a single blog post, but most of it was
fairly straightforward. You can find the full implemenation in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/customautoscaling/servicebussubscription-to-appserviceplan/MetricCollector.cs">my github</a>.</p>
<p>Overall, I&rsquo;ve established an application based on Azure Functions, which
watches the predefined metrics and scales the specified resource up and down
based on target metric values.</p>
<p>The current example works only for the combination of Service Bus Subscription
and App Service Plan, but it is clear how to extend it to more scenarios.</p>
<p>The flexibility of such autoscaling solution exceeds the built-in functionality
that is available in Azure Portal.</p>
<p>The most complex part of my Autoscaling application is the Scaling Logic
function. In the next article of the series, I will refactor it to use
<a href="https://azure.github.io/azure-functions-durable-extension/index.html">Durable Functions</a> -
the upcoming Orchestration framework for Function Apps.</p>
<p>Stay tuned, and happy scaling!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/scalability" term="scalability" label="Scalability" />
                             
                                <category scheme="https://mikhail.io/tags/azure-app-service" term="azure-app-service" label="Azure App Service" />
                             
                                <category scheme="https://mikhail.io/tags/azure-service-bus" term="azure-service-bus" label="Azure Service Bus" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Sending Large Batches to Azure Service Bus]]></title>
            <link href="https://mikhail.io/2017/07/sending-large-batches-to-azure-service-bus/"/>
            <id>https://mikhail.io/2017/07/sending-large-batches-to-azure-service-bus/</id>
            
            <published>2017-07-04T00:00:00+00:00</published>
            <updated>2017-07-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Azure Service Bus client supports sending messages in batches. However, the size of a single batch must stay below 256k bytes, otherwise the whole batch will get rejected.</blockquote><p>Azure Service Bus client supports sending messages in batches (<code>SendBatch</code>
and <code>SendBatchAsync</code> methods of <code>QueueClient</code> and <code>TopicClient</code>). However,
the size of a single batch must stay below 256k bytes, otherwise the whole
batch will get rejected.</p>
<p>How do we make sure that the batch-to-be-sent is going to fit? The rest
of this article will try to answer this seemingly simple question.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Given a list of messages of arbitrary type <code>T</code>, we want to send them to Service
Bus in batches. The amount of batches should be close to minimal, but
obviously each one of them must satisfy the restriction of 256k max size.</p>
<p>So, we want to implement a method with the following signature:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages);
</code></pre></div><p>which would work for collections of any size.</p>
<p>To limit the scope, I will restrict the article to the following assumptions:</p>
<ul>
<li>
<p>Each individual message is less than 256k serialized. If that wasn&rsquo;t true,
we&rsquo;d have to put the body into external blob storage first, and then send
the reference. It&rsquo;s not directly related to the topic of discussion.</p>
</li>
<li>
<p>I&rsquo;ll use <code>public BrokeredMessage(object serializableObject)</code> constructor.
Custom serialization could be used, but again, it&rsquo;s not related to batching,
so I&rsquo;ll ignore it.</p>
</li>
<li>
<p>We won&rsquo;t care about transactions, i.e. if connectivity dies in the middle
of sending the big batch, we might end up with partially sent batch.</p>
</li>
</ul>
<h2 id="messages-of-known-size">Messages of Known Size</h2>
<p>Let&rsquo;s start with a simple use case: the size of each message is known to us.
It&rsquo;s defined by hypothetical <code>Func&lt;T, long&gt; getSize</code> function. Here is a
helpful extension method that will split an arbitrary collection based on
a metric function and maximum chunk size:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> List&lt;List&lt;T&gt;&gt; ChunkBy&lt;T&gt;(<span style="color:#00f">this</span> IEnumerable&lt;T&gt; source, Func&lt;T, <span style="color:#2b91af">long</span>&gt; metric, <span style="color:#2b91af">long</span> maxChunkSize)
{
    <span style="color:#00f">return</span> source
        .Aggregate(
            <span style="color:#00f">new</span>
            {
                Sum = 0L,
                Current = (List&lt;T&gt;)<span style="color:#00f">null</span>,
                Result = <span style="color:#00f">new</span> List&lt;List&lt;T&gt;&gt;()
            },
            (agg, item) =&gt;
            {
                <span style="color:#2b91af">var</span> <span style="color:#00f">value</span> = metric(item);
                <span style="color:#00f">if</span> (agg.Current == <span style="color:#00f">null</span> || agg.Sum + <span style="color:#00f">value</span> &gt; maxChunkSize)
                {
                    <span style="color:#2b91af">var</span> current = <span style="color:#00f">new</span> List&lt;T&gt; { item };
                    agg.Result.Add(current);
                    <span style="color:#00f">return</span> <span style="color:#00f">new</span> { Sum = <span style="color:#00f">value</span>, Current = current, agg.Result };
                }

                agg.Current.Add(item);
                <span style="color:#00f">return</span> <span style="color:#00f">new</span> { Sum = agg.Sum + <span style="color:#00f">value</span>, agg.Current, agg.Result };
            })
        .Result;
}
</code></pre></div><p>Now, the implementation of <code>SendBigBatchAsync</code> is simple:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">async</span> Task SendBigBatchAsync(IEnumerable&lt;T&gt; messages, Func&lt;T, <span style="color:#2b91af">long</span>&gt; getSize)
{
    <span style="color:#2b91af">var</span> chunks = messages.ChunkBy(getSize, MaxServiceBusMessage);
    <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> chunk <span style="color:#00f">in</span> chunks)
    {
        <span style="color:#2b91af">var</span> brokeredMessages = chunk.Select(m =&gt; <span style="color:#00f">new</span> BrokeredMessage(m));
        <span style="color:#00f">await</span> client.SendBatchAsync(brokeredMessages);
    }
}

<span style="color:#00f">private</span> <span style="color:#00f">const</span> <span style="color:#2b91af">long</span> MaxServiceBusMessage = 256000;
<span style="color:#00f">private</span> <span style="color:#00f">readonly</span> QueueClient client;
</code></pre></div><p>Note that I do <code>await</code> for each chunk sequentially to preserve message ordering.
Another thing to notice is that we lost all-or-nothing guarantee: we might
be able to send the first chunk, and then get an exception from subsequent
parts. Some sort of retry mechanism is probably needed.</p>
<h2 id="brokeredmessagesize">BrokeredMessage.Size</h2>
<p>OK, how do we determine the size of each message? How do we implement
<code>getSize</code> function?</p>
<p><code>BrokeredMessage</code> class exposes <code>Size</code> property, so it might be tempting to
rewrite our method the following way:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
{
    <span style="color:#2b91af">var</span> brokeredMessages = messages.Select(m =&gt; <span style="color:#00f">new</span> BrokeredMessage(m));
    <span style="color:#2b91af">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; bm.Size, MaxServiceBusMessage);
    <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> chunk <span style="color:#00f">in</span> chunks)
    {
        <span style="color:#00f">await</span> client.SendBatchAsync(chunk);
    }
}
</code></pre></div><p>Unfortunately, this won&rsquo;t work properly. A quote from documentation:</p>
<blockquote>
<p>The value of Size is only accurate after the BrokeredMessage
instance is sent or received.</p>
</blockquote>
<p>My experiments show that <code>Size</code> of a draft message returns the size of
the message body, ignoring headers. If the message bodies are large, and
each chunk has just a handful of them, the code might work ok-ish.</p>
<p>But it will significantly underestimate the size of large batches of messages
with small payload.</p>
<p>So, for the rest of this article I&rsquo;ll try to adjust the calculation for headers.</p>
<h2 id="fixed-header-size">Fixed Header Size</h2>
<p>It could be that the header size of each message is always the same.
Quite often people will set the same headers for all their messages,
or set no custom headers at all.</p>
<p>In this case, you might just measure this size once, and then put this
fixed value inside a configuration file.</p>
<p>Here is how you measure the headers of a <code>BrokeredMessage</code> message:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> sizeBefore = message.Size;
client.Send(message);
<span style="color:#2b91af">var</span> sizeAfter = message.Size;
<span style="color:#2b91af">var</span> headerSize = sizeAfter - sizeBefore;
</code></pre></div><p>Now you just need to adjust one line from the previous version of
<code>SendBigBatchAsync</code> method</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> chunks = brokeredMessages.ChunkBy(bm =&gt; FixedHeaderSize + bm.Size, MaxServiceBusMessage);
</code></pre></div><p><code>FixedHeaderSize</code> might be simply hard-coded, or taken from configuration
per application.</p>
<h2 id="measuring-of-header-size-per-message">Measuring of Header Size per Message</h2>
<p>If the size of headers varies per message, you need a way to adjust batching
algorithm accordingly.</p>
<p>Unfortunately, I haven&rsquo;t found a straightforward way to accomplish that. It looks like
you&rsquo;d have to serialize the headers yourself, and then measure the size of
resulting binary. This is not a trivial operation to do correctly,
and also implies some performance penalty.</p>
<p>Sean Feldman <a href="https://weblogs.asp.net/sfeldman/asb-batching-brokered-messages">came up</a>
with a way to <em>estimate</em> the size of headers. That might be a good way to go,
though the estimation tends to err on the safe side for messages with small
payload.</p>
<h2 id="heuristics--retry">Heuristics &amp; Retry</h2>
<p>The last possibility that I want to consider is actually allow yourself
violating the max size of the batch, but then handle the exception, retry
the send operation and adjust future calculations based on actual measured size
of the failed messages. The size is known after trying to <code>SendBatch</code>, even if
operation failed, so we can use this information.</p>
<p>Here is a sketch of how to do that in code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Sender is reused across requests
</span><span style="color:#008000"></span><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">BatchSender</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> QueueClient queueClient;
    <span style="color:#00f">private</span> <span style="color:#2b91af">long</span> batchSizeLimit = 262000;
    <span style="color:#00f">private</span> <span style="color:#2b91af">long</span> headerSizeEstimate = 54; <span style="color:#008000">// start with the smallest header possible
</span><span style="color:#008000"></span>
    <span style="color:#00f">public</span> BatchSender(QueueClient queueClient)
    {
        <span style="color:#00f">this</span>.queueClient = queueClient;
    }

    <span style="color:#00f">public</span> <span style="color:#00f">async</span> Task SendBigBatchAsync&lt;T&gt;(IEnumerable&lt;T&gt; messages)
    {
        <span style="color:#2b91af">var</span> packets = (<span style="color:#00f">from</span> m <span style="color:#00f">in</span> messages
                     <span style="color:#00f">let</span> bm = <span style="color:#00f">new</span> BrokeredMessage(m)
                     <span style="color:#00f">select</span> <span style="color:#00f">new</span> { Source = m, Brokered = bm, BodySize = bm.Size }).ToList();
        <span style="color:#2b91af">var</span> chunks = packets.ChunkBy(p =&gt; <span style="color:#00f">this</span>.headerSizeEstimate + p.Brokered.Size, <span style="color:#00f">this</span>.batchSizeLimit);
        <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> chunk <span style="color:#00f">in</span> chunks)
        {
            <span style="color:#00f">try</span>
            {
                <span style="color:#00f">await</span> <span style="color:#00f">this</span>.queueClient.SendBatchAsync(chunk.Select(p =&gt; p.Brokered));
            }
            <span style="color:#00f">catch</span> (MessageSizeExceededException)
            {
                <span style="color:#2b91af">var</span> maxHeader = packets.Max(p =&gt; p.Brokered.Size - p.BodySize);
                <span style="color:#00f">if</span> (maxHeader &gt; <span style="color:#00f">this</span>.headerSizeEstimate)
                {
                    <span style="color:#008000">// If failed messages had bigger headers, remember this header size 
</span><span style="color:#008000"></span>                    <span style="color:#008000">// as max observed and use it in future calculations
</span><span style="color:#008000"></span>                    <span style="color:#00f">this</span>.headerSizeEstimate = maxHeader;
                }
                <span style="color:#00f">else</span>
                {
                    <span style="color:#008000">// Reduce max batch size to 95% of current value
</span><span style="color:#008000"></span>                    <span style="color:#00f">this</span>.batchSizeLimit = (<span style="color:#2b91af">long</span>)(<span style="color:#00f">this</span>.batchSizeLimit * .95);
                }

                <span style="color:#008000">// Re-send the failed chunk
</span><span style="color:#008000"></span>                <span style="color:#00f">await</span> <span style="color:#00f">this</span>.SendBigBatchAsync(packets.Select(p =&gt; p.Source));
            }

        }
    }
}
</code></pre></div><p>The code example is quite involved, here is what actually happens:</p>
<ol>
<li>
<p>Create a brokered message for each message object, but also save the
corresponding source message. This is critical to be able to re-send items:
there&rsquo;s no way to send the same <code>BrokeredMessage</code> instance twice.</p>
</li>
<li>
<p>Also save the body size of the brokered message. We&rsquo;ll use it for retry
calculation.</p>
</li>
<li>
<p>Start with some guess of header size estimate. I start with 54 bytes,
which seems to be the minimal header size possible.</p>
</li>
<li>
<p>Split the batch into chunks the same way we did before.</p>
</li>
<li>
<p>Try sending chunks one by one.</p>
</li>
<li>
<p>If send operation fails with <code>MessageSizeExceededException</code>, iterate
through failed items and find out the actual header size of the message.</p>
</li>
<li>
<p>If that actual size is bigger than our known estimate, increase the estimate
to the newly observed value. Retry sending the chunk (not the whole batch) with
this new setting.</p>
</li>
<li>
<p>If the header is small, but message size is still too big - reduce the
allowed total size of the chunk. Retry again.</p>
</li>
</ol>
<p>The combination of checks of steps 7 and 8 should make the mechanism reliable
and self-adopting to message header payloads.</p>
<p>Since we reuse the sender between send operations, the size parameters will
also converge quite quickly and no more retries will be needed. Thus the
performance overhead should be minimal.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It seems like there is no &ldquo;one size fits all&rdquo; solution for this problem at
the moment. The best implementation might depend on your messaging
requirements.</p>
<p>But if you have the silver bullet solution, please leave a comment under
this post and answer <a href="https://stackoverflow.com/questions/44779707/split-batch-of-messages-to-be-sent-to-azure-service-bus">my StackOverflow question</a>!</p>
<p>Otherwise, let&rsquo;s hope that the new
<a href="https://github.com/azure/azure-service-bus-dotnet">.NET Standard-compatible Service Bus client</a>
will solve this issue for us. Track <a href="https://github.com/Azure/azure-service-bus-dotnet/issues/109">this github issue</a>
for status updates.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-service-bus" term="azure-service-bus" label="Azure Service Bus" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Application Insights]]></title>
            <link href="https://mikhail.io/tags/application-insights/"/>
            <id>https://mikhail.io/tags/application-insights/</id>
            
            <published>2017-06-07T00:00:00+00:00</published>
            <updated>2017-06-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Finding Lost Events in Azure Application Insights]]></title>
            <link href="https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights/"/>
            <id>https://mikhail.io/2017/06/finding-lost-events-in-azure-application-insights/</id>
            
            <published>2017-06-07T00:00:00+00:00</published>
            <updated>2017-06-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>One of the ways we use Azure Application Insights is tracking custom
application-specific events. For instance, every time a data point from an
IoT device comes in, we log an AppInsights event. Then we are able to
aggregate the data and plot charts to derive trends and detect possible
anomalies.</p>
<p>And recently we found such anomaly, which looked like this:</p>
<p><img src="dashboard-chart.png" alt="Amount of Events on Dashboard Chart"></p>
<p>This is a chart from our Azure dashboard, which shows the total amount of
events of specific type received per day.</p>
<p>The first two &ldquo;hills&rdquo; are two weeks, so we can clearly see that we get more
events on business days compared to weekends.</p>
<p>But then something happened on May 20: we started getting much less events,
and the hill pattern disappeared, days looks much more alike.</p>
<p>We haven&rsquo;t noticed any other problems in the system, but the trend looked
quite bothering. Are we loosing data?</p>
<p>I headed towards Analytics console of Application Insights to dig deeper.
Here is the query that reproduces the problem:</p>
<pre><code>customEvents
| where name == &quot;EventXReceived&quot;
| where timestamp &gt;= ago(22d)
| project PointCount = todouble(customMeasurements[&quot;EventXReceived_Count&quot;]), timestamp
| summarize EventXReceived = sum(PointCount) by bin(timestamp, 1d)
| render timechart
</code></pre><p>and I got the same chart as before:</p>
<p><img src="analytics1.png" alt="Trend on Application Insights Analytics"></p>
<p>I checked the history of our source code repository and deployments and I
figured out that we upgraded the version of Application Insights SDK from
version 2.1 to version 2.3.</p>
<p>My guess at this point was that Application Insights started sampling our
data instead of sending all events to the server. After reading
<a href="https://docs.microsoft.com/en-us/azure/application-insights/app-insights-sampling">Sampling in Application Insights</a>
article, I came up with the following query to see the sampling rate:</p>
<pre><code>customEvents
| where name == &quot;EventXReceived&quot;
| where timestamp &gt;= ago(22d)
| summarize 100/avg(itemCount) by bin(timestamp, 1d) 
| render areachart 
</code></pre><p>and the result is self-explanatory:</p>
<p><img src="sampling-rate.png" alt="Sampling Rate"></p>
<p>Clearly, the sampling rate dropped from 100% down to about 30% right when
the anomaly started. The sampling-adjusted query (note <code>itemCount</code> multiplication)</p>
<pre><code>customEvents
| where name == &quot;EventXReceived&quot;
| where timestamp &gt;= ago(22d)
| project PointCount = todouble(customMeasurements[&quot;EventXReceived_Count&quot;]) * itemCount, timestamp
| summarize EventXReceived = sum(PointCount) by bin(timestamp, 1d)
| render timechart
</code></pre><p>puts us back to the point when results make sense:</p>
<p><img src="analytics2.png" alt="Adjusted Trend on Application Insights Analytics"></p>
<p>The third week&rsquo;s Thursday was bank holiday in several European countries, so
we got a drop there.</p>
<p>Should Azure dashboard items take sampling into account - to avoid
confusing people and to show more useful charts?</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/application-insights" term="application-insights" label="Application Insights" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[CloudFlare]]></title>
            <link href="https://mikhail.io/tags/cloudflare/"/>
            <id>https://mikhail.io/tags/cloudflare/</id>
            
            <published>2017-06-06T00:00:00+00:00</published>
            <updated>2017-06-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[HTTPS]]></title>
            <link href="https://mikhail.io/tags/https/"/>
            <id>https://mikhail.io/tags/https/</id>
            
            <published>2017-06-06T00:00:00+00:00</published>
            <updated>2017-06-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Mikhail.io Upgraded to HTTPS and HTTP/2]]></title>
            <link href="https://mikhail.io/2017/06/mikhail-io-upgraded-to-https-and-http2/"/>
            <id>https://mikhail.io/2017/06/mikhail-io-upgraded-to-https-and-http2/</id>
            
            <published>2017-06-06T00:00:00+00:00</published>
            <updated>2017-06-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Starting today, this blog has switched to HTTPS secure protocol:</p>
<p><img src="mikhailio-https.png" alt="HTTPS"></p>
<p>While there&rsquo;s not that much to secure on my blog, HTTPS is still considered to be a good practice for any site in 2017. One of the benefits that we can get from it is the usage of HTTP/2 protocol:</p>
<p><img src="mikhailio-http2.png" alt="HTTP/2"></p>
<p>This should be beneficial to any reader which uses a modern browser!</p>
<p>Thanks to <a href="https://cloudflare.com">CloudFlare</a> for providing me with free HTTPS and HTTP/2 support.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/https" term="https" label="HTTPS" />
                             
                                <category scheme="https://mikhail.io/tags/http/2" term="http/2" label="HTTP/2" />
                             
                                <category scheme="https://mikhail.io/tags/cloudflare" term="cloudflare" label="CloudFlare" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Reliable Consumer of Azure Event Hubs]]></title>
            <link href="https://mikhail.io/2017/05/reliable-consumer-of-azure-event-hubs/"/>
            <id>https://mikhail.io/2017/05/reliable-consumer-of-azure-event-hubs/</id>
            
            <published>2017-05-29T00:00:00+00:00</published>
            <updated>2017-05-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><a href="https://azure.microsoft.com/en-us/services/event-hubs/">Azure Event Hubs</a> is
a log-based messaging system-as-a-service in Azure cloud. It&rsquo;s designed to be able to handle huge
amount of data, and naturally supports multiple consumers.</p>
<h2 id="event-hubs-and-service-bus">Event Hubs and Service Bus</h2>
<p>While Event Hubs are formally part of Azure Service Bus family of products,
in fact its model is quite different.</p>
<p>&ldquo;Traditional&rdquo; Service Bus service is organized around queues (subscriptions
are just queues with the topic being the source of messages). Each consumer
can peek messages from the queue, do the required processing and then
complete the message to remove it from the queue, or abort the processing.
Abortion will leave the message at the queue, or will move it to the Dead Letter
Queue. Completion/abortion are granular per message; and the status of each
message is managed by the Service Bus broker.</p>
<p><img src="service-bus-processors.png" alt="Service Bus Processors"></p>
<p>Event Hubs service is different. Each Hub represnts a log of messages.
Event producer appends data to the end of the log, and consumers can read this log,
but they can&rsquo;t remove or change the status of events there.</p>
<p>Each event has an offset associated with it. And the only operation that is
supported for consumers is &ldquo;give me some messages starting at the offset X&rdquo;.</p>
<p><img src="event-hub-processors.png" alt="Event Hub Processors"></p>
<p>While this approach might seem simplistic, it actually  makes consumers
more powerful:</p>
<ul>
<li>
<p>The messages do not disappear from the Hub after being processed for the
first time. So, if needed, the consumer can go back and re-process older
events again;</p>
</li>
<li>
<p>Multiple consumers are always supported, out of the box. They just read
the same log, after all;</p>
</li>
<li>
<p>Each consumer can go at its own pace, drop and resume processing whenever
needed, with no effect on other consumers.</p>
</li>
</ul>
<p>There are some disadvantages too:</p>
<ul>
<li>
<p>Consumers have to manage their own state of the processing progress, i.e.
they have to save the offset of the last processed event;</p>
</li>
<li>
<p>There is no way to mark any specific event as failed to be able to reprocess
it later. There&rsquo;s no notion of Dead Letter Queue either.</p>
</li>
</ul>
<h2 id="event-processor-host">Event Processor Host</h2>
<p>To overcome the first complication, Microsoft provides the consumer API called
<a href="https://github.com/Microsoft/azure-docs/blob/master/articles/event-hubs/event-hubs-programming-guide.md">EventProcessorHost</a>.
This API has an implementation of consumers based on checkpointing. All you
need to do is to provide a callback to process a batch of events, and then call
<code>CheckpointAsync</code> method, which saves the current offset of the last message
into Azure Blob Storage. If the consumer restarts at any point in time, it will
read the last checkpoint to find the current offset, and will then continue
processing from that point on.</p>
<p>It works great for some scenarios, but the event delivery/processing guarantees
are relatively low in this case:</p>
<ul>
<li>
<p>Any failures are ignored: there&rsquo;s no retry or Dead Letter Queue</p>
</li>
<li>
<p>There are no transactions between event hub checkpoints and the data sinks
that the processor works with (i.e. data stores where processed messages
end up at)</p>
</li>
</ul>
<p>In this post I want to focus on a way to process events with higher consistency
requirements, in particular:</p>
<ul>
<li>
<p>Event Hub processor modifies data in a SQL Database, and such
processing is transactional per batch of messages</p>
</li>
<li>
<p>Each event should be (successfully) processed exactly once</p>
</li>
<li>
<p>If event processing failed, it should be marked as failed and kept
available to be reprocessed at later point in time</p>
</li>
</ul>
<p>While end-to-end exactly-once processing would require changes of the
producers too, we will only focus on consumer side in this post.</p>
<h2 id="transactional-checkpoints-in-sql">Transactional Checkpoints in SQL</h2>
<p>If checkpoint information is stored in Azure Blobs, there is no obvious way to
implement distributed transactions between SQL Database and Azure Storage.</p>
<p>However, we can override the default checkpointing mechanism and
implement our own checkpoints based on a SQL table. This way each
checkpoint update can become part of a SQL transaction and be committed
or rolled back with normal guarantees provided by SQL Server.</p>
<p>Here is a table that I created to hold my checkpoints:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">TABLE</span> EventHubCheckpoint (
  Topic varchar(100) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  PartitionID varchar(100) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  SequenceNumber bigint <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  <span style="color:#00f">Offset</span> varchar(20) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  <span style="color:#00f">CONSTRAINT</span> PK_EventHubCheckpoint <span style="color:#00f">PRIMARY</span> <span style="color:#00f">KEY</span> CLUSTERED (Topic, PartitionID)
)
</code></pre></div><p>For each topic and partition of Event Hubs, we store two values: sequence
number and offset, which together uniquely identify the consumer position.</p>
<p>Conveniently, Event Host Processor provides an extensibility point to
override the default checkpoint manager with a custom one. For that we
need to implement <code>ICheckpointManager</code> interface to work with our SQL
table.</p>
<p>The implementation mainly consists of 3 methods: <code>CreateCheckpointIfNotExistsAsync</code>,
<code>GetCheckpointAsync</code> and <code>UpdateCheckpointAsync</code>. The names are pretty
much self-explanatory, and my Dapper-based implementation is quite trivial.
You can find the code <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/eventhubs-sqlcheckpoints/SQLCheckpointManager.cs">here</a>.</p>
<p>For now, I&rsquo;m ignoring the related topic of lease management and corresponding
interface <code>ILeaseManager</code>. It&rsquo;s quite a subject on its own; for the sake
of simplicity I&rsquo;ll assume we have just one consumer process per partition,
which makes proper lease manager redundand.</p>
<h2 id="dead-letter-queue">Dead Letter Queue</h2>
<p>Now, we want to be able to mark some messages as failed and to
re-process them later. To make Dead Letters transactional, we need another
SQL table to hold the failed events:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">TABLE</span> EventHubDeadLetter (
  Topic varchar(100) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  PartitionID varchar(100) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  SequenceNumber bigint <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  <span style="color:#00f">Offset</span> varchar(20) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  FailedAt datetime <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  Error nvarchar(<span style="color:#00f">max</span>) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
  <span style="color:#00f">CONSTRAINT</span> PK_EventHubDeadLetter <span style="color:#00f">PRIMARY</span> <span style="color:#00f">KEY</span> CLUSTERED (Topic, PartitionID)
)
</code></pre></div><p>This table looks very similar to <code>EventHubCheckpoint</code> that I
defined above. That is because they are effectively storing pointers to
events in a hub. Dead Letters have two additional columns to store error
timestamp and text.</p>
<p>There is no need to store the message content, because failed events still
sit in the event hub anyway. You could still log it for diagnostics purpose - just make an extra
<code>varbinary</code> column.</p>
<p>There&rsquo;s no notion of dead letters in Event Hubs SDK, so I defined my own
interface <code>IDeadLetterManager</code> with a single <code>AddFailedEvents</code> method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IDeadLetterManager
{
    Task AddFailedEvents(IEnumerable&lt;DeadLetter&lt;EventData&gt;&gt; deadLetters);
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">DeadLetter</span>&lt;T&gt;
{
    <span style="color:#00f">public</span> T Data { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> DateTime FailureTime { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> Exception Exception { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>Dapper-based implementation is trivial again, you can find the code
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/eventhubs-sqlcheckpoints/SQLDeadLetterManager.cs">here</a>.</p>
<h2 id="putting-it-together-event-host">Putting It Together: Event Host</h2>
<p>My final solution is still using <code>EventHostProcessor</code>. I pass <code>SQLCheckpointManager</code>
into its constructor, and then I implement <code>IEventProcessor</code>&rsquo;s
<code>ProcessEventsAsync</code> method in the following way:</p>
<ol>
<li>Instantiate a list of items to store failed events</li>
<li>Start a SQL transaction</li>
<li>Loop through all the received events in the batch</li>
<li>Process each item inside a try-catch block</li>
<li>If exception happens, add the current event to the list of failed events</li>
<li>After all items are processed, save failed events to Dead Letter table</li>
<li>Update the checkpoint pointer</li>
<li>Commit the transaction</li>
</ol>
<p>The code block that illustrates this workflow:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">async</span> Task ProcessEventsAsync(
    PartitionContext context,
    IEnumerable&lt;EventData&gt; eventDatas)
{
    <span style="color:#008000">// 1. Instantiate a list of items to store failed events
</span><span style="color:#008000"></span>    <span style="color:#2b91af">var</span> failedItems = <span style="color:#00f">new</span> List&lt;DeadLetter&lt;EventData&gt;&gt;();

    <span style="color:#008000">// 2. Start a SQL transaction
</span><span style="color:#008000"></span>    <span style="color:#00f">using</span> (<span style="color:#2b91af">var</span> scope = <span style="color:#00f">new</span> TransactionScope())
    {
        <span style="color:#008000">// 3. Loop through all the received events in the batch
</span><span style="color:#008000"></span>        <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> eventData <span style="color:#00f">in</span> eventDatas)
        {
            <span style="color:#00f">try</span>
            {
                <span style="color:#008000">// 4. Process each item inside a try-catch block
</span><span style="color:#008000"></span>                <span style="color:#2b91af">var</span> item = <span style="color:#00f">this</span>.Deserialize(eventData);
                <span style="color:#00f">await</span> <span style="color:#00f">this</span>.DoWork(item);
            }
            <span style="color:#00f">catch</span> (Exception ex)
            {
                <span style="color:#008000">// 5. Add a failed event to the list
</span><span style="color:#008000"></span>                failedItems.Add(<span style="color:#00f">new</span> DeadLetter&lt;EventData&gt;(eventData, DateTime.UtcNow, ex));
            }
        }

        <span style="color:#00f">if</span> (failedItems.Any())
        {
            <span style="color:#008000">// 6. Save failed items to Dead Letter table
</span><span style="color:#008000"></span>            <span style="color:#00f">await</span> <span style="color:#00f">this</span>.dlq.AddFailedEvents(failedItems);
        }

        <span style="color:#008000">// 7. Update the checkpoint pointer
</span><span style="color:#008000"></span>        <span style="color:#00f">await</span> context.CheckpointAsync();

        <span style="color:#008000">// 8. Commit the transaction
</span><span style="color:#008000"></span>        scope.Complete();
    }
}
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>My implementation of Event Hubs consumer consists of 3 parts: checkpoint
manager that saves processing progress per partition into a SQL table;
dead letter manager that persists information about processing errors;
and an event host which uses both to provide transactional processing
of events.</p>
<p>The transaction scope is limited to SQL Server databases, but it might be
sufficient for many real world scenarios.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-event-hubs" term="azure-event-hubs" label="Azure Event Hubs" />
                             
                                <category scheme="https://mikhail.io/tags/azure-service-bus" term="azure-service-bus" label="Azure Service Bus" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Why F# and Functional Programming Talk at .NET Development Nederland Meetup]]></title>
            <link href="https://mikhail.io/2017/05/why-fsharp-and-functional-programming-will-make-you-a-better-developer-talk/"/>
            <id>https://mikhail.io/2017/05/why-fsharp-and-functional-programming-will-make-you-a-better-developer-talk/</id>
            
            <published>2017-05-08T00:00:00+00:00</published>
            <updated>2017-05-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On May 8th 2017 I gave a talk at the
<a href="https://www.meetup.com/dotNET-Development-Nederland/">.NET Development Nederland</a>
group in Amsterdam.</p>
<p>Here are the slides for the people who were there and want to revisit
the covered topics.</p>
<h2 id="why-learn-f-and-functional-programming">Why Learn F# and Functional Programming</h2>
<p>Link to full-screen HTML slides:
<a href="https://mikhail.io/talks/why-fsharp/">Why Learn F# and Functional Programming</a></p>
<p>Slides on SlideShare:</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/iG9omDKb42ogTk"
width="595" height="485"
frameborder="0" marginwidth="0" marginheight="0" scrolling="no"
style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<p>Useful links:</p>
<p><a href="http://fsharpforfunandprofit.com">F# for Fun and Profit</a></p>
<p><a href="http://fsharp.org">F# Foundation</a></p>
<p><a href="https://www.manning.com/books/real-world-functional-programming">Real-World Functional Programming, With examples in F# and C# Book</a></p>
<p>Thanks for attending my talk! Feel free to post any feedback in the comments.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/talk" term="talk" label="Talk" />
                             
                                <category scheme="https://mikhail.io/tags/meetup" term="meetup" label="Meetup" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/slides" term="slides" label="Slides" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Clean Code]]></title>
            <link href="https://mikhail.io/tags/clean-code/"/>
            <id>https://mikhail.io/tags/clean-code/</id>
            
            <published>2017-03-25T00:00:00+00:00</published>
            <updated>2017-03-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Dependency Injection]]></title>
            <link href="https://mikhail.io/tags/dependency-injection/"/>
            <id>https://mikhail.io/tags/dependency-injection/</id>
            
            <published>2017-03-25T00:00:00+00:00</published>
            <updated>2017-03-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Visualization]]></title>
            <link href="https://mikhail.io/tags/visualization/"/>
            <id>https://mikhail.io/tags/visualization/</id>
            
            <published>2017-03-25T00:00:00+00:00</published>
            <updated>2017-03-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Visualizing Dependency Tree from DI Container]]></title>
            <link href="https://mikhail.io/2017/03/visualizing-dependency-tree-from-di-container/"/>
            <id>https://mikhail.io/2017/03/visualizing-dependency-tree-from-di-container/</id>
            
            <published>2017-03-25T00:00:00+00:00</published>
            <updated>2017-03-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>So you are a C# developer. And you need to read the code and understand its
structure. Maybe you&rsquo;ve just joined the project, or it&rsquo;s your own code you
wrote 1 year ago. In any case, reading code is hard.</p>
<p>Luckily, some good thought was applied to this particular piece of code.
It&rsquo;s all broken down into small classes (they might even be SOLID!), and all
the dependencies are injected via constructors. It looks like it&rsquo;s your
code indeed.</p>
<p>So, you figured out that the entry point for your current use case is the
class called <code>ThingService</code>. It&rsquo;s probably doing something with <code>Thing</code>&rsquo;s
and that&rsquo;s what you need. The signature of the class constructor looks
like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> ThingService(
    IGetThings readRepository,
    ISaveThing saveRepository,
    IParseAndValidateExcel&lt;Thing, <span style="color:#2b91af">string</span>&gt; fileParser,
    IThingChangeDetector thingChangeDetector,
    IMap&lt;Thing, ThingDTO&gt; thingToDtoMapper,
    IMap&lt;<span style="color:#2b91af">int</span>, ThingDTO, Thing&gt; dtoToThingMapper)
</code></pre></div><p>OK, so we clearly have 6 dependencies here, and they are all interfaces.
We don&rsquo;t know where those interfaces are implemented, but hey - we&rsquo;ve got
the best tooling in the industry, so right click on <code>IGetThings</code>, then
<code>Go To Implementation</code>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> DapperThingRepository(
    ICRUDAdapter adapter,
    IDatabaseConnectionFactory connectionFactory,
    IMap&lt;Thing, ThingRow&gt; thingRowMapper,
    IMap&lt;ThingRow, Thing&gt; thingMapper)
</code></pre></div><p>Now we know that we get <code>Thing</code> from Dapper, so probably from a SQL database.
Let&rsquo;s go one level deeper and check where those Mappers are implemented.
Right click, <code>Go To Implementation</code>&hellip; But instead of navigating to another code
file you see</p>
<pre><code>Find Symbol Result - 28 matches found
</code></pre><p>Oh, right, looks like we use <code>IMap&lt;T, U&gt;</code> in more places. OK, we&rsquo;ll find the
right one later, let&rsquo;s first check the connection factory&hellip;
Right click, <code>Go To Implementation</code>. Nah:</p>
<pre><code>The symbol has no implementation
</code></pre><p>What? But the application works! Ah, <code>IDatabaseConnectionFactory</code> comes
from an internal library, so most probably the implementation is also
inside that library.</p>
<p>Clearly, navigation doesn&rsquo;t go that well so far.</p>
<h2 id="dependency-graph">Dependency Graph</h2>
<p>When code reading gets tricky, usually an image can boost the understanding.
The picture below actually shows the graph of class dependencies from our
example:</p>
<p><img src="class-dependency-graph.png" alt="Class Dependency Graph"></p>
<p>Each node is a class, each arrow is a dependency - an interface injected
into the constructor.</p>
<p>Just by looking at the picture for a minute of two you can start seeing some
structure, and get at least the high-level opinion about the application
complexity and class relationships.</p>
<p>Picture is also a great way of communication. Once you understand the structure,
you can explain it to a colleague much easier with boxes and lines
on the screen in addition to a plain wall of code.</p>
<p>You can enrich such picture with comments at the time of writing and leave
it to your future self or anyone who would read the code in 2 years time.</p>
<p>But now the question is - what&rsquo;s the easiest way to draw such dependency graph?</p>
<h2 id="di-container">DI Container</h2>
<p>The assumption of this post is that a dependency injection (DI) container
of some kind is used in the project. If so, chances are that you can get such
dependency graph from the container registrations.</p>
<p>My example is based on <a href="https://simpleinjector.org/">Simple Injector</a> DI
container which is used by ourselves. So, further on I will explain how to
draw a dependency graph from Simple Injector container.</p>
<p>My guess is that any mature DI library will provide you with such possibility,
mostly because the dependency graphs are built internally by any container
during its normal operations.</p>
<h2 id="implementation">Implementation</h2>
<p>The implementation idea of dependency graph visualization is quite simple, as
the biggest chunk of work is done by Simple Injector itself. Here are the steps:</p>
<ol>
<li>
<p>Run all your DI registrations as you do in the actual application. This will
initialize Container to the desired state.</p>
</li>
<li>
<p>Define which class should be the root of the dependency tree under study.
You can refine later, but you need to start somewhere.</p>
</li>
<li>
<p>Call <code>GetRegistration</code> method of DI container for the selected type. An instance
of <code>InstanceProducer</code> type is returned.</p>
</li>
<li>
<p>Call <code>GetRelationships</code> method of the instance producer to retrieve all
interface/class pairs that the given type depends on. Save each relation into
your output list.</p>
</li>
<li>
<p>Navigate through each dependency recursively to load further layers of the graph.
Basically, do the depth-first search and save all found relations.</p>
</li>
<li>
<p>Convert the list of found relations into <a href="https://en.wikipedia.org/wiki/Graphviz">GraphViz</a>
textual graph description.</p>
</li>
<li>
<p>Use a tool like <a href="http://www.webgraphviz.com/">WebGraphviz</a> do the actual
visualization by converting text to picture.</p>
</li>
</ol>
<p>There are several potential pitfalls on the way, like cyclic graphs, decorator
registrations etc. To help you avoid those I&rsquo;ve created a small library to automate
steps 3 to 6 from the list above. See my
<a href="https://github.com/mikhailshilkov/SimpleInjector.Visualization">SimpleInjector.Visualization github repo</a>
and let me know if you find it useful.</p>
<h2 id="conclusion">Conclusion</h2>
<p>People are good at making sense of visual representations - use that skill to
improve understanding and communication within your development team.</p>
<p>Dependency injection practice requires a lot of ceremony to set it up and
running. Leverage this work for the best: check what kind of insights you
can get from that setup. Dependency graph visualization is one example of
such leverage, but there might be other gems in there.</p>
<p>Just keep searching!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/dependency-injection" term="dependency-injection" label="Dependency Injection" />
                             
                                <category scheme="https://mikhail.io/tags/visualization" term="visualization" label="Visualization" />
                             
                                <category scheme="https://mikhail.io/tags/clean-code" term="clean-code" label="Clean Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Functions as a Facade for Azure Monitoring]]></title>
            <link href="https://mikhail.io/2017/03/azure-functions-as-facade-for-azure-monitoring/"/>
            <id>https://mikhail.io/2017/03/azure-functions-as-facade-for-azure-monitoring/</id>
            
            <published>2017-03-16T00:00:00+00:00</published>
            <updated>2017-03-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Azure Functions are the Function-as-a-Service offering from Microsoft Azure cloud.
Basically, an Azure Function is a piece of code which gets executed by Azure
every time an event of some kind happens. The environment manages deployment,
event triggers and scaling for you. This approach is often reffered as
Serverless.</p>
<p>In this post I will describe one use case for Azure Functions: we implemented
a number of functions as a proxy layer between our operations/monitoring
tool and Azure metric APIs.</p>
<h2 id="problem">Problem</h2>
<p>Automated monitoring and alerting are crucial in order to ensure 24x7 smooth
operations of our business-critical applications. We host applications both
on-premise and in Azure cloud, and we use a single set of tools for monitoring
across this hybrid environment.</p>
<p>Particularly, we use <a href="https://www.paessler.com/prtg">PRTG Network Monitor</a>
to collect all kinds of metrics about the health of our systems and produce
both real-time alerts and historic trends.</p>
<p>A unit of monitoring in PRTG is called &ldquo;sensor&rdquo;. Each sensor polls a specific
data source to retrieve the current value of a metric. The data source can
be a performance counter, a JSON value in HTTP response, a SQL query result
and so on.</p>
<p>The problem is that there is no PRTG sensor for Azure metrics out of the box.
It might be possible to implement a sensor with custom code, e.g. in PowerShell,
but it would be problematic in two ways (at least):</p>
<ol>
<li>The custom code sensors are cumbersome to develop and maintain.</li>
<li>We would have to put sensitive information like Azure API keys and
connection strings to PRTG.</li>
</ol>
<h2 id="solution-overview">Solution Overview</h2>
<p>To overcome these problems we introduced an intermediate layer, as shown
on the following picture:</p>
<p><img src="prtg-http-azure.png" alt="PRTG to HTTP to Azure"></p>
<p>We use PRTG <code>HTTP XML/REST</code> sensor type. This sensor polls a given HTTP endpoint,
parses the response as JSON and finds a predefined field. This field is then
used as the sensor value. It takes 30 seconds to setup such sensor in PRTG.</p>
<p>The HTTP endpoint is hosted inside Azure. It provides a facade for metric
data access. All the sensitive information needed to access Azure metrics
API is stored inside Azure configuration itself. The implementation knows
which Azure API to use to get a specific metric, and it hides those
complications from the client code.</p>
<h2 id="azure-functions">Azure Functions</h2>
<p>We chose Azure Functions as the technology to implement and host such HTTP
facade.</p>
<p>The functions are very easy to create or modify. They are deployed independently
from any other code, so we can update them at any cadence. And no need to
provision any kind of servers anywhere - Azure will run the code for us.</p>
<p>Here is how the whole setup works:</p>
<p><img src="prtg-azure-flow.png" alt="Retrieval of data from Azure to PRTG"></p>
<ol>
<li>
<p>Every X minutes (configured per sensor), PRTG makes an HTTP request
to a predefined URL. The request includes an Access Key as a query parameter
(the key is stored in sensor URL configuration). Each access key enables
access to just one endpoint and is easily revokable.</p>
</li>
<li>
<p>For each Metric type there is an Azure Function listening for
HTTP requests from PRTG. Azure authorizes requests that contain valid
access keys.</p>
</li>
<li>
<p>Based on query parameters of the request, Azure Function retrieves a proper
metric value from Azure management API. Depending on the metric type, this
is accomplished with Azure .NET SDK or by sending a raw HTTP request to
Azure REST API.</p>
</li>
<li>
<p>Azure Function parses the response from Azure API and converts it to
just the value which is requested by PRTG.</p>
</li>
<li>
<p>The function returns a simple JSON object as HTTP response body. PRTG
parses JSON, extracts the numeric value, and saves it into the sensor history.</p>
</li>
</ol>
<p>At the time of writing, we have 13 sensors served by 5 Azure Functions:</p>
<p><img src="prtg-azure-services.png" alt="Map of PRTG sensors to Functions to Azure services"></p>
<p>I describe several functions below.</p>
<h2 id="service-bus-queue-size">Service Bus Queue Size</h2>
<p>The easiest function to implement is the one which gets the amount of
messages in the backlog of a given Azure Service Bus queue. The
<code>function.json</code> file configures input and output HTTP bindings, including
two parameters to derive from the URL: <code>account</code> (namespace) and queue <code>name</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;bindings&#34;: [
    {
      &#34;authLevel&#34;: <span style="color:#a31515">&#34;function&#34;</span>,
      &#34;name&#34;: <span style="color:#a31515">&#34;req&#34;</span>,
      &#34;type&#34;: <span style="color:#a31515">&#34;httpTrigger&#34;</span>,
      &#34;direction&#34;: <span style="color:#a31515">&#34;in&#34;</span>,
      &#34;route&#34;: <span style="color:#a31515">&#34;Queue/{account}/{name}&#34;</span>
    },
    {
      &#34;name&#34;: <span style="color:#a31515">&#34;$return&#34;</span>,
      &#34;type&#34;: <span style="color:#a31515">&#34;http&#34;</span>,
      &#34;direction&#34;: <span style="color:#a31515">&#34;out&#34;</span>
    }
  ],
  &#34;disabled&#34;: <span style="color:#00f">false</span>
}
</code></pre></div><p>The C# implementation uses standard Service Bus API and a connection string
from App Service configuration to retrieve the required data. And then returns
a dynamic object, which will be converted to JSON by Function App runtime.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="">#</span>r <span style="color:#a31515">&#34;Microsoft.ServiceBus&#34;</span>

<span style="color:#00f">using</span> System.Net;
<span style="color:#00f">using</span> Microsoft.ServiceBus;

<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">object</span> Run(HttpRequestMessage req, <span style="color:#2b91af">string</span> account, <span style="color:#2b91af">string</span> name)
{
    <span style="color:#2b91af">var</span> connectionString = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;sb-&#34;</span> + account);
    <span style="color:#2b91af">var</span> nsmgr = NamespaceManager.CreateFromConnectionString(connectionString);
    <span style="color:#2b91af">var</span> queue = nsmgr.GetQueue(name);
    <span style="color:#00f">return</span> <span style="color:#00f">new</span>
    {
        messageCount = queue.MessageCountDetails.ActiveMessageCount,
        dlq = queue.MessageCountDetails.DeadLetterMessageCount
    };
}
</code></pre></div><p>And that is all the code required to start monitoring the queues!</p>
<h2 id="service-bus-queue-statistics">Service Bus Queue Statistics</h2>
<p>In addition to queue backlog and dead letter queue size, we wanted to see
some queue statistics like amount of incoming and outgoing messages per
period of time. The corresponding API exists, but it&rsquo;s not that straightforward,
so I described the whole approach in a separate post:
<a href="https://mikhail.io/2017/03/azure-service-bus-entity-metrics-dotnet-apis/">Azure Service Bus Entity Metrics .NET APIs</a>.</p>
<p>In my Azure Function I&rsquo;m using the NuGet package that I mentioned in the post.
This is accomplished by adding a <code>project.json</code> file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  &#34;frameworks&#34;: {
    &#34;net46&#34;:{
      &#34;dependencies&#34;: {
        &#34;MikhailIo.ServiceBusEntityMetrics&#34;: <span style="color:#a31515">&#34;0.1.2&#34;</span>
      }
    }
   }
}
</code></pre></div><p>The <code>function.json</code> file is similar to the previous one, but with one added
parameter called <code>metric</code>. I won&rsquo;t repeat the whole file here.</p>
<p>The Function implementation loads a certificate from the store, calls
metric API and returns the last metric value available:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">using</span> System.Linq;
<span style="color:#00f">using</span> System.Security.Cryptography.X509Certificates;
<span style="color:#00f">using</span> MikhailIo.ServiceBusEntityMetrics;

<span style="color:#00f">public</span> <span style="color:#00f">static</span> DataPoint Run(HttpRequestMessage req, <span style="color:#2b91af">string</span> account, <span style="color:#2b91af">string</span> name, <span style="color:#2b91af">string</span> metric)
{
    <span style="color:#2b91af">var</span> subscription = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;SubscriptionID&#34;</span>);
    <span style="color:#2b91af">var</span> thumbprint = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;WEBSITE_LOAD_CERTIFICATES&#34;</span>);

    X509Store certStore = <span style="color:#00f">new</span> X509Store(StoreName.My, StoreLocation.CurrentUser);
    certStore.Open(OpenFlags.ReadOnly);

    X509Certificate2Collection certCollection = certStore.Certificates.Find(
        X509FindType.FindByThumbprint,
        thumbprint,
        <span style="color:#00f">false</span>);

    <span style="color:#2b91af">var</span> client = <span style="color:#00f">new</span> QueueStatistics(certCollection[0], subscription, account, name);
    <span style="color:#2b91af">var</span> metrics = client.GetMetricSince(metric, DateTime.UtcNow.AddMinutes(-30));
    <span style="color:#00f">return</span> metrics.LastOrDefault();
}
</code></pre></div><p>Don&rsquo;t forget to set <code>WEBSITE_LOAD_CERTIFICATES</code> setting to your certificate
thumbprint, otherwise Function App won&rsquo;t load it.</p>
<h2 id="web-app-instance-count">Web App Instance Count</h2>
<p>We are using Azure Web Jobs to run background data processing, e.g. for all
queue message handlers. The jobs are hosted in Web Apps, and have auto-scaling
enabled. When the load on the system grows, Azure spins up additional
instances to increase the overall throughput.</p>
<p>So, the next metric to be monitored is the amount of Web App instances running.</p>
<p>There is a REST endpoint to retrieve this information, but this time
authentication and authorization are implemented with Active Directory. I
created a helper class to wrap the authentication logic:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">RestClient</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;T&gt; Query&lt;T&gt;(<span style="color:#2b91af">string</span> url)
    {
        <span style="color:#2b91af">var</span> token = <span style="color:#00f">await</span> GetAuthorizationHeader();
        <span style="color:#2b91af">var</span> client = <span style="color:#00f">new</span> HttpClient();
        client.DefaultRequestHeaders.Authorization = <span style="color:#00f">new</span> AuthenticationHeaderValue(<span style="color:#a31515">&#34;Bearer&#34;</span>, token);

        <span style="color:#2b91af">var</span> response = <span style="color:#00f">await</span> client.GetAsync(url);
        <span style="color:#2b91af">var</span> content = <span style="color:#00f">await</span> response.Content.ReadAsStringAsync();
        <span style="color:#00f">return</span> JsonConvert.DeserializeObject&lt;T&gt;(content);
    }

    <span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;<span style="color:#2b91af">string</span>&gt; GetAuthorizationHeader()
    {
        <span style="color:#2b91af">var</span> activeDirectoryID = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ActiveDirectoryID&#34;</span>);
        <span style="color:#2b91af">var</span> applicationID = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ActiveDirectoryApplicationID&#34;</span>);
        <span style="color:#2b91af">var</span> secret = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ActiveDirectorySecret&#34;</span>);

        <span style="color:#2b91af">var</span> context = <span style="color:#00f">new</span> AuthenticationContext(<span style="color:#a31515">$&#34;https://login.windows.net/{activeDirectoryID}&#34;</span>);
        <span style="color:#2b91af">var</span> credential = <span style="color:#00f">new</span> ClientCredential(applicationID, secret);
        AuthenticationResult result =
            <span style="color:#00f">await</span> context.AcquireTokenAsync(<span style="color:#a31515">&#34;https://management.core.windows.net/&#34;</span>, credential);
        <span style="color:#00f">return</span> result.AccessToken;
    }
}
</code></pre></div><p>The function then uses this REST client to query Web App management API,
converts JSON to strongly typed C# objects and extracts the amount of
instances into HTTP response:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Instance</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> id { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> name { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Response</span>
{
    <span style="color:#00f">public</span> Instance[] <span style="color:#00f">value</span> { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;HttpResponseMessage&gt; Run(HttpRequestMessage req)
{
    <span style="color:#2b91af">var</span> subscription = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;SubscriptionID&#34;</span>);
    <span style="color:#2b91af">var</span> resourceGroup = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ResourceGroup&#34;</span>);
    <span style="color:#2b91af">var</span> appService = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;AppService&#34;</span>);

    <span style="color:#2b91af">var</span> url = <span style="color:#a31515">$&#34;https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resourceGroup}&#34;</span> +
              <span style="color:#a31515">$&#34;/providers/Microsoft.Web/sites/{appService}/instances?api-version=2015-08-01&#34;</span>;
    <span style="color:#2b91af">var</span> response = <span style="color:#00f">await</span> RestClient.Query&lt;Response&gt;(url);

    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK, <span style="color:#00f">new</span>
    {
        instanceCount = response.<span style="color:#00f">value</span>.Length
    });
}
</code></pre></div><p>Please follow <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#create-an-active-directory-application">this walkthrough</a>
to setup your application in Active Directory, assign required permissions and
get the proper keys.</p>
<h2 id="azure-health">Azure Health</h2>
<p>Azure has a service which reports the health of different services at any
given moment, as acknowledged by Microsoft.</p>
<p>The handy part is that you can provide your subscription ID and then only
services used by that subscription will be reported.</p>
<p>The exact usage of health service may depend on your use case, but the
following example shows how to retrieve the basic counts of services per
reported status.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ResourceProperties</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> availabilityState { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> summary { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> detailedStatus { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> reasonType { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> occuredTime { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> reasonChronicity { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> reportedTime { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Resource</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> id { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> ResourceProperties properties { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Response</span>
{
    <span style="color:#00f">public</span> Resource[] <span style="color:#00f">value</span> { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;HttpResponseMessage&gt; Run(HttpRequestMessage req)
{
    <span style="color:#2b91af">var</span> subscription = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;SubscriptionID&#34;</span>);

    <span style="color:#2b91af">var</span> url = <span style="color:#a31515">$&#34;https://management.azure.com/subscriptions/{subscription}/providers/Microsoft.ResourceHealth/availabilityStatuses?api-version=2015-01-01&#34;</span>;
    <span style="color:#2b91af">var</span> r = <span style="color:#00f">await</span> RestClient.Query&lt;Response&gt;(url);
    <span style="color:#2b91af">var</span> available = r.<span style="color:#00f">value</span>
        .Where(v =&gt; v.properties.availabilityState == <span style="color:#a31515">&#34;Available&#34;</span>)
        .Count();

    <span style="color:#2b91af">var</span> unknown = r.<span style="color:#00f">value</span>
        .Where(v =&gt; v.properties.availabilityState == <span style="color:#a31515">&#34;Unknown&#34;</span>)
        .Count();

    <span style="color:#2b91af">var</span> other = r.<span style="color:#00f">value</span>.Length - available - unknown;

    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK, <span style="color:#00f">new</span>
    {
        available = available,
        unknown = unknown,
        other = other,
        details = r.<span style="color:#00f">value</span>
    });
}
</code></pre></div><h2 id="users-online">Users Online</h2>
<p>The last example I want to share is related to Application Insights data.
For instance, we inject a small tracking snippet on our front-end page
and then Application Insights track all the page views and other user
activity.</p>
<p>We use the amount of users currently online as another metric for the
monitoring solution. The Application Insights API is currently in
preview, but at least it is nicely described at
<a href="https://dev.applicationinsights.io/">dev.applicationinsights.io</a>. Be sure
to check out <a href="https://dev.applicationinsights.io/apiexplorer/metrics">API Explorer</a> too.</p>
<p>The following sample function returns the amount of users online:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">UsersCount</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">long</span> unique { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Value</span>
{
    [JsonProperty(&#34;users/count&#34;)]
    <span style="color:#00f">public</span> UsersCount UsersCount { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Response</span>
{
    <span style="color:#00f">public</span> Value <span style="color:#00f">value</span> { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">async</span> Task&lt;HttpResponseMessage&gt; Run(HttpRequestMessage req)
{
    <span style="color:#2b91af">var</span> appID = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ApplicationInsightsID&#34;</span>);
    <span style="color:#2b91af">var</span> key = Environment.GetEnvironmentVariable(<span style="color:#a31515">&#34;ApplicationInsightsKey&#34;</span>);

    <span style="color:#2b91af">var</span> client = <span style="color:#00f">new</span> HttpClient();
    client.DefaultRequestHeaders.Add(<span style="color:#a31515">&#34;x-api-key&#34;</span>, key);
    <span style="color:#2b91af">var</span> url = <span style="color:#a31515">$&#34;https://api.applicationinsights.io/beta/apps/{appID}/metrics/users/count&#34;</span>;

    <span style="color:#2b91af">var</span> response = <span style="color:#00f">await</span> client.GetAsync(url);
    <span style="color:#2b91af">var</span> content = <span style="color:#00f">await</span> response.Content.ReadAsStringAsync();
    <span style="color:#2b91af">var</span> r = JsonConvert.DeserializeObject&lt;Response&gt;(content);

    <span style="color:#00f">return</span> req.CreateResponse(HttpStatusCode.OK, <span style="color:#00f">new</span>
    {
        usersCount = r.<span style="color:#00f">value</span>.UsersCount.unique
    });
}
</code></pre></div><h2 id="combine-several-metrics-in-one-sensor">Combine Several Metrics in One Sensor</h2>
<p>Thanks to suggestion from Luciano Lingnau, we have migrated our PRTG
sensors to <code>HTTP Data Advanced</code>. This sensor type allows bundling several
related metrics into one sensor with multiple channels. PRTG is then
able to display all those channels on the single chart.</p>
<p>For instance, we use the following channels for Service Bus related
sensors:</p>
<ul>
<li>Active message count</li>
<li>Age of the oldest message sitting inside the queue</li>
<li>Dead letter message count</li>
<li>Incoming messages per 5 minutes</li>
<li>Outgoing messages per 5 minutes</li>
<li>Scheduled message count</li>
</ul>
<p>For each channel, we define units of measure, warning and error thresholds.</p>
<p><code>HTTP Data Advanced</code> expects a URL which returns JSON of the predefined format.
Here is a sample C# code to create a <code>dynamic</code> object which is then converted
to the proper JSON:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">return</span> <span style="color:#00f">new</span>
{
    prtg = <span style="color:#00f">new</span>
    {
        result = <span style="color:#00f">new</span>[]
        {
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;ActiveMessageCount&#34;</span>,
                <span style="color:#00f">value</span> = messageCountDetails.ActiveMessageCount,
                unit = <span style="color:#a31515">&#34;Count&#34;</span>,
                customunit = (<span style="color:#2b91af">string</span>)<span style="color:#00f">null</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)<span style="color:#00f">null</span>,
                limitmode = 0
            },
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;DeadLetterMessageCount&#34;</span>,
                <span style="color:#00f">value</span> = messageCountDetails.DeadLetterMessageCount,
                unit = <span style="color:#a31515">&#34;Count&#34;</span>,
                customunit = (<span style="color:#2b91af">string</span>)<span style="color:#00f">null</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)0,
                limitmode = 1
            },
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;OutgoingMessageCount&#34;</span>,
                <span style="color:#00f">value</span> = outgoing,
                unit = <span style="color:#a31515">&#34;custom&#34;</span>,
                customunit = <span style="color:#a31515">&#34;#/5min&#34;</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)<span style="color:#00f">null</span>,
                limitmode = 0
            },
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;IncommingMessageCount&#34;</span>,
                <span style="color:#00f">value</span> = incoming,
                unit = <span style="color:#a31515">&#34;custom&#34;</span>,
                customunit = <span style="color:#a31515">&#34;#/5min&#34;</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)<span style="color:#00f">null</span>,
                limitmode = 0
            },
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;ScheduledMessageCount&#34;</span>,
                <span style="color:#00f">value</span> = messageCountDetails.ScheduledMessageCount,
                unit = <span style="color:#a31515">&#34;Count&#34;</span>,
                customunit = (<span style="color:#2b91af">string</span>)<span style="color:#00f">null</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)<span style="color:#00f">null</span>,
                limitmode = 0
            },
            <span style="color:#00f">new</span>
            {
                channel = <span style="color:#a31515">&#34;Age&#34;</span>,
                <span style="color:#00f">value</span> = age,
                unit = <span style="color:#a31515">&#34;TimeSeconds&#34;</span>,
                customunit = (<span style="color:#2b91af">string</span>)<span style="color:#00f">null</span>,
                limitmaxwarning = (<span style="color:#2b91af">int?</span>)<span style="color:#00f">null</span>,
                limitmode = 0
            }
        }
    }
};

</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>It seems that monitoring metrics retrieval is an ideal scenario to start
using Azure Functions. The Functions are very easy to create and modify,
they abstract away the details of hosting Web API endpoints, and at the same
time give you the full power of C# (or F#) and Azure.</p>
<p>And because we only call those functions about 1 time per minute,
they are free to run!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-functions" term="azure-functions" label="Azure Functions" />
                             
                                <category scheme="https://mikhail.io/tags/metrics" term="metrics" label="Metrics" />
                             
                                <category scheme="https://mikhail.io/tags/monitoring" term="monitoring" label="Monitoring" />
                             
                                <category scheme="https://mikhail.io/tags/prtg" term="prtg" label="PRTG" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Metrics]]></title>
            <link href="https://mikhail.io/tags/metrics/"/>
            <id>https://mikhail.io/tags/metrics/</id>
            
            <published>2017-03-16T00:00:00+00:00</published>
            <updated>2017-03-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[PRTG]]></title>
            <link href="https://mikhail.io/tags/prtg/"/>
            <id>https://mikhail.io/tags/prtg/</id>
            
            <published>2017-03-16T00:00:00+00:00</published>
            <updated>2017-03-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure Service Bus Entity Metrics .NET APIs]]></title>
            <link href="https://mikhail.io/2017/03/azure-service-bus-entity-metrics-dotnet-apis/"/>
            <id>https://mikhail.io/2017/03/azure-service-bus-entity-metrics-dotnet-apis/</id>
            
            <published>2017-03-02T00:00:00+00:00</published>
            <updated>2017-03-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Getting advanced metrics out of Azure Service Bus in C# code</blockquote><p>Azure Service Bus is a key component of many background processing applications hosted in Azure, so it definitely requires monitoring and alerting. My goal for our monitoring solution was to provide an API to retrieve the following parameters for each Service Bus queue/topic in our application:</p>
<ul>
<li>Message count (backlog)</li>
<li>Dead letter queue count</li>
<li>Amount of Incoming messages per time period</li>
<li>Amount of Processed messages per time period</li>
</ul>
<p>The first two are easily retrieved from <code>QueueDescription</code> object (see
<a href="https://msdn.microsoft.com/library/azure/hh780773.aspx">MSDN</a>):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> nsmgr = NamespaceManager.CreateFromConnectionString(connectionString);
<span style="color:#2b91af">var</span> queue = nsmgr.GetQueue(name);
<span style="color:#2b91af">var</span> backlog = queue.MessageCountDetails.ActiveMessageCount;
<span style="color:#2b91af">var</span> dlq = q.MessageCountDetails.DeadLetterMessageCount;
</code></pre></div><p>The other two metrics are not readily available from the .NET SDK though.
There are some extra metrics described in
<a href="https://docs.microsoft.com/en-us/rest/api/servicebus/service-bus-entity-metrics-rest-apis">Service Bus Entity Metrics REST APIs</a>
but the docs are really brief, wague and lack any examples.</p>
<p>So the rest of this post will be a walkthrough of how to consume those
REST API from your .NET code.</p>
<h2 id="management-certificate">Management Certificate</h2>
<p>The API authenticates the caller by its client certificate. This authentication
approach seems to be deprecated for Azure services, but for this particular
API it&rsquo;s still the way to go.</p>
<p>First, you need to obtain a certificate itself, which means:</p>
<ul>
<li>It&rsquo;s installed in certificate store on the machine where API call is made</li>
<li>You have a <code>.cer</code> file for it</li>
</ul>
<p>If you are calling API from your workstation, you may just
<a href="https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-certs-create">Create a new self-signed certificate</a>.</p>
<p>I am calling API from Azure Function App, so I reused the certificate that we already
uploaded to Azure for SSL support.</p>
<p>Once you have the certificate, you have to
<a href="https://docs.microsoft.com/en-us/azure/azure-api-management-certs">Upload it as a management certificate</a>
to <a href="https://manage.windowsazure.com">&ldquo;Classic&rdquo; Azure portal</a>. Yes,
management certificates are not supported by the the new portal. If you don&rsquo;t
have access to the old portal, ask your system administrator to grant it.</p>
<p>Finally, here is a code sample to load the certificate in C# code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">X509Store store = <span style="color:#00f">new</span> X509Store(<span style="color:#a31515">&#34;My&#34;</span>, StoreLocation.CurrentUser);
store.Open(OpenFlags.ReadOnly);
<span style="color:#2b91af">var</span> cert = store.Certificates.Find(
    X509FindType.FindBySubjectName,
    <span style="color:#a31515">&#34;&lt;certificate name of yours&gt;&#34;</span>,
    <span style="color:#00f">false</span>)[0];
</code></pre></div><h2 id="request-headers">Request Headers</h2>
<p>Here is a helper class which adds the specified certificate to each request
and sets the appropriate headers too:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">internal</span> <span style="color:#00f">class</span> <span style="color:#2b91af">AzureManagementClient</span> : WebClient
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> X509Certificate2 certificate;

    <span style="color:#00f">public</span> AzureManagementClient(X509Certificate2 certificate)
    {
        <span style="color:#00f">this</span>.certificate = certificate;
    }

    <span style="color:#00f">protected</span> <span style="color:#00f">override</span> WebRequest GetWebRequest(Uri address)
    {
        <span style="color:#2b91af">var</span> request = (HttpWebRequest)<span style="color:#00f">base</span>.GetWebRequest(address);

        request.ClientCertificates.Add(<span style="color:#00f">this</span>.certificate);
        request.Headers.Add(<span style="color:#a31515">&#34;x-ms-version: 2013-10-01&#34;</span>);
        request.Accept = <span style="color:#a31515">&#34;application/json&#34;</span>;

        <span style="color:#00f">return</span> request;
    }
}
</code></pre></div><p>This code is mostly copied from the very useful
<a href="https://cincycoder.wordpress.com/2015/11/18/azure-service-bus-entity-metrics-api/">post of Brian Starr</a>,
so thank you Brian.</p>
<h2 id="getting-the-list-of-metrics">Getting the List of Metrics</h2>
<p>To get the list of available metrics you will need 3 string parameters:</p>
<ul>
<li>Azure subscription ID</li>
<li>Service Bus namespace</li>
<li>Queue name</li>
</ul>
<p>The following picture shows all of them on Azure Portal screen:</p>
<p><img src="servicebusparameters.png" alt="Service Bus Parameters"></p>
<p>Now, format the following request URL and query it using our azure client:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> client = <span style="color:#00f">new</span> AzureManagementClient(cert);
<span style="color:#2b91af">var</span> url = <span style="color:#a31515">$&#34;https://management.core.windows.net/{subscriptionId}&#34;</span> +
          <span style="color:#a31515">$&#34;/services/servicebus/namespaces/{serviceBusNamespace}&#34;</span> +
          <span style="color:#a31515">$&#34;/queues/{queueName}/Metrics&#34;</span>;
<span style="color:#2b91af">var</span> result = client.DownloadString(url);
</code></pre></div><p>If you did everything correctly, you will get the list of supported metrics
in JSON. Congratulations, that&rsquo;s a major accomplishment :)</p>
<p>And here is a quick way to convert JSON to C# array:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Metric</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Name { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Unit { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> PrimaryAggregation { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> DisplayName { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> metrics = JsonConvert.DeserializeObject&lt;Metric[]&gt;(result);
</code></pre></div><h2 id="getting-the-metric-values">Getting the Metric Values</h2>
<p>Now, to get the metric values themselves, you will need some extra
parameters:</p>
<ul>
<li>Metric name (take a value of <code>Name</code> properties from <code>Metric</code> class above)</li>
<li>Rollup period, or aggregation period: 5 minute, 1 hour, 1 day, or 1 week,
take the <code>Pxxx</code> code from <a href="https://docs.microsoft.com/en-us/rest/api/servicebus/supported-rollups">here</a></li>
<li>Start date/time (UTC) of the data period to query</li>
</ul>
<p>Here is the sample code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> time = DateTime.UtcNow.AddHours(-1).ToString(<span style="color:#a31515">&#34;s&#34;</span>);

<span style="color:#2b91af">var</span> client = <span style="color:#00f">new</span> AzureManagementClient(cert);
<span style="color:#2b91af">var</span> url = <span style="color:#a31515">$&#34;https://management.core.windows.net/{subscriptionId}&#34;</span> +
          <span style="color:#a31515">$&#34;/services/servicebus/namespaces/{serviceBusNamespace}&#34;</span> +
          <span style="color:#a31515">$&#34;/queues/{queueName}/Metrics/{metric}&#34;</span> +
          <span style="color:#a31515">$&#34;/Rollups/PT5M/Values?$filter=Timestamp%20ge%20datetime&#39;{time}Z&#39;&#34;</span>;

<span style="color:#2b91af">var</span> result = client.DownloadString(url);
</code></pre></div><p>I am using <code>incoming</code> metric to get the amount of enqueued messages per period
and <code>outgoing</code> metric to get the amount of dequeued messages.</p>
<p>The strongly typed version is simple:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">DataPoint</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Timestamp { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">long</span> Total { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> data = JsonConvert.DeserializeObject&lt;DataPoint[]&gt;(result);
</code></pre></div><h2 id="working-example">Working Example</h2>
<p>I&rsquo;ve authored a small library which wraps the HTTP request into strongly
typed .NET classes. You can see it in
<a href="https://github.com/mikhailshilkov/ServiceBusEntityMetrics">my github repository</a>
or grab it from <a href="https://www.nuget.org/packages/MikhailIo.ServiceBusEntityMetrics/">NuGet</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-service-bus" term="azure-service-bus" label="Azure Service Bus" />
                             
                                <category scheme="https://mikhail.io/tags/metrics" term="metrics" label="Metrics" />
                             
                                <category scheme="https://mikhail.io/tags/monitoring" term="monitoring" label="Monitoring" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Coding Puzzle in F#: Find the Number of Islands]]></title>
            <link href="https://mikhail.io/2017/02/coding-puzzle-in-fsharp-find-the-number-of-islands/"/>
            <id>https://mikhail.io/2017/02/coding-puzzle-in-fsharp-find-the-number-of-islands/</id>
            
            <published>2017-02-01T00:00:00+00:00</published>
            <updated>2017-02-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Here&rsquo;s a programming puzzle. Given 2D matrix of 0&rsquo;s and 1&rsquo;s, find the number of islands.
A group of connected 1&rsquo;s forms an island. For example, the below matrix contains 5 islands</p>
<pre><code>Input : mat = {{1, 1, 0, 0, 0},
               {0, 1, 0, 0, 1},
               {1, 0, 0, 1, 1},
               {0, 0, 0, 0, 0},
               {1, 0, 1, 0, 1}}
Output : 5
</code></pre><p>A typical solution to this problem will be implemented in C++, Java or C# and will involve
a loop to iterate through the matrix, and another loop or recursion to traverse islands.
The traversal progress will be tracked in an auxiliary mutable array, denoting the visited
nodes. An example of such solution (and the definition of the problem above) can be
found <a href="http://www.geeksforgeeks.org/find-number-of-islands/">here</a>.</p>
<p>I want to give an example of solution done in F#, with generic immutable data structures
and pure functions.</p>
<h2 id="graph-traversal">Graph Traversal</h2>
<p>First of all, this puzzle is a variation of the standard problem: Counting number of
connected components in a graph.</p>
<p><img src="islands.png" alt="Connected Graph Components"></p>
<p>I will start my implementation with a graph traversal implementation, and then we
will apply it to the 2D matrix at hand.</p>
<p>The graph is defined by the following type:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Graph</span>&lt;<span style="color:#00f">&#39;</span>a&gt; = {
  Nodes: seq&lt;<span style="color:#00f">&#39;</span>a&gt;
  Neighbours: <span style="color:#00f">&#39;</span>a -&gt; seq&lt;<span style="color:#00f">&#39;</span>a&gt;
}
</code></pre></div><p>It is a record type with two fields: a sequence of all nodes, and a function to
get neighbour nodes for a given node. The type of the node is generic: I&rsquo;ll use
numbers for our example, but <code>Graph</code> type doesn&rsquo;t care much.</p>
<p>The traversal plan is the following:</p>
<ol>
<li>
<p>Go through the sequence of graph nodes.</p>
</li>
<li>
<p>Keep two accumulator data structures: the list of disjoint sub-graphs
(sets of nodes connected to each other) and the set of visited nodes.
Both are empty at the beginning.</p>
</li>
<li>
<p>If the current node is not in the visited set, recursively traverse all
neighbours to find the current connected component.</p>
</li>
<li>
<p>The connected component traversal is the Depth-First Search, each node
is added to both current set and total visited set.</p>
</li>
</ol>
<p>Let&rsquo;s start the implementation from inside out. The following recursive function
adds a node to the accumulated sets and calls itself for non-visited neighbours:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> rec visitNode accumulator visited node =
  <span style="color:#00f">let</span> newAccumulator = Set.add node accumulator
  <span style="color:#00f">let</span> newVisited = Set.add node visited

  graph.Neighbours node
  |&gt; Seq.filter (<span style="color:#00f">fun</span> n -&gt; Set.contains n newVisited |&gt; <span style="color:#00f">not</span>)
  |&gt; Seq.fold (<span style="color:#00f">fun</span> (acc, vis) n -&gt; visitNode acc vis n) (newAccumulator, newVisited)
</code></pre></div><p>The type of this function is <code>Set&lt;'a&gt; -&gt; Set&lt;'a&gt; -&gt; 'a -&gt; Set&lt;'a&gt; * Set&lt;'a&gt;</code>.</p>
<p>Step 3 is implemented with <code>visitComponent</code> function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> visitComponent (sets, visited) node =
  <span style="color:#00f">if</span> Set.contains node visited
  <span style="color:#00f">then</span> sets, visited
  <span style="color:#00f">else</span>
    <span style="color:#00f">let</span> newIsland, newVisited = visitNode Set.empty visited node
    newIsland :: sets, newVisited
</code></pre></div><p>Now, the graph traversal is just a <code>fold</code> of graph nodes with <code>visitComponent</code> function.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Graph =
  <span style="color:#00f">let</span> findConnectedComponents graph =
    graph.Nodes
    |&gt; Seq.fold visitComponent ([], Set.empty)
    |&gt; fst
</code></pre></div><p>This is the only public function of our graph API, available for the client
applications. The <code>visitNode</code> and <code>visitComponent</code> are defined as local functions
underneath (and they close over the graph value).</p>
<h2 id="2d-matrix">2D Matrix</h2>
<p>Now, let&rsquo;s forget about the graphs for a second and model the 2D matrix of integers.
The type definition is simple, it&rsquo;s just an alias for the array:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Matrix2D</span> = int[,]
</code></pre></div><p>Now, we need to be able to traverse the matrix, i.e. iterate through all elements and
find the neighbours of each element.</p>
<p>The implementation below is mostly busy validating the boundaries of the array. The
neighbours of a cell are up to 8 cells around it, diagonal elements included.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Matrix2D =
  <span style="color:#00f">let</span> allCells (mx: Matrix2D) = seq {
    <span style="color:#00f">for</span> x <span style="color:#00f">in</span> [0 .. Array2D.length1 mx - 1] <span style="color:#00f">do</span>
      <span style="color:#00f">for</span> y <span style="color:#00f">in</span> [0 .. Array2D.length2 mx - 1] -&gt; x, y
  }

  <span style="color:#00f">let</span> neighbours (mx: Matrix2D) (x,y) =
    Seq.crossproduct [x-1 .. x+1] [y-1 .. y+1]
    |&gt; Seq.filter (<span style="color:#00f">fun</span> (i, j) -&gt; i &gt;= 0 &amp;&amp; j &gt;= 0
                              &amp;&amp; i &lt; Array2D.length1 mx
                              &amp;&amp; j &lt; Array2D.length2 mx)
    |&gt; Seq.filter (<span style="color:#00f">fun</span> (i, j) -&gt; i &lt;&gt; x || j &lt;&gt; y)
</code></pre></div><h2 id="putting-it-all-together">Putting It All Together</h2>
<p>Now we are all set to solve the puzzle. Here is our input array:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> mat = array2D
            [| [|1; 1; 0; 0; 0|];
               [|0; 1; 0; 0; 1|];
               [|1; 0; 0; 1; 1|];
               [|0; 0; 0; 0; 0|];
               [|1; 0; 1; 0; 1|]
            |]
</code></pre></div><p>We need a function to define if a given cell is a piece of an island:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> isNode (x, y) = mat.[x, y] = 1
</code></pre></div><p>And here is the essence of the solution - our graph definition. Both <code>Nodes</code>
and <code>Neightbours</code> are matrix cells filtered to contain 1&rsquo;s.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> graph = {
  Nodes = Matrix2D.allCells mat |&gt; Seq.filter isNode
  Neighbours = Matrix2D.neighbours mat &gt;&gt; Seq.filter isNode
}
</code></pre></div><p>The result is calculated with one-liner:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">graph |&gt; Graph.findConnectedComponents |&gt; List.length
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>The implementation above represents my attempt to solve in a functional way
the puzzle which is normally solved in imperative style. I took a step
back and tried to model the underlying concepts with separate data structures.
The types and functions might be reused for similar problems in the same
domain space.</p>
<p>While not a rocket science, the Connected Islands puzzle is a good exercise
and provides a nice example of functional concepts, which I&rsquo;m planning to
use while discussing FP and F#.</p>
<p>The full code can be found in <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/ConnectedIslands.fs">my github</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/programming-puzzles" term="programming-puzzles" label="Programming Puzzles" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Programming Puzzles]]></title>
            <link href="https://mikhail.io/tags/programming-puzzles/"/>
            <id>https://mikhail.io/tags/programming-puzzles/</id>
            
            <published>2017-02-01T00:00:00+00:00</published>
            <updated>2017-02-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Event Sourcing]]></title>
            <link href="https://mikhail.io/tags/event-sourcing/"/>
            <id>https://mikhail.io/tags/event-sourcing/</id>
            
            <published>2017-01-29T00:00:00+00:00</published>
            <updated>2017-01-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Event Sourcing: Optimizing NEventStore SQL read performance]]></title>
            <link href="https://mikhail.io/2017/01/event-sourcing-optimizing-neventstore-sql-read-performance/"/>
            <id>https://mikhail.io/2017/01/event-sourcing-optimizing-neventstore-sql-read-performance/</id>
            
            <published>2017-01-29T00:00:00+00:00</published>
            <updated>2017-01-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In <a href="https://mikhail.io/2016/11/event-sourcing-and-io-complexity/">my previous post about Event Store read complexity</a>
I described how the growth of reads from the event database might be
quadratic in respect to amount of events per aggregate.</p>
<p>On the higher level, the conclusion was that the event sourced database should be optimized
for reads rather that writes, which is not always obvious from the definition
of the &ldquo;append-only store&rdquo;.</p>
<h2 id="neventstore">NEventStore</h2>
<p>In this post I want to look at
<a href="https://github.com/NEventStore/NEventStore">NEventStore</a> on top of
<a href="https://azure.microsoft.com/en-us/services/sql-database/">Azure SQL Database</a>
which is the combination we currently use for event sourcing in Azure-based
web application.</p>
<p>NEventStore library provides a C# abstraction over event store with multiple
providers for several database backends. We use the
<a href="https://github.com/NEventStore/NEventStore.Persistence.SQL">Persistence.SQL provider</a>.
When you initialize
it with a connection string to an empty database, the provider will go
on and create two tables with schema, indexes etc. The most important
table is <code>Commits</code> and it gets the following schema:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">TABLE</span> dbo.Commits
(
  BucketId          varchar(40),
  StreamId          char(40),
  StreamRevision    int,
  Items             tinyint,
  CommitId          uniqueidentifier,
  CommitSequence    int,
  CheckpointNumber  bigint <span style="color:#00f">IDENTITY</span>(1, 1),
  Payload           varbinary(<span style="color:#00f">max</span>),
  CommitStamp       datetime2
)
<span style="color:#00f">GO</span>
<span style="color:#00f">ALTER</span> <span style="color:#00f">TABLE</span> dbo.Commits
<span style="color:#00f">ADD</span> <span style="color:#00f">CONSTRAINT</span> PK_Commits
<span style="color:#00f">PRIMARY</span> <span style="color:#00f">KEY</span> CLUSTERED (CheckpointNumber)
</code></pre></div><p>I removed several columns, most indexes and constraints to make the script
more readable.</p>
<p>The primary key is based upon <code>CheckpointNumber</code> - an <code>IDENTITY</code> column, which means
the new events (commits) are appended to the end of the clustered index.
Clearly, this is good for <code>INSERT</code> performance.</p>
<p>There is a number of secondary non-clustered indexes that are optimized
for rich API of NEventStore library, e.g. dispatching events to observers,
searching for streams, time-based queries etc.</p>
<h2 id="our-use-case">Our Use Case</h2>
<p>It turns out that we don&rsquo;t need those extended API provided by <code>NEventStore</code>.
Effectively, we only need two operations to be supported:</p>
<ul>
<li>Add a new event to a stream</li>
<li>Read all events of a stream</li>
</ul>
<p>Our experience of running production-like workloads showed that the read
operation performance suffers a lot when the size of a stream grows. Here
is a sample query plan for the read query with the default schema:</p>
<p><img src="defaultqueryplan.png" alt="Query Plan with default primary key"></p>
<p>SQL Server uses non-clustered index to find all events of the given
steam, and then does key lookups, which might get very expensive for
large streams with hundreds or thousands of events.</p>
<h2 id="tuning-for-reads">Tuning for Reads</h2>
<p>After seeing this, I decided to re-think the primary index of the
<code>Commits</code> table. Here is what I came down to:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">ALTER</span> <span style="color:#00f">TABLE</span> dbo.Commits
<span style="color:#00f">ADD</span> <span style="color:#00f">CONSTRAINT</span> PK_Commits
<span style="color:#00f">PRIMARY</span> <span style="color:#00f">KEY</span> CLUSTERED (BucketId, StreamId, CommitSequence)
</code></pre></div><p>Now, all the commits of one stream are physically located together in the
clustered index.</p>
<p>The change makes <code>INSERT</code>&rsquo;s less efficient. It&rsquo;s not a simple append to the
end of the clustered index anymore.</p>
<p>But at this price, the reads just got much faster. Here is the plan for
the same query over the new schema:</p>
<p><img src="optimizedqueryplan.png" alt="Query Plan with the new primary key"></p>
<p>Simple, beautiful and fast!</p>
<h2 id="our-results">Our Results</h2>
<p>The results look great for us. We are able to run our 50 GB Commits table
on a 100-DTU SQL Database instance, with typical load of 10 to 25 percent.
The reads are still taking the biggest chunk of the load, with writes
being far behind.</p>
<p>The mileage may vary, so be sure to test your NEventStore schema versus
your workload.</p>
<h2 id="further-improvements">Further Improvements</h2>
<p>Here are some further steps that we might want to take to make <code>Commits</code>
table even faster:</p>
<ul>
<li>
<p>The table comes with 5 non-clustered indexes. One of them became our
clustered index. Two indexes are unique, so they might be useful for
duplicate prevention (e.g. in concurrency scenarios). The remaining two
are non-unique, so they can probably be safely deleted unless we start
using other queries that they are intended for.</p>
</li>
<li>
<p>There are several columns which are not used in our implementation:
<code>StreamIdOriginal</code>, <code>Dispatched</code> and <code>Headers</code> to name a few. We could
replace the table with a view of the same name, and always return defaults
for those columns in any <code>SELECT</code>, ignoring the values in any <code>INSERT</code>.</p>
</li>
</ul>
<p>But I expect these changes to have moderate impact on performance in contrast
to the primary key change discussed above.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/event-sourcing" term="event-sourcing" label="Event Sourcing" />
                             
                                <category scheme="https://mikhail.io/tags/neventstore" term="neventstore" label="NEventStore" />
                             
                                <category scheme="https://mikhail.io/tags/sql-server" term="sql-server" label="SQL Server" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[NEventStore]]></title>
            <link href="https://mikhail.io/tags/neventstore/"/>
            <id>https://mikhail.io/tags/neventstore/</id>
            
            <published>2017-01-29T00:00:00+00:00</published>
            <updated>2017-01-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[SQL Server]]></title>
            <link href="https://mikhail.io/tags/sql-server/"/>
            <id>https://mikhail.io/tags/sql-server/</id>
            
            <published>2017-01-29T00:00:00+00:00</published>
            <updated>2017-01-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Advent of Code]]></title>
            <link href="https://mikhail.io/tags/advent-of-code/"/>
            <id>https://mikhail.io/tags/advent-of-code/</id>
            
            <published>2017-01-26T00:00:00+00:00</published>
            <updated>2017-01-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[My Praise of Advent of Code 2016]]></title>
            <link href="https://mikhail.io/2017/01/my-praise-of-advent-of-code-2016/"/>
            <id>https://mikhail.io/2017/01/my-praise-of-advent-of-code-2016/</id>
            
            <published>2017-01-26T00:00:00+00:00</published>
            <updated>2017-01-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>During the last days of December I was pleasing my internal need for solving
puzzles and tricky tasks by going through
<a href="http://adventofcode.com">Advent of Code 2016</a> challenge.</p>
<p>The idea is simple: every day since December 1st to 25th, the site publishes
a new brain teaser. They are all aligned into one story: the Bad Easter Bunny
has stolen all the Chrismas gifts from Santa, and now you are the hero who
should break into the Bunny&rsquo;s headquarters and save the gifts for the kids.</p>
<p>Having said that, each challenge is independent from the others, so you can
solve them in arbitrary order if you want.</p>
<p><img src="levelmap.png" alt="Advent Of Code Levels">
<em>Advent Calendar in dark ASCII</em></p>
<p>A puzzle consists of a description and an input data set associated with it.
The solution is typically represented as a number or a short string, so it
can be easily typed into the textbox. However, to get this solution you need to
implement a program: computing it manually is not feasible.</p>
<p>I started a bit late and got just the first 11 puzzles solved. Each puzzle
is doable in one sitting, usually half-an-hour to a couple hours of work,
which is very nice.</p>
<p>Some problems are purely about the correctness of your solution. The most
engaging tasks were also computationally intensive, such that a straightforward
solution took too much time to run to completion. You need to find a
shortcut to make it faster, which is always fun.</p>
<p><img src="solved.png" alt="Problem Solved!">
<em>You collect stars for providing the correct answers</em></p>
<p>Apart from generic joy and satisfaction that one gets from solving programming
challenges like these, I also consider it a good opportunity to try a
new programming language or a paradygm.</p>
<p>As I said, the tasks are relatively small, so you can feel the sense of
accomplishment quite often, even being not very familiar with the programming
language of choice.</p>
<p>There are many other people solving the same puzzles and also sharing their
solutions online. You can go and find the other implementations of a task
that you just solved, and compare it to your approach. That&rsquo;s the great way
to learn from other people, broaden your view and expose yourself to new
tricks, data structures and APIs.</p>
<p>I picked F# as my programming language for Advent of Code 2016. I chose to restrict
myself to immutable data structures and pure functions. And it played out really nice,
I am quite happy with speed of development, readability and performance of
the code.</p>
<p><img src="day8.png" alt="Day 8 solved">
<em>Solution to one of the puzzles</em></p>
<p>You can find my code for the first 11 puzzles in
<a href="https://github.com/mikhailshilkov/AdventOfCode2016">my github account</a>.
Full sets of F# solutions are available from
<a href="https://github.com/markheath/advent-of-code-2016/">Mark Heath</a> and
<a href="https://github.com/theburningmonk/AdventOfCodeFs">Yan Cui</a>.</p>
<p>I included one of the solutions into
<a href="https://mikhail.io/2017/01/functional-programming-fsharp-talks/">The Taste of F# talk</a> that I did
at a user group earlier this month.</p>
<p>Next year I&rsquo;ll pick another language and will start on December 1st. I invite
you to join me in solving Advent of Code 2017.</p>
<p>Kudos to <a href="https://twitter.com/ericwastl">Eric Wastl</a> for creating and
maintaining the <a href="http://adventofcode.com">Advent of Code web site</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/programming-puzzles" term="programming-puzzles" label="Programming Puzzles" />
                             
                                <category scheme="https://mikhail.io/tags/advent-of-code" term="advent-of-code" label="Advent of Code" />
                             
                                <category scheme="https://mikhail.io/tags/programming-puzzles" term="programming-puzzles" label="Programming Puzzles" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[My Functional Programming & F# Talks at Webscale Architecture Meetup]]></title>
            <link href="https://mikhail.io/2017/01/functional-programming-fsharp-talks/"/>
            <id>https://mikhail.io/2017/01/functional-programming-fsharp-talks/</id>
            
            <published>2017-01-10T00:00:00+00:00</published>
            <updated>2017-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On January 10th of 2017 I gave two talks at the
<a href="https://www.meetup.com/Webscale-Architecture-NL/events/235727572/">Webscale Architecture NL meetup</a> group in Utrecht.</p>
<p>Here are the slides for the people who were there and want to revisit
the covered topics.</p>
<h2 id="introduction-of-functional-programming">Introduction of Functional Programming</h2>
<p>Link to full-screen HTML slides:
<a href="https://mikhail.io/talks/webscale-fp/">Introduction of Functional Programming</a></p>
<p>Slides on SlideShare:</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/1L3y6bQDoibPrN"
width="778" height="590" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"
style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<h2 id="the-taste-of-f">The Taste of F#</h2>
<p>Link to full-screen HTML slides:
<a href="https://mikhail.io/talks/webscale-fsharp/">The Taste of F#</a></p>
<p>Example problem that I was solving:
<a href="http://adventofcode.com">Advent of Code</a> and
<a href="http://adventofcode.com/2016/day/8">Day 8: Two-Factor Authentication</a></p>
<p>Slides on SlideShare:</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jqd9mSKQdrkyYL"
width="778" height="590" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"
style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<p>Thanks for attending my talks! Feel free to post any feedback in the comments.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/talk" term="talk" label="Talk" />
                             
                                <category scheme="https://mikhail.io/tags/meetup" term="meetup" label="Meetup" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/slides" term="slides" label="Slides" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2016s]]></title>
            <link href="https://mikhail.io/2016/"/>
            <id>https://mikhail.io/2016/</id>
            
            <published>2016-11-29T00:00:00+00:00</published>
            <updated>2016-11-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Introducing Stream Processing in F#]]></title>
            <link href="https://mikhail.io/2016/11/introducing-stream-processing-in-fsharp/"/>
            <id>https://mikhail.io/2016/11/introducing-stream-processing-in-fsharp/</id>
            
            <published>2016-11-29T00:00:00+00:00</published>
            <updated>2016-11-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>The post was published for
<a href="https://sergeytihon.wordpress.com/2016/10/23/f-advent-calendar-in-english-2016/">F# Advent Calendar 2016</a>,
thus the examples are themed around the Christmas gifts.</em></p>
<p>This article is my naive introduction to the data processing discipline called <strong>Stream Processing</strong>.</p>
<p>I describe stream processing from the developer perspective, using the
following (rather unusual) angle:</p>
<ul>
<li>F# as the primary language</li>
<li>Concepts and principles are more important than frameworks and tools</li>
<li>Start with modeling the domain problem and then solve just the problems
that your domain has</li>
</ul>
<p>We begin the journey by introducing what stream processing is all about.</p>
<h2 id="whats-and-whys">What&rsquo;s and Why&rsquo;s</h2>
<p>There are several techniques to process data that are flowing into your
applications, and stream processing is one of them.</p>
<p>Stream processing is focused on the real-time processing of data
continuously, concurrently, and in a record-by-record fashion. Stream
processing is designed to analyze and act on live data flow, using
&ldquo;continuous queries&rdquo; expressed in user code. Data is structured
as a continuous stream of events over time:</p>
<p><img src="eventflow.png" alt="Flow of events"></p>
<p>In contrast to some other approaches to reason about application
structure, stream processing concepts are drawn around the data structures,
flows and transformations rather than services or remote calls.</p>
<p>Although the approach is nothing new, it gained much more traction
during the last years, especially in big data community. Products
like <em>Storm</em>, <em>Kafka</em>, <em>Flink</em>, <em>Samza</em> (all under <em>Apache Foundation</em>),
<em>Google Cloud Dataflow</em>, <em>Akka Streams</em> are popularizing the programming
model and bringing the tools to make it reliable and scalable.</p>
<p>These products are born from the need to run data processing in
massively distributed environments. They are all about scaling out
and solving or mitigating the issues of distributed systems which
are inherintly not reliable.</p>
<p>While this is a noble and mission-critical goal for internet-scale companies, most
applications do not require such massive performances and scale.</p>
<p>There is something to be said for the domain-driven approach, when
an application is built around the main asset and burden of enterprise
systems: the core business logic. It may happen that you don&rsquo;t need
a general purpose framework with the lowest processing latency. Instead
your choice of tools might lean towards the cleanest code possible, tailored
for your own needs, and maintainable over time.</p>
<p>Knowing the landscape can help you do the right trade-off.</p>
<p>The recent Stream Processing boom comes from <em>Apache</em> / <em>JVM</em> world.
Unfortunately, stream processing frameworks and underlying concepts
are mostly unfamiliar to <em>.NET</em> developers.</p>
<p>While <em>Azure Cloud</em> provides a managed service called <em>Azure Stream Analytics</em>,
the product is built around SQL-like language and is rather limited in
extensibility.</p>
<p>We will have a look at other options in .NET space in the further posts
of the series.</p>
<p>For now, I want to start filling the gap and introduce the basic concepts
with F#. As a bonus, we are not limited by particular tools and
implementations, but can start from the ground up.</p>
<h2 id="elements-of-a-stream">Elements of a Stream</h2>
<p>As I already mentioned above, people are doing stream processing for long
time. In fact, if you receive events and then apply the transformation
logic structured around a single event at a time - you are already
doing stream processing.</p>
<p>Here is a simple picture which illustrates the elements of processing:</p>
<p><img src="stage.png" alt="Stream Processing Stage"></p>
<p>The data <strong>Source</strong> is responsible for injection of events
into the pipeline. They are the input intergration points, typically they can
be persistent message queues, logs or subscription feeds.</p>
<p>A sequence of events in the same Source is called <strong>Stream</strong> (thus Stream Processing).
Streams have <strong>unbounded</strong> nature, which means that the amount
of data points is not limited in size or time. There is no &ldquo;end&rdquo; of data: events
will potentially keep coming as long as the processing application is alive.</p>
<p>The high-level purpose of the <strong>Transformation</strong> is to extract
value from the events. That&rsquo;s where the business logic resides, and that&rsquo;s
where development effort goes to. Transformations can also be refered as Stages,
Flows, Tasks, Jobs and so on, depending on the context.</p>
<p>The most simple transformation like format conversion can be stateless.
However, other transformations will often use some kind of <strong>State Store</strong>,
as a means to</p>
<ul>
<li>Aggregate data from multiple events of the same stream</li>
<li>Correlate events from several streams</li>
<li>Enrich event data with external lookup data</li>
</ul>
<p>Data <strong>Sink</strong> represents the output of the pipeline, the place where the transformed,
aggregated and enriched events end up at.</p>
<p>A Sink can be a database of any kind, which stores the processed data, ready
to be consumed by user queries and reports.</p>
<p>Another Sink can become a Source for another stream of events. This way
the series of transformations are sequenced together into <strong>Processing Pipelines</strong>
(or <strong>Topologies</strong>).</p>
<p>On the high-level, the pipelines can usually be represented as directed graphs,
with data streams in nodes and transformations in edges:</p>
<p><img src="topology.png" alt="Stream Processing Pipeline"></p>
<p>In real-world applications, the pipelines can have lots of interconnected
elements and flow branches. We will start with a simplistic example.</p>
<h2 id="gift-count-pipeline">Gift Count Pipeline</h2>
<p>Word Count is the Hello World and TODO app of the data
processing world. Here are the reference implementations for
<a href="https://cloud.google.com/dataflow/examples/wordcount-example">Dataflow</a>,
<a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/wordcount/WordCount.java">Flink</a> and
<a href="https://github.com/nathanmarz/storm-starter/blob/master/src/jvm/storm/starter/WordCountTopology.java">Storm</a>.</p>
<p>To make it a bit more fun, we&rsquo;ll make a Gift Count pipeline out of it.
The following image summarizes our Gift Count topology:</p>
<p><img src="giftcount.png" alt="Gift Count Pipeline"></p>
<p>The pipeline consists of one source, one sink and two transformations.
The input of the pipeline is the source of gift lists (each list is a comma
separated line of text).</p>
<p>The purpose of the processing is to tokenize gift lists into separate gifts,
and then count the occurances of each gift in the stream. The output
is written into a database sink, e.g. a key value store with gifts as keys and
amounts as values.</p>
<p>Note that while Split stage is stateless, the Count stage needs to keep some
internal state to be able to aggregate data over multiple entries.</p>
<p>Let&rsquo;s start thinking about the implementation. How do we model transformations
and pipelines in code?</p>
<h2 id="transformations">Transformations</h2>
<p>Here&rsquo;s how a transformation is typically represented in <em>Apache Storm</em>, the grand
daddy of Big Data Stream Processing systems (transformations are called
<em>Bolts</em> in Storm, and code is Java):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TokenizerBolt</span> <span style="color:#00f">implements</span> IRichBolt {
    <span style="color:#00f">public</span> <span style="color:#2b91af">void</span> execute(Tuple input) {
        String wishlist = input.getString(0);
        <span style="color:#00f">for</span>(String gift: wishlist.split(<span style="color:#a31515">&#34;, &#34;</span>)) {
            collector.emit(<span style="color:#00f">new</span> Values(gift));
        }
        collector.ack(input);
    }
    <span style="color:#008000">// Other code is omitted
</span><span style="color:#008000"></span>}
</code></pre></div><p>Tokenizer is a class which implements a predefined interface with <code>execute</code>
method in it. The method accepts a container class <code>Tuple</code> from where
we can extract real data using position-based indexing. Something like a
<code>DataRow</code> from ADO.NET. The method does not return anything, but instead
calls an effect-ful method <code>emit</code>, passing a tuple to the next transformation
in the pipeline.</p>
<p>Clearly, there is some room for improvement here. We don&rsquo;t want to put our important
domain logic into amorphous functions of type <code>Collector -&gt; Tuple -&gt; unit</code>. Here
is what we can do:</p>
<ul>
<li>Use strong typing to see what function does based on its signature</li>
<li>Use pure functions to make them easy to test and reason about</li>
<li>Use domain-specific types with F# records and ADTs</li>
</ul>
<p>Our domain is very simple in Gift Count example. Still, we could describe <code>Gift</code>
type to restrict it to be lowercase, not empty etc. But for the sake of simplisity
I&rsquo;ll limit it to one liner:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Gift</span> = <span style="color:#2b91af">string</span>
</code></pre></div><p>Now, the type of the first transformation should be <code>string -&gt; Gift list</code>. So,
our transformation is based on a function</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> tokenize (wishlist: <span style="color:#2b91af">string</span>) =
  wishlist.ToLowerInvariant().Split(<span style="color:#a31515">&#34;, &#34;</span>)
  |&gt; List.ofArray
  |&gt; List.map (<span style="color:#00f">fun</span> x -&gt; x.Trim())
  |&gt; List.filter (<span style="color:#00f">fun</span> x -&gt; x.Length &gt; 0)
</code></pre></div><p>The counting transformation is modeled in a similar way. The base function
is of type <code>Gift list -&gt; (Gift * int) list</code> and is actually implemented as</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> count xs = List.countBy id xs
</code></pre></div><p>Instead of using a real database, we will just print the counts to console.
So the last optional step for our examples will be to print out the counts
one by one. Here is a helper function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> print (gift, count) = sprintf <span style="color:#a31515">&#34;%i %s&#34;</span> count gift
</code></pre></div><p>Now, we can tokenize and count the gifts in a single list. But how do we
aggregate data over time? Let&rsquo;s leave this to the pipelines.</p>
<h2 id="pipelines">Pipelines</h2>
<p>Let&rsquo;s have a look at a definition of a Storm pipeline (in Java):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java">TopologyBuilder builder = <span style="color:#00f">new</span> TopologyBuilder();
builder.setSpout(<span style="color:#a31515">&#34;line-reader&#34;</span>, <span style="color:#00f">new</span> LineReaderSpout());
builder.setBolt(<span style="color:#a31515">&#34;gifts-spitter&#34;</span>, <span style="color:#00f">new</span> GiftSpitterBolt()).shuffleGrouping(<span style="color:#a31515">&#34;line-reader&#34;</span>);
builder.setBolt(<span style="color:#a31515">&#34;gift-counter&#34;</span>, <span style="color:#00f">new</span> GiftCounterBolt()).shuffleGrouping(<span style="color:#a31515">&#34;gifts-spitter&#34;</span>);
</code></pre></div><p>There is a <code>Builder</code> class, which is capable to add Sources (<code>Spouts</code>) and Transformations
(<code>Bolts</code>) to the pipeline. But again, there&rsquo;s no type story here: the stages are linked
by name, and the types are just implementations of predefined interfaces.</p>
<p>Here is another example of the pipeline, now from <em>Google Dataflow SDK</em> (still Java):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-java" data-lang="java">Pipeline
    .create(options)
    .apply(TextIO.Read.from(<span style="color:#a31515">&#34;...&#34;</span>))
    .apply(ParDo.named(<span style="color:#a31515">&#34;ExtractGifts&#34;</span>).of(<span style="color:#00f">new</span> DoFn&lt;String, String&gt;() {
         <span style="color:#00f">public</span> <span style="color:#2b91af">void</span> processElement(ProcessContext c) { <span style="color:#008000">/* Implements tokenizer */</span> }
    }))
    .apply(Count.&lt;String&gt;perElement())
    .apply(MapElements.via(<span style="color:#00f">new</span> SimpleFunction&lt;KV&lt;String, Long&gt;, String&gt;() {
         <span style="color:#00f">public</span> String apply(KV&lt;String, Long&gt; element) { <span style="color:#008000">/* Formats results */</span> }
    }))
    .apply(TextIO.Write.to(<span style="color:#a31515">&#34;...&#34;</span>));
</code></pre></div><p>I consider this to be more descriptive. There is a clear flow of operations chained
together. The types are visible and checked at compile time.</p>
<p>How would we like to see this pipeline in F#? Our motivation example is going to
be the same processing applied to a normal F# list of wishlists (strings). The following
code snippet counts the gifts in wishlists and prints the result:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">wishlists
|&gt; List.collect tokenize
|&gt; List.countBy id
|&gt; List.map print
|&gt; List.iter (Console.WriteLine)
</code></pre></div><p>My goal for the rest of the article will be to define a <code>Flow</code> module which would
enable me to write a Stream Processing pipeline in the same fashion. Here is the
target code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">sourceOfWishlists
|&gt; Flow.collect tokenize
|&gt; Flow.countBy id
|&gt; Flow.map print
|&gt; Flow.connectTo sinkForCounts
</code></pre></div><p>So, how do we implement something like this? Let&rsquo;s start with clarifying how
Source and Sink can be defined.</p>
<h2 id="source-and-sink">Source and Sink</h2>
<p>We declared the source of a stream to be unbounded, not limited in time
or event count.</p>
<p>However, the modern stream processing systems like <em>Flink</em> and <em>Dataflow</em>
(both with ideas from <em>Apache Beam</em>) are trying to sit on two chairs at the same time by declaring
that bounded data sources are just sub-case of unbounded streams. If your
goal is to process a fixed batch of data, you could represent it as
one-by-one sequence of events.</p>
<p>Big Data world has a well known approach when batch processing and real time
stream processing are done in parallel, with separate tools and separate code base.
The approach is called <em>Lambda Architecture</em>. Beam is declaring this approach outdated,
offering a way to reuse streaming code and capacity also for bounded data workloads.</p>
<p>To follow this modern path, we will declare our <code>Source</code> as following:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">BoundedSource</span>&lt;<span style="color:#00f">&#39;</span>T&gt; = <span style="color:#2b91af">unit</span> -&gt; <span style="color:#00f">&#39;</span>T seq
<span style="color:#00f">type</span> <span style="color:#2b91af">UnboundedSource</span>&lt;<span style="color:#00f">&#39;</span>T&gt; = (<span style="color:#00f">&#39;</span>T -&gt; <span style="color:#2b91af">unit</span>) -&gt; <span style="color:#2b91af">unit</span>

<span style="color:#00f">type</span> <span style="color:#2b91af">Source</span>&lt;<span style="color:#00f">&#39;</span>T&gt; =
  | Bounded <span style="color:#00f">of</span> BoundedSource&lt;<span style="color:#00f">&#39;</span>T&gt;
  | Unbounded <span style="color:#00f">of</span> UnboundedSource&lt;<span style="color:#00f">&#39;</span>T&gt;
</code></pre></div><p><code>Source</code> is a generic discriminated union with two cases. <code>Bounded</code> case represents
a side effect-ful function, which returns a sequence of elements when called.
The argument of <code>unit</code> is there to delay the processing: we need to declare
sources long before they start to emit values.</p>
<p>The <code>Unbounded</code> case is a bit harder to understand. It accepts an action to be
executed as the argument and returns nothing meaningful (<code>unit</code> again). You will
see a usage example later.</p>
<p>The <code>Sink</code> represents an action to happen at the end of pipeline. I&rsquo;ve made it
a discriminated union too, but with just one case:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Sink</span>&lt;<span style="color:#00f">&#39;</span>T&gt; = | Action <span style="color:#00f">of</span> (<span style="color:#00f">&#39;</span>T -&gt; <span style="color:#2b91af">unit</span>)
</code></pre></div><p>Now, we should be able to simulate an empty processing pipeline: directly connect
a source to a sink. Let&rsquo;s start with bounded data:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> copyPipeline source sink =
  <span style="color:#00f">match</span> source, sink <span style="color:#00f">with</span>
  | Bounded b, Action a -&gt; b() |&gt; Seq.iter a
  | _ -&gt; failwith <span style="color:#a31515">&#34;Not supported yet&#34;</span>

<span style="color:#00f">let</span> gifts = seq [<span style="color:#a31515">&#34;Ball&#34;</span>; <span style="color:#a31515">&#34;Train&#34;</span>; <span style="color:#a31515">&#34;Doll&#34;</span>]
<span style="color:#00f">let</span> giftBoundedSource = (<span style="color:#00f">fun</span>() -&gt; gifts) |&gt; Bounded

<span style="color:#00f">let</span> consoleSink = (<span style="color:#00f">fun</span> (s: <span style="color:#2b91af">string</span>) -&gt; Console.WriteLine s) |&gt; Action

copyPipeline giftBoundedSource consoleSink
</code></pre></div><p>This code will print out all the gift names from the sequence. Now, let&rsquo;s extend it to
stream unbounded data. Before we can do that, let&rsquo;s introduce a helper class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Triggered</span>&lt;<span style="color:#00f">&#39;</span>T&gt;() =
  <span style="color:#00f">let</span> subscribers = <span style="color:#00f">new</span> List&lt;<span style="color:#00f">&#39;</span>T -&gt; <span style="color:#2b91af">unit</span>&gt;()
  <span style="color:#00f">member</span> this.DoNext x =
    subscribers.ForEach(<span style="color:#00f">fun</span> s -&gt; s x)
  <span style="color:#00f">member</span> this.Subscribe = subscribers.Add
</code></pre></div><p>An instance of such class keeps a mutable list of subscribers. Subscribers are
added by calling <code>Subscribe</code> method. Someone else can then call <code>DoNext</code> method,
and each subscriber will get an item every time.</p>
<p>Here&rsquo;s how we can use it for unbounded data processing:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> copyPipeline source sink =
  <span style="color:#00f">match</span> source, sink <span style="color:#00f">with</span>
  | Bounded b, Action a -&gt; b() |&gt; Seq.iter a
  | Unbounded ub, Action a -&gt; ub a

<span style="color:#00f">let</span> consoleSource = <span style="color:#00f">new</span> Triggered&lt;<span style="color:#2b91af">string</span>&gt;()
<span style="color:#00f">let</span> unboundedSource = consoleSource.Subscribe |&gt; Unbounded
copyPipeline unboundedSource consoleSink

Seq.initInfinite (<span style="color:#00f">fun</span> _ -&gt; Console.ReadLine())
|&gt; Seq.takeWhile ((&lt;&gt;) <span style="color:#a31515">&#34;q&#34;</span>)
|&gt; Seq.iter consoleSource.DoNext
</code></pre></div><p>This little program will echo whatever you enter into the console until you type
<code>q</code> to quit. That is an example of unbounded data: you can type as long as
you want, there is no hard limit.</p>
<p>Here&rsquo;s how it works:</p>
<ol>
<li><code>Triggered</code> source is created.</li>
<li>Unbounded source is declared by subscribing to the trigger.</li>
<li>Our dummy pipeline links the source to the action of writing to console.</li>
<li>Every time a new line is entered, <code>DoNext</code> method of the trigger is called
and the data flows to the sink.</li>
</ol>
<p>Stop here and make sure you understand the example before going further.</p>
<h2 id="flow">Flow</h2>
<p>Now it&rsquo;s time to implement the contracts for the flow that we defined
in Gift Count example. The contract consists of two parts. The first part
is a generic interface which defines all the operations that we need:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Runnable</span> = <span style="color:#2b91af">unit</span> -&gt; <span style="color:#2b91af">unit</span>

<span style="color:#00f">type</span> <span style="color:#2b91af">IFlow</span>&lt;<span style="color:#00f">&#39;</span>a&gt; =
  <span style="color:#00f">abstract</span> <span style="color:#00f">member</span> Map&lt;<span style="color:#00f">&#39;</span>b&gt; : (<span style="color:#00f">&#39;</span>a -&gt; <span style="color:#00f">&#39;</span>b) -&gt; IFlow&lt;<span style="color:#00f">&#39;</span>b&gt;
  <span style="color:#00f">abstract</span> <span style="color:#00f">member</span> Collect&lt;<span style="color:#00f">&#39;</span>b&gt; : (<span style="color:#00f">&#39;</span>a -&gt; <span style="color:#00f">&#39;</span>b <span style="color:#2b91af">list</span>) -&gt; IFlow&lt;<span style="color:#00f">&#39;</span>b&gt;
  <span style="color:#00f">abstract</span> <span style="color:#00f">member</span> CountBy&lt;<span style="color:#00f">&#39;</span>b <span style="color:#00f">when</span> <span style="color:#00f">&#39;</span>b: equality&gt; : (<span style="color:#00f">&#39;</span>a -&gt; <span style="color:#00f">&#39;</span>b) -&gt; IFlow&lt;<span style="color:#00f">&#39;</span>b * int&gt;
  <span style="color:#00f">abstract</span> <span style="color:#00f">member</span> To: Sink&lt;<span style="color:#00f">&#39;</span>a&gt; -&gt; Runnable
</code></pre></div><p>Then we define a module which is just a wrapper around the interface to
enable F#-style API:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Flow =
  <span style="color:#00f">let</span> map&lt;<span style="color:#00f">&#39;</span>TI, <span style="color:#00f">&#39;</span>TO&gt; (f: <span style="color:#00f">&#39;</span>TI -&gt; <span style="color:#00f">&#39;</span>TO) (flow: IFlow&lt;<span style="color:#00f">&#39;</span>TI&gt;) = flow.Map f
  <span style="color:#00f">let</span> collect&lt;<span style="color:#00f">&#39;</span>TI, <span style="color:#00f">&#39;</span>TO&gt; (f: <span style="color:#00f">&#39;</span>TI -&gt; <span style="color:#00f">&#39;</span>TO <span style="color:#2b91af">list</span>) (flow: IFlow&lt;<span style="color:#00f">&#39;</span>TI&gt;) = flow.Collect f
  <span style="color:#00f">let</span> countBy&lt;<span style="color:#00f">&#39;</span>T, <span style="color:#00f">&#39;</span>TK <span style="color:#00f">when</span> <span style="color:#00f">&#39;</span>TK: equality&gt; (f: <span style="color:#00f">&#39;</span>T -&gt; <span style="color:#00f">&#39;</span>TK) (flow: IFlow&lt;<span style="color:#00f">&#39;</span>T&gt;) = flow.CountBy f
  <span style="color:#00f">let</span> connectTo&lt;<span style="color:#00f">&#39;</span>T&gt; sink (flow: IFlow&lt;<span style="color:#00f">&#39;</span>T&gt;) = flow.To sink
  <span style="color:#00f">let</span> run (r: Runnable) = r()
</code></pre></div><p>Then, we just need an implementation of <code>IFlow</code> and a factory method to create
an initial instance of flow given a data source.</p>
<p>Now I&rsquo;d like to emphasize that there are multiple possible implementations
of <code>IFlow</code> depending on the required properties for the pipeline. They might make
use of different libraries or frameworks, or be a naive simple implementation like
the one below, suitable for modeling and testing.</p>
<p>In fact, one of my implementations doesn&rsquo;t run the pipeline, but instead uses reflection
to build a visual graph of processing stages, to be used for documentation and discussion
purposes.</p>
<p>We will have a look at more advanced implementations in the further articles, but
for now here is a naive version:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">module</span> Runner =
  <span style="color:#00f">let</span> private mapTransform map = <span style="color:#00f">function</span>
    | Bounded bs -&gt; bs &gt;&gt; Seq.map map |&gt; Bounded
    | Unbounded us -&gt;
      <span style="color:#00f">fun</span> subscriber -&gt; map &gt;&gt; subscriber |&gt; us
      |&gt; Unbounded

  <span style="color:#00f">let</span> private collectTransform mapToMany = <span style="color:#00f">function</span>
    | Bounded bs -&gt; bs &gt;&gt; Seq.map mapToMany &gt;&gt; Seq.concat |&gt; Bounded
    | Unbounded us -&gt;
      <span style="color:#00f">fun</span> subscriber -&gt; mapToMany &gt;&gt; Seq.iter subscriber |&gt; us
      |&gt; Unbounded

  <span style="color:#00f">let</span> private countByTransform&lt;<span style="color:#00f">&#39;</span>a, <span style="color:#00f">&#39;</span>b <span style="color:#00f">when</span> <span style="color:#00f">&#39;</span>b: equality&gt; (getKey: <span style="color:#00f">&#39;</span>a -&gt; <span style="color:#00f">&#39;</span>b) source =
    <span style="color:#00f">let</span> state = <span style="color:#00f">new</span> Dictionary&lt;<span style="color:#00f">&#39;</span>b, int&gt;()
    <span style="color:#00f">let</span> addItem i =
      <span style="color:#00f">let</span> key = getKey i
      <span style="color:#00f">let</span> value = <span style="color:#00f">if</span> state.ContainsKey key <span style="color:#00f">then</span> state.[key] <span style="color:#00f">else</span> 0
      <span style="color:#00f">let</span> newValue = value + 1
      state.[key] &lt;- newValue
      (key, newValue)

    <span style="color:#00f">match</span> source <span style="color:#00f">with</span>
    | Bounded bs -&gt; bs &gt;&gt; Seq.countBy getKey |&gt; Bounded
    | Unbounded us -&gt; (<span style="color:#00f">fun</span> s -&gt; addItem &gt;&gt; s) &gt;&gt; us |&gt; Unbounded

  <span style="color:#00f">let</span> private stage source transform sink () =
    <span style="color:#00f">match</span> transform source, sink <span style="color:#00f">with</span>
    | Bounded bs, Action a -&gt; bs() |&gt; Seq.iter a
    | Unbounded us, Action a -&gt; us a

  <span style="color:#00f">type</span> <span style="color:#2b91af">private</span> Flow&lt;<span style="color:#00f">&#39;</span>a&gt;(source: Source&lt;<span style="color:#00f">&#39;</span>a&gt;, connect: Sink&lt;<span style="color:#00f">&#39;</span>a&gt; -&gt; Runnable) =
    <span style="color:#00f">member</span> this.Apply&lt;<span style="color:#00f">&#39;</span>b&gt; t = <span style="color:#00f">new</span> Flow&lt;<span style="color:#00f">&#39;</span>b&gt;(t source, stage source t) :&gt; IFlow&lt;<span style="color:#00f">&#39;</span>b&gt;

    <span style="color:#00f">interface</span> IFlow&lt;<span style="color:#00f">&#39;</span>a&gt; <span style="color:#00f">with</span>
      <span style="color:#00f">member</span> this.Map&lt;<span style="color:#00f">&#39;</span>b&gt; map = this.Apply&lt;<span style="color:#00f">&#39;</span>b&gt; (mapTransform map)

      <span style="color:#00f">member</span> this.Collect&lt;<span style="color:#00f">&#39;</span>b&gt; map = this.Apply&lt;<span style="color:#00f">&#39;</span>b&gt; (collectTransform map)

      <span style="color:#00f">member</span> this.CountBy&lt;<span style="color:#00f">&#39;</span>b <span style="color:#00f">when</span> <span style="color:#00f">&#39;</span>b: equality&gt;(getKey) =
        this.Apply&lt;<span style="color:#00f">&#39;</span>b * int&gt; (countByTransform&lt;<span style="color:#00f">&#39;</span>a, <span style="color:#00f">&#39;</span>b&gt; getKey)

      <span style="color:#00f">member</span> this.To(sink) = connect(sink)

  <span style="color:#00f">let</span> from&lt;<span style="color:#00f">&#39;</span>a&gt; source =
    <span style="color:#00f">new</span> Flow&lt;<span style="color:#00f">&#39;</span>a&gt;(source, stage source (mapTransform id)) :&gt; IFlow&lt;<span style="color:#00f">&#39;</span>a&gt;
</code></pre></div><p>The implementation details are not that important at the moment (even though it&rsquo;s
just 37 lines of code), so I&rsquo;ll just proceed to the pipeline definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">unboundedSource                <span style="color:#008000">// e.g. same console source as before
</span><span style="color:#008000"></span>|&gt; Runner.from                 <span style="color:#008000">// create Runner implementation of IFlow
</span><span style="color:#008000"></span>|&gt; Flow.collect tokenize
|&gt; Flow.countBy id
|&gt; Flow.map print
|&gt; Flow.connectTo consoleSink  <span style="color:#008000">// connect to the Sink
</span><span style="color:#008000"></span>|&gt; Flow.run                    <span style="color:#008000">// start listening for events
</span></code></pre></div><p>Here you can find
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/streamprocessing/GiftCount.fs">the full code of the Gift Count example</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, we started reasoning about low-latency processing
pipelines from the domain logic point of view. We
tried to reuse well known F# idioms like ADTs and HOFs
to show how stream processing is not much different from other types
of applications.</p>
<p>Although this post is quite long by now, we just scratched the surface
of the stream processing. Here are some focus areas for the
follow-ups:</p>
<ul>
<li>More complex pipeline topologies</li>
<li>State management</li>
<li>Concepts of time, windowing and out-of-order events</li>
<li>Reliability, retries and guarantees</li>
<li>Scaling out</li>
<li>Using 3rd parties for all of that</li>
<li>Documentation and formal analysis</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/stream-processing" term="stream-processing" label="Stream Processing" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/advent-calendar" term="advent-calendar" label="Advent Calendar" />
                             
                                <category scheme="https://mikhail.io/tags/data-processing" term="data-processing" label="Data Processing" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[CQRS]]></title>
            <link href="https://mikhail.io/tags/cqrs/"/>
            <id>https://mikhail.io/tags/cqrs/</id>
            
            <published>2016-11-18T00:00:00+00:00</published>
            <updated>2016-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Event Sourcing and IO Complexity]]></title>
            <link href="https://mikhail.io/2016/11/event-sourcing-and-io-complexity/"/>
            <id>https://mikhail.io/2016/11/event-sourcing-and-io-complexity/</id>
            
            <published>2016-11-18T00:00:00+00:00</published>
            <updated>2016-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><strong>Event Sourcing</strong> is an approach, when an append-only store is used to record the full series of events that
describe actions taken on a particular domain entity. This event store becomes the main source of truth
to reconstruct the current state of the entity and its complete history.</p>
<p>In essence, that means that we store the log of all business events that occurred in the system, and then
we use them to make new decisions and produce new events.</p>
<h2 id="how-event-souring-works">How Event Souring Works</h2>
<p>Event Sourcing is usually used in combination with <strong>Command-Query Responsibility Segregation</strong>, when all writes
to the event store are initiated by commands.</p>
<p>The following picture illustrates the storage and command handling:</p>
<p><img src="event-store.png" alt="Event Store Command Handler"></p>
<p>Every time a new command comes in (1), the command handler understands
which entity is affected and retrieves all the previous events from the store (2).</p>
<p>The handler aggregates the events and derives the current state of the entity (3). If command is valid given that
state, the command handler produces a new event or several events (4), and writes them back to the event store (5).</p>
<h2 id="disk-space-usage">Disk Space Usage</h2>
<p>It&rsquo;s quite obvious that Event Sourcing requires more storage space than traditional approach of only storing
the current state. The storage size is proportional to the total amount of events in the system,
i.e. it&rsquo;s <code>O(n)</code> or <code>O(e * l)</code> where <code>e</code> is the count of entities in the system and <code>l</code> is the average
amount of events per entity.</p>
<p>Here is the chart of disk space usage in a simplified situation of events of equal size:</p>
<p><img src="disk-space.png" alt="Disk space simulation"></p>
<p>We saved 1000 events and consumed 1000 storage units. The disk space is cheap, so we are willing to take
the trade-off of extra storage for the benefits that Event Sourcing provides.</p>
<h2 id="disk-io-usage">Disk IO Usage</h2>
<p>Let&rsquo;s look at how much IO operations we are going to perform over time. Let&rsquo;s say that reading or writing of
one event consumes one unit of IO capacity.</p>
<p>Every time a new event is received, we consume one write operation: it&rsquo;s still linear. The storage is append-only,
so it makes sense that disk space and writes are essentially the same thing.</p>
<p>Reads are a different beast. Every time we receive a command, we need to perform <code>i</code> reads, where <code>i</code>
is the amount of events so far for the entity. Let&rsquo;s have a look at several examples, each one is a
simulation of saving a thousand of new events.</p>
<p>In the <strong>first scenario</strong> we have a steady flow of incoming events. Events belong to different entities (aggregates)
with <strong>10</strong> events per entity on average:</p>
<p><img src="reads-low.png" alt="Reads for low amount of events per entity"></p>
<p>We can see that we do 5x more reads than writes. That is because for each event written we have to read
all the previous events for the same entity, and on average there are 5 of them.</p>
<p>In the <strong>second scenario</strong> we receive the same amount of events in total. While most entities still have
10 events on average, there is just one outlier entity which received <strong>100</strong> events, all in this time period.</p>
<p><img src="reads-outlier.png" alt="Reads with an outlier entity"></p>
<p>Hey, the amount of reads almost doubled! The line also doesn&rsquo;t look linear anymore&hellip;</p>
<p>Let&rsquo;s look at the <strong>third extreme scenario</strong> when all 1000 events were generated by the <strong>same entity</strong>:</p>
<p><img src="reads-single.png" alt="Reads from single entity"></p>
<p>The amount of reads skyrockets to <strong>100 times more</strong> compared to the first scenario. It&rsquo;s clearly quadratic!
The amount of reads for a single entity is <code>O(l)</code> where <code>l</code> is the event count for that entity.</p>
<h2 id="real-life-scenario">Real-Life Scenario</h2>
<p>In many use cases it&rsquo;s unlikely that you get outlier entities which have orders of magnitude difference in amount
of events per entity. E.g. if your entity is an order in a webshop, there&rsquo;s just a few events that humans can
generate about it.</p>
<p>However, if the events are generated from telemetry data or IoT devices, or if the entities tend to live for very
long time (like bank accounts), that&rsquo;s a good sign you should not ignore the potential problem. A handful of anomaly
devices can bring the whole storage to its knees, if protection is not carefully designed.</p>
<p>If your domain has a chance to belong to the second group, you better get prepared.</p>
<h2 id="capacity-planning-and-monitoring">Capacity Planning and Monitoring</h2>
<p>It&rsquo;s not enough to know just the total number of events in your store, nor is the incoming rate of new events
descriptive enough.</p>
<p>Start with modeling your Event Store against real data. Put some monitoring in place to see the distribution of
event density per entity. Average number is not descriptive enough, so you need to build percentiles and
know the maximum too.</p>
<p>Monitor the amount of reads on your data store. Set the baseline based on the real data
pattern, not imaginary numbers.</p>
<h2 id="throttling--sampling">Throttling / Sampling</h2>
<p>In IoT scenarios the easiest way out could be to discard events if they arrive too frequently from the same device,
or use some sampling/aggregation at the ingress point. Only your business domain can define what kind of
data loss is acceptable, if any.</p>
<h2 id="snapshots">Snapshots</h2>
<p>Event Sourcing concept provides the solution for the reads problem in form of Snapshots. Once in every <code>x</code> events,
you should produce a snapshot of the entity state. The next time an event comes in, you just read
the snapshot and the events which happened after the latest snapshot time (amount is less than <code>x</code>).</p>
<p>It might be tricky to come up with a good snapshot strategy in some cases, especially when the business domain
requires multiple projections to be built.</p>
<p>The snapshot size might also grow over time, if entity keeps some internal event-based lists. But snapshots
seem to be the only real solution when the amount of events gets out of control. Choose your Event Store
technology with this consideration in mind.</p>
<p>Happy Event Sourcing!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/event-sourcing" term="event-sourcing" label="Event Sourcing" />
                             
                                <category scheme="https://mikhail.io/tags/cqrs" term="cqrs" label="CQRS" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Leaflet]]></title>
            <link href="https://mikhail.io/tags/leaflet/"/>
            <id>https://mikhail.io/tags/leaflet/</id>
            
            <published>2016-10-17T00:00:00+00:00</published>
            <updated>2016-10-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Leaflet plugin to render geographic corridors]]></title>
            <link href="https://mikhail.io/2016/10/leaflet-plugin-to-render-geographic-corridors/"/>
            <id>https://mikhail.io/2016/10/leaflet-plugin-to-render-geographic-corridors/</id>
            
            <published>2016-10-17T00:00:00+00:00</published>
            <updated>2016-10-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Yesterday I&rsquo;ve published a simple <a href="http://leafletjs.com/">Leaflet</a> plugin called
<a href="https://github.com/mikhailshilkov/leaflet-corridor">leaflet-corridor</a>.
The plugin defines a new Leaflet primitive <code>L.Corridor</code>.</p>
<p>When initialized with an array of geo points and width, it renders a polyline
with width fixed in meters, not in pixels. That means that line width changes whenever
zoom level changes.</p>
<p><img src="leaflet-corridor.gif" alt="Leaflet-corridor animation"></p>
<p>The plugin is handy to denote geographic corridors: ranges of specified width around
a polyline. In our project we used it to show a predefined vehicle route from Origin to
Destination, with only limited allowed violation from this predefined route. Whenever
vehicle&rsquo;s position falls out of this corridor, the event of Out-of-corridor violation
is recorded and shown on the map.</p>
<p>Here are all the links for the corridor plugin:</p>
<ul>
<li><a href="https://github.com/mikhailshilkov/leaflet-corridor">Github repository</a> with source code, documentation and usage example</li>
<li><a href="https://mikhail.io/demos/leaflet-corridor/">Demo page</a> to try it out</li>
<li><a href="http://stackoverflow.com/questions/26206636/is-there-any-method-to-draw-path-polyline-on-leaflet-with-constant-width-strok/40064379">Stackoverflow question</a> which inspired me to open-source the implementation</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/leaflet" term="leaflet" label="Leaflet" />
                             
                                <category scheme="https://mikhail.io/tags/maps" term="maps" label="Maps" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Azure SQL Databases: Backups, Disaster Recovery, Import and Export]]></title>
            <link href="https://mikhail.io/2016/10/azure-sql-databases-backups-disaster-recovery-import-export/"/>
            <id>https://mikhail.io/2016/10/azure-sql-databases-backups-disaster-recovery-import-export/</id>
            
            <published>2016-10-11T00:00:00+00:00</published>
            <updated>2016-10-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Azure SQL Database is a managed cloud database-as-a-service. It provides
application developers with SQL Server databases which are hosted in the
cloud and fully managed by Microsoft.</p>
<p>The service is very easy to start with. Several clicks in the portal and
you have a database running. Now you can copy the connection string to
your application config file, and boom - you have all up and running.
No installation, no license to buy - just pay the hourly fee.</p>
<p>Any production database is a very important asset, so we are used to
give it a good care in self-hosted scenario. A number of questions appear
when you try to apply those practices to the cloud offering:</p>
<ul>
<li>How do I make a backup of my database? Where should I store it?</li>
<li>How do I move my database including schema and data from on-premise
to the cloud?</li>
<li>How do I move it from the cloud to my local server?</li>
<li>What is a point-in-time restore offered by Azure?</li>
<li>Should I use geo-replication? What is geo-restore?</li>
</ul>
<p>In this post I&rsquo;ll give the short answers to these questions and the links
for further reading.</p>
<h2 id="what-is-point-in-time-restore">What is Point-in-time Restore?</h2>
<p>All your databases are always automatically backed-up by Azure. They take
full, differential and log backups in the background to guarantee you always
have your data safe.</p>
<p>These backups are retained for 7 days for Basic, 14 days for Standard and
35 days for Premium tier.</p>
<p>Within this period, you can choose any <em>minute</em> and restore your database
to that point in time. The restore always happens to a <strong>new</strong> database,
it does not overwrite your current database.</p>
<p><img src="PointInTimeRestore.png" alt="Point-in-time Restore"></p>
<p>That&rsquo;s very handy to recover from &ldquo;oops&rdquo; operations when data was deleted
from one or more tables by a human or code error. In this case, you restore
a copy of the database, and then move the data missing without stopping
the original database.</p>
<p>If the restored database must replace the current one, be prepared to change
connection strings once the restore operation is done. Alternatively, you
can rename both databases to point applications to the new database without
any other configuration changes.</p>
<p>Depending on the database size, the restore may take long time, up to several
hours, 12 hours max guaranteed. So, point-in-time restore is very flexible
but not instant.</p>
<p>Further reading:
<a href="https://azure.microsoft.com/en-us/blog/azure-sql-database-point-in-time-restore/">Azure SQL Database Point in Time Restore</a></p>
<h2 id="what-about-disaster-recovery">What about disaster recovery?</h2>
<p>The same Point-in-time Restore can be used for disaster recovery. The backups
are automatically replicated to other Azure regions, and can be restored
in <em>any</em> Azure region. This is called <strong>Geo Restore</strong>.</p>
<p>In case of failure of the primary region, you can immediately start restoring
the database in another region. Remember that the restore might still take
up to several hours depending on the database size.</p>
<p>Also, because the replication is done asynchronously, the geo-restore will
probably lead to some data loss. Usually it will be under 5 minutes of data,
but guarantee is 1 hour at max.</p>
<p>Further reading:
<a href="https://azure.microsoft.com/en-us/blog/azure-sql-database-geo-restore/">Azure SQL Database Geo-Restore</a></p>
<h2 id="can-i-reduce-the-downtime-and-data-loss">Can I reduce the downtime and data loss?</h2>
<p>If you want to be prepared to the failure of the database&rsquo;s Azure region
and be able to fail over much faster, you can use <strong>Active Geo Replication</strong>.
Effectively, you are creating other (up to 5 in total) database(s) which
would be replicated from the primary database.</p>
<p><img src="GeoReplication.png" alt="Geo Replication"></p>
<p>The replication happens asynchronously, which means that the latency
of the primary database does not increase. That also means that some data
may be lost when replica database is promoted to be the new primary.
Microsoft guarantees that the loss will be limited to 5 seconds worth of data.</p>
<p>The failover can be done any time, manually or by your script.</p>
<p>Having replica databases means that you pay for them too. The performance
level (and the fee) is configurable per database.</p>
<p>As a bonus, you can use secondary databases as read-only replicas. Just
remember that the data might be slightly stale.</p>
<p>Geo Replication is only available for Standard and Premium pricing tiers.</p>
<p>Further reading:
<a href="https://azure.microsoft.com/ru-ru/blog/spotlight-on-sql-database-active-geo-replication/">Spotlight on SQL Database Active Geo-Replication</a>,
<a href="https://azure.microsoft.com/en-us/documentation/articles/sql-database-geo-replication-overview/">Overview: SQL Database Active Geo-Replication</a></p>
<h2 id="do-i-still-need-to-make-manual-backups">Do I still need to make manual backups?</h2>
<p>Well, it&rsquo;s possible that you don&rsquo;t have to.</p>
<p>But there are at least two scenarios when you might still need to make
manual backups:</p>
<ol>
<li>
<p>You need to keep a copy of your database for longer period than
Point-in-time restore allows (7 to 35 days depending on the service tier).</p>
</li>
<li>
<p>You need a copy of your Azure database to be restored on premise.</p>
</li>
</ol>
<p>Let&rsquo;s look at manual backups.</p>
<h2 id="how-do-i-make-a-bak-file-from-my-azure-database">How do I make a BAK file from my Azure Database?</h2>
<p>The <code>BAK</code> backup files are not directly supported by Azure SQL Databases.
Instead, there is a feature called <code>Export Data tier application</code>, which
creates a <code>BACPAC</code> file in Azure Storage account.</p>
<p>The easiest way to do that is to use SQL Server Management Studio, connect to
Azure SQL Database, then right-click and select <code>Tasks -&gt; Export Data tier application</code>
in the menu.</p>
<p><img src="ExportDataTier.png" alt="Export Data Tier Application"></p>
<p>You can export the file to the local storage name or Azure Storage account.</p>
<p><img src="ExportSettings.png" alt="Export Settings"></p>
<p>Export will take some time and will consume your database DTUs, so you shouldn&rsquo;t
do it too often.</p>
<p>Export can also be triggered from Azure Portal and PowerShell scripts.</p>
<h2 id="how-do-i-restore-a-copy-of-my-cloud-database-to-a-local-server">How do I restore a copy of my cloud database to a local server?</h2>
<p>Now, when you have a <code>BACPAC</code> file, it&rsquo;s really easy to restore it to any
SQL server instance. Right-click <code>Databases</code> node in SQL Server Management
Studio and select <code>Import Data-tier Application...</code>.</p>
<p><img src="ImportDataTier.png" alt="Import Data Tier Application"></p>
<p>Then pick the location of the saved file.</p>
<h2 id="how-do-i-move-my-existing-database-to-azure-sql-database">How do I move my existing database to Azure SQL Database?</h2>
<p>The process is exactly the same as described above, just the other direction:</p>
<ul>
<li>Export Data-tier Application from your local SQL Server to Azure Storage</li>
<li>Import Data-tier Application to a new Azure SQL Database</li>
</ul>
<h2 id="summary">Summary</h2>
<p>Azure SQL Database is a production-ready fully managed service, which can
dramatically reduce the amount of manual administration compared to on-premise
setup. You can choose between several disaster recovery scenarios based on
your objectives and budget. Import and export of databases are available,
allowing operators to move databases between the cloud and self-hosted servers.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/azure-sql-database" term="azure-sql-database" label="Azure SQL Database" />
                             
                                <category scheme="https://mikhail.io/tags/disaster-recovery" term="disaster-recovery" label="Disaster Recovery" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Disaster Recovery]]></title>
            <link href="https://mikhail.io/tags/disaster-recovery/"/>
            <id>https://mikhail.io/tags/disaster-recovery/</id>
            
            <published>2016-10-11T00:00:00+00:00</published>
            <updated>2016-10-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Aurelia]]></title>
            <link href="https://mikhail.io/tags/aurelia/"/>
            <id>https://mikhail.io/tags/aurelia/</id>
            
            <published>2016-08-21T00:00:00+00:00</published>
            <updated>2016-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Getting started with Azure Application Insights in Aurelia]]></title>
            <link href="https://mikhail.io/2016/08/getting-started-with-azure-application-insights-in-aurelia/"/>
            <id>https://mikhail.io/2016/08/getting-started-with-azure-application-insights-in-aurelia/</id>
            
            <published>2016-08-21T00:00:00+00:00</published>
            <updated>2016-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><a href="https://azure.microsoft.com/en-us/documentation/articles/app-insights-overview/">Azure Application Insights</a>
is an analytics service to monitor live web applications,
diagnose performance issues, and understand what users actually do with the app.
<a href="http://aurelia.io">Aurelia</a> is a modern and slick single-page application framework.
Unfortunately, there&rsquo;s not much guidance on the web about how to use AppInsights and
Aurelia together in a proper manner. The task gets even more challenging in case you are
using TypeScript and want to stay in type-safe land. This post will set you up and
running in no time.</p>
<h2 id="get-your-appinsights-instrumentation-key">Get Your AppInsights Instrumentation Key</h2>
<p>If not done yet, go register in Azure Application Insights portal. To start sending
telemetry data from your application you would need a unique identifier of
your web application, which is called an Instrumentation Key (it&rsquo;s just a guid).
See <a href="https://azure.microsoft.com/en-us/documentation/articles/app-insights-javascript/">Application Insights for web pages</a>
walk-through.</p>
<h2 id="install-a-jspm-package">Install a JSPM Package</h2>
<p>I&rsquo;m using JSPM as a front-end package manager for Aurelia applications. If you use it
as well, run the following command to install AppInsights package:</p>
<pre><code>jspm install github:Microsoft/ApplicationInsights-js
</code></pre><p>it will add a line to <code>config.js</code> file:</p>
<pre><code>map: {
  &quot;Microsoft/ApplicationInsights-js&quot;: &quot;github:Microsoft/ApplicationInsights-js@1.0.0&quot;,
...
</code></pre><p>To keep the names simple, change the line to</p>
<pre><code>  &quot;ApplicationInsights&quot;: &quot;github:Microsoft/ApplicationInsights-js@1.0.0&quot;,
</code></pre><p>Do exactly the same change in <code>project.json</code> file, <code>jspm</code> -&gt; <code>dependencies</code> section.</p>
<h2 id="create-an-aurelia-plugin">Create an Aurelia Plugin</h2>
<p>In order to track Aurelia page views, we are going to plug into the routing pipeline
with a custom plugin. Here is how my plugin looks like in JavaScript (see TypeScript
version below):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#008000">// app-insights.js
</span><span style="color:#008000"></span><span style="color:#00f">export</span> <span style="color:#00f">class</span> AppInsights {
  client;

  constructor() {
    <span style="color:#00f">let</span> snippet = {
      config: {
        instrumentationKey: <span style="color:#a31515">&#39;YOUR INSTRUMENTATION KEY GUID&#39;</span>
      }
    };
    <span style="color:#00f">let</span> init = <span style="color:#00f">new</span> Microsoft.ApplicationInsights.Initialization(snippet);
    <span style="color:#00f">this</span>.client = init.loadAppInsights();
  }

  run(routingContext, next) {
    <span style="color:#00f">this</span>.client.trackPageView(routingContext.fragment, window.location.href);
    <span style="color:#00f">return</span> next();
  }
}
</code></pre></div><p>The constructor instantiates an AppInsights client. It is used inside a <code>run</code> method,
which would be called by Aurelia pipeline during page navigation.</p>
<h2 id="add-the-plugin-to-aurelia-pipeline">Add the Plugin to Aurelia Pipeline</h2>
<p>Go the the <code>App</code> class of your Aurelia application. Import the new plugin</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#008000">// app.js
</span><span style="color:#008000"></span><span style="color:#00f">import</span> {AppInsights} from <span style="color:#a31515">&#39;./app-insights&#39;</span>;
</code></pre></div><p>and change the <code>configureRouter</code> method to register a new pipeline step:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js">configureRouter(config, router): <span style="color:#00f">void</span> {
  config.addPipelineStep(<span style="color:#a31515">&#39;modelbind&#39;</span>, AppInsights);
  config.map(<span style="color:#008000">/*routes are initialized here*/</span>);
}
</code></pre></div><p>After re-building the application, you should be all set to go. Navigate several pages
and wait for events to appear in Application Insights portal.</p>
<h2 id="typescript-obtain-the-definition-file">TypeScript: Obtain the Definition File</h2>
<p>If you are using TypeScript, you are not done yet. In order to compile the <code>AppInsights</code>
plugin you need the type definitions for <code>ApplicationInsights</code> package. Unfortunately,
at the time of writing there is no canonical definition in <code>typings</code> registry, so
you will have to provide a custom <code>.d.ts</code> file. You can download mine from
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/aurelia-app-insights/applicationinsights.d.ts">my github</a>.
I created it based on a file from
<a href="https://www.nuget.org/packages/Microsoft.ApplicationInsights.TypeScript">this NuGet repository</a>.</p>
<p>I&rsquo;ve put it into the <code>custom_typings</code> folder and then made the following adjustment
to <code>build/paths.js</code> file of Aurelia setup:</p>
<pre><code>  dtsSrc: [
    'typings/**/*.d.ts',
    'custom_typings/**/*.d.ts'
  ],
</code></pre><p>For the reference, here is my TypeScript version of the <code>AppInsights</code> plugin:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ts" data-lang="ts"><span style="color:#00f">import</span> {NavigationInstruction, Next} <span style="color:#00f">from</span> <span style="color:#a31515">&#39;aurelia-router&#39;</span>;
<span style="color:#00f">import</span> {Microsoft} <span style="color:#00f">from</span> <span style="color:#a31515">&#39;ApplicationInsights&#39;</span>;

<span style="color:#00f">export</span> <span style="color:#00f">class</span> AppInsights {
  <span style="color:#00f">private</span> client: <span style="color:#2b91af">Microsoft.ApplicationInsights.AppInsights</span>;

  <span style="color:#00f">constructor</span>() {
    <span style="color:#00f">let</span> snippet = {
      config: {
        instrumentationKey: <span style="color:#a31515">&#39;YOUR INSTRUMENTATION KEY GUID&#39;</span>
      },
      queue: []
    };
    <span style="color:#00f">let</span> init = <span style="color:#00f">new</span> Microsoft.ApplicationInsights.Initialization(snippet);
    <span style="color:#00f">this</span>.client = init.loadAppInsights();
  }

  run(routingContext: <span style="color:#2b91af">NavigationInstruction</span>, next: <span style="color:#2b91af">Next</span>): Promise&lt;any&gt; {
    <span style="color:#00f">this</span>.client.trackPageView(routingContext.fragment, window.location.href);
    <span style="color:#00f">return</span> next();
  }
}
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>This walk-through should get you started with Azure Application Insights in your
Aurelia application. Once you have page view metrics coming into the dashboard,
spend more time to discover all the exciting ways to improve your application
with Application Insights.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/application-insights" term="application-insights" label="Application Insights" />
                             
                                <category scheme="https://mikhail.io/tags/aurelia" term="aurelia" label="Aurelia" />
                             
                                <category scheme="https://mikhail.io/tags/javascript" term="javascript" label="Javascript" />
                             
                                <category scheme="https://mikhail.io/tags/typescript" term="typescript" label="Typescript" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Typescript]]></title>
            <link href="https://mikhail.io/tags/typescript/"/>
            <id>https://mikhail.io/tags/typescript/</id>
            
            <published>2016-08-21T00:00:00+00:00</published>
            <updated>2016-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Comparing Scala to F#]]></title>
            <link href="https://mikhail.io/2016/08/comparing-scala-to-fsharp/"/>
            <id>https://mikhail.io/2016/08/comparing-scala-to-fsharp/</id>
            
            <published>2016-08-05T00:00:00+00:00</published>
            <updated>2016-08-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>F# and Scala are quite similar languages from 10.000 feet view. Both are
functional-first languages developed for the virtual machines where imperative
languages dominate. C# for .NET and Java for JVM are still <em>lingua franca</em>,
but alternatives are getting stronger.</p>
<p>My background is in .NET ecosystem, so F# was the first of the two that I started
learning. At the same time, Scala seems to have more traction, largely due to
successful products and frameworks like Spark, Akka and Play. That&rsquo;s why I decided
to broaden my skill set and pick up some Scala knowledge. I&rsquo;ve started with
<a href="https://www.coursera.org/specializations/scala">Functional Programming in Scala Specialization</a> at Coursera.
While following the coursera, I&rsquo;m doing some notes about which language features
in Scala I find interesting, or vice versa - missing compared to F#.</p>
<p>In no particular order, I want to share my notes of Scala vs F# in this blog post.</p>
<p><em>Post updated based on comments by Mark Lewis and Giacomo Citi.</em></p>
<h2 id="implicit-parameters">Implicit Parameters</h2>
<p>A parameter of a function can be marked as implicit</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">def</span> work(<span style="color:#00f">implicit</span> i<span style="color:#00f">:</span><span style="color:#2b91af">Int</span>) <span style="color:#00f">=</span> print(i)
</code></pre></div><p>and that means you can call the function without specifying the value for this parameter
and the compiler will try to figure out that value you (according to
the extensive set of rules), e.g.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">implicit</span> <span style="color:#00f">val</span> v <span style="color:#00f">=</span> 2;
<span style="color:#008000">// ... somewhere below
</span><span style="color:#008000"></span>work <span style="color:#008000">// prints &#39;2&#39;
</span></code></pre></div><p>I am not aware of any similar features in other language that I know, so I&rsquo;m pretty sure
I don&rsquo;t understand it well enough yet :) At the same time, I think implicits are
very characteristic for Scala: they are a powerful tool, which can be used in many
valid scenarios, or can be abused to shoot in one&rsquo;s feet.</p>
<h2 id="underscore-in-lambdas">Underscore In Lambdas</h2>
<p>Underscores <code>_</code> can be used to represent parameters in lambda expressions
without explicitly naming them:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">employees.sortBy(<span style="color:#00f">_</span>.dateOfBirth)
</code></pre></div><p>I think that&rsquo;s brilliant - very short and readable. Tuple values are represented
by <code>_1</code> and <code>_2</code>, so we can sort an array of tuples like</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">profitByYear.sortBy(<span style="color:#00f">_</span>._1)
</code></pre></div><p>This looks a bit hairy and should probably be used only when the meaning is obvious.
(In the example above I&rsquo;m not sure if we sort by year or by profit&hellip;)</p>
<p>In F# underscore is used in a different sense - as &ldquo;something to ignore&rdquo;. That makes
sense, but I would love to have a shorter way of writing lambda in</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">empoyees |&gt; List.sort (<span style="color:#00f">fun</span> e -&gt; e.dateOfBirth)
</code></pre></div><p>Any hint how?</p>
<h2 id="tail-recursion-mark">Tail-Recursion Mark</h2>
<p>Any recursive function in Scala can be marked with <code>@tailrec</code> annotation,
which would result in compilation error if the function is not tail-recursive.
This guarantees that you won&rsquo;t get a nasty stack overflow exception.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">@tailrec
<span style="color:#00f">def</span> boom(x<span style="color:#00f">:</span> <span style="color:#2b91af">Int</span>)<span style="color:#00f">:</span> <span style="color:#2b91af">Int</span> = {
  <span style="color:#00f">if</span> (x == 0) 0
  <span style="color:#00f">else</span> boom(x-1) + 1
}
</code></pre></div><p>The code above won&rsquo;t compile, as the recursion can&rsquo;t be optimized by the
compiler.</p>
<p>The feature sounds very reasonable, although I must admit that I have
never needed it in <em>my</em> F# code yet.</p>
<h2 id="call-by-name">Call By Name</h2>
<p>When you call a function in F#, the parameter values are evaluated before
the function body. This style of function substitution model is known as
Call by Value.</p>
<p>Same is the default in Scala. But there is an alternative: you can defer the
evaluation of parameters by marking them with an <code>=&gt;</code> symbol:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">def</span> callByName(x<span style="color:#00f">:</span> =&gt; <span style="color:#2b91af">Int</span>) <span style="color:#00f">=</span> {
  println(<span style="color:#a31515">&#34;x is &#34;</span> + x)
}
</code></pre></div><p>This style is known as Call by Name, and the evaluation is defered until the
parameter is actually used. So, if parameter is never used, its value
will never be evaluated. This code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">val</span> a<span style="color:#00f">:</span><span style="color:#2b91af">Option</span>[<span style="color:#2b91af">Int</span>] <span style="color:#00f">=</span> <span style="color:#2b91af">Some</span>(1)
<span style="color:#00f">val</span> b <span style="color:#00f">=</span> a getOrElse (2/0)
</code></pre></div><p>will set <code>b</code> to <code>1</code>, and no error will be thrown, even though we are dividing by zero
in function parameter. This is because the parameter of <code>getOrElse</code> is passed
by name.</p>
<p>The F# alternative <code>defaultArg</code> doesn&rsquo;t work this way, so the following code
will blow up:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> a = Some(1)
<span style="color:#00f">let</span> b = defaultArg b (2/0) <span style="color:#008000">// boom
</span></code></pre></div><p>You can get deferred evaluation by passing a function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> defaultArgFunc o (f: <span style="color:#2b91af">unit</span> -&gt; <span style="color:#00f">&#39;</span>a) =
  <span style="color:#00f">match</span> o <span style="color:#00f">with</span> | Some v -&gt; v | None -&gt; f()

<span style="color:#00f">let</span> b2 = defaultArgFunc a (<span style="color:#00f">fun</span> () -&gt; 2 / 0)
</code></pre></div><p>That&rsquo;s essentially what happens in Scala too, but the Scala syntax is
arguably cleaner.</p>
<h2 id="lack-of-type-inference">Lack of Type Inference</h2>
<p>Slowly moving towards language design flavours, I&rsquo;ll start with Type Inference.
In Scala, type inference seems to be quite limited. Yes, you don&rsquo;t have to
explicitly define the types of local values or (most of the time) function return
types, but that&rsquo;s about it.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">def</span> max (a<span style="color:#00f">:</span> <span style="color:#2b91af">Int</span>, b<span style="color:#00f">:</span><span style="color:#2b91af">Int</span>) <span style="color:#00f">=</span> <span style="color:#00f">if</span> (a &gt; b) a <span style="color:#00f">else</span> b
</code></pre></div><p>You have to specify the types of all input parameters, and that&rsquo;s quite a bummer
for people who are used to short type-less code of F# (or Haskell, OCaml and others,
for that matter).</p>
<p>Type inference in F# plays another significant role: automatic type generalization.
F# compiler would make types as generic as possible, based on implementation.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> max a b = <span style="color:#00f">if</span> a &gt; b <span style="color:#00f">then</span> a <span style="color:#00f">else</span> b
</code></pre></div><p>The type of the function above is <code>'a -&gt; 'a -&gt; 'a</code>. Most people wouldn&rsquo;t make
it generic from get-go, but compiler helps in this case.</p>
<h2 id="functional-vs-object-oriented-style">Functional vs Object-Oriented Style</h2>
<p>Both F# and Scala are running on top of managed object-oriented virtual machines,
and at the same time both languages enable developers to write functional code.
Functional programming means operating immutable data structures in pure, free of
side effects operations. Without questioning all this, I find pure functional
Scala code to be written in much more object-oriented <em>style</em> compared to F#.</p>
<p>Classes and objects are ubiquitous in Scala: they are in each example given
in Martin Odersky&rsquo;s courses. Most F# examples refrain from classes unless needed.
F# official guidance is to never expose non-abstract classes from F# API!</p>
<p>Scala is really heavy about inheritance. They even introduced quasi-multiple inheritance:
traits. <code>Stream</code> inherits from <code>List</code>, and <code>Nothing</code> is a subtype of every other type,
to be used for some covariance tricks.</p>
<p>Operations are usually defined as class methods instead of separate functions. For
example the following Scala code</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">word filter (c <span style="color:#00f">=&gt;</span> c.isLetter)
</code></pre></div><p>would filter a string to letters only. Why is <code>isLetter</code> defined as a method of
<code>Char</code>? I don&rsquo;t think it&rsquo;s essential for the type itself&hellip;</p>
<h2 id="usage-of-operators">Usage of Operators</h2>
<p>It looks like Scala culture inclines more towards the usage of different
operators, not only for arithmetic operations but also for different classes
from standard library and domain-specific code too. The basic ones are nice,
e.g. list concatenation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#2b91af">List</span>(1, 2) ++ <span style="color:#2b91af">List</span>(3, 4)
</code></pre></div><p>but others look awkward to me, e.g. stream concatenation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#2b91af">Stream</span>(1) #<span style="color:#00f">:</span><span style="color:#2b91af">::</span> <span style="color:#2b91af">Stream</span>(<span style="">2</span>)
</code></pre></div><p>Akka streams sweetness:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala">in ~&gt; f1 ~&gt; bcast ~&gt; f2 ~&gt; merge ~&gt; f3 ~&gt; out
            bcast ~&gt; f4 ~&gt; merge
</code></pre></div><p>This can go to quite an extreme, similar to what <code>scalaz</code> library does.</p>
<p>My default would be not to use operators unless you are sure that every
reader is able to instantly understand what it means.</p>
<h2 id="partial-application">Partial Application</h2>
<p>Not a huge difference, but F# functions are curried by default, while Scala
functions aren&rsquo;t. Thus, in F# partial application just works, all the time</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> add a b = a + b
<span style="color:#00f">let</span> add3 = add 3
<span style="color:#00f">let</span> sum = add3 5 <span style="color:#008000">// 8
</span></code></pre></div><p>Scala function</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">def</span> add (a<span style="color:#00f">:</span> <span style="color:#2b91af">Int</span>, b<span style="color:#00f">:</span> <span style="color:#2b91af">Int</span>) <span style="color:#00f">=</span> a + b
</code></pre></div><p>is not curried, but Underscore comes to the rescue</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#00f">val</span> add3<span style="color:#00f">:</span> (<span style="color:#2b91af">Int</span>) =&gt; <span style="color:#2b91af">Int</span> <span style="color:#00f">=</span> add(3, <span style="color:#00f">_</span>)
<span style="color:#00f">val</span> sum <span style="color:#00f">=</span> add3(5) <span style="color:#008000">// 8
</span></code></pre></div><p>Note how I miss the type inference again.</p>
<p>The parameter order is very important in F#: the short syntax
will partially apply parameters from left to right. In Scala, you can
put <code>_</code> at any position, which gives you some flexibility.</p>
<h2 id="single-direction-dependency">Single-Direction Dependency</h2>
<p>F# compiler doesn&rsquo;t allow circular dependencies. You can&rsquo;t use a function
before you&rsquo;ve defined it. Here is what Expert F# book has to say about
that:</p>
<blockquote>
<p>Managing dependencies and circularity is one of the most difficult
and fundamental problems in good software design. The files in
an F# project are presented to the F# compiler in a compilation
order: constructs in the earlier files can&rsquo;t refer to declarations
in the later files. This is a mechanism to enforce layered design,
where software is carefully organized into layers, and where one
layer doesn&rsquo;t refer to other layers in a cyclic way (&hellip;) to help you
write code that is reusable and organized
into components that are, where possible, independent and not
combined into a &ldquo;tangle&rdquo; of &ldquo;spaghetti code&rdquo;.</p>
</blockquote>
<p>I think this is huge. F# forces you to structure your code in a way that
avoid mutual dependencies between different functions, types and modules.
This reduces the complexity and coupling, makes the developers avoid some
of the design pitfalls.</p>
<p>There&rsquo;s nothing like that in Scala. You are on your own.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Of course I did not cover all the distinctions, for instance active patterns,
type providers, computation expressions in F# and type classes, higher
kinded types, macros in Scala.</p>
<p>Obviously, both Scala and F# are very capable languages, and I am still
picking up the basics of them. While similar in many aspects, they made
several different choices along the language design trade-offs.</p>
<p>P.S. Overheard on Twitter:</p>
<blockquote>
<p>F# isn&rsquo;t a bad language, it&rsquo;s just attached to a bad platform&hellip;
The opposite of Scala actually.</p>
</blockquote>
<p>UPDATE: Thanks everyone for the great comments; please check out
<a href="https://redd.it/4whxhj">this reddit</a> and <a href="https://lobste.rs/s/ewhrpt">lobste.rs</a>
to see more of them.</p>]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/scala" term="scala" label="Scala" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/coursera" term="coursera" label="Coursera" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Coursera]]></title>
            <link href="https://mikhail.io/tags/coursera/"/>
            <id>https://mikhail.io/tags/coursera/</id>
            
            <published>2016-08-05T00:00:00+00:00</published>
            <updated>2016-08-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Scala]]></title>
            <link href="https://mikhail.io/tags/scala/"/>
            <id>https://mikhail.io/tags/scala/</id>
            
            <published>2016-08-05T00:00:00+00:00</published>
            <updated>2016-08-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[API]]></title>
            <link href="https://mikhail.io/tags/api/"/>
            <id>https://mikhail.io/tags/api/</id>
            
            <published>2016-07-27T00:00:00+00:00</published>
            <updated>2016-07-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[BrowserSync]]></title>
            <link href="https://mikhail.io/tags/browsersync/"/>
            <id>https://mikhail.io/tags/browsersync/</id>
            
            <published>2016-07-27T00:00:00+00:00</published>
            <updated>2016-07-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Mocking API calls in Aurelia]]></title>
            <link href="https://mikhail.io/2016/07/mocking-api-calls-in-aurelia/"/>
            <id>https://mikhail.io/2016/07/mocking-api-calls-in-aurelia/</id>
            
            <published>2016-07-27T00:00:00+00:00</published>
            <updated>2016-07-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><a href="http://aurelia.io">Aurelia</a> is a modern and slick single-page application framework.
&ldquo;Single-page application&rdquo; aspect means that it&rsquo;s loaded
into the browser once, and then the navigation happens on the client side and
all the data are loaded from a REST API endpoint.</p>
<p>Let&rsquo;s say that our front-end Aurelia app is hosted at
<code>myaureliaapp.com</code> while the REST API is hosted at
<code>myaureliaapp.com/api</code>. The REST API is a server-side application,
which can be implemented in .NET, Java, Node.JS etc., and it talks to
a database of some kinds.</p>
<p>For the front-end development purpose, it&rsquo;s usually useful to be able to
mock the connection to API with some static manually generated data. This
cuts the hard dependency between the client code, the backend code and database.
It&rsquo;s much easier to mock the exact data set which is needed for the current
development task.</p>
<p>Fortunately, it can be easily done, and here is how.</p>
<h2 id="identify-your-requests">Identify your requests</h2>
<p>Create a list of the requests that you need to mock. For our example let&rsquo;s
say you do the following requests from the application:</p>
<pre><code>GET /api/products
GET /api/products/{id}
POST /api/products
</code></pre><h2 id="put-your-mock-data-into-files">Put your mock data into files</h2>
<p>Go to the root folder of your Aurelia app and create an <code>/api</code> folder.</p>
<p>Create a <code>/api/products</code> subfolder and put a new file called <code>GET.json</code>. This
file should contain the JSON of the product list, e.g.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">[ { &#34;id&#34;: 1, &#34;name&#34;: <span style="color:#a31515">&#34;Keyboard&#34;</span>, &#34;price&#34;: <span style="color:#a31515">&#34;60$&#34;</span> },
  { &#34;id&#34;: 2, &#34;name&#34;: <span style="color:#a31515">&#34;Mouse&#34;</span>, &#34;price&#34;: <span style="color:#a31515">&#34;20$&#34;</span> },
  { &#34;id&#34;: 3, &#34;name&#34;: <span style="color:#a31515">&#34;Headphones&#34;</span>, &#34;price&#34;: <span style="color:#a31515">&#34;80$&#34;</span> }
]
</code></pre></div><p>Create a new file called <code>POST.json</code> in the same folder. POST response won&rsquo;t
return any data, so the file can be as simple as</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{}
</code></pre></div><p>Create subfolders <code>1</code>, <code>2</code> and <code>3</code> under <code>products</code> and create a <code>GET.json</code>
file in each of them. Every file contains the data for a specific product, e.g.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{ &#34;id&#34;: 1, 
  &#34;name&#34;: <span style="color:#a31515">&#34;Keyboard&#34;</span>, 
  &#34;price&#34;: <span style="color:#a31515">&#34;60$&#34;</span>,
  &#34;category&#34;: <span style="color:#a31515">&#34;Computer Accessories&#34;</span>,
  &#34;brand&#34;: <span style="color:#a31515">&#34;Mousytech&#34;</span>
}
</code></pre></div><h2 id="configure-browsersync-to-mock-your-api-calls">Configure BrowserSync to mock your API calls</h2>
<p>For the purpose of this post, I assume you are using
<a href="https://github.com/aurelia/skeleton-navigation">Aurelia Skeleton Navigation</a>
starter kit, specifically
<a href="https://github.com/aurelia/skeleton-navigation/tree/master/skeleton-esnext">the version with Gulp-based tasks and BrowserSync</a>.
If so, you should be familiar with <code>gulp serve</code> command, which serves your
application at <code>http://localhost:9000</code>. We will extend this command to host
your API mock too.</p>
<p>Navigate to <code>/build/tasks</code> folder and edit the <code>serve.js</code> file. Change the
definition of <code>serve</code> task to the following code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js">gulp.task(<span style="color:#a31515">&#39;serve&#39;</span>, [<span style="color:#a31515">&#39;build&#39;</span>], <span style="color:#00f">function</span>(done) {
  browserSync({
    online: <span style="color:#00f">false</span>,
    open: <span style="color:#00f">false</span>,
    port: 9000,
    server: {
      baseDir: [<span style="color:#a31515">&#39;.&#39;</span>],
      middleware: <span style="color:#00f">function</span>(req, res, next) {
        res.setHeader(<span style="color:#a31515">&#39;Access-Control-Allow-Origin&#39;</span>, <span style="color:#a31515">&#39;*&#39;</span>);

        <span style="color:#008000">// Mock API calls
</span><span style="color:#008000"></span>        <span style="color:#00f">if</span> (req.url.indexOf(<span style="color:#a31515">&#39;/api/&#39;</span>) &gt; -1) {
          console.log(<span style="color:#a31515">&#39;[serve] responding &#39;</span> + req.method + <span style="color:#a31515">&#39; &#39;</span> + req.originalUrl);
          
          <span style="color:#00f">var</span> jsonResponseUri = req._parsedUrl.pathname + <span style="color:#a31515">&#39;/&#39;</span> + req.method + <span style="color:#a31515">&#39;.json&#39;</span>;
          
          <span style="color:#008000">// Require file for logging purpose, if not found require will 
</span><span style="color:#008000"></span>          <span style="color:#008000">// throw an exception and middleware will cancel the retrieve action
</span><span style="color:#008000"></span>          <span style="color:#00f">var</span> jsonResponse = require(<span style="color:#a31515">&#39;../..&#39;</span> + jsonResponseUri);
          
          <span style="color:#008000">// Replace the original call with retrieving json file as reply
</span><span style="color:#008000"></span>          req.url = jsonResponseUri;
          req.method = <span style="color:#a31515">&#39;GET&#39;</span>;
        }

        next();
      }
    }
  }, done);
});
</code></pre></div><h2 id="run-it">Run it</h2>
<p>Now just run <code>gulp serve</code> (or <code>gulp watch</code>, which does <code>serve</code> and then watches
files for changes). Every time your app makes an API call, you will see
a line in the gulp console:</p>
<pre><code>[serve] responding GET /api/products
</code></pre><p>If you happen to make an invalid request with no mock defined, you will
get an error:</p>
<pre><code>[serve] responding GET /api/notproducts
Error: Cannot find module '../../api/notproducts/GET.json'
</code></pre><p>A complete example can be found in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/tree/master/aurelia-api-mocks">my github repository</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aurelia" term="aurelia" label="Aurelia" />
                             
                                <category scheme="https://mikhail.io/tags/browsersync" term="browsersync" label="BrowserSync" />
                             
                                <category scheme="https://mikhail.io/tags/rest" term="rest" label="REST" />
                             
                                <category scheme="https://mikhail.io/tags/api" term="api" label="API" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[REST]]></title>
            <link href="https://mikhail.io/tags/rest/"/>
            <id>https://mikhail.io/tags/rest/</id>
            
            <published>2016-07-27T00:00:00+00:00</published>
            <updated>2016-07-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Building a Poker Bot: Functional Fold as Decision Tree Pattern]]></title>
            <link href="https://mikhail.io/2016/07/building-a-poker-bot-functional-fold-as-decision-tree-pattern/"/>
            <id>https://mikhail.io/2016/07/building-a-poker-bot-functional-fold-as-decision-tree-pattern/</id>
            
            <published>2016-07-22T00:00:00+00:00</published>
            <updated>2016-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>This is the fifth part of <strong>Building a Poker Bot</strong> series where I describe my experience developing bot software
to play in online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively
easy and very enjoyable. Here are the previous parts:</em></p>
<ul>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/"><em>Building a Poker Bot: Card Recognition</em></a></li>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/"><em>Building a Poker Bot: String and Number Recognition</em></a></li>
<li><a href="https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/"><em>Building a Poker Bot: Mouse Movements</em></a></li>
<li><a href="https://mikhail.io/2016/04/building-a-poker-bot-with-akka-net-actors/"><em>Building a Poker Bot with Akka.NET Actors</em></a></li>
</ul>
<p>In this post I describe a simple pattern to structure the complex decision making
code using partial function application and fold operation applied to a list of functions.</p>
<h2 id="context">Context</h2>
<p>Poker decisions are complex and depend on the multitude of parameters and attributes.
We can visualize the decision making process as a Decision Tree where leaf nodes are
decisions being made, and the branches are different conditions. Here is a simplistic
example of such a poker decision tree:</p>
<p><img src="simple-poker-decision-tree.png" alt="Simplistic Poker Decision Tree"></p>
<p>Now, if we need to implement a similar tree in code, the most straightforward way to
do that is to translate each condition to an <code>if</code> statement. This way, the nested
conditions will guide the application through the branches right to the point where
an appropriate decision can be returned.</p>
<p>This approach works for small cases, but in reality it does not scale particularly
well in terms of the tree size. Namely, the two problems are:</p>
<p><strong>Tree depth</strong>. In many cases, you might need to pass ten or more conditions before
you find your way to the leaf. Obviously, ten levels of <code>if</code> statements are not
particularly readable and maintainable. We can try to split the sub-trees into
sub-functions, but that only gives a limited relief.</p>
<p><strong>Subtree correlation</strong>. Some tree branches deep down the hiereachy might be correlated
to each other. Say, you pass 10 levels of conditions and make a bet on flop. Now,
on turn, you would probably take quite a different decision path, but the logic
would be based on similar &lsquo;thinking&rsquo; in human terms. Ideally, we want to keep this
kind of related decisions together, while isolating them from the other unrelated
decision paths.</p>
<p>In fact, the decision tree should be generalized to the Decision <em>Graph</em> to allow
different decision branches to merge back at some point, e.g.</p>
<blockquote>
<p>If there is one Ace on flop, or an overcard came on turn or river</p>
<p>and stacks pre-flop were 20+ BB, or 12+ BB in limped pot</p>
<p>then bet 75% of the pot</p>
</blockquote>
<p>There are multiple paths to the same decisions.</p>
<h2 id="solution">Solution</h2>
<p><strong>Break the decision graph down</strong> vertically into smaller chunks. Each chunk should
represent multiple layers of conditions and lead to eventual decisions. All
conditions in sub-graph should be related to each other (high cohesion) and as
isolated from other sub-graphs as possible (low coupling).</p>
<p>Here are two examples of such sub-graphs:</p>
<p><img src="decision-subgraphs.png" alt="Isolated Decision Sub-graphs"></p>
<p>Each sub-graph is very focused on very specific paths and ignores all the branches
which do not belong to this decision process. The idea is that those branches
will be handled by other sub-graphs.</p>
<p><strong>Represent each sub-graph as a function</strong> with arbitrary signature which accepts
all the parameters that are required for this sub-graph. Do not accept any parameters
which are not related.</p>
<p>The last parameter of each function should be a
<a href="https://mikhail.io/2016/01/monads-explained-in-csharp/#maybe">Maybe</a> of Decision,
so should be the function&rsquo;s return type.</p>
<p><strong>Produce a flat list</strong> of all the sub-graph functions. Partially apply the parameters
to those functions to unify the signature of all of them.</p>
<p>Now, when making a decision, <strong>left-fold the list of functions</strong> with the data of
current poker hand. If a function returns <code>Some</code> value of decision, return it
as the decision produced from the graph.</p>
<h2 id="code-sample">Code sample</h2>
<p>We define a number of functions, each one of which represents one piece of decision
logic. Then we put them all into the list:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> rules = [
  overtakeLimpedPot overtakyHand snapshot value history;
  increaseTurnBetEQvsAI snapshot;
  allInTurnAfterCheckRaiseInLimpedPot snapshot history;
  checkCallPairedTurnAfterCallWithSecondPairOnFlop snapshot value.Made history;
  bluffyCheckRaiseFlopInLimpedPotFlop bluffyCheckRaiseFlopsLimp snapshot value history;
  bluffyOvertakingRiver bluffyOvertaking snapshot history
]
</code></pre></div><p>The type of this list is <code>(Decision option -&gt; Decision option) list</code>.</p>
<p>Note how each individual function accepts different set of parameters. Current hand&rsquo;s
<code>snapshot</code> is used by all of them, while calculated hand <code>value</code> and previous
action <code>history</code> are used only by some of the functions.</p>
<p>Now, here is the definition of the facade decision making function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">rules |&gt; List.fold (<span style="color:#00f">fun</span> opt rule -&gt; rule opt) None
</code></pre></div><p>It calculates the decision by folding the list of rules and passing current decision
between them. <code>None</code> is the initial seed of the fold.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Vertical slices are an efficient way to break down the complex decision making
into smaller cohesive manageable parts. Once you get the parts right, it&rsquo;s easy
to compose them by folding a flat list of partially applied functions into a
<code>Maybe</code> of decision result.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/poker-bot" term="poker-bot" label="Poker Bot" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Poker Bot]]></title>
            <link href="https://mikhail.io/tags/poker-bot/"/>
            <id>https://mikhail.io/tags/poker-bot/</id>
            
            <published>2016-07-22T00:00:00+00:00</published>
            <updated>2016-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Dependency Inversion]]></title>
            <link href="https://mikhail.io/tags/dependency-inversion/"/>
            <id>https://mikhail.io/tags/dependency-inversion/</id>
            
            <published>2016-05-27T00:00:00+00:00</published>
            <updated>2016-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Dependency Inversion Implies Interfaces Are Owned by High-level Modules]]></title>
            <link href="https://mikhail.io/2016/05/dependency-inversion-implies-interfaces-are-owned-by-high-level-modules/"/>
            <id>https://mikhail.io/2016/05/dependency-inversion-implies-interfaces-are-owned-by-high-level-modules/</id>
            
            <published>2016-05-27T00:00:00+00:00</published>
            <updated>2016-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Dependency Inversion is one of the five principles of widely known and
acknowledged S.O.L.I.D. design guidelines. This principle is very powerful
and useful when applied consistently. But in my experience, it&rsquo;s actually
quite easy to misunderstand the idea, or at least to mentally simplify it
to somewhat less profound technique of Dependency Injection.</p>
<p>In this post I will try to give my understanding of the principle, and
the difference between <em>Inversion</em> and <em>Injection</em>.</p>
<p>Let&rsquo;s start with the Dependency Inversion principle definition. It was given
by Uncle Bob Martin, and consists of two parts.</p>
<h2 id="part-1-abstractions">Part 1: Abstractions</h2>
<blockquote>
<p>High-level modules should not depend on low-level modules.
Both should depend on abstractions.</p>
</blockquote>
<p>Ok, this is easy to understand. High-level modules are also high-importance
modules, they are about the business domain and are not specific about
technical details. Low-level modules are about wiring those high-level
functions to execution environment, tools and third parties.</p>
<p>Thus, the implementation of high level policy should not depend on
implementation of low level code, but rather on interfaces (or other
abstractions).</p>
<p>Let&rsquo;s take a look at an example. Our high-level business domain is about
planning and executing trips from geographical point A to point B. Our
low-level code talks to a service which knows how to calculate the time
required for a vehicle to go from A to B:</p>
<p><img src="uml-dependency-inversion-violated.png" alt="UML: dependency inversion violated"></p>
<p>So the following code <strong>violates</strong> the first part of the Dependency Inversion:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">namespace</span> Mapping
{
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">RouteCalculator</span>
    {
        <span style="color:#00f">public</span> TimeSpan CalculateDuration(
            <span style="color:#2b91af">double</span> fromLat, <span style="color:#2b91af">double</span> fromLng, <span style="color:#2b91af">double</span> toLat, <span style="color:#2b91af">double</span> toLng)
        {
            <span style="color:#008000">// Call a 3rd party web service
</span><span style="color:#008000"></span>        }
    }
}

<span style="color:#00f">namespace</span> Planning
{
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TripPlanner</span>
    {
        <span style="color:#00f">public</span> DateTime ExpectedArrival(Trip trip)
        {
            <span style="color:#2b91af">var</span> calculator = <span style="color:#00f">new</span> RouteCalculator();
            <span style="color:#2b91af">var</span> duration = calculator.CalculateDuration(
                trip.Origin.Latitude,
                trip.Origin.Longitude,
                trip.Destination.Latitude,
                trip.Destination.Longitude);
            <span style="color:#00f">return</span> trip.Start.Add(duration);
        }
    }
}
</code></pre></div><p>It&rsquo;s not compliant to the principle because the high-level code (<code>TripPlanner</code>)
explicitly depends on low-level service (<code>RouteCalculator</code>). Note that I&rsquo;ve put
them to distinct namespaces to emphasize the required separation.</p>
<p>To improve on that, we might introduce an interface to decouple the
implementations:</p>
<p><img src="uml-dependency-inversion-with-dependency-injection.png" alt="UML: dependency inversion with dependency injection"></p>
<p>In Trip Planner we accept the interface as constructor parameter, and we&rsquo;ll get the
specific implementation at run time:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">namespace</span> Mapping
{
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">IRouteCalculator</span>
    {
        TimeSpan CalculateDuration(
            <span style="color:#2b91af">double</span> fromLat, <span style="color:#2b91af">double</span> fromLng, <span style="color:#2b91af">double</span> toLat, <span style="color:#2b91af">double</span> toLng);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">RouteCalculator</span> : IRouteCalculator
    {
        <span style="color:#008000">// Same implementation as before...
</span><span style="color:#008000"></span>    }
}

<span style="color:#00f">namespace</span> Planning
{
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TripPlanner</span>
    {
        <span style="color:#00f">private</span> IRouteCalculator calculator;

        <span style="color:#00f">public</span> TripPlanner(IRouteCalculator calculator)
        {
            <span style="color:#00f">this</span>.calculator = calculator;
        }

        <span style="color:#00f">public</span> DateTime ExpectedArrival(Trip trip)
        {
            <span style="color:#2b91af">var</span> duration = <span style="color:#00f">this</span>.calculator.CalculateDuration(
                trip.Origin.Latitude,
                trip.Origin.Longitude,
                trip.Destination.Latitude,
                trip.Destination.Longitude);
            <span style="color:#00f">return</span> trip.Start.Add(duration);
        }
    }
}
</code></pre></div><p>This technique is called <em>dependency
injection</em> or, more specifically, <em>constructor injection</em>. This way
we can easily substitute the implementation later or inject a test
double while unit testing.</p>
<p>But that&rsquo;s just one part of the principle. Let&rsquo;s move on to part 2.</p>
<h2 id="part-2-details">Part 2: Details</h2>
<p>The second part of the principle says</p>
<blockquote>
<p>Abstractions should not depend upon details.
Details should depend upon abstractions.</p>
</blockquote>
<p>I find this wording unfortunate because it might be confusing. There are
some valid examples which explain it with base and derived classes.
But in our example we solved the part 1 with an interface. So now we are told that
the abstraction (interface) should not depend upon details (implementation).</p>
<p>That probably means that the interface should not leak any entities which
are specific to the given implementation, to make other implementation
equally possible.</p>
<p>While this is try, this second part of the principle may seem to be subordinate
to part one, reducing to an idea &ldquo;design your interfaces well&rdquo;. So
many people tend to leave the part 2 out (
<a href="https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design#dependency-inversion-principle">example 1</a>,
<a href="http://www.codeproject.com/Articles/495019/Dependency-Inversion-Principle-and-the-Dependency">example 2</a>), focusing
solely on part 1 - the Dependency Injection.</p>
<h2 id="interface-ownership">Interface Ownership</h2>
<p>But Dependency Inversion is <strong>not</strong> just Dependency Injection. So, to revive
the part 2 I would add the following statement to make it clearer:</p>
<blockquote>
<p><strong>Abstractions should be owned by higher-level modules and implemented by
lower-level modules.</strong></p>
</blockquote>
<p>This rule is violated in our last example. The interface is defined together
with implementation, and is basically just extracted from it. It&rsquo;s <strong>owned</strong>
by the mapping namespace.</p>
<p>To improve the design, we can transfer the interface ownership to domain
level:</p>
<p><img src="uml-dependency-inversion.png" alt="UML: dependency inversion"></p>
<p>As you can see, I also renamed the interface. The name should reflect the way
how the domain experts would think of this abstraction. Here is the result:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">namespace</span> Planning
{
    <span style="color:#00f">public</span> <span style="color:#00f">interface</span> IDurationCalculator
    {
        TimeSpan CalculateDuration(Hub origin, Hub destination);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TripPlanner</span>
    {
        <span style="color:#00f">private</span> IDurationCalculator calculator;

        <span style="color:#00f">public</span> TripPlanner(IDurationCalculator calculator)
        {
            <span style="color:#00f">this</span>.calculator = calculator;
        }

        <span style="color:#00f">public</span> DateTime ExpectedArrival(Trip trip)
        {
            <span style="color:#2b91af">var</span> duration = <span style="color:#00f">this</span>.calculator.CalculateDuration(
                trip.Origin, trip.Destination);
            <span style="color:#00f">return</span> trip.Start.Add(duration);
        }
    }
}

<span style="color:#00f">namespace</span> Mapping
{
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">RouteCalculator</span> : IDurationCalculator
    {
        <span style="color:#00f">public</span> TimeSpan CalculateDuration(Hub origin, Hub destination)
        {
            <span style="color:#008000">// Extract latitude and longitude from Hubs
</span><span style="color:#008000"></span>            <span style="color:#008000">// Call a 3rd party web service
</span><span style="color:#008000"></span>        }
    }
}
</code></pre></div><p>Now, the interface is defined in <code>Planning</code> namespace, close to its <em>Client</em>,
not its <em>Implementation</em>. That&rsquo;s the dependency inversion in action. Even
more importantly, it&rsquo;s defined in terms of our domain - notice the use of
<code>Hub</code> in the interface instead of low-level <code>double</code>.</p>
<h2 id="why-high-level-code-should-own-interfaces">Why High Level Code Should Own Interfaces</h2>
<p>There are multiple benefits to this approach, here are the most important
advantages:</p>
<h3 id="concise-readable-high-level-code">Concise, readable high-level code</h3>
<p>The high-level domain code has the highest value, so the ultimate goal
is to keep it as clean as possible. The interface ownership enables us to
design the most concise interfaces to achieve this goal. We avoid any kind
of adaptation of domain entities to whatever lower-level details.</p>
<h3 id="better-abstractions">Better abstractions</h3>
<p>The interfaces themselves get better as well. They are closer to business,
so abstractions get more ubiquitous and better understood by everyone.</p>
<p>They tend to live longer, just because they are born from the domain side,
not the infrastructure side.</p>
<h3 id="dependencies-in-outer-layers">Dependencies in outer layers</h3>
<p>Code organization tends to improve too. If an interface is defined in the
same module as the implementation, the domain module now has
to reference the infrastructure module just to use the interface.</p>
<p>With domain-level interface, the reference goes in the other direction, so
dependencies are pushed up to the outer layers of application.</p>
<p>This principle is the foundation of domain-centric architectures
<a href="https://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html">Clean architecture</a>,
<a href="http://alistair.cockburn.us/Hexagonal+architecture">Ports and Adapters</a> and the likes.</p>
<h3 id="less-cross-domain-dependencies">Less cross-domain dependencies</h3>
<p>In large systems, the business domains should be split into smaller sub-domains, or
bounded contexts. Still, sub-domains are not totally isolated and must
cooperate to achieve the ultimate business goal.</p>
<p>It might be compelling to reference the interfaces of one sub-domain
from another sub-domain and then say that the dependency is minimal because
they are hidden behind abstractions.</p>
<p>But coupling with abstractions is still coupling. Instead, each domain should
operate its own abstractions at the high level, and then different abstractions
should be wired together on lower level with techniques like adapters, facades,
context mapping etc.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Here is my working definition of Dependency Inversion principle:</p>
<blockquote>
<p>High-level modules should not depend on low-level modules.
Both should depend on abstractions.</p>
</blockquote>
<blockquote>
<p>Abstractions should not depend upon details.
Details should depend upon abstractions.</p>
</blockquote>
<blockquote>
<p>Abstractions should be owned by higher-level modules and implemented by
lower-level modules.</p>
</blockquote>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/solid" term="solid" label="SOLID" />
                             
                                <category scheme="https://mikhail.io/tags/dependency-inversion" term="dependency-inversion" label="Dependency Inversion" />
                             
                                <category scheme="https://mikhail.io/tags/dependency-injection" term="dependency-injection" label="Dependency Injection" />
                             
                                <category scheme="https://mikhail.io/tags/clean-code" term="clean-code" label="Clean Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[SOLID]]></title>
            <link href="https://mikhail.io/tags/solid/"/>
            <id>https://mikhail.io/tags/solid/</id>
            
            <published>2016-05-27T00:00:00+00:00</published>
            <updated>2016-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Code Generation]]></title>
            <link href="https://mikhail.io/tags/code-generation/"/>
            <id>https://mikhail.io/tags/code-generation/</id>
            
            <published>2016-05-13T00:00:00+00:00</published>
            <updated>2016-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Fody]]></title>
            <link href="https://mikhail.io/tags/fody/"/>
            <id>https://mikhail.io/tags/fody/</id>
            
            <published>2016-05-13T00:00:00+00:00</published>
            <updated>2016-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Immutability]]></title>
            <link href="https://mikhail.io/tags/immutability/"/>
            <id>https://mikhail.io/tags/immutability/</id>
            
            <published>2016-05-13T00:00:00+00:00</published>
            <updated>2016-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Tweaking immutable objects with C# and Fody]]></title>
            <link href="https://mikhail.io/2016/05/tweaking-immutable-objects-with-csharp-and-fody/"/>
            <id>https://mikhail.io/2016/05/tweaking-immutable-objects-with-csharp-and-fody/</id>
            
            <published>2016-05-13T00:00:00+00:00</published>
            <updated>2016-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Immutable data structures provide a lot of benefits
to programmers, especially when representing domain
<a href="https://lostechies.com/joeocampo/2007/04/23/a-discussion-on-domain-driven-design-value-objects/">value objects</a>.
Immutability is an essential part of functional programming paradigm.
The concept is useful in object-oriented languages too, but you have
to pay some price to get this advantage.</p>
<p>In C# immutable classes are usually implemented with read-only
properties which are populated from constructor parameters. One of the
disadvantages of this approach is the verbosity of creating a copy of an object
with one property value modified.</p>
<h2 id="example">Example</h2>
<p>Let&rsquo;s have a look at an illustration of this problem. Let&rsquo;s say we have a value
type representing poker player statistics:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">PlayerStats</span>
{
    <span style="color:#00f">public</span> PlayerStats(
        <span style="color:#2b91af">int</span> hands, 
        <span style="color:#2b91af">int</span> daysOnline,
        Money won, 
        Money expectedValue)
    {
        <span style="color:#00f">this</span>.Hands = hands;
        <span style="color:#00f">this</span>.DaysOnline = daysOnline;
        <span style="color:#00f">this</span>.Won = won;
        <span style="color:#00f">this</span>.ExpectedValue = expectedValue;
    }

    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Hands { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> DaysOnline { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Money Won { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Money ExpectedValue { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>We already see that it&rsquo;s quite verbose: basically we repeat each property name
five times. But the issue I&rsquo;m discussing today is related to how we create
a new object based on another object. Let&rsquo;s say we need to make a copy of
a given statistics, but with <code>Hands</code> property increased by 1:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> increasedHands = <span style="color:#00f">new</span> PlayerStats(
    existing.Hands + 1,
    existing.DaysOnline,
    existing.Won,
    existing.ExpectedValue);
</code></pre></div><p>Not as simple as we could hope. Also, there is some room for mistakes here. For
instance, we could swap <code>Won</code> and <code>ExpectedValue</code> property calls
and compiler won&rsquo;t let us know because the types are the same. So we probably
want to use explicit constructor parameter names:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> increasedHands = <span style="color:#00f">new</span> PlayerStats(
    hands: existing.Hands + 1,
    daysOnline: existing.DaysOnline,
    won: existing.Won,
    expectedValue: existing.ExpectedValue);
</code></pre></div><p>But that leads to even more typing and repetition&hellip;</p>
<h2 id="inspiration">Inspiration</h2>
<p>F# is a functional-first language with immutability as first-class concept.
In F# value objects are usually modelled with Records, here is our example
reimplemented:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">PlayerStats</span> = {
    Hands: int
    DaysOnline: int
    Won: Money
    ExpectedValue: Money
}
</code></pre></div><p>Creation of new objects based on other objects is also solved properly in F#,
thanks to the <strong><code>with</code></strong> keyword :</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> increasedHands = { existing <span style="color:#00f">with</span> Hands = existing.Hands + 1 }
</code></pre></div><p>All the properties are copied from the source record except for the ones
explicitly mentioned in the expression.</p>
<h2 id="defining-with-in-c">Defining With in C#</h2>
<p>There&rsquo;s no <strong><code>with</code></strong> operator in C#, but we can try to come up with an
alternative. We can define some fluent methods which would change
property values one by one (they don&rsquo;t change the original object, but
return a copy with changed value):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> PlayerStats WithHands(<span style="color:#2b91af">int</span> hands) 
{
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> PlayerStats(
        hands: hands,
        daysOnline: existing.DaysOnline,
        won: existing.Won,
        expectedValue: existing.ExpectedValue);
}

<span style="color:#00f">public</span> PlayerStats WithDaysOnline(<span style="color:#2b91af">int</span> daysOnline) { ... }
<span style="color:#00f">public</span> PlayerStats WithWon(Money won) { ... }
<span style="color:#00f">public</span> PlayerStats WithExpectedValue (Money expectedValue) { ... }
</code></pre></div><p>The method implementation is very tedious but the usage gets much cleaner:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> increasedHands = existing.WithHands(existing.Hands + 1);
</code></pre></div><p>One way to avoid repetitive code is to generate it.</p>
<h2 id="withfody-plugin">With.Fody Plugin</h2>
<p>In <a href="https://mikhail.io/2015/12/weaving-your-domain-classes-with-fody/">one of my previous posts</a>
I described how C# value objects can be made less painful with
<a href="https://github.com/Fody/Fody">Fody</a> - a tool which changes your assembly at
compilation time to provide some desired properties in automated and reliable
fashion.</p>
<p>Please welcome the new Fody plugin <a href="https://github.com/mikhailshilkov/With.Fody"><strong>With.Fody</strong></a>
which auto-implements <code>With</code> method bodies for C# immutable classes.</p>
<p>Here is how to use this plugin for our imaginary example.</p>
<p>First, add a reference to NuGet pakages <code>Fody</code> and <code>With.Fody</code>.</p>
<p>Then, keep the <code>PlayerStats</code> class definition, but get rid of the bodies
of <code>WithXyz</code> methods. Keep the signature but return something trivial like
<code>null</code> or <code>this</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> PlayerStats WithHands(<span style="color:#2b91af">int</span> hands) =&gt; <span style="color:#00f">this</span>;
<span style="color:#00f">public</span> PlayerStats WithDaysOnline(<span style="color:#2b91af">int</span> daysOnline) =&gt; <span style="color:#00f">this</span>;
<span style="color:#00f">public</span> PlayerStats WithWon(Money won) =&gt; <span style="color:#00f">this</span>;
<span style="color:#00f">public</span> PlayerStats WithExpectedValue (Money expectedValue) =&gt; <span style="color:#00f">this</span>;
</code></pre></div><p>Compile the project and you will see the following line in Build Output:</p>
<pre><code>&gt;      Fody/With:   Added method 'With' to type 'PlayerStats'.
</code></pre><p>It means that the method bodies were re-implemented with calls to
class constructor with proper parameter values.</p>
<p>The method stubs are needed to satisfy code completion tools like
IntelliSense and Resharper, otherwise we could skip them altogether.</p>
<h2 id="single-with-for-multiple-properties">Single With() for Multiple Properties</h2>
<p>In case you avoid <a href="https://mikhail.io/2015/08/units-of-measurement-in-domain-design/">Primitive Obsession</a>
antipattern, you will often end up with classes which have unique types of
properties, e.g.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TripProfile</span>
{
    <span style="color:#00f">public</span> TripProfile(
        Distance totalDistance,
        Speed averageSpeed,
        Volume fuelConsumed)
    {
        <span style="color:#00f">this</span>.TotalDistance = totalDistance;
        <span style="color:#00f">this</span>.AverageSpeed = averageSpeed;
        <span style="color:#00f">this</span>.FuelConsumed = fuelConsumed;
    }

    <span style="color:#00f">public</span> Distance TotalDistance { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Speed AverageSpeed { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Volume FuelConsumed { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>In this case, the plugin can be smart enough to figure out which property
you want to modify just by looking at the type of the argument. The single stub
method can look like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> TripProfile With&lt;T&gt;(T <span style="color:#00f">value</span>) =&gt; <span style="color:#00f">this</span>;
</code></pre></div><p>This would get compiled into 3 strongly typed <code>With</code> methods with <code>Distance</code>,
<code>Speed</code> and <code>Volume</code> arguments respectively. Resharper is still happy. And
if you make a mistake and try to call the generic <code>With</code> method with an
argument of wrong type (say <code>int</code>), the compiler will give you an error.</p>
<p>It is safe to call <code>With</code> methods in the same assembly where the class is defined:
the calls get adapted to the real implementation automatically.</p>
<h2 id="how-to-get-started-with-your-classes">How to Get Started with Your Classes</h2>
<p>Here are the requirements for the classes to be picked up by <strong><code>Fody.With</code></strong>:</p>
<ol>
<li>Have a single constructor.</li>
<li>The constructor should have more than one argument.</li>
<li>For each constructor agrument, there must be a gettable property with
same name (case insensitive).</li>
<li>At least one <code>With</code> stub must be defined as described above.</li>
</ol>
<p>You can check out more examples, look at the source code or raise an issue in
<a href="https://github.com/mikhailshilkov/With.Fody">With.Fody github repository</a>.</p>
<p>Give it a try and let me know what your think!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fody" term="fody" label="Fody" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/immutability" term="immutability" label="Immutability" />
                             
                                <category scheme="https://mikhail.io/tags/code-generation" term="code-generation" label="Code Generation" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Dapper]]></title>
            <link href="https://mikhail.io/tags/dapper/"/>
            <id>https://mikhail.io/tags/dapper/</id>
            
            <published>2016-04-15T00:00:00+00:00</published>
            <updated>2016-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[T-SQL]]></title>
            <link href="https://mikhail.io/tags/t-sql/"/>
            <id>https://mikhail.io/tags/t-sql/</id>
            
            <published>2016-04-15T00:00:00+00:00</published>
            <updated>2016-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[T-SQL MERGE statement is underrated]]></title>
            <link href="https://mikhail.io/2016/04/t-sql-merge-statement-is-underrated/"/>
            <id>https://mikhail.io/2016/04/t-sql-merge-statement-is-underrated/</id>
            
            <published>2016-04-15T00:00:00+00:00</published>
            <updated>2016-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>How many times did you write a SQL to <strong>save</strong> a row without knowing whether the same
primary key already exists or not? You just get an object in your data access layer and
you want to save all fields into the database.</p>
<p>But there is no SAVE statement in SQL, so effectively you need to come up with your
implementation of &ldquo;INSERT or UPDATE&rdquo; command.</p>
<h2 id="example">Example</h2>
<p>Let&rsquo;s take a concrete example. You have a person object with just 3 fields, here is the
type definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Person</span>
{
    <span style="color:#00f">public</span> Guid Id { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Name { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Email { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>Persons are entities, so we chose a GUID for the primary key column. We always generate
the unique identifier at client side and just want to save the <code>Person</code>.</p>
<h2 id="typical-solutions">Typical Solutions</h2>
<p>The typical T-SQL developer&rsquo;s toolbox contains <code>SELECT</code>, <code>INSERT</code>,
<code>UPDATE</code> and <code>DELETE</code> statements. <code>DELETE</code> is of no use here, but the combination of the other
three can be employed to complete the task. The most straightforward option is</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">IF</span> <span style="color:#00f">NOT</span> <span style="color:#00f">EXISTS</span>(<span style="color:#00f">SELECT</span> 1 <span style="color:#00f">FROM</span> Person <span style="color:#00f">WHERE</span> Id = @Id)
  <span style="color:#00f">INSERT</span> Person (Id, Name, Email) <span style="color:#00f">VALUES</span> (@Id, @Name, @Email)
<span style="color:#00f">ELSE</span>
  <span style="color:#00f">UPDATE</span> Person <span style="color:#00f">SET</span> Name = @Name <span style="color:#00f">AND</span> Email = @Email <span style="color:#00f">WHERE</span> Id = @Id
</code></pre></div><p>It&rsquo;s 4 lines of code instead of one, but it works. Being more fancy, we can reduce the code to
3 lines of code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">UPDATE</span> Person <span style="color:#00f">SET</span> Name = @Name <span style="color:#00f">AND</span> Email = @Email <span style="color:#00f">WHERE</span> Id = @Id
<span style="color:#00f">IF</span> @@ROWCOUNT = 0 <span style="color:#00f">THEN</span>
  <span style="color:#00f">INSERT</span> Person (Id, Name, Email) <span style="color:#00f">VALUES</span> (@Id, @Name, @Email)
</code></pre></div><p>It should also perform faster if you update more often then insert.</p>
<p>Solved? Not completely&hellip;</p>
<p>The problem is that sometimes it doesn&rsquo;t
work. By default, <code>SELECT</code> doesn&rsquo;t lock the table, so in race condition scenario there
may be another thread which would insert another row with same Id between the execution of
two statements (or delete the existing row for that matter). Ouch.</p>
<p>If you think that&rsquo;s just a theoretical problem&hellip; Well, it might be for our tiny shiny <code>Person</code>
table, but it will happen for the tables of decent sizes with complex update patterns.</p>
<h2 id="transactions">Transactions</h2>
<p>What do we do when the execution of two statements can cause race conditions with
unpredictable results? We use transactions! So, start a transaction before the statement,
then lock the table in <code>SELECT</code> and commit after all is done. It works, but quite some downsides
again:</p>
<ul>
<li>Lots of boilerplate code</li>
<li>Easy to make a mistake (Which lock do we need? <code>updlock</code>? <code>holdlock</code>? <code>tablockx</code>?)</li>
<li>You might get into a deadlock, so need to handle it gracefully</li>
</ul>
<h2 id="merge">MERGE</h2>
<p>Starting with SQL Server 2008, Microsoft introduced the <a href="https://msdn.microsoft.com/ru-ru/library/bb510625.aspx">MERGE</a>
statement. Generally, it&rsquo;s quite powerful and can be used to save all the different rows of a source
table into a target table. But we can also use it for our simple task of saving a person.</p>
<p><code>MERGE</code> is just one statement, so it&rsquo;s atomic and consistent. It performs very well.
But the syntax is&hellip; oh my god, it&rsquo;s horrible. Your eyes might bleed:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql">MERGE Person <span style="color:#00f">AS</span> target
<span style="color:#00f">USING</span> (<span style="color:#00f">SELECT</span> @Id, @Name, @Email) <span style="color:#00f">AS</span> <span style="color:#00f">source</span> (Id, Name, Email)
   <span style="color:#00f">ON</span> (target.Id = <span style="color:#00f">source</span>.Id)
 <span style="color:#00f">WHEN</span> MATCHED <span style="color:#00f">THEN</span> 
      <span style="color:#00f">UPDATE</span> <span style="color:#00f">SET</span> Name = <span style="color:#00f">source</span>.Name, Email = <span style="color:#00f">source</span>.Email
 <span style="color:#00f">WHEN</span> <span style="color:#00f">NOT</span> MATCHED <span style="color:#00f">THEN</span>
      <span style="color:#00f">INSERT</span> (Id, Name, Email) <span style="color:#00f">VALUES</span> (<span style="color:#00f">source</span>.Id, <span style="color:#00f">source</span>.Name, <span style="color:#00f">source</span>.Email)
</code></pre></div><p>Yes, we repeat the name of each column 6 times. And we say <code>source</code> 7 times. And you can imagine
how the <code>MERGE</code> of a table with 50 columns would look like. And how painful it is to add a new column
to an existing statement written 2 years ago.</p>
<p>By the way, the deadlocks are still possible with <code>MERGE</code> statement, so you need to handle them
properly.</p>
<p>So the developers, even the ones who know about the <code>MERGE</code>, usually choose to use the good old <code>CRUD</code>
combination. But when isn&rsquo;t the syntax a problem?</p>
<h2 id="generate-it">Generate It!</h2>
<p>More and more developers shift from writing the stored procedures to using ORMs. With full-blown
ORMs you don&rsquo;t need to care about particular SQL statements, but you get a bunch of other problems
related to <a href="https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch">Object-relational impedance mismatch</a>.</p>
<p>One possible approach is to use a mini-ORM, for instance <a href="https://github.com/StackExchange/dapper-dot-net">Dapper</a>.
You do your work in your favourite
general-purpose language, but stay &ldquo;close to the metal&rdquo;, or rather to SQL engine statements.</p>
<p>Here is how I invoke a <code>MERGE</code> statement for a Person object (given a connection from the pool):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> person = <span style="color:#00f">new</span> Person(...);
DapperAdapter.Merge(connection, person);
</code></pre></div><p>Voila! The implementation of generic <code>Merge</code> method takes care of the syntax complications.
Write once, use everywhere:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">void</span> Merge&lt;TEntity&gt;(IDbConnection dbConnection, TEntity entity) <span style="color:#00f">where</span> TEntity : <span style="color:#00f">class</span>
{
    <span style="color:#2b91af">var</span> props = entity.GetType().GetProperties().Select(p =&gt; p.Name).ToList();
    <span style="color:#2b91af">var</span> names = <span style="color:#2b91af">string</span>.Join(<span style="color:#a31515">&#34;, &#34;</span>, props);
    <span style="color:#2b91af">var</span> values = <span style="color:#2b91af">string</span>.Join(<span style="color:#a31515">&#34;, &#34;</span>, props.Select(n =&gt; <span style="color:#a31515">&#34;@&#34;</span> + n));
    <span style="color:#2b91af">var</span> updates = <span style="color:#2b91af">string</span>.Join(<span style="color:#a31515">&#34;, &#34;</span>, props.Select(n =&gt; <span style="color:#a31515">$&#34;{n} = @{n}&#34;</span>));
    dbConnection.Execute(
        <span style="color:#a31515">$@&#34;MERGE {entity.GetType().Name} as target
</span><span style="color:#a31515">          USING (VALUES({values}))
</span><span style="color:#a31515">          AS SOURCE ({names})
</span><span style="color:#a31515">          ON target.Id = @Id
</span><span style="color:#a31515">          WHEN matched THEN
</span><span style="color:#a31515">            UPDATE SET {updates}
</span><span style="color:#a31515">          WHEN not matched THEN
</span><span style="color:#a31515">            INSERT({names}) VALUES({values});&#34;</span>,
        entity);
}
</code></pre></div><p>Of course, it will only work if you use the convention of naming the <code>Person</code> properties
after the database table. In most cases, there will be a domain class <code>Person</code> and a property
bag class <code>PersonRow</code>, so you&rsquo;ll have to do the mapping between them. But that might be
easier than writing T-SQL code.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Don&rsquo;t let the bulky syntax scare you away from the <code>MERGE</code> T-SQL statement. Extend your
toolbox, and use the tools wisely.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/t-sql" term="t-sql" label="T-SQL" />
                             
                                <category scheme="https://mikhail.io/tags/sql-server" term="sql-server" label="SQL Server" />
                             
                                <category scheme="https://mikhail.io/tags/dapper" term="dapper" label="Dapper" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Actor Model]]></title>
            <link href="https://mikhail.io/tags/actor-model/"/>
            <id>https://mikhail.io/tags/actor-model/</id>
            
            <published>2016-04-09T00:00:00+00:00</published>
            <updated>2016-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Akka.NET]]></title>
            <link href="https://mikhail.io/tags/akka.net/"/>
            <id>https://mikhail.io/tags/akka.net/</id>
            
            <published>2016-04-09T00:00:00+00:00</published>
            <updated>2016-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Building a Poker Bot with Akka.NET Actors]]></title>
            <link href="https://mikhail.io/2016/04/building-a-poker-bot-with-akka-net-actors/"/>
            <id>https://mikhail.io/2016/04/building-a-poker-bot-with-akka-net-actors/</id>
            
            <published>2016-04-09T00:00:00+00:00</published>
            <updated>2016-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>This post lays out the most exciting part of the bot. I&rsquo;ll compose the recognition, flow, decision and mouse clicking parts together into the bot application. The application is a console executable interacting with multiple windows of poker room software.</blockquote><p><i>This is the fourth part of <strong>Building a Poker Bot</strong> series where I describe my experience developing bot software to play in online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively easy and very enjoyable. Here are the previous parts:</i></p>
<ul>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/"><em>Building a Poker Bot: Card Recognition</em></a></li>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/"><em>Building a Poker Bot: String and Number Recognition</em></a></li>
<li><a href="https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/"><em>Building a Poker Bot: Mouse Movements</em></a></li>
</ul>
<p>This post lays out the most exciting part of the bot. I&rsquo;ll compose the recognition, flow, decision and mouse clicking
parts together into the bot application. The application is a console executable interacting with multiple
windows of poker room software.</p>
<h2 id="flow">Flow</h2>
<p>The following picture shows the outline of the application data flow:</p>
<p><img src="pokeractors.png" alt="Actor Diagram"></p>
<p><strong>Find Tables</strong> - Every half a second or so we scan all the windows and search for open poker tables among them.
For each poker table we make a screenshot and send those to recognition.</p>
<p><strong>Recognize Screen</strong> - Parse the data from the screenshot. Check whether it&rsquo;s our turn to make a play now, what
the <a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/">hole cards</a> and
<a href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/">stacks</a> are, produce the detailed
screen information and send it to decision maker.</p>
<p><strong>Make Decision</strong> - Understand if that&rsquo;s a new hand or there was a past history before. See
what the villains did and which new cards we got. Here the secret sauce comes to play and produces
a move to be made. Send the action to the mouse clicker.</p>
<p><strong>Click Buttons</strong> - Based on the decision made, click the right buttons. It should be done with proper delays
and <a href="https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/">human-like movements</a> so that the villain
and poker room don&rsquo;t understand that it&rsquo;s bot who is playing.</p>
<hr>
<h2 id="let-the-actors-play">Let the Actors Play</h2>
<p>Because of the multi-tabling, the application is intrinsically multi-threaded. At the same time,
the different parts of the flow are executed at different cadence:</p>
<ul>
<li>Finding tables is triggered by time and is single-threaded</li>
<li>Screen recognition, history detection and decision making run in sequence and can be executed in parallel
for multiple tables</li>
<li>Clicking the buttons is again single-threaded, as it must synchronize the outputs from the previous steps,
put them in sequence with appropriate delays</li>
</ul>
<p>Here are the other treats of the flow:</p>
<ul>
<li>It is reactive and event based</li>
<li>The flow is unidirectional, the output of one step goes to the input of the next step</li>
<li>Most steps are stateless, but the history state needs to be preserved and, ideally, isolated from the other
steps</li>
</ul>
<p>This list of features made me pick the Actor-based <a href="http://getakka.net">Akka.NET</a> framework to implement the flow.</p>
<p>For sure, the application could be done with a bunch of procedural code instead.
But I found actors to be a useful modeling technique to be employed.
It goes well with reactive nature of the application and builds the nice
foundation for more complicated scenarios in the future.</p>
<p>Also, I was curious how F# and Akka.NET would work together.</p>
<h2 id="supervision-hierachy">Supervision Hierachy</h2>
<p>In Akka.NET each actor has a supervisor actor who is managing its lifecycle. All actors together form a
supervision tree. Here is the tree shown for the Player application:</p>
<p><img src="actorhierachy.png" alt="Actor Hierachy"></p>
<p>There is just one copy of both Table Finder and Button Clicker actors and they are supervised by the root
User actor.</p>
<p>For each poker table a Recognizer actor gets created. These actors are managed by Table
Finder.</p>
<p>Each Recognizer actor creates an instance of Decision actor who keeps the hand history
and makes decisions.</p>
<p>Finally, all decisions are sent to one centralized Button Clicker actor whose job is
to click all the tables with proper delays and in order.</p>
<hr>
<h2 id="implementation-patterns">Implementation Patterns</h2>
<p>All actors are implemented with <a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/">Functional Actor Patterns</a>
which are described in <a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/">my previous post</a>.</p>
<p>The basic idea is that each actor is defined in functional style with these
building blocks:</p>
<ul>
<li>Type of incoming and, if needed, outgoing messages</li>
<li>A domain function with business logic</li>
<li>Actor function which puts those parts together</li>
<li>Expression to spawn an actor based on actor function</li>
</ul>
<p>Let&rsquo;s look at the examples to understand this structure better.</p>
<h2 id="table-finder">Table Finder</h2>
<p>Table Finder does not have any meaningful input message. It gets a message from
Akka.NET scheduling system just to be periodically activated.</p>
<p>The domain function is called <code>findWindows</code> and has the type <code>unit -&gt; WindowInfo seq</code>.
It returns the poker window screenshots and titles.</p>
<p>Actor function of type <code>int -&gt; seq&lt;string * WindowInfo&gt;</code> is used by the
<a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/#RouterSupervisor">Router-Supervisor</a> pattern to
define the behavior. The ouput tuple defines an ID of an output actor and a
message to send to it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> findActor msg =
  findWindows ()
  |&gt; Seq.map (<span style="color:#00f">fun</span> x -&gt; (<span style="color:#a31515">&#34;recognizer-actor-&#34;</span> + x.TableName, x))
</code></pre></div><p>Here is how I spawn the singleton instance of this actor:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> tableFinderRef =
  actorOfRouteToChildren findActor (spawnChild recognizer)
  |&gt; spawn system <span style="color:#a31515">&#34;table-finder-actor&#34;</span>
</code></pre></div><p>Where <code>spawnChild</code> is a helper function - essentially an adapter of standard
<code>spawn</code> function with proper parameter order:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> spawnChild childActor name (mailbox : Actor&lt;<span style="color:#00f">&#39;</span>a&gt;) =
  spawn mailbox.Context name childActor
</code></pre></div><p>We can also extend it to debug messages when new actors get created.</p>
<h2 id="recognizer">Recognizer</h2>
<p>Recognizer receives the <code>WindowInfo</code> produced by the Table Finder.</p>
<p>The domain function has the type of <code>Bitmap -&gt; Screen</code>. You can read more about table
recognition in <a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/">Part 1</a> and
<a href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/">Part 2</a>
of these series.</p>
<p>Actor function is an implementation of
<a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/#ConverterSupervisor">Converter-Supervisor</a> pattern.
The output is a decision message for Decision Maker actor which is a supervised
child of the Recognizer. Here is the actor function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> recognizeActor (window : WindowInfo) =
  <span style="color:#00f">let</span> result = recognize window.Bitmap
  { WindowTitle = window.Title
    TableName = window.TableName
    Screen = result
    Bitmap = window.Bitmap }
</code></pre></div><p>And here is the spawn function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> recognizer = actorOfConvertToChild recognizeActor (spawnChild decider <span style="color:#a31515">&#34;decider&#34;</span>)
</code></pre></div><p>Notice how this expression was used in Table Finder instantiation above.</p>
<h2 id="decision-maker">Decision Maker</h2>
<p>Decision Maker actor function is an implementation of
<a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/#StatefulConverter">Stateful Converter</a> pattern. It receives
a decision message from a Recognizer. The output is a click message for a
singleton Clicker actor. It also needs to preserve some state between two calls.
In the minimalistic implementation this state holds the previous screen that
it received, so that if the same message is received twice, the later message is
ignored.</p>
<p>This way the actor function has the type of
<code>DecisionMessage -&gt; Screen option -&gt; ClickerMessage * Screen option</code>
and looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> decisionActor msg lastScreen =
  <span style="color:#00f">let</span> screen = msg.Screen
  <span style="color:#00f">match</span> lastScreen <span style="color:#00f">with</span>
  | Some s <span style="color:#00f">when</span> s = screen -&gt; (None, lastScreen)
  | _ -&gt;
    <span style="color:#00f">let</span> action = decide screen
    <span style="color:#00f">let</span> outMsg = { WindowTitle = msg.WindowTitle; Clicks = action }
    (Some outMsg, Some screen)
</code></pre></div><p>Here is the spawn function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> decider = actorOfStatefulConvert decisionActor None clickerRef
</code></pre></div><p>where <code>None</code> represents the initial state.</p>
<h2 id="button-clicker">Button Clicker</h2>
<p>Clicker actor has the simplest implementation because it does not send messages to other actors.
Here is the message that it receives from Decision Maker:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">ClickTarget</span> = (int * int * int * int)
<span style="color:#00f">type</span> <span style="color:#2b91af">ClickerMessage</span> = {
  WindowTitle: <span style="color:#2b91af">string</span>
  Clicks: ClickTarget[]
}
</code></pre></div><p>The domain function has the simple type <code>ClickerMessage -&gt; unit</code> with mouse
clicks as side effect. You can read more about the mouse movements in
<a href="https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/">Part 3</a>
of these series.</p>
<p><a href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/#MessageSink">Message Sink</a>
pattern is used for this actor, so actor function isn&rsquo;t
really needed. We spawn the singleton instance with the following statement:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> clickerRef = actorOfSink click |&gt; spawn system <span style="color:#a31515">&#34;clicker-actor&#34;</span>
</code></pre></div><p>Actor goes under supervision by actor system with <code>click</code> as message handler.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The top layer of poker player application is composed of small single-purpose
actors which talk to each other by sending messages.</p>
<p>Thanks to succinct F# language and functional actor patterns this layer is
very thin, and thus easy to understand and maintain.</p>
<p>The business logic is isolated and by itself has no dependency on Akka.NET.</p>
<p><em>Proceed to <a href="/2016/07/building-a-poker-bot-functional-fold-as-decision-tree-pattern/">Part 5 of Building a Poker Bot: Functional Fold as Decision Tree Pattern</a>.</em></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/poker-bot" term="poker-bot" label="Poker Bot" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/akka.net" term="akka.net" label="Akka.NET" />
                             
                                <category scheme="https://mikhail.io/tags/actor-model" term="actor-model" label="Actor Model" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Functional Actor Patterns with Akka.NET and F#]]></title>
            <link href="https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/"/>
            <id>https://mikhail.io/2016/03/functional-actor-patterns-with-akkadotnet-and-fsharp/</id>
            
            <published>2016-03-21T00:00:00+00:00</published>
            <updated>2016-03-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>My exploration of Actor model started with <a href="http://getakka.net">Akka.NET</a> framework - a .NET port of
JVM-based <a href="http://akka.io">Akka</a>. Actor programming model made a lot of sense to me, but once
I started playing with it, some questions arose. Most of those questions were related to the
following <a href="http://doc.akka.io/docs/akka/2.4.2/general/actors.html">definition</a>:</p>
<blockquote>
<p>An actor is a container for <code>State</code>, <code>Behavior</code>, a <code>Mailbox</code>, <code>Children</code> and a <code>Supervisor Strategy</code>.</p>
</blockquote>
<p>So, based on the <a href="https://github.com/petabridge/akka-bootcamp">Akka.NET Bootcamp</a> course I understood that
an Actor</p>
<ul>
<li>knows what kind of messages it can accept</li>
<li>does some processing of each message</li>
<li>holds some state which is changed during message processing</li>
<li>potentially changes its behavior based on the current state</li>
<li>creates and stores references to child actors</li>
<li>obtains references to other actors</li>
<li>sends messages to children and other actors</li>
</ul>
<p>While it&rsquo;s nice that the framework enables us to develop for different aspects of actor
behavior, it might also be dangerous in case you do all the things in one place. Ball of spaghetti mud
was where I ended up during my first attempt. My actors were doing all the things from
the above list and the code got messy very quick. So, the following questions popped up
in my head:</p>
<p><em>How do I avoid mixing several concerns in one piece of code?</em></p>
<p><em>How do I make the code easily testable?</em></p>
<p><em>How do I minimize the usage of mutable state?</em></p>
<p><em>How do I avoid boilerplate code when it&rsquo;s not needed?</em></p>
<h2 id="functional-actors">Functional Actors</h2>
<p>I am now developing the actor-based application in F#, the functional first
language. Functions are easy to reason about, reusable and testable. But the
actors are usually defined in terms of objects and classes. F# supports classes
but that&rsquo;s not the path that I&rsquo;m willing to go.</p>
<p>How do we make actors out of functions? Well, most of the time actors don&rsquo;t
need all the features of the framework. In this case we can define the required actor
behavior in terms of a minimal function and then use creational patterns to
spawn actor instances out of it.</p>
<p>Let&rsquo;s look at some common patterns that I identified. For each pattern, I will
define</p>
<ul>
<li>an example of a core function which implements the business logic</li>
<li>a generic function to create actors with behavior of a core function</li>
<li>an example of actor instantiation using the two functions above</li>
</ul>
<hr>
<h2 id="message-sink"><a name="MessageSink"></a>
Message Sink</h2>
<p>Stateless Message Sink is the simplest type of actor.</p>
<p><img src="messagesink.png" alt="Message Sink actor"></p>
<p>It receives a message and executes some action on it. The action is not related
to any other actors, and there is no state, so the processing of each message
is always the same. Obviously, it&rsquo;s related to some kind of side effects:
logging the message, saving the data to the external storage and so on.</p>
<p>So, we don&rsquo;t need the majority of actor features in this case. The whole actor
processing could be represented by a function of type <code>'a -&gt; unit</code>. Here
is an example of a core function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> print msg =
  printn <span style="color:#a31515">&#34;Message received: %A&#34;</span> msg
</code></pre></div><p>So how do we make an actor out of this function? Well, it&rsquo;s already implemented
as <code>actorOf</code> helper function in Akka.NET F# extensions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> actorOfSink (f : <span style="color:#00f">&#39;</span>a -&gt; <span style="color:#2b91af">unit</span>) = actorOf f
</code></pre></div><p>And here is how we spawn an actor instance:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> printActorRef =
  actorOfSink print
  |&gt; spawn system <span style="color:#a31515">&#34;print-actor&#34;</span>

printActorRef &lt;! 3
<span style="color:#008000">// &#34;Message received: 3&#34; is printed
</span></code></pre></div><p>That&rsquo;s the simplicity that I&rsquo;m searching for. Let&rsquo;s look at a slightly more
complex example.</p>
<hr>
<h2 id="converter"><a name="Converter"></a>
Converter</h2>
<p>Stateless Converter maps the incoming message into another message and sends
it to another predefined actor.</p>
<p><img src="converter.png" alt="Converter actor"></p>
<p>The core of this actor is a classic function with one input and one output
parameter (type <code>'a - 'b</code>):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> square msg =
  msg * msg
</code></pre></div><p>The actor function is similar to the one of Message Sink, but it also accepts
a reference to the output actor and knows how to send messages to it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> actorOfConvert f outputRef =
  actorOf2 (<span style="color:#00f">fun</span> _ msg -&gt; outputRef &lt;! f msg)
</code></pre></div><p>Here is how we spawn an instance of a Converter using our <code>print-actor</code> as the
output:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> squareActorRef =
  actorOfConvert square printActorRef
  |&gt; spawn system <span style="color:#a31515">&#34;square-actor&#34;</span>

squareActorRef &lt;! 3
<span style="color:#008000">// &#34;Message received: 9&#34; is printed
</span></code></pre></div><p>Both actor patterns had no notion of state so far. Let&rsquo;s see how we can
treat the statefulness in a functional way.</p>
<hr>
<h2 id="stateful-sink">Stateful Sink</h2>
<p>Let&rsquo;s get back to our Message Sink actor with side-effects, and make it
dependent on its internal state. The state is affected by the incoming
messages and is preserved until the next message comes in.</p>
<p><img src="statefulsink.png" alt="Stateful Sink actor"></p>
<p>Does not look very functional, right? But this impression is wrong in fact.
We can represent the state before a message came in - as an extra input parameter,
and  the state after the message got processed - as an output parameter.
We start with an initial state and the output of the
first message becomes the input state of the second message:</p>
<p><img src="statefulsinkfunctional.png" alt="Stateful Sink functional actor"></p>
<p>Here is an example of a function which prints out the index of a message together
with the message contents:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> printIndex index msg =
  printn <span style="color:#a31515">&#34;Message [%i] received: %A&#34;</span> index msg
  index + 1
</code></pre></div><p>For the actor implementation we need a recursive function so we can&rsquo;t use
<code>actorOf2</code> anymore. Actor workflow is a bit more lines but still very simple:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> actorOfStatefulSink f initialState (mailbox : Actor&lt;<span style="color:#00f">&#39;</span>a&gt;) =

  <span style="color:#00f">let</span> rec imp lastState =
    actor {
      <span style="color:#00f">let!</span> msg = mailbox.Receive()
      <span style="color:#00f">let</span> newState = f msg lastState
      <span style="color:#00f">return</span>! imp newState
    }

  imp initialState
</code></pre></div><p>And here is a usage example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> printIndexActorRef =
  actorOfSink printIndex 1
  |&gt; spawn system <span style="color:#a31515">&#34;print-ix-actor&#34;</span>

printActorRef &lt;! 3
<span style="color:#008000">// &#34;Message [1] received: 3&#34; is printed
</span><span style="color:#008000"></span>
printActorRef &lt;! 4
<span style="color:#008000">// &#34;Message [2] received: 4&#34; is printed
</span></code></pre></div><hr>
<h2 id="stateful-converter"><a name="StatefulConverter"></a>
Stateful Converter</h2>
<p>By now, the core function of the Stateful Converter actor should be a no-brainer for you. The actor
would have two input parameters and two outputs (in a tuple). One of the outputs
is a message and goes to another actor, the other output becomes an input for the
next actor:</p>
<p><img src="statefulconverter.png" alt="Stateful Converter actor"></p>
<p>Here is a function which squares the messaged number, then calculates the running total
and sends it forward:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> squareAndSum sum msg =
  <span style="color:#00f">let</span> result = sum + msg*msg
  (result, result)
</code></pre></div><p>In this particular case the output message and state are equal, but they don&rsquo;t
have to be. Here is the actor implementation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> actorOfStatefulConvert f initialState outputRef (mailbox : Actor&lt;<span style="color:#00f">&#39;</span>a&gt;) =

  <span style="color:#00f">let</span> rec imp lastState =
    actor {
      <span style="color:#00f">let!</span> msg = mailbox.Receive()
      <span style="color:#00f">let</span> (result, newState) = f msg lastState
      outputRef &lt;! result
      <span style="color:#00f">return</span>! imp newState
    }

  imp initialState
</code></pre></div><p>And a usage example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> squareAndSumActorRef =
  actorOfConvert square 0 printIndexActorRef
  |&gt; spawn system <span style="color:#a31515">&#34;square-sum-actor&#34;</span>

squareAndSumActorRef &lt;! 3
<span style="color:#008000">// &#34;Message [1] received: 9&#34; is printed
</span><span style="color:#008000"></span>
squareAndSumActorRef &lt;! 4
<span style="color:#008000">// &#34;Message [2] received: 25&#34; is printed
</span></code></pre></div><hr>
<h2 id="converter-supervisor"><a name="ConverterSupervisor"></a>
Converter-Supervisor</h2>
<p>In the previous patterns the Converter actors were sending messages
to predefined actor references which were not managed (or supervised in Akka terms)
by those actors. Now, let&rsquo;s say that the actor needs to create a child
to send converted messages to it afterwards:</p>
<p><img src="supervisedchild.png" alt="Supervised Child actor"></p>
<p>We can treat such child reference as the state and instantiate it when the first message
comes in. (We can&rsquo;t spawn it before the first message because the
<code>mailbox</code> object is required.) The message goes to the actor
reference that we store in the state, something like this:</p>
<p><img src="childasstate.png" alt="Supervised Child as State"></p>
<p>Here is the generic actor implementation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> actorOfConvertToChild f spawnChild (mailbox : Actor&lt;<span style="color:#00f">&#39;</span>a&gt;) =

  <span style="color:#00f">let</span> rec imp state =
    actor {
      <span style="color:#00f">let</span> newstate =
        <span style="color:#00f">match</span> state <span style="color:#00f">with</span>
        | Some s -&gt; s
        | None -&gt; spawnChild mailbox

      <span style="color:#00f">let!</span> msg = mailbox.Receive()
      newstate &lt;! f msg
      <span style="color:#00f">return</span>! imp (Some newstate)
    }

  imp None
</code></pre></div><p>The only difference is that we accept a <code>spawnChild</code> function instead of
pre-baked actor reference. Here is the first calculator example refactored
to Print actor being a child of Square actor.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> squareWithChildRef =
  actorOfConvertToChild print (spawnChild square <span style="color:#a31515">&#34;print-actor&#34;</span>)
  |&gt; spawn system <span style="color:#a31515">&#34;square-with-child-actor&#34;</span>
</code></pre></div><p>Notice that the <code>square</code> and <code>print</code> functions have exactly the same signatures
and implementations as we used before, and the concern of actor hierarchy is
completely separated from the business logic of the actors.</p>
<p>This hierarchy is handy whenever you need multiple instances of one actor type
(<code>f</code>-actor from the picture) and corresponding instances of another actor type
(<code>g</code>-actor):</p>
<p><img src="multiparents.png" alt="Multiple Parents and Children actor"></p>
<hr>
<h2 id="router-supervisor"><a name="RouterSupervisor"></a>
Router-Supervisor</h2>
<p>Routers are the kind of actors which forward each incoming message to one
or more downstream actors. In this example the downstream actors are supervised
by the Router itself.
So, the Router-Supervisor can have multiple children and send the result of
message processing to one or more of them:</p>
<p><img src="router.png" alt="Router actor"></p>
<p>To keep the spirit of functional actors, we represent the router logic with a function
of type <code>'a -&gt; seq&lt;string * 'b&gt;</code>, where <code>'a</code> is the type of incoming messages,
<code>'b</code> is the type of outgoing messages, and <code>string</code> represents the identifier of the
actor to get the message. Here is a sample implementation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> routeSensorData msg =
  msg |&gt; Seq.map (<span style="color:#00f">fun</span> x -&gt; (<span style="color:#a31515">&#34;sensor-actor-&#34;</span> + x.SensorId, x.Temperature))
</code></pre></div><p>Based on the incoming metadata (sensor identifier) the actor forwards its
temperature to corresponding sensor-specific actor.</p>
<p>Here is the implementation of the generic actor function:</p>
<pre><code>let actorOfRouteToChildren f spawnChild (mailbox : Actor&lt;'a&gt;) =

  let getActor id =
    let actorRef = mailbox.Context.Child(id)
    if actorRef.IsNobody() then
      spawnChild id mailbox
    else
      actorRef

  let rec imp () =
    actor {
      let! msg = mailbox.Receive()
      f msg |&gt; Seq.iter (fun (id, x) -&gt; (getActor id) &lt;! x)
      return! imp ()
    }

  imp ()
</code></pre><p>And a usage example:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> sensorRouterRef =
  actorOfRouteToChildren routeSensorData (spawnChild square)
  |&gt; spawn system <span style="color:#a31515">&#34;route-sensor-actor&#34;</span>
</code></pre></div><p>Note that <code>spawnChild</code> does not accept the child ID anymore because it&rsquo;s being
controlled by the router itself.</p>
<hr>
<h2 id="conclusion">Conclusion</h2>
<p>With the patterns that we have so far we should be able to build quite powerful
hierarchies like the one shown below:</p>
<p><img src="usecase.png" alt="Actor use case"></p>
<p>There might be many other scenarios and types of actors that would make sense
in your use case. I&rsquo;m just showing the basic patterns, but more importantly the way of
reasoning about the code. Don&rsquo;t
let the multitude of actor aspects push you into the world of poorly structured
code.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/akka.net" term="akka.net" label="Akka.NET" />
                             
                                <category scheme="https://mikhail.io/tags/actor-model" term="actor-model" label="Actor Model" />
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/patterns" term="patterns" label="Patterns" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Patterns]]></title>
            <link href="https://mikhail.io/tags/patterns/"/>
            <id>https://mikhail.io/tags/patterns/</id>
            
            <published>2016-03-21T00:00:00+00:00</published>
            <updated>2016-03-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Aurelia Map Component with Leaflet]]></title>
            <link href="https://mikhail.io/2016/03/aurelia-map-component-with-leaflet/"/>
            <id>https://mikhail.io/2016/03/aurelia-map-component-with-leaflet/</id>
            
            <published>2016-03-11T00:00:00+00:00</published>
            <updated>2016-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>This is a short tutorial on how to create a map control in an Aurelia.js application. I am using the Leaflet library with custom tile source and I also show the way to implement your own overlay layer.</blockquote><p>This is a short tutorial on how to create a map control in <a href="http://aurelia.io">Aurelia.js</a>
application. I am using the <a href="http://leafletjs.com">Leaflet</a> library with custom tile
source and I also show the way to implement your own overlay layer. Here is what
my map looks like:</p>
<p><img src="map.png" alt="Map"></p>
<p>So, I assume you already have an existing Aurelia application, and let&rsquo;s start.</p>
<h2 id="install-leaflet">Install Leaflet</h2>
<p>The following command will install Leaflet module to the application:</p>
<pre><code>jspm install leaflet
</code></pre><p>If you are using TypeScript, don&rsquo;t forget to add type definitions</p>
<pre><code>tsd install leaflet
</code></pre><h2 id="define-a-map-component">Define a Map Component</h2>
<p>Create a new <code>map.html</code> file and put the following contents there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-html" data-lang="html">&lt;template&gt;
  &lt;require from=<span style="color:#a31515">&#34;leaflet/dist/leaflet.css&#34;</span>&gt;&lt;/require&gt;
  &lt;div id=<span style="color:#a31515">&#34;mapid&#34;</span> style=<span style="color:#a31515">&#34;height: 100%&#34;</span>&gt;&lt;/div&gt;
&lt;/template&gt;
</code></pre></div><p>We import the CSS required by leaflet and define the <code>div</code> element to host
the map in. Then, create a new <code>map.js</code> file (or <code>map.ts</code> for typescript),
here is the minimum code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">export</span> <span style="color:#00f">class</span> Map {
}
</code></pre></div><h2 id="load-the-map-with-tiles">Load the Map with Tiles</h2>
<p>First, import the leaflet module in your codebehind:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">import</span> * as L from <span style="color:#a31515">&#39;leaflet&#39;</span>;
</code></pre></div><p>Now, define the <code>attached</code> function, which would be called by Aurelia when
control&rsquo;s HTML is loaded, and make a map there:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">export</span> <span style="color:#00f">class</span> Map {
  attached() {
    <span style="color:#00f">let</span> map = L.map(<span style="color:#a31515">&#39;mapid&#39;</span>).setView([51.505, -0.09], 13);

    <span style="color:#00f">let</span> urlTemplate = <span style="color:#a31515">&#39;http://{s}.tile.osm.org/{z}/{x}/{y}.png&#39;</span>;
    map.addLayer(L.tileLayer(urlTemplate, { minZoom: 4 }));
  }
}
</code></pre></div><p>The example above uses the URL template of Open Street Maps as per the Leaflet&rsquo;s
tutorial, but I needed to use our privately hosted maps, so I changed it to
something like:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">let</span> urlTemplate = <span style="color:#a31515">&#39;http://www.mysite.com/tiles?layer=background&amp;level={z}&amp;x={x}&amp;y={y}&#39;</span>;
map.addLayer(L.tileLayer(urlTemplate, { minZoom: 4, zoomOffset: 8 }));
</code></pre></div><p>The <code>zoomOffset</code> parameter was required to fix impedance mismatch of zoom levels.</p>
<h2 id="custom-overlay-layer">Custom Overlay Layer</h2>
<p>For our custom maps we needed to show two layers on top of each other:</p>
<ul>
<li>The usual tile layer for the map background</li>
<li>The overlay layer for the map labels and additional information</li>
</ul>
<p>The overlay layer can&rsquo;t be broken down into tiles (not supported by our map provider),
so we need to show the whole layer as a single picture and then refresh it every
time user pans or zooms the map.</p>
<p>The overlay layer can be implemented with <code>onAdd</code> and <code>onRemove</code> functions
and then feeding an image element to the Leaflet as a layer. Here is the code:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">import</span> * as L from <span style="color:#a31515">&#39;leaflet&#39;</span>;

<span style="color:#00f">export</span> <span style="color:#00f">class</span> LabelOverlayLayer {
  map;
  image;

  onAdd(map) {
    <span style="color:#00f">this</span>.map = map;

    <span style="color:#00f">this</span>.image = L.DomUtil.create(<span style="color:#a31515">&#39;img&#39;</span>, <span style="color:#a31515">&#39;leaflet-tile-loaded&#39;</span>);
    map.getPanes().overlayPane.appendChild(<span style="color:#00f">this</span>.image);

    map.on(<span style="color:#a31515">&#39;moveend&#39;</span>, <span style="color:#00f">this</span>.render, <span style="color:#00f">this</span>);
    <span style="color:#00f">this</span>.render();
  }

  onRemove (map) {
    map.getPanes().overlayPane.removeChild(<span style="color:#00f">this</span>.image);
    map.off(<span style="color:#a31515">&#39;moveend&#39;</span>, <span style="color:#00f">this</span>.render, <span style="color:#00f">this</span>);
  }

  render() {
    <span style="color:#00f">let</span> bounds = <span style="color:#00f">this</span>.map.getBounds(), mapSize = <span style="color:#00f">this</span>.map.getSize();
    <span style="color:#00f">let</span> se = bounds.getSouthEast(), nw = bounds.getNorthWest();

    <span style="color:#00f">let</span> tileUrl = <span style="color:#a31515">`http://www.mysite.com/tiles?layer=labels&amp;lonmin=</span><span style="color:#a31515">${</span>nw.lng<span style="color:#a31515">}</span><span style="color:#a31515">&amp;latmin=</span><span style="color:#a31515">${</span>se.lat<span style="color:#a31515">}</span><span style="color:#a31515">&amp;lonmax=</span><span style="color:#a31515">${</span>se.lng<span style="color:#a31515">}</span><span style="color:#a31515">&amp;latmax=</span><span style="color:#a31515">${</span>nw.lat<span style="color:#a31515">}</span><span style="color:#a31515">&amp;width=</span><span style="color:#a31515">${</span>Math.floor(mapSize.x)<span style="color:#a31515">}</span><span style="color:#a31515">&amp;height=</span><span style="color:#a31515">${</span>Math.floor(mapSize.y)<span style="color:#a31515">}</span><span style="color:#a31515">`</span>;
    <span style="color:#00f">this</span>.image.src = tileUrl;

    <span style="color:#00f">let</span> pos = <span style="color:#00f">this</span>.map.latLngToLayerPoint(nw);
    L.DomUtil.setPosition(<span style="color:#00f">this</span>.image, pos, <span style="color:#00f">false</span>);
  }
};
</code></pre></div><p>The usage of this layer in the map component is trivial:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">this</span>.map.addLayer(<span style="color:#00f">new</span> LabelOverlayLayer());
</code></pre></div><h2 id="use-the-map-component">Use the Map Component</h2>
<p>The map component is ready to be used in the application:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-html" data-lang="html">&lt;require from=<span style="color:#a31515">&#34;./components/map&#34;</span>&gt;&lt;/require&gt;
&lt;div  style=<span style="color:#a31515">&#34;height: 700px&#34;</span>&gt;
  &lt;map&gt;&lt;/map&gt;
&lt;/div&gt;
</code></pre></div><p>The container around the map should have a non-zero height, so I made it fixed
in the example above.</p>
<p>Don&rsquo;t forget to bundle the leaflet assets by including the following lines
into your <code>bundles.json</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#a31515">&#34;includes&#34;</span><span style="">:</span> [
  <span style="color:#a31515">&#34;aurelia-framework&#34;</span>,
  <span style="">//</span> <span style="">...</span>
  <span style="color:#a31515">&#34;leaflet&#34;</span>,
  <span style="color:#a31515">&#34;leaflet/dist/leaflet.css!text&#34;</span>
]<span style="">,</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/aurelia" term="aurelia" label="Aurelia" />
                             
                                <category scheme="https://mikhail.io/tags/leaflet" term="leaflet" label="Leaflet" />
                             
                                <category scheme="https://mikhail.io/tags/typescript" term="typescript" label="Typescript" />
                             
                                <category scheme="https://mikhail.io/tags/javascript" term="javascript" label="Javascript" />
                             
                                <category scheme="https://mikhail.io/tags/maps" term="maps" label="Maps" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Building a Poker Bot: Mouse Movements]]></title>
            <link href="https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/"/>
            <id>https://mikhail.io/2016/03/building-a-poker-bot-mouse-movements/</id>
            
            <published>2016-03-01T00:00:00+00:00</published>
            <updated>2016-03-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The last step of the poker bot flow: clicking the buttons. The screen is already recognized, the hand is understood, the decisions are made and now the bot needs to execute the actions. This means clicking the right button at the poker table.</blockquote><p>This is the third part of <strong>Building a Poker Bot</strong> series where I describe my experience developing bot software
to play in online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively easy and very enjoyable.</p>
<p>Here are the previous parts:</p>
<ul>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/"><em>Building a Poker Bot: Card Recognition</em></a></li>
<li><a href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/"><em>Building a Poker Bot: String and Number Recognition</em></a></li>
</ul>
<p>In this short post I write about the last step of the poker bot flow: clicking
the buttons. So, the screen is already recognized, the hand is understood,
the decisions are made and now the bot needs to execute the actions. Except for
the bet sizing, this simply means clicking the right button at the poker table.</p>
<p>The stealthiness of such clicks is a valid concern here. Ideally, we want all
the mouse movements to look as similar as possible to the movements produced
by a human being. For this post, I will simplify the task to the following steps:</p>
<ul>
<li>Identify where the mouse cursor is right now</li>
<li>Decide where the mouse should be moved to</li>
<li>Gradually move the mouse cursor</li>
<li>Click the button</li>
</ul>
<h2 id="cursor-position">Cursor Position</h2>
<p>It&rsquo;s really easy to understand where the mouse cursor currently is: just
use <code>Control.MousePosition</code> property from the standard library:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> currentPosition () =
  <span style="color:#00f">let</span> mp = System.Windows.Forms.Control.MousePosition
  (mp.X, mp.Y)
</code></pre></div><p>Note that your application doesn&rsquo;t have to be based on WinForms, just reference
the required assembly.</p>
<h2 id="move-the-cursor">Move the Cursor</h2>
<p>I use the third party <a href="https://inputsimulator.codeplex.com/">WindowsInput</a>
library to control the mouse and the keyboard programmatically. It uses some
weird coordinate system, so the function to move the mouse cursor looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> simulator = <span style="color:#00f">new</span> InputSimulator()

<span style="color:#00f">let</span> moveTo x y =
  <span style="color:#00f">let</span> toX = 65535. * x / (Screen.PrimaryScreen.Bounds.Width |&gt; <span style="color:#2b91af">float</span>)
  <span style="color:#00f">let</span> toY = 65535. * y / (Screen.PrimaryScreen.Bounds.Height |&gt; <span style="color:#2b91af">float</span>)
  simulator.Mouse.MoveMouseTo(toX, toY)
</code></pre></div><p>The input parameters <code>x</code> and <code>y</code> are the pixel location starting at
the top-left corner of the screen.</p>
<h2 id="move-it-smoothly">Move It Smoothly</h2>
<p>Now we want to simulate the human-like movements. It won&rsquo;t be perfect, but
at least it should look decent. For this gradual movement function I used
a nice F# feature called asynchronous workflows. Effectively, it looks like
a loop with async sleep statements inside.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> moveToWorkflow step (toX, toY) = async {
  <span style="color:#00f">let</span> (fromX, fromY) = currentPosition()
  <span style="color:#00f">let</span> count = Math.Max(10, (Math.Abs (toX - fromX) + Math.Abs (toY - fromY)) / 20)
  <span style="color:#00f">for</span> i = 0 <span style="color:#00f">to</span> count <span style="color:#00f">do</span>
    <span style="color:#00f">let</span> x = step fromX toX count i |&gt; <span style="color:#2b91af">float</span>
    <span style="color:#00f">let</span> y = step fromY toY count i |&gt; <span style="color:#2b91af">float</span>
    moveTo x y
    <span style="color:#00f">do</span>! Async.Sleep 3
  }
</code></pre></div><p>The key parameter here is the <code>step</code> function of obscure type <code>int -&gt; int -&gt; int -&gt; int -&gt; int</code>.
Basically, it calculates a coordinate for n-th step of the movement. We can
plug different implementations of this function to find the right balance of
the movement style. Here is the simplest linear implementation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> linearStep from until max i =
  from + (until - from) * i / max
</code></pre></div><p>The sinus-based implementation is a bit more verbose because of float-int
conversions:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> sinStep (from:int) (until:int) (max:int) (index:int) =
  <span style="color:#00f">let</span> fromf = from |&gt; <span style="color:#2b91af">float</span>
  <span style="color:#00f">let</span> untilf = until |&gt; <span style="color:#2b91af">float</span>
  <span style="color:#00f">let</span> maxf = max |&gt; <span style="color:#2b91af">float</span>
  <span style="color:#00f">let</span> indexf = index |&gt; <span style="color:#2b91af">float</span>
  fromf + (untilf - fromf) * Math.Sin(Math.PI / 2. * indexf / maxf) |&gt; int
</code></pre></div><p>The following animation illustrates the concept:</p>
<svg width="778" height="190" viewBox="0 0 500 190">
  <image id="mouse1" x="0" y="20" width="16" height="16" xlink:href="/2016/03/building-a-poker-bot-mouse-movements/mouse_cursor-16.png" />
  <image id="mouse2" x="0" y="90" width="16" height="16" xlink:href="/2016/03/building-a-poker-bot-mouse-movements/mouse_cursor-16.png" />
  <image id="mouse3" x="0" y="160" width="16" height="16" xlink:href="/2016/03/building-a-poker-bot-mouse-movements/mouse_cursor-16.png" />
  <animate xlink:href="#mouse1" attributeName="x" from="0" to="0" values="0;450;0" keyTimes="0;0.5;1" repeatCount="indefinite" dur="2s" begin="0s" fill="none" calcMode="discrete" id="img-anim1"/>
  <animate xlink:href="#mouse2" attributeName="x" from="0" to="0" values="0;450;0" keyTimes="0;0.5;1" repeatCount="indefinite" dur="2s" begin="0s" fill="none" id="img-anim2"/>
  <animate xlink:href="#mouse3" attributeName="x" from="0" to="0" values="0;70;139;204;264;318;364;401;428;444;450;380;311;246;186;132;86;49;22;6;0" keyTimes="0;0.05;0.1;0.15;0.2;0.25;0.3;0.35;0.4;0.45;0.5;0.55;0.6;0.65;0.7;0.75;0.8;0.85;0.9;0.95;1" repeatCount="indefinite" dur="2s" begin="0s" fill="none" id="img-anim3"/>
</svg>
<p>The top mouse cursor just
jumps from left to right and back (no animation). The middle cursor moves with
linear speed (<code>linearStep</code> function above). The bottom cursor moves based on
the <code>sinStep</code> function derived from sinus of time.</p>
<h2 id="click-the-button">Click the Button</h2>
<p>A button is a rectangle and we want to click a random point inside it. So, all
we need is to pick random coordinates, move the mouse there and send a
click event via the simulator:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> clickButton (minX, minY, maxX, maxY) =
  <span style="color:#00f">let</span> r = <span style="color:#00f">new</span> Random()
  <span style="color:#00f">let</span> p = (r.Next(minX, maxX), r.Next(minY, maxY))
  moveToWorkflow sinStep p |&gt; Async.RunSynchronously
  simulator.Mouse.LeftButtonClick()
</code></pre></div><h2 id="demo-time">Demo Time</h2>
<p>Here is the demo of the mouse movements:</p>
<p><img src="mouseclicking.gif" alt="Mouse clicking the button"></p>
<p>It looks fun, doesn&rsquo;t it? The full code for the mouse movements can be found in
<a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/Clicker.fs">my github repo</a>.</p>
<p><em>Proceed to <a href="/2016/04/building-a-poker-bot-with-akka-net-actors/">Part 4 of Building a Poker Bot with Akka.NET Actor</a>.</em></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/poker-bot" term="poker-bot" label="Poker Bot" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[ORM]]></title>
            <link href="https://mikhail.io/tags/orm/"/>
            <id>https://mikhail.io/tags/orm/</id>
            
            <published>2016-02-23T00:00:00+00:00</published>
            <updated>2016-02-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Unit testing Dapper repositories]]></title>
            <link href="https://mikhail.io/2016/02/unit-testing-dapper-repositories/"/>
            <id>https://mikhail.io/2016/02/unit-testing-dapper-repositories/</id>
            
            <published>2016-02-23T00:00:00+00:00</published>
            <updated>2016-02-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Dapper is a micro-ORM library which is very simple and super fast. Quite often the data access code is difficult to be unit tested. In this post, I present some ideas of testing Dapper-based database access.</blockquote><p><a href="https://github.com/StackExchange/dapper-dot-net">Dapper</a> is a micro-ORM library which is
very simple and super fast. In our projects we use Dapper for the tasks where something like
EntityFramework or NHibernate would be an overkill.</p>
<p>Quite often the data access code is difficult to be unit tested. Objects like
database connections, commands, transactions and contexts are hard to mock, and
thus the data access code is not easily isolated. Dapper relies heavily on SQL
statements inside C# code, which gives an extra complication. Some people would
argue that unit tests are not warranted for data access layer, and integration
tests should be used instead. Let&rsquo;s have a look at another possibility.</p>
<h2 id="an-example-of-a-repository">An Example of a Repository</h2>
<p>Let&rsquo;s say we have a simple class and we want to populate instances of this class
from the database:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Product</span>
{
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Id { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Name { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> Description { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>To be able to use Dapper for data access, we need an instance of <code>IDbConnection</code>.
As we want to be able to mock the connection for unit tests, we need to create
a factory interface to abstract it away:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IDatabaseConnectionFactory
{
    IDbConnection GetConnection();
}
</code></pre></div><p>Now the repository would get a connection from this factory and execute
Dapper queries on it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ProductRepository</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> IDatabaseConnectionFactory connectionFactory;

    <span style="color:#00f">public</span> ProductRepository(IDatabaseConnectionFactory connectionFactory)
    {
        <span style="color:#00f">this</span>.connectionFactory = connectionFactory;
    }

    <span style="color:#00f">public</span> Task&lt;IEnumerable&lt;Product&gt;&gt; GetAll()
    {
        <span style="color:#00f">return</span> <span style="color:#00f">this</span>.connectionFactory.GetConnection().QueryAsync&lt;Product&gt;(
            <span style="color:#a31515">&#34;select * from Product&#34;</span>);
    }
}
</code></pre></div><h2 id="testing-without-a-real-database">Testing Without a real Database</h2>
<p>Here is my approach to testing the repository:</p>
<ol>
<li>Use an in-memory <a href="https://www.sqlite.org/">SQLite3</a> database.</li>
<li>Create a table there and put some data in.</li>
<li>Run the repository against this database.</li>
<li>Compare the result to the expected values.</li>
</ol>
<p>Here is a helper class which uses another micro-ORM library <a href="http://ormlite.com/">OrmLite</a> to talk
to SQLite database:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">InMemoryDatabase</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> OrmLiteConnectionFactory dbFactory =
        <span style="color:#00f">new</span> OrmLiteConnectionFactory(<span style="color:#a31515">&#34;:memory:&#34;</span>, SqliteOrmLiteDialectProvider.Instance);

    <span style="color:#00f">public</span> IDbConnection OpenConnection() =&gt; <span style="color:#00f">this</span>.dbFactory.OpenDbConnection();

    <span style="color:#00f">public</span> <span style="color:#00f">void</span> Insert&lt;T&gt;(IEnumerable&lt;T&gt; items)
    {
        <span style="color:#00f">using</span> (<span style="color:#2b91af">var</span> db = <span style="color:#00f">this</span>.OpenConnection())
        {
            db.CreateTableIfNotExists&lt;T&gt;();
            <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> item <span style="color:#00f">in</span> items)
            {
                db.Insert(item);
            }
        }
    }
}
</code></pre></div><p>And here is the test for our <code>ProductRepository</code> class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">async</span> Task QueryTest()
{
    <span style="color:#008000">// Arrange
</span><span style="color:#008000"></span>    <span style="color:#2b91af">var</span> products = <span style="color:#00f">new</span> List&lt;Product&gt;
    {
        <span style="color:#00f">new</span> Product { ... },
        <span style="color:#00f">new</span> Product { ... }
    };
    <span style="color:#2b91af">var</span> db = <span style="color:#00f">new</span> InMemoryDatabase();
    db.Insert(products);
    connectionFactoryMock.Setup(c =&gt; c.GetConnection()).Returns(db.OpenConnection());

    <span style="color:#008000">// Act
</span><span style="color:#008000"></span>    <span style="color:#2b91af">var</span> result = <span style="color:#00f">await</span> <span style="color:#00f">new</span> ProductRepository(connectionFactoryMock.Object).GetAll();

    <span style="color:#008000">// Assert
</span><span style="color:#008000"></span>    result.ShouldBeEquivalentTo(products);
}
</code></pre></div><h2 id="is-it-a-unit-test">Is It a Unit Test?</h2>
<p>Well, not completely. This approach does not mock the database, but instead puts
an in-memory database in place of the normal one. The problem is that we don&rsquo;t
control all the details how it works, so it might not be as flexible as we need.
For instance, SQLite type system is quite simplistic, so while <code>INT</code> and <code>BIGINT</code>
are different column types in SQL Server, they are the same <code>INTEGER</code> type in
SQLite. This can lead to false positive or false negative tests in edge cases.</p>
<p>Nevertheless, the concept is simple and requires very little amount of code,
so it&rsquo;s useful to have it in the toolbox anyway. The resulting tests are fast,
have no external dependencies and are always consistent between multiple runs.
That makes them better than real integration tests for the simple scenarios
during TDD development.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/unit-testing" term="unit-testing" label="Unit Testing" />
                             
                                <category scheme="https://mikhail.io/tags/dapper" term="dapper" label="Dapper" />
                             
                                <category scheme="https://mikhail.io/tags/orm" term="orm" label="ORM" />
                             
                                <category scheme="https://mikhail.io/tags/tdd" term="tdd" label="TDD" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Building a Poker Bot: String and Number Recognition]]></title>
            <link href="https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/"/>
            <id>https://mikhail.io/2016/02/building-a-poker-bot-string-recognition/</id>
            
            <published>2016-02-10T00:00:00+00:00</published>
            <updated>2016-02-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>This is the second part of Building a Poker Bot series where I describe my experience developing bot software to play in online poker rooms. Reading cards and other fixed images was the first step. The bot should also be able to read different text-based information from the screen.</blockquote><p>This is the second part of <strong>Building a Poker Bot</strong> series where I describe my experience developing bot software
to play in online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively easy and very enjoyable.</p>
<p>Here is the first part: <a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/">Building a Poker Bot: Card Recognition</a></p>
<h2 id="why-string-recognition">Why string recognition</h2>
<p>Reading cards and other fixed images was the first step. The bot should also
be able to read different text-based information from the screen, e.g.</p>
<ul>
<li>Current blind levels</li>
<li>Current pot size</li>
<li>The size of bets made by each player</li>
<li>Player names</li>
<li>Stack sizes</li>
<li>Chat messages (for advanced scenarios)</li>
</ul>
<p>We need this vital information to make proper decisions, so let&rsquo;s look at
how to parse the textual data.</p>
<h2 id="new-challenges">New challenges</h2>
<p>String recognition has some specific difficulties when compared to fixed
images like cards:</p>
<ul>
<li>The size of a string is not predefined. Obviously, the longer the string, the
more space it takes on the screen</li>
<li>The position of a string is not fixed either. Some strings are aligned to
the center, others may diverge based on other variable parts like stakes or blinds</li>
<li>Different strings might be rendered in different font size</li>
</ul>
<p>Here is what needs to be done to overcome these complications:</p>
<ul>
<li>Pick the layout which makes your life easier</li>
<li>Adjust fonts and positions if possible</li>
<li>Make sure that all important strings are always visible and not overlapping to other information</li>
<li>For each string define a region where it belongs to in 100% cases. The background
of this region should be more or less evenly filled with a color in contrast to the font color.</li>
</ul>
<h2 id="string-recognition-steps">String recognition steps</h2>
<p>We start with a screenshot of a poker table again:</p>
<p><img src="table.png" alt="Poker table screenshot"></p>
<p>We know our fixed regions where our labels are located, so we take those
regions for processing:</p>
<p><img src="regions.png" alt="Regions of string recognition"></p>
<p>For each region we trim away the blank margins around the text (i.e. left,
top, right and bottom padding):</p>
<p><img src="nomargin.png" alt="Margins being removed"></p>
<p>We find dark lines between bright symbols and we consider them as gaps
between characters:</p>
<p><img src="splitchars.png" alt="Split to characters"></p>
<p>The final step is to compare each symbol to the known patterns and find the best
match (in case of my layout the match for symbols is always 100% perfect). Let&rsquo;s
look how these steps are implemented.</p>
<h2 id="removing-padding-around-the-text">Removing padding around the text</h2>
<p>Because the padding is removed from all 4 sides of the region, I decided to use
<code>Array2D</code> data type to be able to iterate in different order. The whole algorithm operates
with black or white points defined as a helper type:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">BW</span> = B | W
</code></pre></div><p>So the <code>removePadding</code> function has type of <code>BW[,] -&gt; BW[,]</code> and looks
like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> removePadding pixels =
  <span style="color:#00f">let</span> allBlack s = Seq.exists ((=) W) s
  <span style="color:#00f">let</span> maxWidth = Array2D.length1 pixels - 1
  <span style="color:#00f">let</span> maxHeight = Array2D.length2 pixels - 1
  <span style="color:#00f">let</span> firstX = [0..maxWidth]
    |&gt; Seq.tryFindIndex (<span style="color:#00f">fun</span> y -&gt; allBlack pixels.[y, 0..maxHeight])
  <span style="color:#00f">let</span> lastX = [0..maxWidth]
    |&gt; Seq.tryFindIndexBack (<span style="color:#00f">fun</span> y -&gt; allBlack pixels.[y, 0..maxHeight])
  <span style="color:#00f">let</span> firstY = [0..maxHeight]
    |&gt; Seq.tryFindIndex (<span style="color:#00f">fun</span> x -&gt; allBlack pixels.[0..maxWidth, x])
  <span style="color:#00f">let</span> lastY = [0..maxHeight]
    |&gt; Seq.tryFindIndexBack (<span style="color:#00f">fun</span> x -&gt; allBlack pixels.[0..maxWidth, x])

  <span style="color:#00f">match</span> (firstX, lastX, firstY, lastY) <span style="color:#00f">with</span>
  | (Some fx, Some lx, Some fy, Some ly) -&gt; pixels.[fx..lx, fy..ly]
  | _ -&gt; Array2D.init 0 0 (<span style="color:#00f">fun</span> _ _ -&gt; B)
</code></pre></div><p>The first part finds the amount of fully-black columns and rows in the array.
Then, if white points are found, the second part returns a sub array based on
the indices, otherwise empty array is returned.</p>
<h2 id="split-the-text-into-characters">Split the text into characters</h2>
<p>First, we convert our 2D array into the list of lists, where each item in the
top-level list represents a single column of pixels:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> pixelColumns =
  [0..Array2D.length1 pixels - 1]
  |&gt; Seq.map (<span style="color:#00f">fun</span> x -&gt; pixels.[x, 0..Array2D.length2 pixels - 1] |&gt; List.ofArray)
</code></pre></div><p>Then we can fold this list of columns into the symbols, where each symbol itself
is the list of columns:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> splitIntoSymbols (e : BW <span style="color:#2b91af">list</span>) (state: BW <span style="color:#2b91af">list</span> <span style="color:#2b91af">list</span> <span style="color:#2b91af">list</span>) =
  <span style="color:#00f">match</span> state <span style="color:#00f">with</span>
  | cur::rest -&gt;
      <span style="color:#00f">if</span> isSeparator e <span style="color:#00f">then</span>
        <span style="color:#00f">match</span> cur <span style="color:#00f">with</span>
        | _::_ -&gt; []::state <span style="color:#008000">// add new list
</span><span style="color:#008000"></span>        | _ -&gt; state        <span style="color:#008000">// skip if we already have empty item
</span><span style="color:#008000"></span>      <span style="color:#00f">else</span> (e::cur)::rest   <span style="color:#008000">// add e to current list
</span><span style="color:#008000"></span>  | _ -&gt; [[e]]

Seq.foldBack splitIntoSymbols pixelColumns []
</code></pre></div><p>The type of <code>state</code> is a bit of brain teaser, I guess it could be improved
by introducing some intermediate type with descriptive name, but I decided
to leave that part for now. Read it as list of symbols, which are lists of
columns, which are lists of pixels.</p>
<h2 id="match-the-symbols-vs-the-known-patterns">Match the symbols vs the known patterns</h2>
<p>This part was already described in <a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/">my first article</a>.
Basically we compare the list of black or white points to the patterns of
the known symbols:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getChar patterns bws =
  <span style="color:#00f">let</span> samePatterns h p =
    Seq.zip h p
    |&gt; Seq.forall (<span style="color:#00f">fun</span> (v1, v2) -&gt; v1 = v2)
  <span style="color:#00f">let</span> matchingPattern =
    patterns
      |&gt; Array.filter (<span style="color:#00f">fun</span> p -&gt; List.length p.Pattern = List.length bws)
      |&gt; Array.filter (<span style="color:#00f">fun</span> p -&gt; samePatterns bws p.Pattern)
      |&gt; Array.tryHead
  defaultArg (Option.map (<span style="color:#00f">fun</span> p -&gt; p.Char) matchingPattern) <span style="color:#a31515">&#39;?&#39;</span>
</code></pre></div><h2 id="putting-it-all-together">Putting it all together</h2>
<p>The <code>recognizeString</code> function accepts lower-order functions to match
symbols and get pixels together with width and height of the region:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp">recognizeString: (BW <span style="color:#2b91af">list</span> <span style="color:#2b91af">list</span> -&gt; <span style="color:#2b91af">char</span>) -&gt; (int -&gt; int -&gt; color) -&gt; int -&gt; int -&gt; <span style="color:#2b91af">string</span>
</code></pre></div><p>It builds an array of pixels, removes padding and folds with recognition.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> recognizeString matchSymbol getPixel width height =

  <span style="color:#00f">let</span> pixels =
    Array2D.init width height (<span style="color:#00f">fun</span> x y -&gt; isWhite (getPixel x y))
    |&gt; removePadding

  <span style="color:#00f">let</span> pixelColumns =
    [0..Array2D.length1 pixels - 1]
    |&gt; Seq.map (<span style="color:#00f">fun</span> x -&gt; pixels.[x, 0..Array2D.length2 pixels - 1] |&gt; List.ofArray)

  Seq.foldBack splitIntoSymbols pixelColumns []
  |&gt; List.map matchSymbol
  |&gt; Array.ofSeq
  |&gt; String.Concat
</code></pre></div><p>Then we use it with a specific recognition patterns, e.g. known digits in case
of numbers recognition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> recognizeNumber x =
  recognizeString (getChar numberPatterns) x
</code></pre></div><p>A way to produce these patterns is discussed in <a href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/">the previous part</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>String recognition takes a bit more steps to execute comparing to the recognition
of fixed objects. Nevertheless it&rsquo;s pretty straightforward to implement once
we split it into small and well-understood conversion steps. The full code
for card recognition can be found in <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/StringRecognition.fs">my github repo</a>.</p>
<p><em>Proceed to <a href="/2016/03/building-a-poker-bot-mouse-movements/">Part 3 of Building a Poker Bot: Mouse Movements</a>.</em></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/poker-bot" term="poker-bot" label="Poker Bot" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/image-recognition" term="image-recognition" label="Image Recognition" />
                             
                                <category scheme="https://mikhail.io/tags/ocr" term="ocr" label="OCR" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Image Recognition]]></title>
            <link href="https://mikhail.io/tags/image-recognition/"/>
            <id>https://mikhail.io/tags/image-recognition/</id>
            
            <published>2016-02-10T00:00:00+00:00</published>
            <updated>2016-02-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[OCR]]></title>
            <link href="https://mikhail.io/tags/ocr/"/>
            <id>https://mikhail.io/tags/ocr/</id>
            
            <published>2016-02-10T00:00:00+00:00</published>
            <updated>2016-02-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Akka.NET-style actors in Service Fabric]]></title>
            <link href="https://mikhail.io/2016/02/akka-net-style-actors-in-service-fabric/"/>
            <id>https://mikhail.io/2016/02/akka-net-style-actors-in-service-fabric/</id>
            
            <published>2016-02-08T00:00:00+00:00</published>
            <updated>2016-02-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Akka.NET and Service Fabric are the two actor frameworks that emerged in .NET world in the last year. The two implementations of actor models are quite different. These differences are multi-faceted but today I want to focus on API to define an actor and to communicate to it.</blockquote><p>Akka.NET and Service Fabric are the two actor frameworks that emerged in .NET world in the last year.
The two implementations of actor models are quite different. These differences are multi-faceted but
today I want to focus on API to define an actor and to communicate to it.</p>
<h2 id="service-fabric-actors">Service Fabric Actors</h2>
<p>Every actor in Service Fabric has a public interface which describes its behaviour. For this article
I&rsquo;m going to use a toy example based on weather reports. Our actor will be able to get whether reports
and then return the maximum temperature for a given period. An instance of actor will be created
for each city (geo partitioning). Here is our interface in Service Fabric:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IWeatherActor : IActor
{
    Task AddWeatherReport(WeatherReport report);

    Task&lt;<span style="color:#2b91af">int?</span>&gt; GetMaxTemperature(Period period);
}
</code></pre></div><p>We have two operations: a command and a query. They are both async (return <code>Task</code>). The data classes
are required to be mutable DTOs based on <code>DataContract</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[DataContract]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WeatherReport</span>
{
    [DataMember]
    <span style="color:#00f">public</span> DateTime Moment { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    [DataMember]
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Temperature { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    [DataMember]
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Humidity { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}

[DataContract]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Period</span>
{
    [DataMember]
    <span style="color:#00f">public</span> DateTime From { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    [DataMember]
    <span style="color:#00f">public</span> DateTime Until { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
</code></pre></div><p>And here is the implementation of the weather actor:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">internal</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WeatherActor</span> : StatefulActor&lt;List&lt;WeatherReport&gt;&gt;, IWeatherActor
{
    <span style="color:#00f">public</span> Task AddWeatherReport(WeatherReport report)
    {
        <span style="color:#00f">this</span>.State = <span style="color:#00f">this</span>.State ?? <span style="color:#00f">new</span> List&lt;WeatherReport&gt;();
        <span style="color:#00f">this</span>.State.Add(report);
        <span style="color:#00f">return</span> Task.FromResult(0);
    }

    <span style="color:#00f">public</span> Task&lt;<span style="color:#2b91af">int?</span>&gt; GetMaxTemperature(Period period)
    {
        <span style="color:#00f">return</span> Task.FromResult(
            (<span style="color:#00f">this</span>.State ?? Enumerable.Empty&lt;WeatherReport&gt;())
            .Where(r =&gt; r.Moment &gt; period.From &amp;&amp; r. Moment &lt;= period.Until)
            .Max(r =&gt; (<span style="color:#2b91af">int?</span>)r.Temperature));
    }
}
</code></pre></div><p>Service Fabric provides reliable storage out of the box, so we are using it to
store our reports. There&rsquo;s no code required to instantiate an actor. Here is the
code to use it:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Submit a new report
</span><span style="color:#008000"></span>IWeatherActor actor = ActorProxy.Create&lt;IWeatherActor&gt;(<span style="color:#00f">new</span> ActorId(<span style="color:#a31515">&#34;Amsterdam&#34;</span>));
actor.AddWeatherReport(
    <span style="color:#00f">new</span> WeatherReport { Moment = DateTime.Now, Temperature = 22, Humidity = 55 });

<span style="color:#008000">// Make a query somewhere else
</span><span style="color:#008000"></span>IWeatherActor actor = ActorProxy.Create&lt;IWeatherActor&gt;(<span style="color:#00f">new</span> ActorId(<span style="color:#a31515">&#34;Amsterdam&#34;</span>));
<span style="color:#2b91af">var</span> result = actor.GetMaxTemperature(<span style="color:#00f">new</span> Period { From = monthAgo, Until = now });
</code></pre></div><h2 id="akkanet-actors">Akka.NET Actors</h2>
<p>Actors in Akka.NET are message-based. The messages are immutable POCOs, which
is a great design decision. Here are the messages for our scenario:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WeatherReport</span>
{
    <span style="color:#00f">public</span> WeatherReport(DateTime moment, <span style="color:#2b91af">int</span> temperature, <span style="color:#2b91af">int</span> humidity)
    {
        <span style="color:#00f">this</span>.Moment = moment;
        <span style="color:#00f">this</span>.Temperature = temperature;
        <span style="color:#00f">this</span>.Humidity = humidity;
    }

    <span style="color:#00f">public</span> DateTime Moment { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Temperature { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> <span style="color:#2b91af">int</span> Humidity { <span style="color:#00f">get</span>; }
}

<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Period</span>
{
    <span style="color:#00f">public</span> Period(DateTime <span style="color:#00f">from</span>, DateTime until)
    {
        <span style="color:#00f">this</span>.From = <span style="color:#00f">from</span>;
        <span style="color:#00f">this</span>.Until = until;
    }

    <span style="color:#00f">public</span> DateTime From { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> DateTime Until { <span style="color:#00f">get</span>; }
}

</code></pre></div><p>There&rsquo;s no need to define any interfaces. The basic actor implementation derives from
<code>ReceiveActor</code> and calls <code>Receive</code> generic method to setup a callback which is called
when a message of specified type is received:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WeatherActor</span> : ReceiveActor
{
    <span style="color:#00f">private</span> List&lt;WeatherReport&gt; state = <span style="color:#00f">new</span> List&lt;WeatherReport&gt;();

    <span style="color:#00f">public</span> WeatherActor()
    {
        Receive&lt;WeatherReport&gt;(<span style="color:#00f">this</span>.AddWeatherReport);
        Receive&lt;Period&gt;(<span style="color:#00f">this</span>.GetMaxTemperature);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">void</span> AddWeatherReport(WeatherReport report)
    {
        <span style="color:#00f">this</span>.state.Add(report);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">void</span> GetMaxTemperature(Period period)
    {
        <span style="color:#2b91af">var</span> response = <span style="color:#00f">this</span>.state
            .Where(r =&gt; r.Moment &gt; period.From &amp;&amp; r. Moment &lt;= period.Until)
            .Max(r =&gt; (<span style="color:#2b91af">int?</span>)r.Temperature);
        Sender.Tell(response, Self);
    }
}
</code></pre></div><p>Note a couple more differences in this implementation comparing to Fabric style:</p>
<ul>
<li>
<p>State is stored in a normal class field and is not persistent or replicated
by default. This can be solved by Akka.NET Persistence, which would save all
messages (and potentially snapshots) to the external database. Still, it won&rsquo;t
be the same level of convenience as in-built Service Fabric statefullness.</p>
</li>
<li>
<p><code>GetMaxTemperature</code> method does not return anything, because nobody would look
at the returned value. Instead, it sends yet another message to the sender actor.
So, <code>Request-Response</code> workflow is supported but is a bit less convenient and
explicit.</p>
</li>
</ul>
<p>Let&rsquo;s have a look at the client code. <code>ActorSelection</code> is the closest notion to
Fabric&rsquo;s <code>ActorProxy</code>: it does not create an actor, but just gets an endpoint
based on the name. Note that Akka.NET actor needs to be explicitly created by
another actor, but lifetime management is a separate discussion, so we&rsquo;ll skip
it for now. Here is the report sender:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Submit a new report
</span><span style="color:#008000"></span><span style="color:#2b91af">var</span> msg = <span style="color:#00f">new</span> WeatherReport { Moment = DateTime.Now, Temperature = 22, Humidity = 55 };
Context.ActorSelection(<span style="color:#a31515">&#34;/user/weather/Amsterdam&#34;</span>).Tell(msg);
</code></pre></div><p>Asking <code>ActorSelection</code> is not directly possible, we would need to setup an
inbox and receive callback messages. We&rsquo;ll pretend that we have an <code>ActorRef</code>
for the sake of simplicity:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Make a query somewhere else
</span><span style="color:#008000"></span>ActoRef actor = ... ; <span style="color:#008000">// we have it
</span><span style="color:#008000"></span><span style="color:#2b91af">var</span> result = <span style="color:#00f">await</span> actor.Ask(<span style="color:#00f">new</span> Period { From = monthAgo, Until = now });
</code></pre></div><h2 id="the-best-of-two-worlds">The Best of Two Worlds</h2>
<p>Now my goals is to come up with an implementation of Service Fabric actors with
the properties that combine the good parts of both frameworks (without explicitly
using Akka.NET), i.e.</p>
<ul>
<li>Use the full power of Service Fabric actors, including lifetime management,
cluster management and reliable state</li>
<li>Use the simplicity of Request-Response pattern implementation of Service Fabric</li>
<li>Support immutable POCO messages instead of <code>DataContract</code> DTOs</li>
<li>Use <code>ReceiveActor</code>-like API for message processing</li>
</ul>
<p>Here is the third implementation of our Weather Actor (the definitions of messages
from Akka.NET example are intact):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[ActorService(Name = &#34;WeatherActor&#34;)]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">WeatherActor</span> : StetefulReceiveActor&lt;List&lt;WeatherReport&gt;&gt;
{
    <span style="color:#00f">public</span> WeatherActor()
    {
        Receive&lt;WeatherReport&gt;(<span style="color:#00f">this</span>.AddWeatherReport);
        Receive&lt;Period, <span style="color:#2b91af">int</span>&gt;(<span style="color:#00f">this</span>.GetMaxTemperature);
    }

    <span style="color:#00f">public</span> Task&lt;List&lt;WeatherReport&gt;&gt; AddWeatherReport(
        List&lt;WeatherReport&gt; state, WeatherReport report)
    {
        state = state ?? <span style="color:#00f">new</span> List&lt;WeatherReport&gt;();
        state.Add(report);
        <span style="color:#00f">return</span> Task.FromResult(state);
    }

    <span style="color:#00f">public</span> Task&lt;<span style="color:#2b91af">int?</span>&gt; GetMaxTemperature(List&lt;WeatherReport&gt; state, Period period)
    {
        <span style="color:#00f">return</span> Task.FromResult(
            (state ?? Enumerable.Empty&lt;WeatherReport&gt;())
            .Where(r =&gt; r.Moment &gt; period.From &amp;&amp; r. Moment &lt;= period.Until)
            .Max(r =&gt; (<span style="color:#2b91af">int?</span>)r.Temperature));
    }
}
</code></pre></div><p>The base <code>ReceiveActor</code> class is not defined yet, we&rsquo;ll do it in the next section. Here is
how it&rsquo;s being used:</p>
<ul>
<li>The base class is generic and it accepts the type of the state (similar to normal Fabric actors)</li>
<li>Constructor registers two <code>Receive</code> handlers: message handler and request handler. Note
that the later one accepts two type parameters: request type and response type</li>
<li>Both handlers get the current state as the first argument instead of pulling it from the property of
the base class</li>
<li>The both return <code>Task</code>&lsquo;ed data. Message handler is allowed to change the state, while
request handler does  not change the state but just returns the response back</li>
<li><code>ServiceName</code> attribute is required because there are (may be) multiple classes implementing
the same interface</li>
</ul>
<p>The client code uses our own <code>MessageActorProxy</code> class to create non-generic proxies which
are capable to <code>Tell</code> (send a message one way) and <code>Ask</code> (do request and wait for response):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Submit a new report
</span><span style="color:#008000"></span><span style="color:#2b91af">var</span> actor = MessageActorProxy.Create(<span style="color:#00f">new</span> ActorId(<span style="color:#a31515">&#34;Amsterdam&#34;</span>), <span style="color:#a31515">&#34;WeatherActor&#34;</span>);
actor.Tell(<span style="color:#00f">new</span> WeatherReport { Moment = DateTime.Now, Temperature = 22, Humidity = 55 });

<span style="color:#008000">// Make a query somewhere else
</span><span style="color:#008000"></span><span style="color:#2b91af">var</span> actor = MessageActorProxy.Create(<span style="color:#00f">new</span> ActorId(<span style="color:#a31515">&#34;Amsterdam&#34;</span>), <span style="color:#a31515">&#34;WeatherActor&#34;</span>);
<span style="color:#2b91af">var</span> result = actor.Ask(<span style="color:#00f">new</span> Period { From = monthAgo, Until = now });
</code></pre></div><h2 id="implementation-of-receiveactor">Implementation of ReceiveActor</h2>
<p>Let&rsquo;s start with the interface definition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IReceiveActor : IActor
{
    Task Tell(<span style="color:#2b91af">string</span> typeName, <span style="color:#2b91af">byte</span>[] message);

    [Readonly]
    Task&lt;<span style="color:#2b91af">byte</span>[]&gt; Ask(<span style="color:#2b91af">string</span> typeName, <span style="color:#2b91af">byte</span>[] message);
}
</code></pre></div><p>The two methods for <code>Tell</code> and <code>Ask</code> accept serializes data together with fully qualified
type name. This will allow passing any kind of objects which can be handled by a serializer
of choice (I used Newtonsoft JSON serializer).</p>
<p>Actor implementation derives from <code>StatefulActor</code> and uses another type/bytes pair to store
the serialized state:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">    <span style="color:#00f">public</span> <span style="color:#00f">abstract</span> <span style="color:#00f">class</span> <span style="color:#2b91af">StatefulReceiveActor</span> : StatefulActor&lt;StateContainer&gt;,
                                                 IReceiveActor
    {
        <span style="color:#008000">// ...
</span><span style="color:#008000"></span>    }

    [DataContract]
    <span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">StateContainer</span>
    {
        [DataMember]
        <span style="color:#00f">public</span> <span style="color:#2b91af">string</span> TypeName { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }

        [DataMember]
        <span style="color:#00f">public</span> <span style="color:#2b91af">byte</span>[] Data { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    }
</code></pre></div><p>The simplistic implementation of <code>Receive</code> generic methods uses two dictionaries
to store the handlers:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> Dictionary&lt;Type, Func&lt;<span style="color:#2b91af">object</span>, <span style="color:#2b91af">object</span>, Task&lt;<span style="color:#2b91af">object</span>&gt;&gt;&gt; handlers;
<span style="color:#00f">private</span> Dictionary&lt;Type, Func&lt;<span style="color:#2b91af">object</span>, <span style="color:#2b91af">object</span>, Task&lt;<span style="color:#2b91af">object</span>&gt;&gt;&gt; askers;

<span style="color:#00f">public</span> ReceiveActor()
{
    <span style="color:#00f">this</span>.handlers = <span style="color:#00f">new</span> Dictionary&lt;Type, Func&lt;<span style="color:#2b91af">object</span>, <span style="color:#2b91af">object</span>, Task&lt;<span style="color:#2b91af">object</span>&gt;&gt;&gt;();
    <span style="color:#00f">this</span>.askers = <span style="color:#00f">new</span> Dictionary&lt;Type, Func&lt;<span style="color:#2b91af">object</span>, <span style="color:#2b91af">object</span>, Task&lt;<span style="color:#2b91af">object</span>&gt;&gt;&gt;();
}

<span style="color:#00f">protected</span> <span style="color:#00f">void</span> Receive&lt;T&gt;(Func&lt;<span style="color:#2b91af">object</span>, T, Task&lt;<span style="color:#2b91af">object</span>&gt;&gt; handler)
    =&gt; <span style="color:#00f">this</span>.handlers.Add(<span style="color:#00f">typeof</span>(T), <span style="color:#00f">async</span> (s, m) =&gt; <span style="color:#00f">await</span> handler(s, (T)m));

<span style="color:#00f">protected</span> <span style="color:#00f">void</span> Receive&lt;TI, TO&gt;(Func&lt;<span style="color:#2b91af">object</span>, TI, Task&lt;TO&gt;&gt; asker)
    =&gt; <span style="color:#00f">this</span>.askers.Add(<span style="color:#00f">typeof</span>(TI), <span style="color:#00f">async</span> (s, m) =&gt; <span style="color:#00f">await</span> asker(s, (TI)m));

</code></pre></div><p>The <code>Tell</code> method deserializes the message and state, then picks a handler based on
the message type, executes it and serializes the produced state back:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">async</span> Task Tell(<span style="color:#2b91af">string</span> typeName, <span style="color:#2b91af">byte</span>[] message)
{
    <span style="color:#2b91af">var</span> type = Type.GetType(typeName);
    <span style="color:#2b91af">var</span> typedMessage = <span style="color:#00f">this</span>.serializer.Deserialize(message, type);

    <span style="color:#2b91af">var</span> typedState = <span style="color:#00f">this</span>.State != <span style="color:#00f">null</span>
        ? <span style="color:#00f">this</span>.serializer.Deserialize(<span style="color:#00f">this</span>.State.Data, Type.GetType(<span style="color:#00f">this</span>.State.TypeName))
        : <span style="color:#00f">null</span>;
    <span style="color:#2b91af">var</span> handler = <span style="color:#00f">this</span>.handlers.FirstOrDefault(t =&gt; t.Key.IsAssignableFrom(type)).Value;
    <span style="color:#00f">if</span> (handler != <span style="color:#00f">null</span>)
    {
        <span style="color:#2b91af">var</span> newState = <span style="color:#00f">await</span> handler(typedState, typedMessage);
        <span style="color:#00f">this</span>.State =
            newState != <span style="color:#00f">null</span>
            ? <span style="color:#00f">new</span> StateContainer
              {
                  Data = <span style="color:#00f">this</span>.serializer.Serialize(newState),
                  TypeName = newState.GetType().AssemblyQualifiedName
              }
            : <span style="color:#00f">null</span>;
    }
}
</code></pre></div><p>The implementation of <code>Ask</code> is almost identical, so I&rsquo;ll skip it. <code>MessageActorProxy</code>
encapsulates the serialization around passing data to normal <code>ActorProxy</code> class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">MessageActorProxy</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> IStatefulMessageActor proxy;
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> ISerializer serializer = <span style="color:#00f">new</span> JsonByteSerializer();

    <span style="color:#00f">private</span> MessageActorProxy(ActorId actorId, <span style="color:#2b91af">string</span> serviceName)
    {
        <span style="color:#00f">this</span>.proxy = ActorProxy.Create&lt;IReceiveActor&gt;(actorId, serviceName: serviceName);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">async</span> Task Tell(<span style="color:#2b91af">object</span> message)
    {
        <span style="color:#2b91af">var</span> serialized = <span style="color:#00f">this</span>.serializer.Serialize(message);
        <span style="color:#00f">await</span> <span style="color:#00f">this</span>.proxy.Send(message.GetType().AssemblyQualifiedName, serialized);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">async</span> Task&lt;T&gt; Ask&lt;T&gt;(<span style="color:#2b91af">object</span> message)
    {
        <span style="color:#2b91af">var</span> serialized = <span style="color:#00f">this</span>.serializer.Serialize(message);
        <span style="color:#2b91af">var</span> fullName = message.GetType().AssemblyQualifiedName;
        <span style="color:#2b91af">var</span> response = <span style="color:#00f">await</span> <span style="color:#00f">this</span>.proxy.Ask(fullName, serialized);
        <span style="color:#00f">return</span> (T)<span style="color:#00f">this</span>.serializer.Deserialize(response, <span style="color:#00f">typeof</span>(T));
    }

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> MessageActorProxy Create(ActorId actorId, <span style="color:#2b91af">string</span> serviceType)
    {
        <span style="color:#00f">return</span> <span style="color:#00f">new</span> MessageActorProxy(actorId, serviceType);
    }
}
</code></pre></div><p>Let&rsquo;s briefly wrap it up.</p>
<h2 id="conclusion">Conclusion</h2>
<p>At this stage Azure Service Fabric lacks support of some actor model best practices
like message-based API and immutable POCO classes. At the same time, it provides
super powerful setup regarding cluster resource management, state replication, fault
tolerance and reliable communication. We can borrow some approaches that are used in Akka.NET
framework to improve the developer experience who wants to leverage the power
of Service Fabric.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/service-fabric" term="service-fabric" label="Service Fabric" />
                             
                                <category scheme="https://mikhail.io/tags/akka.net" term="akka.net" label="Akka.NET" />
                             
                                <category scheme="https://mikhail.io/tags/actor-model" term="actor-model" label="Actor Model" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Service Fabric]]></title>
            <link href="https://mikhail.io/tags/service-fabric/"/>
            <id>https://mikhail.io/tags/service-fabric/</id>
            
            <published>2016-02-08T00:00:00+00:00</published>
            <updated>2016-02-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Building a Poker Bot: Card Recognition]]></title>
            <link href="https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/"/>
            <id>https://mikhail.io/2016/02/building-a-poker-bot-card-recognition/</id>
            
            <published>2016-02-01T00:00:00+00:00</published>
            <updated>2016-02-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>This is the first part of Building a Poker Bot series where I describe my experience developing bot software for online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively easy and very enjoyable.</blockquote><p>This is the first part of <strong>Building a Poker Bot</strong> series where I describe my experience developing bot software
for online poker rooms. I&rsquo;m building the bot with .NET framework and F# language which makes the task relatively
easy and very enjoyable.</p>
<h2 id="screen-recognition">Screen recognition</h2>
<p>For a human, the very first step to the ability to play poker is to understand the cards, what a hand is and
what the value of your hand is. E.g. in Texas Holdem each player gets 2 hole cards which form a hand. At
the showdown the player with the best hand wins.</p>
<p>Poker bots are no different, they also need to be taught the notion of cards and hands. A bot should &ldquo;watch&rdquo;
the table and see which cards he is dealt with. There are several ways to achieve that but I go for a technique
called screen recognition, i.e. the bot makes a screenshot of a table and then reads the pixels to understand
what&rsquo;s going on. Very similar to what people do.</p>
<p>Image recognition in general is a tough task. Human beings are very good at interpreting vague images and
recognizing familiar objects. It&rsquo;s much more difficult for computers. General image recognition (think showing
a photo to your computer and asking whether there is an animal there) is very tough; corporations like Google
and Microsoft are spending numerous man-years and employ techniques like machine learning and neural networks.</p>
<p>Fortunately, poker table recognition is much easier. The images to be recognized are machine-generated, so
the same things are rendered more or less the same way all the time. It makes sense to keep the poker table
size fixed to some predefined value which makes recognition task fairly easy.</p>
<h2 id="card-recognition-steps">Card recognition steps</h2>
<p>There are 13 card faces (from Deuce to Ace) and 4 suits. All of them are just fixed-size images which we need to be able to
match with. So we start with a screenshot of a poker table:</p>
<p><img src="table.png" alt="Poker table screenshot"></p>
<p>The table size is fixed, so are the left and the top pixel positions of hole cards. So, our first step is to extract
the small images of cards out of the big screenshot:</p>
<p><img src="cards.png" alt="Extracted card images"></p>
<p>Now, we can take the recognition of card faces and suits separately. In our sample layout, suits are color coded.
This is very friendly to humans and super simple for the bot. We pick the suit based on the color (ignoring
the white pixels):</p>
<p><img src="suits.png" alt="Recognized suits"></p>
<p>This leaves us with the task of choosing between 13 card faces. The color information is not important
here, we can make the image grey-scale. Moreover, we can reduce the color information to the single bit per
pixel - call it white or black:</p>
<p><img src="blackandwhite.png" alt="Black and white pixels"></p>
<p>Now this mask is very simple, and we can compare it with 13 predefined masks for 13 cards pixel by pixel.
The one with the biggest amount of matches wins.</p>
<h2 id="suit-recognition">Suit recognition</h2>
<p>Let&rsquo;s put some code at the table. We start with suit recognition. <code>getSuit</code> function has type
<code>Color -&gt; string option</code> and converts the color of a pixel into the suit name, if possible. Hearts (&ldquo;h&rdquo;)
are red, Diamonds (&ldquo;d&rdquo;) are blue, Clubs (&ldquo;c&rdquo;) are green and Spades (&ldquo;s&rdquo;) are black:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getSuit (c : Color) =
  <span style="color:#00f">match</span> c <span style="color:#00f">with</span>
  | _ <span style="color:#00f">when</span> c.B &lt; 127uy &amp;&amp; c.G &lt; 127uy &amp;&amp; c.R &gt; 127uy -&gt; Some <span style="color:#a31515">&#34;h&#34;</span>
  | _ <span style="color:#00f">when</span> c.B &gt; 127uy &amp;&amp; c.G &lt; 127uy &amp;&amp; c.R &lt; 127uy -&gt; Some <span style="color:#a31515">&#34;d&#34;</span>
  | _ <span style="color:#00f">when</span> c.B &lt; 127uy &amp;&amp; c.G &gt; 127uy &amp;&amp; c.R &lt; 127uy -&gt; Some <span style="color:#a31515">&#34;c&#34;</span>
  | _ <span style="color:#00f">when</span> c.B &lt; 127uy &amp;&amp; c.G &lt; 127uy &amp;&amp; c.R &lt; 127uy -&gt; Some <span style="color:#a31515">&#34;s&#34;</span>
  | _ -&gt; None
</code></pre></div><p>This function is used by <code>getCardSuit</code> function of type <code>(int -&gt; int -&gt; Color) -&gt; int -&gt; int -&gt; string</code>.
Its first argument is a function which returns the color of a pixel based on <code>(x, y)</code>
relative coordinates (starting with 0). The next two arguments are width and height of the cards. Result is
the same suit name that we described above. The function loops through all the pixels, gets a suit per
pixel and then returns the suit which is the most popular among them. Alternatively, we could just return
the first suit found, but my implementation looks more resilient:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getCardSuit getPixel width height =
  seq { <span style="color:#00f">for</span> x <span style="color:#00f">in</span> 0 .. width - 1 <span style="color:#00f">do</span>
          <span style="color:#00f">for</span> y <span style="color:#00f">in</span> 0 .. height - 1 <span style="color:#00f">do</span>
            <span style="color:#00f">yield</span> getSuit (getPixel x y) }
  |&gt; Seq.choose id
  |&gt; Seq.countBy id
  |&gt; Seq.maxBy (<span style="color:#00f">fun</span> (v, c) -&gt; c)
  |&gt; fst
</code></pre></div><h2 id="producing-the-black--white-pattern">Producing the black &amp; white pattern</h2>
<p><code>getCardPattern</code> accepts the same parameters as <code>getSuits</code> but returns <code>seq&lt;BW&gt;</code> instead. This is
a sequence of black or white pixels with a helper union type:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">BW</span> = B | W
</code></pre></div><p>The function body enumerates the pixels and return black or white result as a flat sequence:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getCardPattern getPixel width height =
  <span style="color:#00f">let</span> isWhite (c : Color) =
    <span style="color:#00f">if</span> c.B &gt; 127uy &amp;&amp; c.G &gt; 127uy &amp;&amp; c.R &gt; 127uy <span style="color:#00f">then</span> W
    <span style="color:#00f">else</span> B

  seq { <span style="color:#00f">for</span> x <span style="color:#00f">in</span> 0 .. width - 1 <span style="color:#00f">do</span>
          <span style="color:#00f">for</span> y <span style="color:#00f">in</span> 0 .. height - 1 <span style="color:#00f">do</span>
            <span style="color:#00f">yield</span> isWhite (getPixel x y) }
</code></pre></div><h2 id="card-face-recognition">Card face recognition</h2>
<p>Having a black and white pattern, we can compare it with the predefined patterns and pick the
most similar one. A pattern is defined with a helper type</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">CardPattern</span> = {
  Card: <span style="color:#2b91af">string</span>
  Pattern: BW array
}
</code></pre></div><p><code>Pattern</code> is a sequence which is equivalent to the sequence we got on the previous step.
<code>Card</code> is a string of hand face value 2, 3, 4 .. A. <code>getCardFace</code> has the type
<code>CardPattern[] -&gt; seq&lt;BW&gt; -&gt; string</code>, it accepts an array of known patterns and a pattern
of the card to be recognized. It compares patterns pixel by pixel and returns the card
which has the biggest amount of matches:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> getCardFace patterns bws =
  <span style="color:#00f">let</span> matchCount h p =
    Seq.zip h p
    |&gt; Seq.map (<span style="color:#00f">fun</span> (v1, v2) -&gt; <span style="color:#00f">if</span> v1 = v2 <span style="color:#00f">then</span> 1 <span style="color:#00f">else</span> 0)
    |&gt; Seq.sum
    |&gt; <span style="color:#2b91af">decimal</span>
  <span style="color:#00f">let</span> maxPattern = patterns |&gt; Array.maxBy (<span style="color:#00f">fun</span> p -&gt; matchCount bws p.Pattern)
  maxPattern.Card
</code></pre></div><h2 id="getting-the-known-patterns">Getting the known patterns</h2>
<p>So how do we create an array of known patterns? It&rsquo;s tedious to do manually, so
we use a bit of code generation.
Basically we just take several screenshots of poker tables and feed them to the following
helper function:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> parsePattern getPixel width height =
  getCardPattern getPixel width height
  |&gt; Seq.map (<span style="color:#00f">fun</span> x -&gt; <span style="color:#00f">if</span> x = B <span style="color:#00f">then</span> <span style="color:#a31515">&#34;B&#34;</span> <span style="color:#00f">else</span> <span style="color:#a31515">&#34;W&#34;</span>)
  |&gt; String.concat <span style="color:#a31515">&#34;;&#34;</span>
</code></pre></div><p>The function creates a string which can be copy-pasted into F# array of <code>BW</code>.</p>
<h2 id="putting-it-all-together">Putting it all together</h2>
<p>Here is the facade function that will be called from the outside:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> recognizeCard getPixel width height =
  <span style="color:#00f">let</span> value =
    getCardPattern getPixel width height
    |&gt; getCardValue patterns
  <span style="color:#00f">let</span> suit = getCardSuit getPixel width height
  value + suit
</code></pre></div><p>The calling code looks like this:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">let</span> image = <span style="color:#00f">new</span> Bitmap(<span style="color:#a31515">&#34;...&#34;</span>);

<span style="color:#00f">let</span> getPixel offsetX offsetY x y =
  image.GetPixel(offsetX + x, offsetY + y)

<span style="color:#00f">let</span> hand = (recognizeCard (getPixel leftX top) width height) + (recognizeCard (getPixel rightX top) width height)
</code></pre></div><p><code>leftX</code>, <code>rightX</code>, <code>top</code>, <code>width</code> and <code>height</code> are well-known parameters of cards locations within a screenshot,
which are hard coded for a given table size.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The full code for card recognition can be found in my <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/HandRecognition.fs">github repo</a>. It&rsquo;s just 75 lines of code which is
much less that one could imagine for a task of image recognition. Similar code could be used to recognize other
fixed objects at poker table: dealer button location, action buttons, checkboxes etc. In the next part of this
series I will show how to recognize non-fixed parts: text and numbers.</p>
<p><em>Proceed to <a href="/2016/02/building-a-poker-bot-string-recognition/">Part 2 of Building a Poker Bot: String and Number Recognition</a>.</em></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/poker-bot" term="poker-bot" label="Poker Bot" />
                             
                                <category scheme="https://mikhail.io/tags/fsharp" term="fsharp" label="FSharp" />
                             
                                <category scheme="https://mikhail.io/tags/image-recognition" term="image-recognition" label="Image Recognition" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Monads explained in C#]]></title>
            <link href="https://mikhail.io/2016/01/monads-explained-in-csharp/"/>
            <id>https://mikhail.io/2016/01/monads-explained-in-csharp/</id>
            
            <published>2016-01-25T00:00:00+00:00</published>
            <updated>2016-01-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><em>The newer and much longer version of this article is now available:</em>
<a href="https://mikhail.io/2018/07/monads-explained-in-csharp-again/">Monads explained in C# (again)</a></p>
<p>It looks like there is a mandatory post that every blogger who learns functional programming should write:
what a Monad is. Monads have the reputation of being something very abstract and very confusing for every
developer who is not a hipster Haskell programmer. They say that once you understand what a monad is, you
loose the ability to explain it in simple language. Doug Crockford was the first one to lay this rule down, but
it becomes kind of obvious once you read 3 or 5 explanations on the web. Here is my attempt, probably doomed
to fail :)</p>
<h2 id="monads-are-container-types">Monads are container types</h2>
<p>Monads represent a class of types which behave in the common way.</p>
<p>Monads are containers which encapsulate some kind of functionality. On top of
that, they provide a way to combine two containers into one. And that&rsquo;s about it.</p>
<p>The goals of monads are similar to generic goals of any encapsulation in
software development practices: hide the implementation details from the client,
but provide a proper way to use the hidden functionality.</p>
<p>It&rsquo;s not because we
want to be able to change the implementation, it&rsquo;s because we want to make the
client as simple as possible and to enforce the best way of code structure.
Quite often monads provide the way to avoid imperative code in favor of
functional style.</p>
<p>Monads are flexible, so in C# we could try to represent a monadic type as
a generic class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Monad</span>&lt;T&gt;
{
}
</code></pre></div><h2 id="monad-instances-can-be-created">Monad instances can be created</h2>
<p>Quite an obvious statement, isn&rsquo;t it. Having a class <code>Monad&lt;T&gt;</code>, there should
be a way to create an object of this class out of an instance of type <code>T</code>.
In functional world this operation is known as <code>Return</code> function. In C# it
can be as simple as a constructor:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Monad</span>&lt;T&gt;
{
    <span style="color:#00f">public</span> Monad(T instance)
    {
    }
}
</code></pre></div><p>But usually it makes sense to define an extension method to enable fluent
syntax of monad creation:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">MonadExtensions</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Monad&lt;T&gt; Return&lt;T&gt;(<span style="color:#00f">this</span> T instance) =&gt; <span style="color:#00f">new</span> Monad&lt;T&gt;(instance);
}
</code></pre></div><h2 id="monads-can-be-chained-to-create-new-monads">Monads can be chained to create new monads</h2>
<p>This is the property which makes monads so useful, but also a bit confusing.
In functional world this operation is expressed with the <code>Bind</code> function
(or <code>&gt;&gt;=</code> operator). Here is the signature of <code>Bind</code> method in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Monad</span>&lt;T&gt;
{
    <span style="color:#00f">public</span> Monad&lt;TO&gt; Bind&lt;TO&gt;(Func&lt;T, Monad&lt;TO&gt;&gt; func)
    {
    }
}
</code></pre></div><p>As you can see, the <code>func</code> argument is a complicated thing. It accepts an
argument of type <code>T</code> (not a monad) and returns an instance of <code>Monad&lt;TO&gt;</code>
where <code>TO</code> is another type. Now, our first instance of <code>Monad&lt;T&gt;</code> knows
how to bind itself to this function to produce another instance of monad
of the new type. The full power of monads comes when we compose several of
them in one chain:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">initialValue
    .Return()
    .Bind(v1 =&gt; produceV2OutOfV1(v1))
    .Bind(v2 =&gt; produceV3OutOfV2(v2))
    .Bind(v3 =&gt; produceV4OutOfV3(v3))
    <span style="color:#008000">//...
</span></code></pre></div><p>Let&rsquo;s have a look at some examples.</p>
<h2 id="example-maybe-option-type"><a name="maybe"></a>
Example: Maybe (Option) type</h2>
<p><code>Maybe</code> is the 101 monad which is used everywhere. <code>Maybe</code> is another approach
to dealing with &lsquo;no value&rsquo; value, alternative to the concept of <code>null</code>.
Basically your object should never be null, but it can either have <code>Some</code>
value or be <code>None</code>. F# has a maybe implementation built into the language:
it&rsquo;s called <code>option</code> type. Here is a sample implementation in C#:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Maybe</span>&lt;T&gt; <span style="color:#00f">where</span> T : <span style="color:#00f">class</span>
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> T <span style="color:#00f">value</span>;

    <span style="color:#00f">public</span> Maybe(T someValue)
    {
        <span style="color:#00f">if</span> (someValue == <span style="color:#00f">null</span>)
            <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(nameof(someValue));
        <span style="color:#00f">this</span>.<span style="color:#00f">value</span> = someValue;
    }

    <span style="color:#00f">private</span> Maybe()
    {
    }

    <span style="color:#00f">public</span> Maybe&lt;TO&gt; Bind&lt;TO&gt;(Func&lt;T, Maybe&lt;TO&gt;&gt; func) <span style="color:#00f">where</span> TO : <span style="color:#00f">class</span>
    {
        <span style="color:#00f">return</span> <span style="color:#00f">value</span> != <span style="color:#00f">null</span> ? func(<span style="color:#00f">value</span>) : Maybe&lt;TO&gt;.None();
    }

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Maybe&lt;T&gt; None() =&gt; <span style="color:#00f">new</span> Maybe&lt;T&gt;();
}
</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">MaybeExtensions</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Maybe&lt;T&gt; Return&lt;T&gt;(<span style="color:#00f">this</span> T <span style="color:#00f">value</span>) <span style="color:#00f">where</span> T : <span style="color:#00f">class</span>
    {
        <span style="color:#00f">return</span> <span style="color:#00f">value</span> != <span style="color:#00f">null</span> ? <span style="color:#00f">new</span> Maybe&lt;T&gt;(<span style="color:#00f">value</span>) : Maybe&lt;T&gt;.None();
    }
}
</code></pre></div><p>Return function is implemented with a combination of a public constructor
which accepts <code>Some</code> value (notice that <code>null</code> is not allowed) and a static
<code>None</code> method returning an object of &lsquo;no value&rsquo;. <code>Return</code> extension method
combines both of them in one call.</p>
<p><code>Bind</code> function is implemented explicitly.</p>
<p>Let&rsquo;s have a look at a use case. Imagine we have a traditional repository
which loads data from an external storage (no monads yet):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> ITraditionalRepository
{
    Customer GetCustomer(<span style="color:#2b91af">int</span> id);
    Address GetAddress(<span style="color:#2b91af">int</span> id);
    Order GetOrder(<span style="color:#2b91af">int</span> id);
}
</code></pre></div><p>Now, we write a client class which loads data one by one and tries to find
a shipper:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">Shipper shipperOfLastOrderOnCurrentAddress = <span style="color:#00f">null</span>;
<span style="color:#2b91af">var</span> customer = repo.GetCustomer(customerId);
<span style="color:#00f">if</span> (customer?.Address != <span style="color:#00f">null</span>)
{
    <span style="color:#2b91af">var</span> address = repo.GetAddress(customer.Address.Id);
    <span style="color:#00f">if</span> (address?.LastOrder != <span style="color:#00f">null</span>)
    {
        <span style="color:#2b91af">var</span> order = repo.GetOrder(address.LastOrder.Id);
        shipperOfLastOrderOnCurrentAddress = order?.Shipper;
    }
}
<span style="color:#00f">return</span> shipperOfLastOrderOnCurrentAddress;
</code></pre></div><p>Note, that the code assumes that repository returns <code>null</code> if some entity
is not found, although nothing in the type system shows that. Then, there
is a number of <code>null</code> checks (facilitated with elvis operator). The code gets
a bit cluttered and less linear.</p>
<p>Here is an alternative repository which returns <code>Maybe</code> type:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IMonadicRepository
{
    Maybe&lt;Customer&gt; GetCustomer(<span style="color:#2b91af">int</span> id);
    Maybe&lt;Address&gt; GetAddress(<span style="color:#2b91af">int</span> id);
    Maybe&lt;Order&gt; GetOrder(<span style="color:#2b91af">int</span> id);
}
</code></pre></div><p>The contract is more explicit: you see that <code>Maybe</code> type is used, so you
will be forced to handle the case of absent value.</p>
<p>And here is how the above example can be rewritten with <code>Bind</code> method
composition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">Maybe&lt;Shipper&gt; shipperOfLastOrderOnCurrentAddress =
    repo.GetCustomer(customerId)
        .Bind(c =&gt; c.Address)
        .Bind(a =&gt; repo.GetAddress(a.Id))
        .Bind(a =&gt; a.LastOrder)
        .Bind(lo =&gt; repo.GetOrder(lo.Id))
        .Bind(o =&gt; o.Shipper);
</code></pre></div><p>There&rsquo;s no branching anymore, the code is fluent and linear.</p>
<p>If you think that the syntax looks very much like a LINQ query with a bunch
of <code>Select</code> statements, you are not the only one ;) One of the common
implementations of <code>Maybe</code> implements <code>IEnumerable</code> interface which allows
a more C#-idiomatic binding composition. Actually:</p>
<h2 id="ienumerable--selectmany-is-a-monad">IEnumerable + SelectMany is a monad</h2>
<p><code>IEnumerable</code> is an interface for enumerable containers.</p>
<p>Enumerable containers can be created - thus the <code>Return</code> monadic operation.</p>
<p>The <code>Bind</code> operation is defined by the standard LINQ extension method, here
is its signature:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> IEnumerable&lt;B&gt; SelectMany&lt;A, B&gt;(
    <span style="color:#00f">this</span> IEnumerable&lt;A&gt; first,
    Func&lt;A, IEnumerable&lt;B&gt;&gt; selector)
</code></pre></div><p>And here is an example of composition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">IEnumerable&lt;Shipper&gt; someWeirdListOfShippers =
    customers
        .SelectMany(c =&gt; c.Addresses)
        .SelectMany(a =&gt; a.Orders)
        .SelectMany(o =&gt; o.Shippers);
</code></pre></div><p>The query has no idea about how the collections are stored (encapsulated in
containers). We use functions <code>A -&gt; IEnumerable&lt;B&gt;</code> to produce new enumerables
(<code>Bind</code> operation).</p>
<h2 id="monad-laws">Monad laws</h2>
<p>There are a couple of laws that <code>Return</code> and <code>Bind</code> need to adhere to, so
that they produce a proper monad.</p>
<p><strong>Identity law</strong> says that that <code>Return</code> is a neutral operation: you can safely
run it before <code>Bind</code>, and it won&rsquo;t change the result of the function call:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Given
</span><span style="color:#008000"></span>T <span style="color:#00f">value</span>;
Func&lt;T, M&lt;U&gt;&gt; f;

<span style="color:#008000">// == means both parts are equivalent
</span><span style="color:#008000"></span><span style="color:#00f">value</span>.Return().Bind(f) == f(<span style="color:#00f">value</span>)
</code></pre></div><p><strong>Associativity law</strong> means that the order in which <code>Bind</code> operations
are composed does not matter:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#008000">// Given
</span><span style="color:#008000"></span>M&lt;T&gt; m;
Func&lt;T, M&lt;U&gt;&gt; f;
Func&lt;U, M&lt;V&gt;&gt; g;

<span style="color:#008000">// == means both parts are equivalent
</span><span style="color:#008000"></span>m.Bind(f).Bind(g) == m.Bind(a =&gt; f(a).Bind(g))
</code></pre></div><p>The laws may look complicated, but in fact they are very natural
expectations that any developer has when working with monads, so don&rsquo;t
spend too much mental effort on memorizing them.</p>
<h2 id="conclusion">Conclusion</h2>
<p>You should not be afraid of the &ldquo;M-word&rdquo; just because you are a C# programmer.
C# does not have a notion of monads as predefined language constructs, but
it doesn&rsquo;t mean we can&rsquo;t borrow some ideas from the functional world. Having
said that, it&rsquo;s also true that C# is lacking some powerful ways to combine
and generalize monads which are possible in Haskell and other functional
languages.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/monads" term="monads" label="Monads" />
                             
                                <category scheme="https://mikhail.io/tags/linq" term="linq" label="LINQ" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Fire-and-forget in Service Fabric actors]]></title>
            <link href="https://mikhail.io/2016/01/fire-and-forget-in-service-fabric-actors/"/>
            <id>https://mikhail.io/2016/01/fire-and-forget-in-service-fabric-actors/</id>
            
            <published>2016-01-13T00:00:00+00:00</published>
            <updated>2016-01-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>At the recent Webscale Architecture meetup we discussed two implementations of the Actor model in the .NET ecosystem: Akka.NET and Azure Service Fabric Actors. One important discussion was around Ask vs Tell call model.</blockquote><p>At the <a href="http://www.meetup.com/Webscale-Architecture-NL/events/225979118/">recent Webscale Architecture meetup</a>
we discussed two implementations of the Actor model in the .NET ecosystem:
<a href="http://akka.net">Akka.NET</a> and <a href="https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-actors-introduction/">Azure Service Fabric Actors</a>.
One important discussion was
around <strong>Ask</strong> vs <strong>Tell</strong> call model. With <strong>Tell</strong> model, the Sender just sends the
message to the Recepient without waiting for a result to come back. <strong>Ask</strong> model
means the Sender will at some point get a response back from the Receiver, potencially
blocking its own execution.</p>
<p>The default model of Akka.NET is <strong>Tell</strong>:</p>
<blockquote>
<p><strong>Tell: Fire-forget</strong></p>
</blockquote>
<blockquote>
<p>This is the preferred way of sending messages. No blocking waiting for
a message. This gives the best concurrency and scalability characteristics.</p>
</blockquote>
<p>On the contrary, the default model for Service Fabric Actors is RPC-like
<strong>Ask</strong> model. Let&rsquo;s have a close look at this model, and then see how we can
implement <strong>Tell</strong> (or <strong>Fire-and-Forget</strong>) model.</p>
<p>Actor definition starts with an interface:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> IHardWorkingActor : IActor
{
    Task DoWork(<span style="color:#2b91af">string</span> payload);
}
</code></pre></div><p>As you can see, the method does not return any useful data, which means
the client code isn&rsquo;t really interested in waiting for the operation to
complete. Here&rsquo;s how we implement this interface in the Actor class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">HardWorkingActor</span> : Actor, IHardWorkingActor
{
    <span style="color:#00f">public</span> <span style="color:#00f">async</span> Task DoWork(<span style="color:#2b91af">string</span> payload)
    {
        ActorEventSource.Current.ActorMessage(<span style="color:#00f">this</span>, <span style="color:#a31515">&#34;Doing Work&#34;</span>);
        <span style="color:#00f">await</span> Task.Delay(500);
    }
}
</code></pre></div><p>This test implementation simulates the hard work by means of an artificial 500 ms delay.</p>
<p>Now, let&rsquo;s look at the client code. Let&rsquo;s say, the client receives the payloads
from a queue or a web front-end and needs to go as fast as possible. It gets a payload,
creates an actor proxy to dispatch the payload to, then it just wants
to continue with the next payload. Here is the &ldquo;Ask&rdquo; implementation based on
the Service Fabric samples:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">int</span> i = 0;
<span style="color:#2b91af">var</span> timer = <span style="color:#00f">new</span> Stopwatch();
timer.Start();
<span style="color:#00f">while</span> (<span style="color:#00f">true</span>)
{
    <span style="color:#2b91af">var</span> proxy = ActorProxy.Create&lt;IHardWorkingActor&gt;(ActorId.NewId(), <span style="color:#a31515">&#34;fabric:/Application1&#34;</span>);
    <span style="color:#00f">await</span> proxy.DoWork(<span style="color:#a31515">$&#34;Work ${i++}&#34;</span>);
    Console.WriteLine(<span style="color:#a31515">$@&#34;Sent work to Actor {proxy.GetActorId()},
</span><span style="color:#a31515">                         rate is {i / timer.Elapsed.TotalSeconds}/sec&#34;</span>);
}
</code></pre></div><p>Note an <code>await</code> operator related to every call. That means that the client will
block until the actor work is complete. When we run the client, no surprise that
we get the rate of about 2 messages per second:</p>
<pre><code>Sent work to Actor 1647857287613311317, rate is 1,98643230380293/sec
</code></pre><p>That&rsquo;s not very exciting. What we want instead is to tell the actor to do the
work and immediately proceed to the next one. Here&rsquo;s how the client call should
look like:</p>
<pre><code>proxy.DoWork($&quot;Work ${i++}&quot;).FireAndForget();
</code></pre><p>Instead of <code>await</code>-ing, we make a <code>Task</code>, pass it to some (not yet existing)
extension method and proceed immediately. It appears that the implementation
of such extension method is trivial:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">TaskHelper</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">void</span> FireAndForget(<span style="color:#00f">this</span> Task task)
    {
        Task.Run(<span style="color:#00f">async</span>() =&gt; <span style="color:#00f">await</span> task).ConfigureAwait(<span style="color:#00f">false</span>);
    }
}
</code></pre></div><p>The result looks quite different from what we had before:</p>
<pre><code>Sent work to Actor -8450334792912439527, rate is 408,484162592517/sec
</code></pre><p>400 messages per second, which is some 200x difference&hellip;</p>
<p>The conclusions are simple:</p>
<ul>
<li>
<p>Service Fabric is a powerful platform and programming paradigm which doesn&rsquo;t
limit your choice of communication patterns</p>
</li>
<li>
<p>Design the communication models carefully based on your use case, don&rsquo;t
take the defaults for granted</p>
</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/actor-model" term="actor-model" label="Actor Model" />
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/akka.net" term="akka.net" label="Akka.NET" />
                             
                                <category scheme="https://mikhail.io/tags/service-fabric" term="service-fabric" label="Service Fabric" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Validation with Either data type in C#]]></title>
            <link href="https://mikhail.io/2016/01/validation-with-either-data-type-in-csharp/"/>
            <id>https://mikhail.io/2016/01/validation-with-either-data-type-in-csharp/</id>
            
            <published>2016-01-06T00:00:00+00:00</published>
            <updated>2016-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>In this article we will employ a functional monadic concept Either to make validation code more expressive and easier to maintain.</blockquote><p>In this article we will employ a functional monadic concept <strong>Either</strong> to make validation
code more expressive and easier to maintain.</p>
<h2 id="problem">Problem</h2>
<p>Let&rsquo;s say we get a request from some client code and we need to check if this
request is actually valid. If it&rsquo;s not valid, we want to make a detailed description
of the problems that we identified. If it is valid, we want to produce a response
about the successful acceptance of the request. Let&rsquo;s define the classes:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Request</span> { ... }
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Response</span> { ... }
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">ValidationError</span> { ... }
</code></pre></div><p>Now, we need a function which would accept a <code>Request</code> and would return <code>Response</code>
or <code>ValidationError</code>. Let&rsquo;s look at some possible solutions.</p>
<h2 id="throw-an-exception">Throw an exception</h2>
<p>Validation <em>error</em> sounds like it could be an exception:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Response Validate(Request r)
{
   <span style="color:#00f">if</span> (!Valid(r))
       <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ValidationException(<span style="color:#00f">new</span> ValidationError(...));

   <span style="color:#00f">return</span> <span style="color:#00f">new</span> Response(r);
}
</code></pre></div><p>This approach is really bad though. You have to declare a special exception
class to hold the validation error. But even worse, exception handling is not
explicit - you don&rsquo;t see the exception type when you look at method signature.
Client processing code is going to be messed up because of exception handling.
Never use exceptions for your business logic flow.</p>
<h2 id="output-parameter">Output parameter</h2>
<p>We could make an output parameter of <code>ValidationError</code> type:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Response Validate(Request r, <span style="color:#00f">out</span> ValidationError error)
{
    <span style="color:#00f">if</span> (Valid(r))
    {
        error = <span style="color:#00f">new</span> ValidationError(...);
        <span style="color:#00f">return</span> <span style="color:#00f">null</span>;
    }

    error = <span style="color:#00f">null</span>;
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> Response(r);
}
</code></pre></div><p>Now the interface is more explicit: client won&rsquo;t be able to completely ignore
the fact that an error is possible. But output parameters are not really
easy to use in C#, especially in fluent-style client code. Moreover, we are
using nulls as a way to represent missing object, which is a smell by itself,
because nulls are not explicit. Never use nulls in your business logic.</p>
<h2 id="return-the-combined-result">Return the combined result</h2>
<p>We could declare a container class which would keep both <code>Response</code> and
<code>ValidationError</code>, and then return it from the method.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Both</span>&lt;TData, TError&gt;
{
    <span style="color:#00f">public</span> TData Data { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
    <span style="color:#00f">public</span> TError Error { <span style="color:#00f">get</span>; <span style="color:#00f">set</span>; }
}
...
<span style="color:#00f">public</span> Both&lt;Response, ValidationError&gt; Validate(Request r)
{
    <span style="color:#00f">return</span> Valid(r)
        ? <span style="color:#00f">new</span> Both&lt;Response, ValidationError&gt; { Data = <span style="color:#00f">new</span> Response(r) }
        : <span style="color:#00f">new</span> Both&lt;Response, ValidationError&gt; { Error = <span style="color:#00f">new</span> ValidationError(...) };
}
</code></pre></div><p>Looks much nicer, we are getting there. Now it&rsquo;s a pure function with input
and output parameters, but we still use null for result state representation.
Let&rsquo;s see how we can solve it with <strong>Either</strong> data structure.</p>
<h2 id="introducing-either">Introducing Either</h2>
<p>Instead of returning <code>Both</code> with nullable properties, let&rsquo;s return <code>Either</code>
with just one of them. When constructing an object, you can specify either
a &lsquo;left&rsquo; or a &lsquo;right&rsquo; argument, but not both.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Either</span>&lt;TL, TR&gt;
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> TL left;
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> TR right;
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> <span style="color:#2b91af">bool</span> isLeft;

    <span style="color:#00f">public</span> Either(TL left)
    {
        <span style="color:#00f">this</span>.left = left;
        <span style="color:#00f">this</span>.isLeft = <span style="color:#00f">true</span>;
    }

    <span style="color:#00f">public</span> Either(TR right)
    {
        <span style="color:#00f">this</span>.right = right;
        <span style="color:#00f">this</span>.isLeft = <span style="color:#00f">false</span>;
    }
}
</code></pre></div><p>Now, the main difference is in how the client uses it. There are no properties
to accept <code>Left</code> and <code>Right</code> parts. Instead we define the following method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> T Match&lt;T&gt;(Func&lt;TL, T&gt; leftFunc, Func&lt;TR, T&gt; rightFunc)
    =&gt; <span style="color:#00f">this</span>.isLeft ? leftFunc(<span style="color:#00f">this</span>.left) : rightFunc(<span style="color:#00f">this</span>.right);
</code></pre></div><p>That&rsquo;s the concept of pattern matching implemented in C# world. If a left value
is specified, <code>Match</code> will return the result of the left function, otherwise the result
of the right function.</p>
<p>Another improvement would be to create explicit operators for easy conversions
from left and right types:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">implicit</span> <span style="color:#00f">operator</span> Either&lt;TL, TR&gt;(TL left) =&gt; <span style="color:#00f">new</span> Either&lt;TL, TR&gt;(left);

<span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">implicit</span> <span style="color:#00f">operator</span> Either&lt;TL, TR&gt;(TR right) =&gt; <span style="color:#00f">new</span> Either&lt;TL, TR&gt;(right);
</code></pre></div><p>Let&rsquo;s have a look at a complete example.</p>
<h2 id="why-its-great">Why it&rsquo;s great</h2>
<p>Here is the service code written with <code>Either</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Either&lt;Response, ValidationError&gt; Validate(Request r)
{
    <span style="color:#00f">return</span> Valid(r)
        ? Data = <span style="color:#00f">new</span> Response(r)
        : <span style="color:#00f">new</span> ValidationError(...);
}
</code></pre></div><p>Clean and nice! Now a simplistic client:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> validated = service.Validate(request);
Console.WriteLine(
    validated.Match(
        result =&gt; <span style="color:#a31515">$&#34;Success: {result}&#34;</span>,
        error =&gt; <span style="color:#a31515">$&#34;Error: {error}&#34;</span>)
    );
</code></pre></div><p>Simple, readable , no conditionals, no null checks, no way to silently ignore the fact that
validation may fail.</p>
<h2 id="show-me-the-code">Show me the code</h2>
<p>You can find the definition of <code>Either</code> class in my <a href="https://github.com/mikhailshilkov/mikhailio-samples/blob/master/Either%7BTL%2CTR%7D.cs">github repo</a>.</p>
<p><strong>Update.</strong> Here is a link to an awesome talk on this topic:
<a href="https://vimeo.com/113707214">Railway oriented programming: Error handling in functional languages by Scott Wlaschin</a></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/functional-programming" term="functional-programming" label="Functional Programming" />
                             
                                <category scheme="https://mikhail.io/tags/clean-code" term="clean-code" label="Clean Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2015s]]></title>
            <link href="https://mikhail.io/2015/"/>
            <id>https://mikhail.io/2015/</id>
            
            <published>2015-12-22T00:00:00+00:00</published>
            <updated>2015-12-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[DDD]]></title>
            <link href="https://mikhail.io/tags/ddd/"/>
            <id>https://mikhail.io/tags/ddd/</id>
            
            <published>2015-12-22T00:00:00+00:00</published>
            <updated>2015-12-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Weaving your domain classes with Fody]]></title>
            <link href="https://mikhail.io/2015/12/weaving-your-domain-classes-with-fody/"/>
            <id>https://mikhail.io/2015/12/weaving-your-domain-classes-with-fody/</id>
            
            <published>2015-12-22T00:00:00+00:00</published>
            <updated>2015-12-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>When I model the business domain with C#, the resulting data structures tend to contain a lot of boilerplate code. It&rsquo;s repeated from class to class and it gets more difficult to see the essence of the model behind the repetitive cruft. In this article I show off one trick to reduce this boilerplate code with Fody library.</blockquote><p>When I model the business domain with C#, the resulting data structures tend to contain a lot of boilerplate code. It&rsquo;s repeated from class to class and it gets more difficult to see the essence of the model behind the repetitive cruft. Here is a simplistic example, which illustrates the problem. Let&rsquo;s say we are modelling Trips, and for each <code>Trip</code> we need to keep track of <code>Origin</code>, <code>Destination</code> and <code>Vehicle</code> which executes the <code>Trip</code>, nothing else. Here is a code to create an sample trip:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> trip = <span style="color:#00f">new</span> Trip(
    origin: <span style="color:#00f">new</span> Location(<span style="color:#a31515">&#34;Paris&#34;</span>, geoParis),
    destination: <span style="color:#00f">new</span> Location(<span style="color:#a31515">&#34;Amsterdam&#34;</span>, geoAmsterdam),
    vehicle: <span style="color:#00f">new</span> Vehicle(<span style="color:#a31515">&#34;TBL-12-H&#34;</span>, Type.HeavyTruck)
</code></pre></div><p>Let&rsquo;s include these requirements as parts of our trip model:</p>
<ul>
<li>It has a constructor which accepts three arguments (see above)</li>
<li>It has 3 read-only properties which are assigned from constructor parameters</li>
<li>It should not allow null values to be assigned to these properties via constructor</li>
<li>It should be a Value object, that is two objects with same properties should be equal</li>
</ul>
<h2 id="initial-version">Initial version</h2>
<p>First, let&rsquo;s implement these requirement in a usual way:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Trip</span> : IEquatable&lt;Trip&gt;
{
    <span style="color:#00f">public</span> Trip(Location origin, Location destination, Vehicle vehicle)
    {
        <span style="color:#00f">if</span> (origin == <span style="color:#00f">null</span>) <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(nameof(origin));
        <span style="color:#00f">if</span> (destination == <span style="color:#00f">null</span>) <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(nameof(destination));
        <span style="color:#00f">if</span> (vehicle == <span style="color:#00f">null</span>) <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(nameof(vehicle));

        <span style="color:#00f">this</span>.Origin = origin;
        <span style="color:#00f">this</span>.Destination = destination;
        <span style="color:#00f">this</span>.Vehicle = vehicle;
    }

    <span style="color:#00f">public</span> Location Origin { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Location Destination { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Vehicle Vehicle { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> <span style="color:#2b91af">bool</span> Equals(Trip other)
    {
        <span style="color:#00f">return</span> Equals(<span style="color:#00f">this</span>.Origin, other.Origin)
            &amp;&amp; Equals(<span style="color:#00f">this</span>.Destination, other.Destination)
            &amp;&amp; Equals(<span style="color:#00f">this</span>.Vehicle, other.Vehicle);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">override</span> <span style="color:#2b91af">bool</span> Equals(<span style="color:#2b91af">object</span> obj)
    {
        <span style="color:#00f">if</span> (ReferenceEquals(<span style="color:#00f">null</span>, obj))
        {
            <span style="color:#00f">return</span> <span style="color:#00f">false</span>;
        }
        <span style="color:#00f">if</span> (ReferenceEquals(<span style="color:#00f">this</span>, obj))
        {
            <span style="color:#00f">return</span> <span style="color:#00f">true</span>;
        }
        <span style="color:#00f">if</span> (obj.GetType() != <span style="color:#00f">this</span>.GetType())
        {
            <span style="color:#00f">return</span> <span style="color:#00f">false</span>;
        }
        <span style="color:#00f">return</span> Equals((Trip)obj);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">override</span> <span style="color:#2b91af">int</span> GetHashCode()
    {
        <span style="color:#00f">unchecked</span>
        {
            <span style="color:#2b91af">var</span> hashCode = <span style="color:#00f">this</span>.Origin.GetHashCode();
            hashCode = (hashCode * 397) ^ <span style="color:#00f">this</span>.Destination.GetHashCode();
            hashCode = (hashCode * 397) ^ <span style="color:#00f">this</span>.Vehicle.GetHashCode();
            <span style="color:#00f">return</span> hashCode;
        }
    }

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">bool</span> <span style="color:#00f">operator</span> ==(Trip tripA, Trip tripB)
    {
        <span style="color:#00f">return</span> <span style="color:#2b91af">object</span>.Equals(tripA, tripB);
    }

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">bool</span> <span style="color:#00f">operator</span> !=(Trip tripA, Trip tripB)
    {
        <span style="color:#00f">return</span> !<span style="color:#2b91af">object</span>.Equals(tripA, tripB);
    }
}
</code></pre></div><p>That&rsquo;s a lot of code! It&rsquo;s very repetitive but it&rsquo;s also tricky: you can implement it incorrectly in some slight way that wouldn&rsquo;t be easy to catch until it silently fails one day. So imagine how many tests you need to validate it.</p>
<p>I implemented this code with help of Resharper, which makes it so much easier, but the code is still a heavy luggage to carry on. This class is hard to read and hard to change - every time you add a property you should not forget to update all the corresponding methods.</p>
<p>Are there other options?</p>
<h2 id="introducing-fody">Introducing Fody</h2>
<p><a href="https://github.com/Fody/Fody">Fody</a> is an extensible tool for weaving .NET assemblies. It means that you can use it to improve your code automatically at the time of compilation. Fody itself doesn&rsquo;t do much to the code, but it has a collection of plugins to actually change it. For this example I will use two of them:</p>
<ul>
<li><a href="https://github.com/Fody/NullGuard"><strong>NullGuard</strong></a> - guards all the input parameters, output parameters and return values of all types in a current assembly not to be null. If null value is passed or returned, the weaved code with throw an exception.</li>
<li><a href="https://github.com/Fody/Equals"><strong>Equals</strong></a> - you can mark a class with <code>[Equals]</code> attribute and Fody will implement <code>Equals()</code> and <code>GetHashCode()</code> methods and <code>==</code> operator for you by comparing all public properties of the annotated class.</li>
</ul>
<p>To install them just execute</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ps" data-lang="ps">PM&gt; Install-Package NullGuard.Fody
PM&gt; Install-Package Equals.Fody
</code></pre></div><p>The root of your project will now contain the following configuration file:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#00f">&lt;?xml version=&#34;1.0&#34; encoding=&#34;utf-8&#34;?&gt;</span>
&lt;Weavers&gt;
  &lt;NullGuard IncludeDebugAssert=<span style="color:#a31515">&#34;false&#34;</span> /&gt;
  &lt;Equals /&gt;
&lt;/Weavers&gt;
</code></pre></div><p>(I&rsquo;ve added <code>IncludeDebugAssert</code> attribute manually to disable assert statements in debug mode)</p>
<p>Let&rsquo;s adjust our class to make use of the plugins:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Equals]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Trip</span>
{
    <span style="color:#00f">public</span> Trip(Location origin, Location destination, Vehicle vehicle)
    {
        <span style="color:#00f">this</span>.Origin = origin;
        <span style="color:#00f">this</span>.Destination = destination;
        <span style="color:#00f">this</span>.Vehicle = vehicle;
    }

    <span style="color:#00f">public</span> Location Origin { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Location Destination { <span style="color:#00f">get</span>; }
    <span style="color:#00f">public</span> Vehicle Vehicle { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>And that&rsquo;s it! We still get the same functionality but the code is just trivial. Let&rsquo;s see how it works:</p>
<ul>
<li><code>Equals</code> attribute means that we want Fody plugin to implement all the equality-related boilerplate code for this class, including operators and <code>IEquatable&lt;T&gt;</code> implementation. So this plugin is in <em>opt-in</em> mode.</li>
<li>I used no attributes from <code>NullGuard</code> plugin. This plugin works in <em>opt-out</em> mode, i.e. it changes all the classes by default, and if you don&rsquo;t want it for some piece of code - you can always opt out. This default makes a lot of sense to me: I don&rsquo;t want any nulls in my code unless I really need them due to some external contracts.</li>
</ul>
<p>Let&rsquo;s open the resulting assembly in <a href="http://ilspy.net/">ILSpy</a> to see what it compiles to. Here is the constructor:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Trip(Location origin, Location destination, Vehicle vehicle)
{
    <span style="color:#2b91af">bool</span> flag = origin == <span style="color:#00f">null</span>;
    <span style="color:#00f">if</span> (flag)
    {
        <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(<span style="color:#a31515">&#34;origin&#34;</span>);
    }
    <span style="color:#2b91af">bool</span> flag2 = destination == <span style="color:#00f">null</span>;
    <span style="color:#00f">if</span> (flag2)
    {
        <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(<span style="color:#a31515">&#34;destination&#34;</span>);
    }
    <span style="color:#2b91af">bool</span> flag3 = vehicle == <span style="color:#00f">null</span>;
    <span style="color:#00f">if</span> (flag3)
    {
        <span style="color:#00f">throw</span> <span style="color:#00f">new</span> ArgumentNullException(<span style="color:#a31515">&#34;vehicle&#34;</span>);
    }
    <span style="color:#00f">this</span>.&lt;Origin&gt;k__BackingField = origin;
    <span style="color:#00f">this</span>.&lt;Destination&gt;k__BackingField = destination;
    <span style="color:#00f">this</span>.&lt;Vehicle&gt;k__BackingField = vehicle;
}
</code></pre></div><p>It&rsquo;s bit more verbose but essentially equivalent to what I did manually before. By default null guard will be very strict, so you will see that even auto-property&rsquo;s return values are checked:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> Location Origin
{
    [CompilerGenerated]
    <span style="color:#00f">get</span>
    {
        Location expr_06 = <span style="color:#00f">this</span>.&lt;Origin&gt;k__BackingField;
        <span style="color:#00f">if</span> (expr_06 == <span style="color:#00f">null</span>)
        {
            <span style="color:#00f">throw</span> <span style="color:#00f">new</span> InvalidOperationException(<span style="color:#a31515">&#34;[NullGuard] Return value of property &#39;ETA.Domain.Location ETA.Domain.Trip::Origin()&#39; is null.&#34;</span>);
        }
        <span style="color:#00f">return</span> expr_06;
    }
}
</code></pre></div><p>It doesn&rsquo;t make much sense to me, so I configured Fody on assembly level to check only arguments and return values:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[assembly: NullGuard(ValidationFlags.Arguments | ValidationFlags.ReturnValues)]
</code></pre></div><p>Here is a set of operations related to equality (I&rsquo;ll skip the body in sake of brevity):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Trip</span> : IEquatable&lt;Trip&gt;
{
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">private</span> <span style="color:#00f">static</span> <span style="color:#2b91af">bool</span> EqualsInternal(Trip left, Trip right) { ... }
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">public</span> <span style="color:#00f">override</span> <span style="color:#2b91af">bool</span> Equals(Trip other) { ... }
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">public</span> <span style="color:#00f">override</span> <span style="color:#2b91af">bool</span> Equals(<span style="color:#2b91af">object</span> obj) { ... }
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">public</span> <span style="color:#00f">override</span> <span style="color:#2b91af">int</span> GetHashCode() { ... }
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">bool</span> <span style="color:#00f">operator</span> ==(Trip left, Trip right) { ... }
    [GeneratedCode(&#34;Equals.Fody&#34;, &#34;1.4.6.0&#34;), DebuggerNonUserCode]
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#2b91af">bool</span> <span style="color:#00f">operator</span> !=(Trip left, Trip right) { ... }
}
</code></pre></div><p>There is a catch (at least at the time of writing): the auto-generated <code>==</code> and <code>!=</code> operators won&rsquo;t work properly if you use them inside the same assembly where the type is defined. That&rsquo;s because the C# compiler will only use these operators properly if they are defined at compile time, and they only get defined after the compilation (weaving takes place after IL is produced). See <a href="https://github.com/Fody/Equals/issues/10">the issue on GitHub</a> for details.</p>
<h2 id="bonus---a-proper-solution">Bonus - a proper solution</h2>
<p>Here is how you actually should define similar types:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fsharp" data-lang="fsharp"><span style="color:#00f">type</span> <span style="color:#2b91af">Trip</span> =
  { Origin : Location
    Destination : Location
    Vehicle : Vehicle }
</code></pre></div><p>No nulls are possible here and equality works out of the box. There&rsquo;s just one major detail: it&rsquo;s F#&hellip;</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/fody" term="fody" label="Fody" />
                             
                                <category scheme="https://mikhail.io/tags/ddd" term="ddd" label="DDD" />
                             
                                <category scheme="https://mikhail.io/tags/code-generation" term="code-generation" label="Code Generation" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Continuous Deployment]]></title>
            <link href="https://mikhail.io/tags/continuous-deployment/"/>
            <id>https://mikhail.io/tags/continuous-deployment/</id>
            
            <published>2015-12-14T00:00:00+00:00</published>
            <updated>2015-12-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Deploy your SPA to Azure]]></title>
            <link href="https://mikhail.io/2015/12/deploy-your-spa-to-azure/"/>
            <id>https://mikhail.io/2015/12/deploy-your-spa-to-azure/</id>
            
            <published>2015-12-14T00:00:00+00:00</published>
            <updated>2015-12-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>In this post I want to share a simple tutorial on how to deploy your single page application into the Azure cloud. I have a Single Page Application (SPA) done with HTML/JavaScript in a separate local Git repository. I alsp have a ASP.NET 4.6 Web API service which serves the data for SPA in another local Git repository. Now I want to deploy both to the Azure cloud, and make it easy to deploy changes in the future.</blockquote><p>In this post I want to share a simple tutorial on how to deploy your single page application into the Azure cloud.</p>
<h2 id="the-goal">The goal</h2>
<p>Here is the initial setup:</p>
<ul>
<li>
<p>I have a Single Page Application (SPA) done with HTML/JavaScript in a separate local Git repository</p>
</li>
<li>
<p>I have a ASP.NET 4.6 Web API service which serves the data for SPA in another local Git repository</p>
</li>
<li>
<p>Now I want to deploy both to the Azure cloud, and make it easy to deploy changes in the future</p>
</li>
</ul>
<h2 id="the-solution">The solution</h2>
<p>We will deploy our application to Azure Cloud Services / Web application.</p>
<ol>
<li>
<p>Go to <a href="https://portal.azure.com">Azure Portal</a> then <code>App Services -&gt; Add</code> and follow the wizard to create your Web app. Here is mine:
<img src="azurewebapp.jpg" alt="Azure web app"></p>
</li>
<li>
<p>Follow <a href="https://azure.microsoft.com/en-us/documentation/articles/web-sites-publish-source-control/">this guide</a> to create a new Git repository and setup continuous deployment from this repository to Azure web application. You are good once you see this working (step 6):
<img src="git-hello-git.png" alt="Hello git running in Azure"></p>
</li>
<li>
<p>Copy your SPA files into the root of the new Git repository, here is my repo after I did that:
<img src="spa-copied-to-repo.jpg" alt="SPA files in the repo">
and push them to <code>azure</code> remote. Now you should be able to browse to the web app and see your SPA screen, but with all calls to Web API failing.</p>
</li>
<li>
<p>Inside your new Git repository, create a sub-folder to host Web API services. My SPA expects them under <code>/api</code> folder, so that&rsquo;s the folder name that I created:
<img src="webapi-copied-to-repo.jpg" alt="Web API files in the repo"></p>
</li>
<li>
<p>Copy your binary compiled files of your Web API to <code>/api</code> sub-folder. This includes the bin folder, config files, asax files etc - whatever you would need in your local IIS deployment. DO NOT copy the sln/csproj files, otherwise the Azure will also try to do the compilation himself and will change the root of your web application to the folder with csproj files. So, my <code>/api</code> folder looks like this:
<img src="api-folder.jpg" alt="Web API folder contents"></p>
</li>
<li>
<p>Commit the changes and Git push to <code>azure</code> remote. Once the files are deployed, your SPA app should be up and running. Well done!</p>
</li>
<li>
<p>You don&rsquo;t want to copy the files manually all the time, so make a PowerShell script or gulp task to do that for you. Remember, your changes will be applied whenever you push a new version to <code>azure</code> remote of your Git repo.</p>
</li>
</ol>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/azure" term="azure" label="Azure" />
                             
                                <category scheme="https://mikhail.io/tags/spa" term="spa" label="SPA" />
                             
                                <category scheme="https://mikhail.io/tags/continuous-deployment" term="continuous-deployment" label="Continuous Deployment" />
                             
                                <category scheme="https://mikhail.io/tags/git" term="git" label="Git" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Git]]></title>
            <link href="https://mikhail.io/tags/git/"/>
            <id>https://mikhail.io/tags/git/</id>
            
            <published>2015-12-14T00:00:00+00:00</published>
            <updated>2015-12-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[SPA]]></title>
            <link href="https://mikhail.io/tags/spa/"/>
            <id>https://mikhail.io/tags/spa/</id>
            
            <published>2015-12-14T00:00:00+00:00</published>
            <updated>2015-12-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Course]]></title>
            <link href="https://mikhail.io/tags/course/"/>
            <id>https://mikhail.io/tags/course/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[JSPM]]></title>
            <link href="https://mikhail.io/tags/jspm/"/>
            <id>https://mikhail.io/tags/jspm/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Learning]]></title>
            <link href="https://mikhail.io/tags/learning/"/>
            <id>https://mikhail.io/tags/learning/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Pluralsight]]></title>
            <link href="https://mikhail.io/tags/pluralsight/"/>
            <id>https://mikhail.io/tags/pluralsight/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Review of the course "Modern, Modular JavaScript with SystemJS and jspm"]]></title>
            <link href="https://mikhail.io/2015/11/review-of-jspm-course/"/>
            <id>https://mikhail.io/2015/11/review-of-jspm-course/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Recently I was playing with Aurelia SPA framework, which makes heavy use of jspm and SystemJS for modules/packaging. This package manager is new to me, and it looked a bit like magic sometimes. So when I saw a jspm course on Pluralsight, I decided to give it a try. And I did not regret: the course is great.</blockquote><p>Recently I was playing with <a href="http://aurelia.io">Aurelia</a> SPA framework, which makes heavy use of jspm and SystemJS for modules/packaging. This package manager is new to me, and it looked a bit like magic sometimes. So when I saw a jspm course on Pluralsight, I decided to give it a try. And I did not regret: the course is great. It&rsquo;s so good, that I decided to write a review for it, even though I have never done that before.</p>
<p>So, the full course name is &ldquo;<a href="https://www.pluralsight.com/courses/javascript-systemjs-jspm">Modern, Modular JavaScript with SystemJS and jspm</a>&rdquo; by Wes Higbee. The topic sounds pretty narrow (compare to something like &ldquo;Building a Web App with ASP.NET 5, MVC 6, EF7 and AngularJS&rdquo;), but the video track is surprisingly long: 7.5 hours. And every hour is packed with well structured in-depth material which lays the topic out from A to Z.</p>
<p>The majority of online courses are focusing on just explaining the <em>What</em>&rsquo;s of a technology: where you get it, how you start quickly, how you solve typical tasks. Wes does not stop there: he is really focusing on <em>Why</em>&rsquo;s: not only why we should use ES2015 modules and jspm, but the complete logical flow from the best practices in other programming environments to the module patterns to the tools that enable us use the modern approaches in javascript today. The understanding of this reasoning chain sets the solid ground, and you actually start getting the What&rsquo;s deeper.</p>
<p>But Wes goes even further: he mixes the <em>How</em>&rsquo;s in. I love watching him breaking the stuff to show why it fails and how to fix it. The references to module dependency graphs and internals of the tools are insightful; that&rsquo;s how &ldquo;the magic&rdquo; transforms into the comprehension of modern open source tools and libraries.</p>
<p>Of course, just watching the course won&rsquo;t make me an expert in the topic. Now I need to put the knowledge into practice, ask more questions and find answers myself. But I will hopefully save hours of debugging and frustration and will maybe produce better products in the end. Wes and Pluralsight, I definitely need more of courses like this - focused, deep and engaging!</p>
<p>P.S. It was a bit awkward to watch the course in the office because of solitaire cards being shown on my screen. Not sure what my colleagues thought I was doing ;)</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/pluralsight" term="pluralsight" label="Pluralsight" />
                             
                                <category scheme="https://mikhail.io/tags/course" term="course" label="Course" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                             
                                <category scheme="https://mikhail.io/tags/javascript" term="javascript" label="Javascript" />
                             
                                <category scheme="https://mikhail.io/tags/jspm" term="jspm" label="JSPM" />
                             
                                <category scheme="https://mikhail.io/tags/systemjs" term="systemjs" label="SystemJS" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[SystemJS]]></title>
            <link href="https://mikhail.io/tags/systemjs/"/>
            <id>https://mikhail.io/tags/systemjs/</id>
            
            <published>2015-11-24T00:00:00+00:00</published>
            <updated>2015-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Domain Design]]></title>
            <link href="https://mikhail.io/tags/domain-design/"/>
            <id>https://mikhail.io/tags/domain-design/</id>
            
            <published>2015-08-11T00:00:00+00:00</published>
            <updated>2015-08-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Units of Measurement]]></title>
            <link href="https://mikhail.io/tags/units-of-measurement/"/>
            <id>https://mikhail.io/tags/units-of-measurement/</id>
            
            <published>2015-08-11T00:00:00+00:00</published>
            <updated>2015-08-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Units of measurement in domain design]]></title>
            <link href="https://mikhail.io/2015/08/units-of-measurement-in-domain-design/"/>
            <id>https://mikhail.io/2015/08/units-of-measurement-in-domain-design/</id>
            
            <published>2015-08-11T00:00:00+00:00</published>
            <updated>2015-08-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>If you have business application of any decent size, your most important code probably resides in domain logic. Try to make it as simple and readable as possible, and it will always pay you back. Today I want to discuss one aspect of writing clean domain code: units of measurement.</blockquote><p>If you have business application of any decent size, your most important code probably resides in domain logic.
When working with 3rd party code, you can always find an answer on stack overflow or official documentation, but your domain is all yours. Try to make it as simple and readable as possible, and it will always pay you back.</p>
<p>Today I want to discuss one aspect of writing clean domain code: units of measurement. It is important for any domain (or sub-domain) where you operate some physical measurements.</p>
<h2 id="problem-statement">Problem statement</h2>
<p>Our toy example will be about cars and fuel consumption. You receive some data about the trip of your car, e.g. an instance of</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> ITrip
{
    <span style="color:#2b91af">double</span> FuelUsed { <span style="color:#00f">get</span>; }
    <span style="color:#2b91af">double</span> Distance { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>Now you want to calculate the fuel consumption rate of your trip. You write</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#2b91af">var</span> fuelRate = trip.FuelUsed / trip.Distance;
</code></pre></div><p>You get the value, but what is it? Let&rsquo;s say you want a value of liters per 100 kilometers. You can assume that <code>FuelUsed</code> is in liters, and <code>Distance</code> is in kilometers. To be more explicit you refactor your code</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> ITrip
{
    <span style="color:#2b91af">double</span> FuelUsedInLiters { <span style="color:#00f">get</span>; }
    <span style="color:#2b91af">double</span> DistanceInKilometers { <span style="color:#00f">get</span>; }
}

<span style="color:#2b91af">var</span> fuelRateLitersPer100Kilometers = trip.FuelUsedInLiters * 100.0 / trip.DistanceInKilometers;
</code></pre></div><p>Now it&rsquo;s much more explicit, and probably good enough for such a small code example. For larger code bases, you will inevitably get into more problems:</p>
<ol>
<li>
<p>You will start measuring same things in different units. E.g. you will store the distance in meters in the database, so you&rsquo;ll have to multiply by 1000 somewhere in persistence layer.</p>
</li>
<li>
<p>If you need to convert metric to imperial and back, you will get lots of constants here and there.</p>
</li>
<li>
<p>String formatting will become a tedious task. Be sure to call a right formatter for each implicit unit.</p>
</li>
</ol>
<p>This does not work well. The code smell is called <a href="http://blog.ploeh.dk/2011/05/25/DesignSmellPrimitiveObsession/">Primitive Obsession</a> and we should avoid this in production-grade code. Instead, we want the succinctness of first example in combination with strong compile-time checks and well-defined operations.</p>
<h2 id="defining-the-units">Defining the units</h2>
<p>I tried several options like generic classes for units, but I ended up having a struct per measurement. The code is very boring and repetitive, but it provides me with the strongest compile-time checks and nice readability. If you are too bored with typing, you can do some code generation or just use 3rd party that suits you.
So, my end result looks like</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">interface</span> ITrip
{
    Volume FuelUsed { <span style="color:#00f">get</span>; }
    Distance Distance { <span style="color:#00f">get</span>; }
}
</code></pre></div><p>Let&rsquo;s see how Distance is defined (Volume will be almost exactly same):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">struct</span> <span style="color:#2b91af">Distance</span>
{
    <span style="color:#00f">private</span> Distance(<span style="color:#2b91af">double</span> kilometers)
    {
        <span style="color:#00f">this</span>.Kilometers = kilometers;
    }

    <span style="color:#00f">public</span> <span style="color:#2b91af">double</span> Kilometers { <span style="color:#00f">get</span>; }

    <span style="color:#00f">public</span> <span style="color:#2b91af">double</span> Meters =&gt; <span style="color:#00f">this</span>.Kilometers / 1000.0;

    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">readonly</span> Distance Zero = <span style="color:#00f">new</span> Distance(0.0);

    ...
}
</code></pre></div><p>Several important things to notice here:</p>
<ol>
<li>
<p>It&rsquo;s a struct.</p>
</li>
<li>
<p>It&rsquo;s immutable. Once an instance is created, its properties can&rsquo;t be changed anymore.</p>
</li>
<li>
<p>Constructor is private. I don&rsquo;t actually want people to create instances directly: <code>new Distance(123)</code> reads pretty horribly, keep reading to see better options.
Of course, default constructor is still public, but you can only create a zero value with it.</p>
</li>
<li>
<p>Better way of creating zero distance is to call Zero static field.</p>
</li>
</ol>
<h2 id="instantiation">Instantiation</h2>
<p>So, how do we create measurement objects?</p>
<h3 id="factory-method">Factory method</h3>
<p>The classic way is a set of static factory methods:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">static</span> Distance FromKilometers(<span style="color:#2b91af">double</span> kilometers)
{
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> Distance(kilometers);
}

<span style="color:#00f">public</span> <span style="color:#00f">static</span> Distance FromMeters(<span style="color:#2b91af">double</span> meters)
{
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> Distance(meters / 1000.0);
}
</code></pre></div><p>Usage is as simple as <code>var distance = Distance.FromMeters(234);</code></p>
<h3 id="extension-method">Extension method</h3>
<p>Imagine you have the following code which converts an integer value of a database result into our units</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs">trip.Distance = Distance.FromMeters(database.ReadInt32(<span style="color:#a31515">&#34;TotalDistance&#34;</span>)
                        .GetDefaultOrEmpty());
</code></pre></div><p>Such a long expression reads better with a fluent interface like</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs">trip.Distance = database.ReadInt32(<span style="color:#a31515">&#34;TotalDistance&#34;</span>)
                        .GetDefaultOrEmpty()
                        .MetersToDistance();
</code></pre></div><p><code>MetersToDistance</code> in this case is an extension method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">DistanceExtensions</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> Distance MetersToDistance(<span style="color:#00f">this</span> <span style="color:#2b91af">double</span> meters)
    {
        <span style="color:#00f">return</span> Distance.FromMeters(meters);
    }
}
</code></pre></div><h3 id="operator-with-static-class-using">Operator with static class using</h3>
<p>C# 6 brings us a new language construct. Now we can import a static helper class</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">using</span> static Units.Constants;
</code></pre></div><p>And then we can write something like</p>
<pre><code>var distance = 10.0 * km;
</code></pre><p>where <code>km</code> is defined in that static class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">class</span> <span style="color:#2b91af">Constants</span>
{
    <span style="color:#00f">public</span> <span style="color:#00f">static</span> <span style="color:#00f">readonly</span> Distance km = Distance.FromKilometers(1.0);
}
</code></pre></div><p>This may not look like idiomatic C#, but I think it&rsquo;s very good at least for writing unit tests:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> target = <span style="color:#00f">new</span> Trip
{
    DistanceOnFoot = 5 * km,
    DistanceOnBicycle = 10 * km,
    DistanceOnCar = 30 * km
};
target.TotalDistance.Should().Be((30 + 10 + 5) * km);
</code></pre></div><p>For this to compile you just need to define the operator overload:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">static</span> Distance <span style="color:#00f">operator</span>*(<span style="color:#2b91af">int</span> <span style="color:#00f">value</span>, Distance distance)
{
    <span style="color:#00f">return</span> Distance.FromKilometers(<span style="color:#00f">value</span> * distance.Kilometers);
}
</code></pre></div><h2 id="conversion-and-printing">Conversion and printing</h2>
<p>More advanced unit conversions are easy with unit classes. A common use case would be to convert metric units to imperial system. All you need to do is to add another calculated property</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#008000">// Distance class
</span><span style="color:#008000"></span><span style="color:#00f">private</span> <span style="color:#00f">const</span> <span style="color:#2b91af">double</span> MilesInKilometer = 0.621371192;
<span style="color:#00f">private</span> <span style="color:#00f">const</span> <span style="color:#2b91af">double</span> FeetInMeter = = 3.2808399;

<span style="color:#00f">public</span> <span style="color:#2b91af">double</span> Miles =&gt; <span style="color:#00f">this</span>.Kilometers * MilesInKilometer;

<span style="color:#00f">public</span> <span style="color:#2b91af">double</span> Feet =&gt; <span style="color:#00f">this</span>.Meters * FeetInMeter;
</code></pre></div><p>Another common task is printing (formatting) unit values into string. While you can (and should) implement some basic version of it in <code>ToString()</code> method, I advise against doing all the formatting inside the unit class. The formatting scenarios can be quite complex:</p>
<ul>
<li>Format based on user preferences (metric/imperial)</li>
<li>Pick units based on the value (e.g. 30 m but 1.2 km, not 1200 m)</li>
<li>Localization to different languages</li>
<li>Rounding to some closest value</li>
</ul>
<p>If you do all that in the unit class, it&rsquo;s going to violate the single responsibility principle. Just create a separate class for formatting and put all those rules there.</p>
<h2 id="unit-derivation">Unit derivation</h2>
<p>Once you write more unit classes, you will definitely want to derive the calculation result of two units into the third one. In our example, we want to divide <code>Volume</code> of fuel used by <code>Distance</code> to get fuel <code>ConsumptionRate</code>.</p>
<p>There&rsquo;s no magic that you could do here. You will have to define <code>ConsumptionRate</code> class the same way you defined the other two, and then just overload the operation</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#00f">public</span> <span style="color:#00f">static</span> ConsumptionRate <span style="color:#00f">operator</span>/(Volume volume, Distance distance)
{
    <span style="color:#00f">return</span> ConsumptionRate
        .FromLitersPer100Kilometers(volume.Liters * 100.0 / distance.Kilometers);
}
</code></pre></div><p>Of course, you&rsquo;ll have to define all the required combinations explicitly.</p>
<p>If you defined Constants as described above, you&rsquo;ll be able to instantiate values in your tests in the following way:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#2b91af">var</span> fuelRate = 7.5 * lit / (100 * km);
</code></pre></div><h2 id="should-i-use-3rd-party-libraries-for-that">Should I use 3rd party libraries for that?</h2>
<p>It depends. Of course, people implemented all this functionality about 1 million times before you, so there are numerous libraries on GitHub.</p>
<p>I would say, if you start a new project and you don&rsquo;t have a strong opinion about the unit code, just go grab the library and try to use it.</p>
<p>At the same time, for existing code base, it might be easier to introduce your own implementation which would resemble something that you already use.</p>
<p>Also, I have another reason for my own implementation. I&rsquo;m using units all over the code base of domain logic, the very heart of the software, the exact place where I want full control. I find it a bit awkward to introduce a 3rd party dependency in domain layer.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/units-of-measurement" term="units-of-measurement" label="Units of Measurement" />
                             
                                <category scheme="https://mikhail.io/tags/domain-design" term="domain-design" label="Domain Design" />
                             
                                <category scheme="https://mikhail.io/tags/ddd" term="ddd" label="DDD" />
                             
                                <category scheme="https://mikhail.io/tags/clean-code" term="clean-code" label="Clean Code" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Animation]]></title>
            <link href="https://mikhail.io/tags/animation/"/>
            <id>https://mikhail.io/tags/animation/</id>
            
            <published>2015-07-28T00:00:00+00:00</published>
            <updated>2015-07-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Aurelia element animation with custom attribute]]></title>
            <link href="https://mikhail.io/2015/07/aurelia-element-animation-with-custom-attribute/"/>
            <id>https://mikhail.io/2015/07/aurelia-element-animation-with-custom-attribute/</id>
            
            <published>2015-07-28T00:00:00+00:00</published>
            <updated>2015-07-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>I&rsquo;ve been exploring Aurelia javascript UI framework recently to get some experience needed for our next big project. One thing that I couldn&rsquo;t implement out of the box was a kind of animation.</blockquote><p>I&rsquo;ve been exploring <a href="http://aurelia.io">Aurelia</a> javascript UI framework recently to get some experience needed
for our next big project. One thing that I couldn&rsquo;t implement out of the box was a kind
of animation.</p>
<p>I have a grid of values bound to View Model. View Model communicates to server, receives
any updates of data and the grid got immediately updated, all that works great with Aurelia.
Now I want to highlight the cell which has just received an updated value with a small
background animation, like this:</p>
<p><img src="animation.gif" alt="Updated cell animation"></p>
<p>Aurelia has a library called <a href="https://github.com/aurelia/animator-css">aurelia-animator-css</a> with a helper
class to run CSS animation. If you use it directly in your View Model, you will end up with the code like</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">this</span>.newMessageReceived =
    msg =&gt; {
        <span style="color:#00f">this</span>.data.filter(i =&gt; i.id === msg.id).forEach(t =&gt; {
            <span style="color:#00f">let</span> editedItemIdx = <span style="color:#00f">this</span>.data.indexOf(i);
            <span style="color:#00f">var</span> elem = <span style="color:#00f">this</span>.element.querySelectorAll(<span style="color:#a31515">&#39;tbody tr&#39;</span>)[editedItemIdx + 1]
                                   .querySelectorAll(<span style="color:#a31515">&#39;td&#39;</span>)[3];

            <span style="color:#00f">this</span>.animator.addClass(elem, <span style="color:#a31515">&#39;background-animation&#39;</span>).then(() =&gt; {
                <span style="color:#00f">this</span>.animator.removeClass(elem, <span style="color:#a31515">&#39;background-animation&#39;</span>);
            });
        });
    };
</code></pre></div><p>So we get a new message, find the related item in our data, then find the index of that data. Then we use this
index in query selector to get the exact row that needs animation, find the cell by hard coded index, and
finally use animator to highlight the background.</p>
<p>Ouch&hellip; That smells. We spoiled our view model with view details, and all this code is very ugly and fragile.</p>
<p>Good news: we can improve the solution with the Aurelia&rsquo;s feature called Custom Attributes. Let&rsquo;s create a new
javascript file and call it <code>animateonchange.js</code>:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">import</span> {customAttribute} from <span style="color:#a31515">&#39;aurelia-framework&#39;</span>;

<span style="">@</span>customAttribute(<span style="color:#a31515">&#39;animateonchange&#39;</span>)
<span style="color:#00f">export</span> <span style="color:#00f">class</span> AnimateOnChangeCustomAttribute {

}
</code></pre></div><p>I declared a class for our new attribute, so far it&rsquo;s empty. I imported customAttribute decorator from
Aurelia framework: that the way we can define a name for our custom attribute. This can be avoided: if I
change the name to <code>AnimateonchangeCustomAttribute</code>, Aurelia will infer the name from class name, but I want
to stay explicit and keep the class name readable. Note that capital letters are not allowed in attribute name.</p>
<p>Now, let&rsquo;s declare the constructor of the new class and inject all the dependencies:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">import</span> {inject, customAttribute} from <span style="color:#a31515">&#39;aurelia-framework&#39;</span>;
<span style="color:#00f">import</span> {CssAnimator} from <span style="color:#a31515">&#39;aurelia-animator-css&#39;</span>;

<span style="">@</span>customAttribute(<span style="color:#a31515">&#39;animateonchange&#39;</span>)
<span style="">@</span>inject(Element, CssAnimator)
<span style="color:#00f">export</span> <span style="color:#00f">class</span> AnimateOnChangeCustomAttribute {

    constructor(element, animator) {
        <span style="color:#00f">this</span>.element = element;
        <span style="color:#00f">this</span>.animator = animator;
    }

}
</code></pre></div><p>I used dependency injection to get attribute&rsquo;s element and CSS animator and save them into class fields.
Here&rsquo;s how to use them:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#00f">import</span> {inject, customAttribute} from <span style="color:#a31515">&#39;aurelia-framework&#39;</span>;
<span style="color:#00f">import</span> {CssAnimator} from <span style="color:#a31515">&#39;aurelia-animator-css&#39;</span>;

<span style="">@</span>customAttribute(<span style="color:#a31515">&#39;animateonchange&#39;</span>)
<span style="">@</span>inject(Element, CssAnimator)
<span style="color:#00f">export</span> <span style="color:#00f">class</span> AnimateOnChangeCustomAttribute {
    constructor(element, animator) {
        <span style="color:#00f">this</span>.element = element;
        <span style="color:#00f">this</span>.animator = animator;
        <span style="color:#00f">this</span>.initialValueSet = <span style="color:#00f">false</span>;
    }

    valueChanged(newValue){
        <span style="color:#00f">if</span> (<span style="color:#00f">this</span>.initialValueSet) {
            <span style="color:#00f">this</span>.animator.addClass(<span style="color:#00f">this</span>.element, <span style="color:#a31515">&#39;background-animation&#39;</span>).then(() =&gt; {
                <span style="color:#00f">this</span>.animator.removeClass(<span style="color:#00f">this</span>.element, <span style="color:#a31515">&#39;background-animation&#39;</span>);
            });
        }
        <span style="color:#00f">this</span>.initialValueSet = <span style="color:#00f">true</span>;
    }
}
</code></pre></div><p>The new method <code>valueChanged</code> will be called every time the bound value changes. I want to ignore the
first value (it&rsquo;s not an update yet), so I did that with <code>initialValueSet</code> flag. Then I just run CSS
animator. No DOM-related queries!</p>
<p>Here is how we use the custom attribute from a view:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-html" data-lang="html">&lt;template&gt;

    &lt;require from=<span style="color:#a31515">&#34;./animateonchange&#34;</span>&gt;&lt;/require&gt;

    &lt;table class=<span style="color:#a31515">&#34;table&#34;</span>&gt;
        &lt;tr repeat<span style="">.</span>for=<span style="color:#a31515">&#34;item of data&#34;</span>&gt;
            &lt;td&gt;${item.value1}&lt;/td&gt;
            &lt;td&gt;${item.value2}&lt;/td&gt;
            &lt;td animateonchange<span style="">.</span>bind=<span style="color:#a31515">&#34;item.value3ToUpdate&#34;</span>&gt;${item.value3ToUpdate}&lt;/td&gt;
            &lt;td&gt;${item.value4}&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;

&lt;/template&gt;
</code></pre></div><p>First, we use <code>require</code> element to import custom attribute definition (make sure the path is correct
and no <code>.js</code> extension is present).</p>
<p>Second, we use <code>animateonchange.bind</code> to bind the value to the custom attributes. And it works!</p>
<p>Of course, you need to define the CSS class, e.g.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-css" data-lang="css">.<span style="color:#2b91af">background-animation-add</span> {
    <span style="color:#00f">-webkit-</span><span style="color:#00f">animation</span>: changeBack 0.5<span style="color:#2b91af">s</span>;
    <span style="color:#00f">animation</span>: changeBack 0.5<span style="color:#2b91af">s</span>;
}
.<span style="color:#2b91af">background-animation-remove</span> {
    <span style="color:#00f">-webkit-</span><span style="color:#00f">animation</span>: fadeIn 0.5<span style="color:#2b91af">s</span>;
    <span style="color:#00f">animation</span>: fadeIn 0.5<span style="color:#2b91af">s</span>;
}
@<span style="color:#00f">-webkit-keyframes</span> changeBack {
    0% { <span style="color:#00f">background-color</span>: <span style="color:#00f">white</span>; }
    50% { <span style="color:#00f">background-color</span>: <span style="color:#00f">lightgreen</span>; }
    100% { <span style="color:#00f">background-color</span>: <span style="color:#00f">white</span>; }
}
@<span style="color:#00f">keyframes</span> changeBack {
    0% { <span style="color:#00f">background-color</span>: <span style="color:#00f">white</span>; }
    50% { <span style="color:#00f">background-color</span>: <span style="color:#00f">lightgreen</span>; }
    100% { <span style="color:#00f">background-color</span>: <span style="color:#00f">white</span>; }
}
</code></pre></div><p>Here is a <a href="http://plnkr.co/edit/oa0Kb1hf6D9M2jl22vWD">plunkr link to a complete example</a></p>
<p>Happy coding!</p>
<p>Useful links:</p>
<ul>
<li>
<p><a href="http://aurelia.io/docs.html#custom-attributes">Aurelia Custom Attributes documentation</a></p>
</li>
<li>
<p><a href="http://blog.durandal.io/2015/07/17/animating-apps-with-aurelia-part-1/">Animating Apps with Aurelia - Part 1 by Rob Eisenberg</a></p>
</li>
</ul>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/javascript" term="javascript" label="Javascript" />
                             
                                <category scheme="https://mikhail.io/tags/aurelia" term="aurelia" label="Aurelia" />
                             
                                <category scheme="https://mikhail.io/tags/animation" term="animation" label="Animation" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Best Practices]]></title>
            <link href="https://mikhail.io/tags/best-practices/"/>
            <id>https://mikhail.io/tags/best-practices/</id>
            
            <published>2015-05-27T00:00:00+00:00</published>
            <updated>2015-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Code Review]]></title>
            <link href="https://mikhail.io/tags/code-review/"/>
            <id>https://mikhail.io/tags/code-review/</id>
            
            <published>2015-05-27T00:00:00+00:00</published>
            <updated>2015-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Peer code review Why's, How's and What's]]></title>
            <link href="https://mikhail.io/2015/05/peer-code-review-whys-hows-and-whats/"/>
            <id>https://mikhail.io/2015/05/peer-code-review-whys-hows-and-whats/</id>
            
            <published>2015-05-27T00:00:00+00:00</published>
            <updated>2015-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>This post is a short summary on why do code reviews, what the steps are and what should actually be reviewed. The text was created to set some common ground for my team, which was just starting to conduct code reviews in a consistent way.</blockquote><p>This post is a short summary on why do code reviews, what the steps are and what should actually be reviewed. The text was created to set some common ground for my team, which was just starting to conduct code reviews in a consistent way. By any means, this text is not a complete guide but rather a checklist and a starting point for discussion.</p>
<h2 id="whys">WHY&rsquo;s</h2>
<p>So, why spend time on code reviews at all? The most important reasons are listed below:</p>
<p><strong>To find bugs</strong>, to spot and fix defects early in the process;</p>
<p><strong>To spread knowledge</strong>, to share understanding of the code base as team members learn from each other and increase team cohesiveness;</p>
<p><strong>To agree on rules</strong>, to maintain a level of consistency in design and implementation;</p>
<p><strong>To improve as the team</strong>, to identify common defects across the team thus reduce rework;</p>
<p><strong>To get second opinion</strong>, to add a different perspective, &ldquo;another set of eyes&rdquo; adds objectivity.</p>
<h2 id="hows">HOW&rsquo;s</h2>
<p>Which steps should I take during code reviews? This depends a lot on the tools being used, of course. We use the following set of tools: task management tool (<em>Atlassian JIRA</em>), IDE (<em>Visual Studio</em>), source control log (Git log viewer of taste) and code review tool (<em>Atlassian Crucible/FishEye</em>).
You should do the following during your code review process:</p>
<p><strong>Understand the scope</strong>, read the task description, user story, commit or pull request message;</p>
<p><strong>Get the code</strong>, pull the latest code to your hard drive and in your IDE, create a review item in code review tool, make sure that all changes are included;</p>
<p><strong>Read and understand</strong> all the changes by navigating them in your tools;</p>
<p><strong>Ask the author</strong> if something is unclear;</p>
<p><strong>Go through the checklist</strong> (below) and make your peer judgement;</p>
<p><strong>Put your comments</strong> to code review tool, as written comments are usually preferred over verbal;</p>
<p><strong>Merge the code</strong> into main repository or integration branch, if you agree with the proposed changes;</p>
<p><strong>Report the result back</strong> to the team by moving the task to rejected or accepted state, according to your team&rsquo;s workflow.</p>
<h2 id="whats">WHAT&rsquo;s</h2>
<p>Here is the approximate checklist of what should be reviewed during the code review. Start at the top and go down to the bottom.</p>
<p><strong>Readiness</strong>: Is the change under review submitted correctly, are all tests green?</p>
<p><strong>Functionality</strong>: Does the code work? Does it perform its intended function, is the logic correct etc. You don&rsquo;t have to test all scenarios, but you should at least think of possible scenarios and whether the code addresses all of them in your opinion.</p>
<p><strong>Readability</strong>: Is all the code easy to understand? When you read the code, is it easy to understand every detail of it without spending a lot of time and effort?</p>
<p><strong>Testabity</strong>: Is the code testable? I.e. not too many dependencies, unable to initialize objects with stubs or mocks, test frameworks can use methods etc.</p>
<p><strong>No duplication</strong>: Is there any redundant or duplicate code? Do you see some possibility to refactor it to reduce duplication?</p>
<p><strong>Reuse</strong>: Did the author reuse existing classes and libraries for solving common tasks? Is the naming and code structure consistent with similar existing blocks of solution? Can any of the code be replaced with library functions?</p>
<p><strong>Design</strong>: Do you agree with the design and structure of code blocks? Think of coupling and cohesion, SOLID principles, dependency chains etc.</p>
<p><strong>Unit tests</strong>: Do tests exist and are they comprehensive? Are all the changes covered by unit tests? Do they conform to unit testing guidelines? Can you think of more tests which are missing?</p>
<p><strong>Readability of tests</strong>: Are tests short and readable? Do they reveal any design problems?</p>
<p><strong>Error handling</strong>: Is exception handling and logging consistent? Are they tested?</p>
<p><strong>Public contracts</strong>: Could the code violate backwards compatibility when we have to keep it?</p>
<p><strong>No leftovers</strong>: Do you see any code which is commented out or any TODOs? Can any logging or debugging code be removed without loosing functionality?</p>
<p><strong>Code style</strong>: Does it conform to your agreed programming practices and coding style? Do ReSharper and static code analysis tools give no errors and warnings?</p>
<p><strong>Documentation</strong>: Was the documentation updated in case the change touches public API?</p>
<p><strong>Comments</strong>: Are all comments valid? Do you see any dummy comments which are unreadable and were created just to make ReSharper happy?</p>
<p><strong>Performance</strong>: Do you see any potential performance problems that have to be solved before the first version of this code is accepted? When applicable, measure the CPU load, memory, or traffic consumption caused by new code.</p>
<p><strong>High level tests</strong>: When applicable, are integration and/or end-to-end tests created?</p>
<p><strong>Data migration</strong>: When applicable, was database migration script updated?</p>
<p>Am I missing anything important? Probably yes, so please share your suggestions in the comments.
Happy reviews!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/code-review" term="code-review" label="Code Review" />
                             
                                <category scheme="https://mikhail.io/tags/best-practices" term="best-practices" label="Best Practices" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Reflection]]></title>
            <link href="https://mikhail.io/tags/reflection/"/>
            <id>https://mikhail.io/tags/reflection/</id>
            
            <published>2015-04-08T00:00:00+00:00</published>
            <updated>2015-04-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Unit testing null parameter checks]]></title>
            <link href="https://mikhail.io/2015/04/unit-testing-null-parameter-checks/"/>
            <id>https://mikhail.io/2015/04/unit-testing-null-parameter-checks/</id>
            
            <published>2015-04-08T00:00:00+00:00</published>
            <updated>2015-04-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>We use constructor dependency injection throughout our application. Most service classes have constructors, which accept all dependencies in form of interfaces. We also use TDD, which means we must write unit tests for every aspect of our code. So I want to discuss one specific aspect: guarding the constructor parameters from null values and testing this guard.</blockquote><p>We use constructor dependency injection throughout our application. This means that most service classes have constructors, which accept all dependencies in form of interfaces. They are then saved to private fields to be used while class methods are executed. Here is an example (all examples below are in C#):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">VeryUsefulClass</span> : IVeryUsefulClass
{
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> ISomething something;
    <span style="color:#00f">private</span> <span style="color:#00f">readonly</span> ISomethingElse somethingElse;

    <span style="color:#00f">public</span> VeryUsefulClass(ISomething something, ISomethingElse somethingElse)
    {
        <span style="color:#00f">this</span>.something = something;
        <span style="color:#00f">this</span>.somethingElse = somethingElse;
    }

    <span style="color:#00f">public</span> AwesomeResponse DoUsefullStaff(ImportantRequest request)
    {
        <span style="color:#00f">this</span>.something.DoSomething();
        <span style="color:#00f">this</span>.somethingElse.DoSomethingElse();
        <span style="color:#00f">return</span> ...;
    }
}
</code></pre></div><p>We also use TDD, which means we must write unit tests for every aspect of our code. So I want to discuss one specific aspect: guarding the constructor parameters from null values and testing this guard. Here is one possible way to write such tests (with NUnit and Moq):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[TestFixture]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">VeryUsefulClassTests</span>
{
    [Test]
    <span style="color:#00f">public</span> <span style="color:#00f">void</span> WhenSomethingIsNullConstructorThrowsNullException()
    {
        Assert.ThrowsException&lt;ArgumentNullException&gt;(() =&gt;
            <span style="color:#00f">new</span> VeryUsefulClass(<span style="color:#00f">null</span>, <span style="color:#00f">new</span> Mock&lt;ISomethingElse&gt;.Object));
    }

    [Test]
    <span style="color:#00f">public</span> <span style="color:#00f">void</span> WhenSomethingElseIsNullConstructorThrowsNullException()
    {
        Assert.ThrowsException&lt;ArgumentNullException&gt;(() =&gt;
            <span style="color:#00f">new</span> VeryUsefulClass(<span style="color:#00f">new</span> Mock&lt;ISomething&gt;.Object, <span style="color:#00f">null</span>));
    }

    [Test]
    <span style="color:#00f">public</span> <span style="color:#00f">void</span> ImportantRequestProducesAwesomeResponse()
    {
        <span style="color:#2b91af">var</span> target = <span style="color:#00f">new</span> VeryUsefulClass(<span style="color:#00f">new</span> Mock&lt;ISomething&gt;.Object,
            <span style="color:#00f">new</span> Mock&lt;ISomethingElse&gt;.Object);
        ...
    }

    ...
}
</code></pre></div><p>The tests are small and each one tests just one thing. But it looks like we have a bit too much duplication and &ldquo;noise&rdquo;: too much service code around real code under test.</p>
<h2 id="make-it-better">Make it better</h2>
<p>Now let&rsquo;s say we need to introduce another dependency into our useful class: ISomethingNew. The constructor signature will change to</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> VeryUsefulClass(ISomething something,
    ISomethingElse somethingElse,
    ISomethingNew somethingNew)
{
    ...
}
</code></pre></div><p>So, how many places do we have to change in our test class? One per each test, which includes one per each constructor parameter. Quite a lot! If we have a class with many dependencies, we are in trouble. So, before introducing the new dependency, let&rsquo;s refactor the tests. First, let&rsquo;s declare all mocks as private fields and create them in set-up method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[TestFixture]
<span style="color:#00f">public</span> <span style="color:#00f">class</span> <span style="color:#2b91af">VeryUsefulClassTests</span>
{
    <span style="color:#00f">private</span> Mock&lt;ISomething&gt; something;
    <span style="color:#00f">private</span> Mock&lt;ISomethingElse&gt; somethingElse;

    [SetUp]
    <span style="color:#00f">public</span> <span style="color:#00f">void</span> SetUp()
    {
        <span style="color:#00f">this</span>.something = <span style="color:#00f">new</span> Mock&lt;ISomething&gt;();
        <span style="color:#00f">this</span>.somethingElse = <span style="color:#00f">new</span> Mock&lt;ISomethingElse&gt;();
    }
...
</code></pre></div><p>This way the same clean mocks will be available for each and every test. To make use of them, let&rsquo;s create GetTarget method which will create an instance of class under test</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">private</span> VeryUsefulClass GetTarget()
{
    <span style="color:#00f">return</span> <span style="color:#00f">new</span> VeryUsefulClass(<span style="color:#00f">this</span>.something.Object, <span style="color:#00f">this</span>.somethingElse.Object);
}
</code></pre></div><p>Now we are ready to rewrite our test methods with less duplication</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp">[Test]
<span style="color:#00f">public</span> <span style="color:#00f">void</span> WhenSomethingIsNullConstructorThrowsNullException()
{
    <span style="color:#00f">this</span>.something = <span style="color:#00f">null</span>;
    Assert.ThrowsException&lt;ArgumentNullException&gt;(() =&gt; <span style="color:#00f">this</span>.GetTarget());
}

[Test]
<span style="color:#00f">public</span> <span style="color:#00f">void</span> WhenSomethingElseIsNullConstructorThrowsNullException()
{
    <span style="color:#00f">this</span>.somethingElse = <span style="color:#00f">null</span>;
    Assert.ThrowsException&lt;ArgumentNullException&gt;(() =&gt; <span style="color:#00f">this</span>.GetTarget());
}

[Test]
<span style="color:#00f">public</span> <span style="color:#00f">void</span> ImportantRequestProducesAwesomeResponse()
{
    <span style="color:#2b91af">var</span> target = <span style="color:#00f">this</span>.GetTarget();
    ...
}
</code></pre></div><p>So, how many constructor calls do we have to change when we introduce a new dependency now? Just one for the complete test class!</p>
<p>I strongly believe that readability of your test classes is very important. If you make your tests short, expressive and easy to read, your tests will have much higher value: not only the safety net for classes, but also nice documentation which is easy to use and support.</p>
<h2 id="make-it-shine">Make it shine</h2>
<p>I&rsquo;m still not quite satisfied with the amount of code we have to write for such a simple thing as the validation of ArgumentNullException being thrown from constructors. Imagine this: we have hundreds or thousands of classes which follow this same pattern, and we end up writing thousands tests which look almost exactly the same&hellip;</p>
<p>I solved it with a simple helper method:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csharp" data-lang="csharp"><span style="color:#00f">public</span> <span style="color:#00f">void</span> ConstructorMustThrowArgumentNullException(Type type)
{
    <span style="color:#00f">foreach</span> (<span style="color:#2b91af">var</span> constructor <span style="color:#00f">in</span> type.GetConstructors())
    {
        <span style="color:#2b91af">var</span> parameters = constructor.GetParameters();
        <span style="color:#2b91af">var</span> mocks = parameters.Select(
            p =&gt;
                {
                    Type mockType = <span style="color:#00f">typeof</span>(Mock&lt;&gt;).MakeGenericType(
                        <span style="color:#00f">new</span>[] { p.ParameterType });
                    <span style="color:#00f">return</span> (Mock)Activator.CreateInstance(mockType);
                }).ToArray();

        <span style="color:#00f">for</span> (<span style="color:#2b91af">int</span> i = 0; i &lt; parameters.Length; i++)
        {
            <span style="color:#2b91af">var</span> mocksCopy = mocks.Select(m =&gt; m.Object).ToArray();
            mocksCopy[i] = <span style="color:#00f">null</span>;
            <span style="color:#00f">try</span>
            {
                constructor.Invoke(mocksCopy);
                Assert.Fail(<span style="color:#a31515">&#34;ArgumentNullException expected for parameter {0} of
</span><span style="color:#a31515"></span>                             constructor, but no exception was thrown<span style="color:#a31515">&#34;,
</span><span style="color:#a31515"></span>                             parameters[i].Name);
            }
            <span style="color:#00f">catch</span> (TargetInvocationException ex)
            {
                Assert.AreEqual(<span style="color:#00f">typeof</span>(ArgumentNullException),
                    ex.InnerException.GetType(),
                    <span style="color:#2b91af">string</span>.Format(<span style="color:#a31515">&#34;ArgumentNullException expected for parameter {0} of
</span><span style="color:#a31515"></span>                        constructor, but exception of type {1} was thrown<span style="color:#a31515">&#34;,
</span><span style="color:#a31515"></span>                        parameters[i].Name, ex.InnerException.GetType()));
            }
        }
    }
}
</code></pre></div><p>It accepts a type as its only input parameter (obviously, it&rsquo;s easy to make it generic or an extension method). Then, using the reflection, it iterates through the input parameters of a constructor, and passes one null value and mocks all other parameters. It expects ArgumentNullException to be thrown on each call.</p>
<p>You could write one test for all classes in a namespace or in assembly, if the pattern is applied consistently there! And it will let you know when one of your new classes violates the common rule, with zero extra effort.</p>
<p>Does anyone know the library which would do that without me inventing the bicycle myself?</p>
<p>Happy coding!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/unit-testing" term="unit-testing" label="Unit Testing" />
                             
                                <category scheme="https://mikhail.io/tags/dependency-injection" term="dependency-injection" label="Dependency Injection" />
                             
                                <category scheme="https://mikhail.io/tags/best-practices" term="best-practices" label="Best Practices" />
                             
                                <category scheme="https://mikhail.io/tags/reflection" term="reflection" label="Reflection" />
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                             
                                <category scheme="https://mikhail.io/tags/tdd" term="tdd" label="TDD" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Hackathon]]></title>
            <link href="https://mikhail.io/tags/hackathon/"/>
            <id>https://mikhail.io/tags/hackathon/</id>
            
            <published>2015-03-15T00:00:00+00:00</published>
            <updated>2015-03-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[The Green Hackfest - my first hackathon]]></title>
            <link href="https://mikhail.io/2015/03/the-green-hackfest-my-first-hackathon/"/>
            <id>https://mikhail.io/2015/03/the-green-hackfest-my-first-hackathon/</id>
            
            <published>2015-03-15T00:00:00+00:00</published>
            <updated>2015-03-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>In this post I&rsquo;d like to share my experience about the first hackathon that I took part in. It was called The Green Hackfest and took place in Utrecht, the Netherlands on 10-12 October 2014. My team won a price of 1000 EURO, so read on to know what hackathon is, why you should go there, and how to score the price. And of course, pictures, pictures, pictures.</blockquote><p>In this post I&rsquo;d like to share my experience about the first hackathon that I took part in. It was called <a href="http://the-hackfest.com/past-hacks/green-hackfest-utrecht-2014.html">The Green Hackfest</a> and took place in Utrecht, the Netherlands on 10-12 October 2014.</p>
<p><img src="hackfest_banner.jpg" alt="Green Hackfest"></p>
<p>A brief introduction on what hackathon is. A hackathon, or a hackfest in this case (people used these words interchangeably), is an event for programmers, engineers, designers and entrepreneurs where they can get together, form teams and work on solving problems in creative way. The time is very limited, usually from 12 to 24 hours, but in this case we had a &lsquo;marathon&rsquo; of 48 hours. At the start, teams get the challenge definitions and in the end they must pitch their ideas, prototypes and mock-ups to the judges to win the prizes. The participants are traditionally called hackers, which does not mean that they hack bank networks and that kind of stuff, but they rather strive to overcome the usual limitation in whatever area they are competing.</p>
<p>I will start with the most useful part - I&rsquo;ll give several pieces of advice for those who are preparing to their first hackathon. And then I&rsquo;ll get to the description of what was happening this time, so expect quite some photos.</p>
<h2 id="what-i-learnt">What I learnt</h2>
<p>So, in case you are going to take part in one of those hackathons, here is what I can advise you:</p>
<ul>
<li>
<p>Definitely go. It will be quite a new experience for you, well worth the time.</p>
</li>
<li>
<p>Take a laptop with your favourite working environment and any other devices that might be useful. Apart from this and some small personal things, you won&rsquo;t probably need anything else.</p>
</li>
<li>
<p>Don&rsquo;t be late. Otherwise you may miss the important parts: briefs and team formation.</p>
</li>
<li>
<p>Don&rsquo;t worry if you don&rsquo;t have a team before the event, that&rsquo;s normal.</p>
</li>
<li>
<p>Always compete for a prize. Even if it&rsquo;s not particularly important for you, it will help you align your actions and get more drive and fun.</p>
</li>
<li>
<p>Don&rsquo;t be shy, talk to people. If you don&rsquo;t know how to start, just join some other discussion by saying hello and your name. Ask what people think about briefs, what their background is, what are they going to do next and so on. Don&rsquo;t be afraid to get out of comfort zone.</p>
</li>
<li>
<p>Be nice, smile, make jokes, be open and willing to help others, listen to them.</p>
</li>
<li>
<p>Try to join someone who has complementing background for your skills, not someone with the same skill set. If you are a software developer, search for designers, entrepreneurs and pitch speakers.</p>
</li>
<li>
<p>Don&rsquo;t come with anything done in advance. You are welcome to bring relevant ideas, but don&rsquo;t bring implementations. The briefs will be too specific for you to fit. And, in any case, it will be transparent to everyone whether you did your prototype on-site or beforehand. You won&rsquo;t have fun if you start cheating.</p>
</li>
<li>
<p>Focus on pitch preparation quite early. Think through your demo, and focus on making it shiny and impressive, and don&rsquo;t spend much time on anything else. During last hour, practice your speech and check that you fit into time limit.</p>
</li>
<li>
<p>If you are suggested to use a specific technology, use it, you might then compete for two prizes instead of one, which double your chances.</p>
</li>
<li>
<p>If you stay overnight, have some sleep. Take a sleeping bag or a mat with you.</p>
</li>
<li>
<p>Keep your prototype focused. You only need one impressive idea, not 10 boring ideas.</p>
</li>
<li>
<p>There will be someone from sponsors and technology partners to support you. Reach to them and take advice from people who are willing to help you.</p>
</li>
<li>
<p>Have fun!</p>
</li>
</ul>
<h2 id="how-we-scored-at-green-hackfest">How we scored at Green Hackfest</h2>
<p>So we gathered together on Friday evening, and stayed in the hackspace until Sunday night. Participants are provided with all the infrastructure during this time: power plugs, Wi-Fi, tables &amp; chairs, food, drinks and sleeping room. All free of charge.</p>
<p>The whole story starts with introduction from hosts and then with briefs from sponsors. Here is Richard Kastelein, the event organizer, giving his kick-off speech:</p>
<p><img src="Richard_Kastelein.jpg" alt="Richard Kastelein"></p>
<p>This way we got the tasks for these 2 days. There were several tasks, so everyone could choose which one suits them the best. Some folks were late, so they were having troubles afterwards to get the idea of what&rsquo;s expected from them.</p>
<p>I think there were about 70 participants in beginning, and about 50 of them made it to the end. Most people came alone or in pair; and I think there were two pre-formed teams of several people.</p>
<p>Most people were Dutch, but a lot were not. Many international students, scientists and entrepreneurs live in the Netherlands these days, and they were all welcome. So, while some teams were speaking Dutch within the group, English was the default language for other communication all the time, which is very handy. Some participants came to the Netherlands specifically to take part in this event: I remember guys from UK, Belgium and even Serbia.</p>
<p>I am the software developer and I expected the majority of hackers to be developers too, mostly on experienced side. But I was quite wrong. Many people were from entrepreneurial background, coming from ventures on early stage, or research environments. Many folks were students with no career set yet. In result, I was the only person who was able to write code in our team, and one of the most experienced programmers overall (my guess).</p>
<p>So the first task for everyone was to build a team for yourself. There was no formal procedure for that. You just meet people, ask what they think about the challenges and try to start the discussion, then go to someone else, and repeat. Sooner or later, you should find someone with whom you are ready to spend the next 45 hours working on a project. The normal team is of 2 to 5 people, and you should ideally join diverse skills. So if you are a developer, search for entrepreneurs or other idea generators, and also someone with design skills.</p>
<p>This time we had the following briefs, all of them were more or less related to green tech:</p>
<ul>
<li>
<p>Crowdfunding challenge: find new applications for an existing crowdfunding platform</p>
</li>
<li>
<p>Optimize manufacturing line based on monitoring data to use less energy without compromising the production schedule</p>
</li>
<li>
<p>Make people in Utrecht use more bicycles instead of cars (tough task, as Utrecht is one of the cities were pretty much everyone is already using bicycles)</p>
</li>
<li>
<p>Visualize the sustainability of electric car stations</p>
</li>
<li>
<p>Show off anything with IBM BlueMix platform</p>
</li>
</ul>
<p>And there was the main prize for the best overall Hack Green.</p>
<p>Here is the prize structure:</p>
<p><img src="Prize_structure.jpg" alt="Prize structure"></p>
<p>(the student award was eventually transformed to IBM technology award)</p>
<p>Quite quickly I decided that Ebbits brief looks the most promising. First, it provided some test data and API to play with, so we had at least something to start with. Second, it did not sound as romantic as some other topics, so I hoped that not many teams would pick it. And last but not least, there were 3 prizes for this same brief, so any of the top 3 teams were going to cash!</p>
<p>Here is the manager from Ebbits presenting his manufacturing brief:</p>
<p><img src="Manufacturing_brief.jpg" alt="Manufacturing brief"></p>
<p>So I joined several discussions until I found folks who were thinking more or less on the same page, and were equally enthusiastic about smart factory challenge.</p>
<p>The hackspace was available for the whole 48 hours of the hackfest. There was one big open space containing many open cabins with tables, couches and chairs inside:</p>
<p><img src="Hackspace_cabin.jpg" alt="Hackspace four guys 2"></p>
<p>So usually one team could fit into one cabin:</p>
<p><img src="Hackspace_one_team.jpg" alt="Hackspace three guys"></p>
<p>Not too much space, but enough to put two or three laptops + cups or beers aside:</p>
<p><img src="Hackspace_tables.jpg" alt="Hackspace one guy"></p>
<p>The gathering area was located in the centre of this room. People got together for all briefs, intermediate meet-ups, final pitches and prizes.</p>
<p><img src="Hackspace_hall.jpg" alt="Hackspace"></p>
<p>These guys had won the previous hackfest of same series, and I think they didn&rsquo;t speak to anyone, so nobody knew what they were doing:</p>
<p><img src="Last_winners.jpg" alt="Hackspace four guys"></p>
<p>We took the large space behind the scene, and also got a big white board for brainstorming:</p>
<p><img src="Hackfest_our_team.jpg" alt="Hackspace our team"></p>
<p>Wi-Fi was free and fast enough for the participants. Power plugs were everywhere, sometimes hanging from the ceiling:</p>
<p><img src="Hackspace_wires.jpg" alt="Hackspace our team 2"></p>
<p>Food and beverages (including beer) were also provided in effectively unlimited quantities. Breakfasts, lunches, dinners plus night snacks - all for free for hackers. Biological green food for green hack fest:</p>
<p><img src="Food.jpg" alt="Food"></p>
<p>There were two sleeping areas as well. I don&rsquo;t have any photos of those, but they were really basic: just separate empty rooms where you could put a sleeping bag and take a nap. I&rsquo;m not sure how many teams slayed late in the night, but we were lazy and went to sleep at midnight or so :)</p>
<p>In terms of prize winning odds, the most important part of the whole process is the final pitch. Each team had just 2 minutes to make a demo and do the speech. The demo doesn&rsquo;t need to show off a finished product, but everyone had to prepare prototypes, some pieces of working UI. Powerpoint slides were not enough. After the demo, the jury had 3 to 5 minutes to ask questions, and then the next team takes over. There was a training on how to pitch effectively, but I skipped that one.</p>
<p>So, you should leave quite some time before the pitch starts to actually prepare your demo. Focus on what you will say, and implement only the features that will be demoed, don&rsquo;t waste your time on anything else. Try showing something relatively small, specific and easy to understand, but also give a short overview of how you can see this can be expanded. Highlight the strengths of your team and why you are good candidates to do the job that you started.</p>
<p>During the pitch, the team is standing next to a big screen. One member is showing the demo on his laptop, while another member explains the whole story behind. Here is how it looked like:</p>
<p><img src="Pitch.jpg" alt="Pitch"></p>
<p>There were four Judges in the jury, consisting of sponsor representatives and serial entrepreneurs. Here are those smart powerful guys:</p>
<p><img src="Judges.jpg" alt="Judges"></p>
<p>We ended up taking the second place in our category. As we chose to participate in a category with 3 prizes, we scored 1000 euros! This is our photo with the sponsor and our prize:</p>
<p><img src="Our_prize.jpg" alt="Our prize"></p>
<p>And this is the best overall hack team, congratulations to them:</p>
<p><img src="Winners.jpg" alt="Winners"></p>
<p>Here are all the hackers who stayed at the fest until the very end:</p>
<p><img src="All_hackers.jpg" alt="All hackers"></p>
<p>Enjoyable experience, thanks everyone for sharing it.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/hackathon" term="hackathon" label="Hackathon" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Databases]]></title>
            <link href="https://mikhail.io/tags/databases/"/>
            <id>https://mikhail.io/tags/databases/</id>
            
            <published>2015-02-17T00:00:00+00:00</published>
            <updated>2015-02-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How I fixed a bug in SQL Server]]></title>
            <link href="https://mikhail.io/2015/02/how-i-fixed-a-bug-in-sql-server/"/>
            <id>https://mikhail.io/2015/02/how-i-fixed-a-bug-in-sql-server/</id>
            
            <published>2015-02-17T00:00:00+00:00</published>
            <updated>2015-02-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Our production databases were migrated from SQL Server 2012 to SQL Server 2014. Among other improvements in the latter, the new cardinality estimator was introduced. The new estimator is supposed to be smarter than the old one, of course. It takes more factors into account, and thus should give better results with some query plans. But &ldquo;new&rdquo; also means &ldquo;less tested&rdquo;&hellip;</blockquote><p>Well, not really&hellip; I just reported it to Microsoft and then it got fixed. Anyway, here is the story.</p>
<p>Our production databases were migrated from SQL Server 2012 to SQL Server 2014. Among other improvements in the latter, <a href="http://blogs.msdn.com/b/psssql/archive/2014/04/01/sql-server-2014-s-new-cardinality-estimator-part-1.aspx">the new cardinality estimator was introduced</a>. In two words, cardinality estimator is a piece of SQL functionality which estimates how many rows the engine might get for a specific query or a query part. The new estimator is supposed to be smarter than the old one, of course. It takes more factors into account, and thus should give better results with some query plans. But &ldquo;new&rdquo; also means &ldquo;less tested&rdquo;&hellip;</p>
<p>Immediately after migration we found out that one of the stored procedures got much slower than before. That stored procedure was retrieving thousands of rows from a queue, and then did a series of transformations which took ages to complete. During the investigation we found that the query plan is far from being optimal, with massive scans of partitioned tables; and the reason was in ridiculously high estimations on one of the tables. That table was of moderate size (millions of rows), our query touched only some hundreds rows, while the estimate was&hellip; millions!</p>
<p>Here is a repro to show this issue on any test database (database must be in SQL Server 2014 compatibility mode):</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">TABLE</span> [dbo].[Store](
Id int <span style="color:#00f">IDENTITY</span>(1,1) <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
City int <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
<span style="color:#00f">Size</span> int <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>,
Name varchar(<span style="color:#00f">max</span>) <span style="color:#00f">NULL</span>,
     <span style="color:#00f">CONSTRAINT</span> [PK_Store] <span style="color:#00f">PRIMARY</span> <span style="color:#00f">KEY</span> CLUSTERED ([Id] <span style="color:#00f">ASC</span>)
)

<span style="color:#00f">GO</span>

<span style="color:#00f">CREATE</span> NONCLUSTERED <span style="color:#00f">INDEX</span> [IX_Store] <span style="color:#00f">ON</span> [dbo].[Store]
(
City <span style="color:#00f">ASC</span>,
<span style="color:#00f">Size</span> <span style="color:#00f">ASC</span>
)

<span style="color:#00f">GO</span>
<span style="color:#00f">TRUNCATE</span> <span style="color:#00f">TABLE</span> Store
<span style="color:#00f">INSERT</span> Store
<span style="color:#00f">SELECT</span> i % 101, i % 11, <span style="color:#a31515">&#39;Store &#39;</span> + <span style="color:#00f">CAST</span>(i <span style="color:#00f">AS</span> VARCHAR)
  <span style="color:#00f">FROM</span>
  (<span style="color:#00f">SELECT</span> TOP 100000 ROW_NUMBER() OVER (<span style="color:#00f">ORDER</span> <span style="color:#00f">BY</span> s1.[object_id]) <span style="color:#00f">AS</span> i
     <span style="color:#00f">FROM</span> sys.all_objects  s1, sys.all_objects  s2) numbers
<span style="color:#00f">GO</span>

<span style="color:#00f">CREATE</span> <span style="color:#00f">TABLE</span> StoreRequest (City int <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>, <span style="color:#00f">Size</span> int <span style="color:#00f">NOT</span> <span style="color:#00f">NULL</span>)

<span style="color:#00f">GO</span>
<span style="color:#00f">DELETE</span> StoreRequest
<span style="color:#00f">INSERT</span> StoreRequest <span style="color:#00f">values</span> (55, 1)
<span style="color:#00f">INSERT</span> StoreRequest <span style="color:#00f">values</span> (66, 2)

<span style="color:#00f">GO</span>

<span style="color:#00f">SELECT</span> s.City
  <span style="color:#00f">FROM</span> StoreRequest <span style="color:#00f">AS</span> r
       <span style="color:#00f">INNER</span> <span style="color:#00f">JOIN</span> Store <span style="color:#00f">AS</span> s  <span style="color:#00f">WITH</span>(<span style="color:#00f">INDEX</span>(IX_Store), FORCESEEK)
               <span style="color:#00f">ON</span> s.City = r.City <span style="color:#00f">AND</span> s.<span style="color:#00f">Size</span> = r.<span style="color:#00f">Size</span>
 <span style="color:#00f">WHERE</span> r.<span style="color:#00f">Size</span> &lt;&gt; 1 <span style="color:#00f">OR</span> s.City &lt;&gt; 55
</code></pre></div><p>In this script I create two tables: Store with 100k rows and StoreRequest with 2 rows. And then I make a query where I join them on two columns. The WHERE clause contains two restrictions: one for each of the tables with OR clause in between. It sounds more complicated than it really is.</p>
<p>INDEX and FORCESEEK hints are there to force the engine to use this index to be able to see the statistics. Otherwise it considers the index to be too heavy to use. And that is why:</p>
<p><img src="2014estimates.png" alt="Execution plan in SQL Server 2014"></p>
<p>You see the significant error in Estimated Number of Rows vs Actual Number of Rows. The estimator actually thinks that the query is going to yield half of all rows in that table!</p>
<p>Good news: it&rsquo;s actually easy to fix that problem in this case. We should just change the WHERE clause to use same table for both sides of OR condition:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">SELECT</span> s.City
  <span style="color:#00f">FROM</span> StoreRequest <span style="color:#00f">AS</span> r
       <span style="color:#00f">INNER</span> <span style="color:#00f">JOIN</span> Store <span style="color:#00f">AS</span> s  <span style="color:#00f">WITH</span>(<span style="color:#00f">INDEX</span>(IX_Store), FORCESEEK)
               <span style="color:#00f">ON</span> s.City = r.City <span style="color:#00f">AND</span> s.<span style="color:#00f">Size</span> = r.<span style="color:#00f">Size</span>
 <span style="color:#00f">WHERE</span> s.<span style="color:#00f">Size</span> &lt;&gt; 1 <span style="color:#00f">OR</span> s.City &lt;&gt; 55
</code></pre></div><p>Estimator is able to handle this situation correctly:</p>
<p><img src="2014estimates_good.png" alt="Execution plan in SQL Server 2014 as it should be"></p>
<p>Same fix worked for our production query. That query was much more complicated so it took more time to find the reason.</p>
<p>To prove that this bug is fresh for the new cardinality estimator, we can switch the original query to the legacy cardinality estimator with flag 9481:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"> <span style="color:#00f">SELECT</span> s.City
   <span style="color:#00f">FROM</span> StoreRequest <span style="color:#00f">AS</span> r
        <span style="color:#00f">INNER</span> <span style="color:#00f">JOIN</span> Store <span style="color:#00f">AS</span> s  <span style="color:#00f">WITH</span>(<span style="color:#00f">INDEX</span>(IX_Store), FORCESEEK)
                <span style="color:#00f">ON</span> s.City = r.City <span style="color:#00f">AND</span> s.<span style="color:#00f">Size</span> = r.<span style="color:#00f">Size</span>
  <span style="color:#00f">WHERE</span> r.<span style="color:#00f">Size</span> &lt;&gt; 1 <span style="color:#00f">OR</span> s.City &lt;&gt; 55
 <span style="color:#00f">OPTION</span>(QUERYTRACEON 9481)
</code></pre></div><p>And then we get this:</p>
<p><img src="2012estimates.png" alt="Execution plan in SQL Server 2012"></p>
<p>Which is as good as the best attempt of SQL Server 2014 and much-much better than the worst one.</p>
<p>I posted this repro to <a href="https://connect.microsoft.com/SQLServer/feedback/details/1086125/cardinality-estimator-2014-is-off-with-or-in-where-clause">SQL Server forum</a>. Here is the comment that I got from Erland Sommarskog, SQL Server MVP:</p>
<blockquote>
<p>The estimate here is clearly incorrect. SQL Server knows the density of Size and City. It knows the cardinality of the temp table. The density information gives how many rows the join will produce. The WHERE clause will then remove a certain number of rows. With no statistics for the temp table, it does not know how many, but it will apply some standard guess.</p>
</blockquote>
<blockquote>
<p>50000 is a completely bogus number, because the join cannot produce that many rows, and SQL Server is able to compute the join with out the WHERE clause decently. (Well, it estimates 90, when the number is 180.) No, this is obviously a case of the cardinality estimator giving up completely.</p>
</blockquote>
<blockquote>
<p>It is worth noting that both these WHERE clauses gives reasonable estimates:</p>
</blockquote>
<blockquote>
<pre><code>WHERE r.Size &lt;&gt; 11 OR r.City &lt;&gt; 550
WHERE s.Size &lt;&gt; 11 OR s.City &lt;&gt; 550
</code></pre>
</blockquote>
<blockquote>
<p>Whereas these two gives the spooky 50000:</p>
</blockquote>
<blockquote>
<pre><code>WHERE s.Size &lt;&gt; 11 OR r.City &lt;&gt; 550
WHERE r.Size &lt;&gt; 11 OR s.City &lt;&gt; 550
</code></pre>
</blockquote>
<blockquote>
<ul>
<li>Erland Sommarskog</li>
</ul>
</blockquote>
<p>Then I also posted it to <a href="https://connect.microsoft.com/SQLServer/feedback/details/1086125/cardinality-estimator-2014-is-off-with-or-in-where-clause">SQL Server feedback issue tracker</a> and after a couple of weeks got a reply from Microsoft employee:</p>
<blockquote>
<p>Thank you for reporting this issue. I am happy to let you know that we have a fix for this problem, which will be available in one of the upcoming servicing releases for SQL Server 2014 (the details will be published in a KB).</p>
</blockquote>
<blockquote>
<ul>
<li>Thanks, Alexey, SQL Development</li>
</ul>
</blockquote>
<p>Thanks to Erlang and Alexey for their help. I look forward to the servicing release!
Happy coding!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/sql-server" term="sql-server" label="SQL Server" />
                             
                                <category scheme="https://mikhail.io/tags/databases" term="databases" label="Databases" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[How we do message processing]]></title>
            <link href="https://mikhail.io/2015/02/how-we-do-message-processing/"/>
            <id>https://mikhail.io/2015/02/how-we-do-message-processing/</id>
            
            <published>2015-02-05T00:00:00+00:00</published>
            <updated>2015-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Our team develops a back-end system that processes messages from mobile devices. The devices collect information from complex machines and send messages to our data center. In this article I want to share our approaches to building such processing software. The ideas are quite general and can be applied to any system of the following architecture&hellip;</blockquote><p>Our team develops a back-end system that processes messages from mobile devices. The devices collect information from complex machines and send messages to our data center. In this article I want to share our approaches to building such processing software. The ideas are quite general and can be applied to any system of the following architecture:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="architecture.jpg"
            alt="System architecture"
             />
        
    
    <figcaption>
        <h4>System architecture</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The devices use communication channels to send messages to our gateway - the input point of our application. The application&rsquo;s goal is to understand what came in, do the required actions and save the information into the database for further processing. Let&rsquo;s consider the database to be the end point of processing. Sounds easy, right? But some difficulties appear with the growth of amount and diversity of incoming messages; so let&rsquo;s look at some of them.</p>
<p>A few words on the target load level. Our system processes the messages from tens of thousands of devices, and we get from several hundreds to a thousand messages per second. If you numbers are different by orders of magnitude, it might be the case that your problems are going to look completely different and you&rsquo;ll need a different set of tools to solve them.</p>
<p>Apart from the number of messages itself, there is a problem of irregularity and peak times. The application must be ready for relatively short peaks which might be about ten times higher than the average expectation. To address this problem we organize the system as a sequence of queues and corresponding processors.</p>
<p>The input gateway doesn&rsquo;t do much of real job: it just receives a message from a client and puts it into the queue. This operation is very cheap, thus the gateway is capable of accepting a vast number of messages per second. Afterwards a separate process retrieves several messages - the amount it wants to get - from the queue and does the hard work. The processing happens asynchronously while the load on the system remains limited. Perhaps the time in the queue grows at peaks, but that&rsquo;s it.</p>
<p>Normally the message processing is non-trivial and consists of several actions. Here we get to the next logical step: we break down the job into several stages, each one having a separate queue and a dedicated processor. The queues and processors are independent and may reside on separate physical servers; and we can tune and scale them independently:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="sequence.jpg"
            alt="Sequence-based architecture"
             />
        
    
    <figcaption>
        <h4>Sequence-based architecture</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The first queue contains the messages from devices as-is, without decoding or transforming them. The first processor decodes them and puts them into the second queue. The second processor could, for instance, call a third-party service and enrich the message with some relevant information, and the third processor could save that information into the database.</p>
<p>These are the basics, so what do we still need to consider?</p>
<h3 id="define-your-values">Define your values</h3>
<ol>
<li>
<p>Simplicity of creation, change and support</p>
<p>Asynchronous distributed message processing brings quite some extra complexity into the software product. You should constantly work on reducing this price. The code gets optimized to be, at first place, readable and straightforward for all the team members, cheap to be changed and supported. If nobody but the author can decrypt the code, no great architecture will make the team happy.</p>
<p>This statements looks obvious, but it might take quite some time and effort before the team starts to consistently implement this principle and not just declare it. Do the regular refactoring whenever you feel you can make the code a bit better and simpler. All the source code should get reviewed and the most critical parts are better to be developed in pair.</p>
</li>
<li>
<p>Fault tolerance</p>
<p>It makes sense to define your policies of hardware or subsystem failures handling from the very beginning. They will differ for different products. It might be the case that someone can throw away all the messages that come in during 5 minutes of a server reboot.</p>
<p>In our system we don&rsquo;t want to lose messages. If a particular service is not currently available, a database call times out, or there is a random processing error, it must not result in information loss. The affected messages must be saved inside the queue and will be processed right after the problem is fixed.</p>
<p>Suppose your code on one server calls a web service on another server in synchronous manner. If the second server is not available, the processing will fail, and you can&rsquo;t do anything but log the error. In case of asynchronous processing the message will wait for the second server to go live again.</p>
</li>
<li>
<p>Performance</p>
<p>Processing rate per second, latency, load on the servers - all these are important parameters of application performance. That&rsquo;s why we choose the architecture to be flexible.</p>
<p>Although, don&rsquo;t pay TOO much attention to optimization from the very beginning. Usually the majority of performance issues is created by relatively small pieces of code. Unfortunately people tend to be very bad at predicting where exactly those issues are going to appear. People write <a href="http://carlos.bueno.org/optimization/">books on pre-mature optimization</a>. So make sure that your architecture allows you to fine-tune the system and forget about optimization until the first load testing.</p>
<p>At the same time, and for this reason, start running the load tests early on, and then include them into your standard testing procedure. Start optimizing only when the tests reveal a specific performance problem.</p>
</li>
</ol>
<h3 id="fine-tune-your-mindset">Fine tune your mindset</h3>
<ol>
<li>
<p>Operate queues and asynchronous processors</p>
<p>I already described this above. Our main tools are queues and processors. While the classic approach is &ldquo;get request, call remote code, wait for response, return it back to the originator&rdquo;, now we should always use &ldquo;get a message from a queue, process it, send a message to another queue&rdquo;. The right mix of these two approaches should enable both scalability and ease of development.</p>
</li>
<li>
<p>Break the processing down into several stages</p>
<p>If message processing is complex enough to be split into several stages, do that by creating several queues and processors. Make sure that you don&rsquo;t make it too complex to understand by unneeded fragmentation: the right balance is important. Quite often you will see a split which feels natural for developers. If not, try thinking of possible failure points. If there are multiple reasons why a processor may fail, consider its breakdown.</p>
</li>
<li>
<p>Don&rsquo;t mix decoding and processing</p>
<p>Usually the messages arrive encoded with a protocol which can be binary, XML, JSON, etc. Decode them into your native format as soon as possible. This will help you solve two problems. First, you might need to support multiple protocols; and after decoding you unify the format of all further messages. Second, logging and debugging gets simpler.</p>
</li>
<li>
<p>Make the queues topology configurable</p>
<p>Structure your code in a way that allows you to change the configuration of queues relatively easy. Splitting a processor into two parts should not result in tons of refactoring. Don&rsquo;t make your code depend on a specific queue mechanism implementation: tomorrow you might want to change it.</p>
</li>
<li>
<p>Do batch processing</p>
<p>Normally it makes sense to receive messages from a queue in batches, not one by one. The services that you use might accept arrays for faster processing and in this case one call will always be faster than a handful of small ones. One insertion of 100 rows into database is faster than 100 remote insertions.</p>
</li>
</ol>
<h3 id="create-tooling">Create tooling</h3>
<ol>
<li>
<p>Implement total monitoring</p>
<p>Invest into monitoring tools. It should be easy to see the charts of throughput, average processing time, queue size and time since last message with breakdown by queue.
<img src="monitoring.jpg" alt="Queue monitoring"></p>
<p>We use monitoring tools not only on production and staging environments but also on testing servers and even developer&rsquo;s machines. Carefully baked charts are helpful during debug and load testing procedures.</p>
</li>
<li>
<p>Test everything</p>
<p>Message processing systems are perfect fields to apply fully automated testing. Input data protocols are well-defined and there is no human interaction. Cover your code with unit tests. Make your queues pluggable so that you could mock your real queues out with test in-memory queues to run quick intercommunication tests. Finally, create full-blown integration tests which should be run on staging environment (and preferably also on production).</p>
</li>
<li>
<p>Store the failed messages</p>
<p>Usually you don&rsquo;t want one erroneous message to stop the complete queue processing. Being able to diagnose the problem is equally important. So put all the failed messages into a specialized storage and put a spotlight on it. Make a tool to move messages back to the relevant queue as soon as the failure reason is addressed.</p>
<p>Same or similar mechanism can be used to store the messages to be processed at some point of time in the future. Keep them in that special storage and check periodically if now is time to proceed.</p>
</li>
<li>
<p>Automate the deployment</p>
<p>Application setup and update must require just one or two clicks. Strive for frequent updates on production; ideally - automated deployment on every commit to the dedicated branch. Deployment scripts or tools will help developers maintain their personal and testing environments up-to-date.</p>
</li>
</ol>
<h3 id="wrapping-up">Wrapping up</h3>
<p>Clean and understandable architecture provides developers with a good means of communication, helps figure out the similar vision and concepts. Architecture metaphor expressed in form of a picture or short document will bring you closer to smart design, will help find errors or plan a refactoring.</p>
<p>Happy message processing!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/architecture" term="architecture" label="Architecture" />
                             
                                <category scheme="https://mikhail.io/tags/messaging" term="messaging" label="Messaging" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Messaging]]></title>
            <link href="https://mikhail.io/tags/messaging/"/>
            <id>https://mikhail.io/tags/messaging/</id>
            
            <published>2015-02-05T00:00:00+00:00</published>
            <updated>2015-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2014s]]></title>
            <link href="https://mikhail.io/2014/"/>
            <id>https://mikhail.io/2014/</id>
            
            <published>2014-10-16T00:00:00+00:00</published>
            <updated>2014-10-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Debugging]]></title>
            <link href="https://mikhail.io/tags/debugging/"/>
            <id>https://mikhail.io/tags/debugging/</id>
            
            <published>2014-10-16T00:00:00+00:00</published>
            <updated>2014-10-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Nice way to kill your SQL Server]]></title>
            <link href="https://mikhail.io/2014/10/16/nice-way-to-kill-your-sql-server/"/>
            <id>https://mikhail.io/2014/10/16/nice-way-to-kill-your-sql-server/</id>
            
            <published>2014-10-16T00:00:00+00:00</published>
            <updated>2014-10-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>The short story about how to crash your complete system by missing one line of code in SQL Server stored procedure.</blockquote><p>Are you getting enormous amount of</p>
<pre><code>The activated proc '...' running on queue '...' output the following:
'The service queue &quot;...&quot; is currently disabled.
</code></pre><p>errors from SQL Server? Keep on reading!</p>
<p>Yesterday, when I came back from my lunch, I got a heads-up from Windows:</p>
<p><img src="low-disk-space.gif" alt="Low disk space"></p>
<p>And it was literally out of space: 0 (zero) bytes available on C: drive. Wow&hellip;</p>
<p>I cleaned up the binary files from my developments, which gave me 3 GB of free space and some time to do the investigation. A short time, because disk space was counting down at about 300 MB per minute.</p>
<p>It appeared that the SQL Server error log files grew up to 53 GB and were still growing. Here is the default folder where you can find them:</p>
<p><img src="errorlog.png" alt="SQL Server error log"></p>
<p>To gain more time, I executed the command</p>
<pre><code>exec sp_cycle_error
</code></pre>
<p>is Management Studio. It renames current ERRORLOG file to ERRORLOG.1 and starts the new current. So now it was safe to delete all .1/.2/etc files and get those 53 GB back. Huh&hellip;</p>
<p>Now it&rsquo;s time to figure out the actual issue. Go to Management Studio -&gt; your server -&gt; Management -&gt; SQL Server Logs -&gt; Current -&gt; (right click) -&gt; View SQL Server Log</p>
<p>I saw millions of the following messages there (let&rsquo;s say my queue is named XQueue for simplicity):</p>
<pre><code>The activated proc '[dbo].[XQueueProcessor]' running
on queue 'dbo.XQueue' output the following:
'The service queue &quot;XQueue&quot; is currently disabled.
</code></pre>
<p>So, it looks like there is a problem with SQL Server Service Broker, which is constantly trying to read something from disabled queue. Isn&rsquo;t it supposed to stop after 5 attempts (a.k.a. poison messages)? Ok, keep on troubleshooting.</p>
<p>First, let check if the queue is actually disabled</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">SELECT</span> Name, is_receive_enabled, is_enqueue_enabled
  <span style="color:#00f">FROM</span> sys.service_queues
 <span style="color:#00f">WHERE</span> Name = <span style="color:#a31515">&#39;XQueue&#39;</span>
</code></pre></div><p>Yep, it&rsquo;s disabled:
<img src="qdisabled.png" alt="XQueue disabled"></p>
<p>Do we have anything queued?</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">SELECT</span> TOP 1000 *
  <span style="color:#00f">FROM</span> [XQueue] <span style="color:#00f">WITH</span>(NOLOCK)
</code></pre></div><p>I got 9 rows back. This means we got some messages en-queued, and one of them failed to process, the message was considered as poison and the queue was disabled (which is good), but the processor still seems to be running in eternal loop.</p>
<p>So, let&rsquo;s have a look at the processor. We had something like that:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">PROCEDURE</span> [dbo].[XQueueProcessor]
<span style="color:#00f">AS</span>
<span style="color:#00f">BEGIN</span>
    <span style="color:#00f">DECLARE</span>
        @queuing_order BIGINT
       ,@conversation_handle UNIQUEIDENTIFIER
       ,@message_enqueue_time DATETIME
       ,@message_type_name VARCHAR(156)
       ,@message_body VARBINARY(<span style="color:#00f">MAX</span>);

    WHILE(1=1) <span style="color:#008000">-- MAIN WHILE LOOP RECEIVING THE MESSAGE BATCHES
</span><span style="color:#008000"></span>    <span style="color:#00f">BEGIN</span>
        <span style="color:#00f">BEGIN</span> <span style="color:#00f">TRANSACTION</span>
        <span style="color:#00f">BEGIN</span> TRY
            WAITFOR(RECEIVE TOP(1)
                        @queuing_order = [queuing_order],
                        @conversation_handle = [conversation_handle],
                        @message_enqueue_time = message_enqueue_time,
                        @message_type_name = message_type_name,
                        @message_body = message_body
                    <span style="color:#00f">FROM</span> [dbo].[XQueue]), TIMEOUT 10

            <span style="color:#00f">IF</span> (@@ROWCOUNT = 0)
            <span style="color:#00f">BEGIN</span>
                <span style="color:#00f">ROLLBACK</span> <span style="color:#00f">TRANSACTION</span>
                BREAK
            <span style="color:#00f">END</span>

            <span style="color:#008000">-- SOME CRUD OPERATIONS HAPPEN HERE
</span><span style="color:#008000"></span>
            <span style="color:#00f">COMMIT</span> <span style="color:#00f">TRANSACTION</span>
        <span style="color:#00f">END</span> TRY
        <span style="color:#00f">BEGIN</span> CATCH
            <span style="color:#00f">DECLARE</span> @Error varchar(4000) = ERROR_MESSAGE()
            PRINT @Error
            <span style="color:#00f">ROLLBACK</span> <span style="color:#00f">TRANSACTION</span>
        <span style="color:#00f">END</span> CATCH
    <span style="color:#00f">END</span>
<span style="color:#00f">END</span>
</code></pre></div><p>A-ha! So, what happens when an error occurs during the message processing? Control goes to CATCH block, error message is printed out (when we are in Service Broker, print goes directly to ERRORLOG file which we saw already), the transaction is rolled back&hellip; But we still are inside eternal loop, no BREAK or RETURN statement there, so the processor will try again to get the same message from the queue. After 5th attempt, the message will be marked as poison, the queue will get disabled, but the loop will keep running! The exception will change to &ldquo;The service queue is currently disabled&rdquo; and voila, CPU is busy and error logs are floating the disk.</p>
<p>The fix is pretty trivial, just change the CATCH block to</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">BEGIN</span> CATCH
    <span style="color:#00f">DECLARE</span> @Error varchar(4000) = ERROR_MESSAGE()
    PRINT @Error
    <span style="color:#00f">ROLLBACK</span> <span style="color:#00f">TRANSACTION</span>
    <span style="color:#00f">RETURN</span>
<span style="color:#00f">END</span> CATCH
</code></pre></div><p>Be careful!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/debugging" term="debugging" label="Debugging" />
                             
                                <category scheme="https://mikhail.io/tags/sql-server" term="sql-server" label="SQL Server" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[SQL: produce resultset with N rows]]></title>
            <link href="https://mikhail.io/2014/06/03/sql-produce-resultset-with-n-rows/"/>
            <id>https://mikhail.io/2014/06/03/sql-produce-resultset-with-n-rows/</id>
            
            <published>2014-06-03T00:00:00+00:00</published>
            <updated>2014-06-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Sometimes you need to produce a result set, which would contain N rows with numbers 1&hellip;N in each row. For example, I needed to calculate some statistics per week for N weeks starting from today and going back to the past.</blockquote><p>Let&rsquo;s talk about T-SQL today.</p>
<p>Sometimes you need to produce a result set, which would contain N rows with numbers 1&hellip;N in each row. For example, I needed to calculate some statistics per week for N weeks starting from today and going back to the past.</p>
<p>Here is a table funtion which does the enumeration:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">CREATE</span> <span style="color:#00f">FUNCTION</span> dbo.fnEnumerateNumbers(@<span style="color:#00f">max</span> int)
<span style="color:#00f">RETURNS</span> <span style="color:#00f">TABLE</span>
<span style="color:#00f">RETURN</span>
<span style="color:#00f">WITH</span>
 E1(N) <span style="color:#00f">AS</span> (<span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span>
           <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span>
           <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span> <span style="color:#00f">SELECT</span> 1 <span style="color:#00f">UNION</span> <span style="color:#00f">ALL</span>
           <span style="color:#00f">SELECT</span> 1),                 <span style="color:#008000">--10E1  or 10 rows
</span><span style="color:#008000"></span> E2(N) <span style="color:#00f">AS</span> (<span style="color:#00f">SELECT</span> 1 <span style="color:#00f">FROM</span> E1 a, E1 b), <span style="color:#008000">--10E2  or 100 rows
</span><span style="color:#008000"></span> E4(N) <span style="color:#00f">AS</span> (<span style="color:#00f">SELECT</span> 1 <span style="color:#00f">FROM</span> E2 a, E2 b), <span style="color:#008000">--10E4  or 10000 rows
</span><span style="color:#008000"></span> E8(N) <span style="color:#00f">AS</span> (<span style="color:#00f">SELECT</span> 1 <span style="color:#00f">FROM</span> E4 a, E4 b)  <span style="color:#008000">--10E8  or 100000000 rows
</span><span style="color:#008000"></span><span style="color:#00f">SELECT</span> TOP (@<span style="color:#00f">max</span>) N = ROW_NUMBER() OVER (<span style="color:#00f">ORDER</span> <span style="color:#00f">BY</span> (<span style="color:#00f">SELECT</span> <span style="color:#00f">NULL</span>))
  <span style="color:#00f">FROM</span> E8
</code></pre></div><p>Usage is trivial:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">SELECT</span> N
  <span style="color:#00f">FROM</span> dbo.fnEnumerateNumbers(100)
</code></pre></div><p>And here is how I solved the task of enumerating weeks:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#00f">SELECT</span> dateadd(week, 1 - N, getutcdate())
  <span style="color:#00f">FROM</span> dbo.fnEnumerateNumbers(100)
</code></pre></div><p>There are multiple possibilities to implement similar function, but I believe this way is the best one in terms of performance. No temporary tables, no table reads, no dependencies on table existance, and you can define your maximum yourself.</p>
<p>Happy coding!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                             
                                <category scheme="https://mikhail.io/tags/sql-server" term="sql-server" label="SQL Server" />
                             
                                <category scheme="https://mikhail.io/tags/t-sql" term="t-sql" label="T-SQL" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[ASP.NET]]></title>
            <link href="https://mikhail.io/tags/asp.net/"/>
            <id>https://mikhail.io/tags/asp.net/</id>
            
            <published>2014-05-21T00:00:00+00:00</published>
            <updated>2014-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Sgen to precompile classes for XmlSerializer]]></title>
            <link href="https://mikhail.io/2014/05/21/sgen-to-precompile-classes-for-xmpserializer/"/>
            <id>https://mikhail.io/2014/05/21/sgen-to-precompile-classes-for-xmpserializer/</id>
            
            <published>2014-05-21T00:00:00+00:00</published>
            <updated>2014-05-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>During my investigation of our ASP.NET application performance issue, I&rsquo;ve found out that XmlSerializer may require a long warm-up. The first time, when it&rsquo;s used for a specific class (de-)serialization, can take up to 500 ms om my machine!</blockquote><p>During my investigation of our ASP.NET application performance issue, I&rsquo;ve found out that XmlSerializer may require a long warm-up. The first time, when it&rsquo;s used for a specific class (de-)serialization, can take up to 500 ms om my machine! We use XmlSerializer to encode/decode user preferences. Having 40 different classes being deserialized at user login lead to a massive delay of 14 seconds. This is only for the first user login after the application start-up, but you do quite a lot of &lsquo;first times&rsquo; every day while developing the application.</p>
<p>These delays are caused by the runtime which prepares and compiles the strongly typed classes for serialization in runtime. Not sure why it&rsquo;s SO slow, but probably things might go wrong.</p>
<p>There is a simple solution for this problem: create those helper classes at build time, pack it into an assembly and ship it as part of your .NET application. The tool that does this is called Sgen.exe, and it is a part of .NET SDK. So, we just need to use it in our project.</p>
<p>The easiest way is to add a post-build event in your CS project properties. The event would look like</p>
<pre>sgen.exe /force /assembly:"$(TargetPath)"</pre>
<p>After re-compiling the project, you should see a new assembly in Bin directory, which has the name ending with XmlSerializers.dll. That&rsquo;s the new assembly which will save you several seconds on each application start-up. It will be automatically copied to Bin directory of the projects which reference the original assembly with your serializable classes, so no other actions needed.</p>
<p>However, this command might fail. Sgen tool will try to prepare the helper classes for each class in your assembly, which might not be possible. Then you&rsquo;ll get some error messages in Visual Studio. In this case, you might want to add a &ldquo;/types&rdquo; parameter, explicitly listing the class names to precompile. But I have a better solution for you.</p>
<p>It&rsquo;s quite possible that you use MSBuild for regular automated builds of your project. We do. In this case a post-build event in the project properties won&rsquo;t work during those build. Then, do not add any post-build events, but instead open your .csproj file in plain text editor, and search for the following commented section:</p>
<pre><code>&lt;!-- To modify your build process, add your task inside one of the targets below and uncomment it.
   Other similar extension points exist, see Microsoft.Common.targets.
&lt;Target Name=&quot;BeforeBuild&quot;&gt;
&lt;/Target&gt;
&lt;Target Name=&quot;AfterBuild&quot;&gt;
&lt;/Target&gt;
--&gt;
</code></pre>
<p>Uncomment the second XML node and substitute it with something like this:</p>
<pre><code>&lt;Target Name=&quot;AfterBuild&quot; DependsOnTargets=&quot;AssignTargetPaths;Compile;ResolveKeySource&quot; Inputs=&quot;$(MSBuildAllProjects);@(IntermediateAssembly)&quot; Outputs=&quot;$(OutputPath)$(_SGenDllName)&quot;&gt;
    &lt;ItemGroup&gt;
      &lt;SgenTypes Include=&quot;MyNamespace.MySerializableClass1&quot; /&gt;
      &lt;SgenTypes Include=&quot;MyNamespace.MySerializableClass2&quot; /&gt;
      &lt;SgenTypes Include=&quot;MyNamespace.MySerializableClassN&quot; /&gt;
    &lt;/ItemGroup&gt;
    &lt;Delete Files=&quot;$(TargetDir)$(TargetName).XmlSerializers.dll&quot; ContinueOnError=&quot;true&quot; /&gt;
    &lt;SGen BuildAssemblyName=&quot;$(TargetFileName)&quot; BuildAssemblyPath=&quot;$(OutputPath)&quot; References=&quot;@(ReferencePath)&quot; ShouldGenerateSerializer=&quot;true&quot; UseProxyTypes=&quot;false&quot; KeyContainer=&quot;$(KeyContainerName)&quot; KeyFile=&quot;$(KeyOriginatorFile)&quot; DelaySign=&quot;$(DelaySign)&quot; ToolPath=&quot;$(SGenToolPath)&quot; Types=&quot;@(SgenTypes)&quot;&gt;
      &lt;Output TaskParameter=&quot;SerializationAssembly&quot; ItemName=&quot;SerializationAssembly&quot; /&gt;
    &lt;/SGen&gt;
&lt;/Target&gt;
</code></pre>
<p>The ItemGroup lists all the classes that you need to precompile. You can omit this section and Types attribute of SGen node if you want to compile all classes in assembly.
Have a nice application start-up boost!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/asp.net" term="asp.net" label="ASP.NET" />
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                             
                                <category scheme="https://mikhail.io/tags/debugging" term="debugging" label="Debugging" />
                             
                                <category scheme="https://mikhail.io/tags/performance" term="performance" label="Performance" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2013s]]></title>
            <link href="https://mikhail.io/2013/"/>
            <id>https://mikhail.io/2013/</id>
            
            <published>2013-01-06T00:00:00+00:00</published>
            <updated>2013-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Poker]]></title>
            <link href="https://mikhail.io/tags/poker/"/>
            <id>https://mikhail.io/tags/poker/</id>
            
            <published>2013-01-06T00:00:00+00:00</published>
            <updated>2013-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Poker Money application for Windows Store]]></title>
            <link href="https://mikhail.io/2013/01/06/poker-money-application-for-windows-store/"/>
            <id>https://mikhail.io/2013/01/06/poker-money-application-for-windows-store/</id>
            
            <published>2013-01-06T00:00:00+00:00</published>
            <updated>2013-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Windows 8 / Windows RT / Windows Store all arrived in October 2012. I&rsquo;ve decided to take a shot on writing a commercial application for this new platform. And that&rsquo;s how Poker Money app appeared.</blockquote><p>Windows 8 / Windows RT / Windows Store all arrived in October 2012.</p>
<p>I&rsquo;ve decided to take a shot on writing a commercial application for this new platform. And that&rsquo;s how Poker Money app appeared.</p>
<p>Here are the links to it:</p>
<p><a href="http://pokermoneyapp.com" title="Poker money app official website">Poker Money app official website</a></p>
<p><a href="http://apps.microsoft.com/windows/ru-RU/app/poker-money/47c61b67-94ec-4aae-a9c4-f60a37d7cc03" title="Poker Money in Windows Store">Windows Store application page</a></p>
<p><a href="http://forumserver.twoplustwo.com/45/software/poker-money-app-1282491/" title="Support thread for Poker Money">Support thread on 2+2</a></p>
<p>Poker Money is a basic tool for tracking online poker results, which supports all game types for the two major poker rooms: PokerStars and FullTiltPoker.</p>
<p>Here is a sample screen:
<img src="pokermoneyapp-screen.png" alt="Poker Money app on Windows 8"></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/poker" term="poker" label="Poker" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[2012s]]></title>
            <link href="https://mikhail.io/2012/"/>
            <id>https://mikhail.io/2012/</id>
            
            <published>2012-12-26T00:00:00+00:00</published>
            <updated>2012-12-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[One certificate missing, one received unexpectedly]]></title>
            <link href="https://mikhail.io/2012/12/26/one-certificate-missing-one-received-unexpectedly/"/>
            <id>https://mikhail.io/2012/12/26/one-certificate-missing-one-received-unexpectedly/</id>
            
            <published>2012-12-26T00:00:00+00:00</published>
            <updated>2012-12-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Today I want to give a note about two more online courses that I completed. The last one (10 weeks in September-November) was called Introduction to Computational Finance and Financial Econometrics and was lead by Eric Zivot from University of Washington. First of all, it was the most intense course out of all that I took to date.</blockquote><p>Today I want to give a note about two more online courses that I completed.</p>
<p>The last one (10 weeks in September-November) was called <a href="https://www.coursera.org/course/compfinance">Introduction to Computational Finance and Financial Econometrics</a> and was lead by Eric Zivot from University of Washington. First of all, it was the most intense course out of all that I took to date. Every of 10 weeks we had about 3-4 hours of videos + homework. The course material was mostly about maths and statistics, and less about application in economics. However, some concepts were new to me and sounded very interesting.</p>
<p>I scored about 95% on all tests and exams (they were relatively easy comparing to video material). But unfortunately, I got no certificate or even course record for this! It seems that University of Washington did not allow Coursera to issue such certificates and records&hellip; What a shame! I spent lots of time on it, and a record would be satisfactory. Please be careful if you take this course or other courses of UoW in the future.</p>
<p>However, I got another unexpected certificate. In June, I took a course called <a href="https://www.coursera.org/course/hci">Human-Computer Interaction</a> about user interface design concepts and approaches. I started it with a big delay because of my vacation, so I thought I wouldn&rsquo;t score enough for a certificate. But I was wrong! I got the certificate 6 months after the course ended:
<img src="hci.jpg" alt="HCI certificate"></p>
<p>The course was short but useful. I wish there would be an extension course, which would cover more details on the topic.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/coursera" term="coursera" label="Coursera" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Introduction to Finance class at Coursera]]></title>
            <link href="https://mikhail.io/2012/11/01/finance-class-at-coursera/"/>
            <id>https://mikhail.io/2012/11/01/finance-class-at-coursera/</id>
            
            <published>2012-11-01T00:00:00+00:00</published>
            <updated>2012-11-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>I&rsquo;ve recently received a statement of accomplishment document for Coursera&rsquo;s online Introduction to Finance class that I took in July-September this year.</blockquote><p>I&rsquo;ve recently received a statement of accomplishment document for <a href="https://www.coursera.org/" title="Coursera">Coursera</a>&rsquo;s online <a href="https://www.coursera.org/course/introfinance" title="Introduction to Finance class">Introduction to Finance class</a> that I took in July-September this year.</p>
<p><img src="finance.jpg" alt="Finance"></p>
<p>The course was lead by prof. Gautam Kaul from University of Michigan. The level of lectures was really introductory, the pace was very slow (too slow for me, even watching x1.5 speed). You could learn about basic finance principles like time value of money, calculating cash flows and discounted values, about stocks and bonds, and risk/return relations.</p>
<p>I liked a lot the assignment tests, which were much stronger and solid then the examples in lectures. They were challenging most of the time, so I&rsquo;m glad I managed to get 100% score in the end :)</p>
<p>Recommended for anyone who has no idea about what finance is about, but not for experienced students.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/coursera" term="coursera" label="Coursera" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Gamification class at Coursera]]></title>
            <link href="https://mikhail.io/2012/10/29/gamification-class-at-coursera/"/>
            <id>https://mikhail.io/2012/10/29/gamification-class-at-coursera/</id>
            
            <published>2012-10-29T00:00:00+00:00</published>
            <updated>2012-10-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>I&rsquo;ve recently received a statement of accomplishment document for Coursera&rsquo;s online Gamification class that I took in August-October this year.</blockquote><p>I&rsquo;ve recently received a statement of accomplishment document for <a href="https://www.coursera.org/" title="Coursera">Coursera</a>&rsquo;s online <a href="https://www.coursera.org/course/gamification" title="Gamification class">Gamification class</a> that I took in August-October this year.</p>
<p><img src="gamification.jpg" alt="Gamification certificate" title="Gamification certificate"></p>
<p>The course was lead by prof. Kevin Werbach from The Wharton School, University of Pennsylvania. He told us about applying game techniques to non-game world problems: business, social etc. In essence, it&rsquo;s on how to add FUN to processes, that are not supposed to be fun at all. Which is really cool. I loved both informative and well-producted lectures, and peer-reviewed assignments that appeared to be&hellip; fun :)</p>
<p>Want to know why Stackoverflow and Foursquare are such a success? Come join this course next time!</p>
<p>I hope I&rsquo;ll have a chance to apply the ideas from the course in real life!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/coursera" term="coursera" label="Coursera" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Fiddler]]></title>
            <link href="https://mikhail.io/tags/fiddler/"/>
            <id>https://mikhail.io/tags/fiddler/</id>
            
            <published>2012-09-02T00:00:00+00:00</published>
            <updated>2012-09-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Use Fiddler to debug urlfetch requests in Google AppEngine]]></title>
            <link href="https://mikhail.io/2012/09/02/use-fiddler-to-debug-urlfetch-requests-in-google-appengine/"/>
            <id>https://mikhail.io/2012/09/02/use-fiddler-to-debug-urlfetch-requests-in-google-appengine/</id>
            
            <published>2012-09-02T00:00:00+00:00</published>
            <updated>2012-09-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>We use a lot of web crawling to get data from third-party websites. Some crawling is not as easy as just a simple GET request, so we have to send specific POST data, cookies and HTTP headers. And all this needs to be debugged. Fiddler2 is the gold standard for web debugging tools, so I&rsquo;d like to use it in this case too.</blockquote><p>In <a href="http://www.tripbenefit.com" title="TripBenefit.com - travel in St. Petersburg, Russia">TripBenefit</a> application, we use a lot of web crawling to get data from third-party websites. As the application works on top of Google AppEngine, the <a href="https://developers.google.com/appengine/docs/python/urlfetch/fetchfunction?hl=ru" title="urlfetch.fetch() docs">urlfetch.fetch()</a> function is used to send HTTP requests and get responses.</p>
<p>Some crawling is not as easy as just a simple GET request, so we have to send specific POST data, cookies and HTTP headers. And all this needs to be debugged. <a href="http://www.fiddler2.com" title="Fiddler2 web debugging tool">Fiddler2</a> is the gold standard for web debugging tools, so I&rsquo;d like to use it in this case too. I.e. I want to see urlfetch requests displayed in Fiddler, while I&rsquo;m on development env.**
**</p>
<p>However, to make it work, I have to make AppEngine run all fetch requests through the Fiddler&rsquo;s proxy: localhost:8888. As proxies aren&rsquo;t supported within Google production environment, it&rsquo;s not supported by development engine either. There&rsquo;s simply no such &lsquo;proxy&rsquo; parameter in urlfetch!</p>
<p>It seems that the only way to overcome this limitation is to modify the code of AppEngine development server. It isn&rsquo;t that tricky as it sounds! :)</p>
<p>Stop all your developmenet AppEngine instances. Go to your AppEngine folder, and then to \google\appengine\api. Edit urlfetch_stub.py (make a backup beforehand). Search for &ldquo;connection =&rdquo; line, something like</p>
<pre><code>if _CONNECTION_SUPPORTS_TIMEOUT:
  connection = connection_class(host, timeout=deadline)
else:
  connection = connection_class(host)
</code></pre>
<p>Substitute it with</p>
<pre><code>connection = connection_class('127.0.0.1', 8888)
</code></pre>
<p>Then, search for &ldquo;connection.request&rdquo; method call similar to</p>
<pre><code>connection.request(method, full_path, payload, adjusted_headers)
</code></pre>
<p>and insert another line before it:</p>
<pre><code>full_path = protocol + &quot;://&quot; + host + full_path
</code></pre>
<p>Done! After you run the dev server and Fiddler2, you should be able to see urlfetch requests in web debugger. Have a nice debugging session!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/debugging" term="debugging" label="Debugging" />
                             
                                <category scheme="https://mikhail.io/tags/fiddler" term="fiddler" label="Fiddler" />
                             
                                <category scheme="https://mikhail.io/tags/python" term="python" label="Python" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Google adds Public Transit into API]]></title>
            <link href="https://mikhail.io/2012/08/14/google-adds-public-transit-into-api/"/>
            <id>https://mikhail.io/2012/08/14/google-adds-public-transit-into-api/</id>
            
            <published>2012-08-14T00:00:00+00:00</published>
            <updated>2012-08-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Now, both Google Directions Web Services and Google Maps JavaScript API include building the routes with Public Transit.</blockquote><p>Now, both Google Directions Web Services and Google Maps JavaScript API include building the routes with Public Transit.</p>
<p>Blog post:</p>
<p><a href="http://googlegeodevelopers.blogspot.com/2012/06/public-transit-routing-and-layer-now.html">http://googlegeodevelopers.blogspot.com/2012/06/public-transit-routing-and-layer-now.html</a></p>
<p>API docs:</p>
<p><a href="https://developers.google.com/maps/documentation/directions/#TravelModes">https://developers.google.com/maps/documentation/directions/#TravelModes</a></p>
<p><a href="https://developers.google.com/maps/documentation/javascript/directions#TransitOptions">https://developers.google.com/maps/documentation/javascript/directions#TransitOptions</a></p>
<p>Sample request in St. Petersburg:</p>
<p><a href="http://maps.googleapis.com/maps/api/directions/json?origin=59.94473,30.294254&amp;destination=59.80612,30.308552&amp;sensor=false&amp;departure_time=1346197500&amp;mode=transit">http://maps.googleapis.com/maps/api/directions/json?origin=59.94473,30.294254&amp;destination=59.80612,30.308552&amp;sensor=false&amp;departure_time=1346197500&amp;mode=transit</a></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/google-maps" term="google-maps" label="Google Maps" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Google Maps]]></title>
            <link href="https://mikhail.io/tags/google-maps/"/>
            <id>https://mikhail.io/tags/google-maps/</id>
            
            <published>2012-08-14T00:00:00+00:00</published>
            <updated>2012-08-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Enable jinja2 and i18n translations on Google AppEngine]]></title>
            <link href="https://mikhail.io/2012/07/26/enable-jinja2-and-i18n-translations-on-google-appengine/"/>
            <id>https://mikhail.io/2012/07/26/enable-jinja2-and-i18n-translations-on-google-appengine/</id>
            
            <published>2012-07-26T00:00:00+00:00</published>
            <updated>2012-07-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>My initial goal was to make our new application (based on python/AppEngine) translatable. All strings in the application must be translatable. Translations should preferably stored in separate files. It should be easy to use the translations both in .py files and html templates.</blockquote><p>My initial goal was to make our new application (based on python/AppEngine) translatable. This means the following requirements:</p>
<ol>
<li>All strings in the application must be translatable</li>
<li>Translations should preferably stored in separate files</li>
<li>It should be easy to use the translations both in .py files and html templates</li>
</ol>
<p>The solution that I came to after a couple of hours includes the following components: Babel (string file generation), i18n.gettext (getting strings in code) and jinja2 &lt;% trans %&gt; tag (getting strings in templates). The setup of all this is not obvious, so I&rsquo;ll put the steps in this blog post. Let&rsquo;s start!</p>
<ol>
<li>
<p>Intall Babel: <a href="http://babel.edgewall.org/">http://babel.edgewall.org/</a></p>
<p>You need to install it, not just ref from the application, as you&rsquo;ll need its comman &lsquo;pybabel&rsquo; to generate locale-specific files. I use Windows, so I just downloaded the installation package.</p>
<p>Make sure that Python folders are in your PATH variable. I use Python 2.7, so to make Babel work I&rsquo;ll need the following values in PATH: &ldquo;C:\Python27;C:\Python27\Scripts&rdquo;. Scripts folder contains the pybabel executable.</p>
</li>
<li>
<p>Install jinja2: <a href="http://jinja.pocoo.org/">http://jinja.pocoo.org</a>
Once again, you need to install it, as Babel will need it to parse strings in templates. Just run</p>
<pre><code> easy_install Jinja2
</code></pre>
</li>
<li>
<p>Put Babel and <a href="http://pypi.python.org/pypi/gaepytz">gaepytz</a> libraries inside your GAE application. They are required for i18n module.</p>
</li>
<li>
<p>Configure jinja2 to be used in your application. You&rsquo;ll need the following entry in app.yaml:</p>
<pre><code> libraries:
 - name: jinja2
   version: &quot;2.6&quot;
</code></pre>
<p>and your webhandler.py will look something similar to this:</p>
<pre><code> import webapp2
 from webapp2_extras import jinja2
 from webapp2_extras import i18n
 from google.appengine.ext.webapp.util import run_wsgi_app

 class MainHandler(webapp2.RequestHandler):
     @webapp2.cached_property
     def jinja2(self):
         return jinja2.get_jinja2(app=self.app)

     def get(self):
         i18n.get_i18n().set_locale('ru_RU') # sample locale assigned
         ... # your web site functionality goes here

 # jinja2 config with i18n enabled
 config = {'webapp2_extras.jinja2': {
              'template_path': 'templates',
              'environment_args': { 'extensions': ['jinja2.ext.i18n'] }
            }
           }
 application = webapp2.WSGIApplication([('.*', MainHandler)], config=config)

 def main():
     run_wsgi_app(application)

 if __name__ == &quot;__main__&quot;:
     main()
</code></pre>
<p>This code will work if you put your jinja2 templates into &ldquo;templates&rdquo; folder.</p>
</li>
<li>
<p>Create the translations markup. This means, you define the translatable strings in python code with a commonly used &lsquo;_&rsquo; alias:</p>
<pre><code> from webapp2_extras.i18n import gettext as _

 def do_some_text():
     return _('some text')
</code></pre>
<p>or in jijna2 template with {% trans %} block:</p>
<pre><code> {% block buttons %}
 &lt;div&gt;
     &lt;div onclick=&quot;window.print()&quot;&gt;{% trans %}Print{% endtrans %}&lt;/div&gt;
 &lt;/div&gt;
 {% endblock %}
</code></pre>
</li>
<li>
<p>Create a Babel configuration file babel.cfg (put it into the application folder for now):</p>
<pre><code> [jinja2: **/templates/**.html]
 encoding = utf-8
 [python: source/*.py]
 [extractors]
 jinja2 = jinja2.ext:babel_extract
</code></pre>
<p>This file instructs Babel to extract translatable strings from html jinja2 templates in &ldquo;templates&rdquo; folder and python files in &ldquo;source&rdquo; folder.</p>
</li>
<li>
<p>Now it&rsquo;s time to create translations. First, add a &ldquo;locale&rdquo; folder in application root. Still being in root folder, run the following pybabel command to extract the translatable strings from the code</p>
<pre><code> pybabel extract -F ./babel.cfg -o ./locale/messages.pot ./
</code></pre>
<p>then initialize the locales with</p>
<pre><code> pybabel init -l en_US -d ./locale -i ./locale/messages.pot
 pybabel init -l ru_RU -d ./locale -i ./locale/messages.pot
</code></pre>
<p>Now open locale\ru_RU\LC_MESSAGES\messages.po file in your favorite text editor, and produce the translations (you have to change &lsquo;msgstr&rsquo; only):</p>
<pre><code> #: templates/sample.html:10
 msgid &quot;Print&quot;
 msgstr &quot;Печать&quot;
 #: source/test.py:13
 msgid &quot;some text&quot;
 msgstr &quot;немного текста&quot;
</code></pre>
<p>And finally compile the texts with</p>
<pre><code> pybabel compile -f -d ./locale
</code></pre>
</li>
<li>
<p>Every time you need to add more strings, you should do the same steps as in 6, but use &ldquo;update&rdquo; instead of &ldquo;init&rdquo;:</p>
<pre><code> pybabel update -l en_US -d ./locale -i ./locale/messages.pot
 pybabel update -l ru_RU -d ./locale -i ./locale/messages.pot
</code></pre>
</li>
</ol>
<p>Done! You should be able to run the application and see the strings translated.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/gcp" term="gcp" label="GCP" />
                             
                                <category scheme="https://mikhail.io/tags/python" term="python" label="Python" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Model Thinking class at Coursera]]></title>
            <link href="https://mikhail.io/2012/06/21/model-thinking-class-at-coursera/"/>
            <id>https://mikhail.io/2012/06/21/model-thinking-class-at-coursera/</id>
            
            <published>2012-06-21T00:00:00+00:00</published>
            <updated>2012-06-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>I&rsquo;ve recently received a statement of accomplishment document for Coursera&rsquo;s online Model Thinking Class that I took in April-May this year.</blockquote><p>I&rsquo;ve recently received a statement of accomplishment document for <a href="https://www.coursera.org/" title="Coursera">Coursera</a>&rsquo;s online <a href="https://www.coursera.org/course/modelthinking" title="Model Thinking Class">Model Thinking Class</a> that I took in April-May this year.</p>
<p><img src="modelthinking.jpg" alt="Model Thinking" title="Model Thinking"></p>
<p>The course was kind of short but very intense, with a broad variety of models reviewed and many many very cool examples of models applied to real-world processes like population segregation, economic growth, disease spread, auctions and sport results. Scott E Page is a cool teacher who knows how to explain things in simple words and speaks very fast :)</p>
<p>Thanks to Scott E and University of Michigan. Sign up for more courses coming at <a href="Coursera" title="Coursera">Coursera</a>!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/coursera" term="coursera" label="Coursera" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[API to get the list of hotels]]></title>
            <link href="https://mikhail.io/2012/05/17/api-to-get-the-list-of-hotels/"/>
            <id>https://mikhail.io/2012/05/17/api-to-get-the-list-of-hotels/</id>
            
            <published>2012-05-17T00:00:00+00:00</published>
            <updated>2012-05-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>In our travelling application, we need to show the list of hotels in a city (St. Petersburg, Russia at the moment, but more will be needed in the future). The idea was to find a hotel information provider, and then upload the complete list into our own database.</blockquote><p>In our travelling application, we need to show the list of hotels in a city (St. Petersburg, Russia at the moment, but more will be needed in the future). The idea was to find a hotel information provider, and then upload the complete list into our own database. The following info is needed for each hotel:</p>
<ul>
<li>Name in English</li>
<li>Location (latitude / longitude)</li>
<li>An image would be nice</li>
<li>Probably, some sort of rating
We started with <a href="http://www.booking.com/index.html?aid=352212" title="Booking.com">Booking.com</a>, which does have API, but the API is NOT public, and one has to provide website/bank account information to become an affiliate and get the access.</li>
</ul>
<p>Then, I had a try with <a href="http://GeoNames.org" title="GeoNames.org">GeoNames.org</a>, which I once used for the list of populated localities in Europe. Unfortunately, the POI list in Russia is quite poor there.</p>
<p>We had next try with <a href="http://www.openstreetmap.org" title="OpenStreetMap">OpenStreetMap</a> data. I&rsquo;ve downloaded a 7 GB XML file with POIs of Russia. But I got disappointed once again after parsing it: only about 100 hotels in St. Petersburg + most names are in Russian.</p>
<p>Finally, we&rsquo;ve found the solution. <a href="http://www.hotelscombined.com/?a_aid=61901" title="HotelsCombined">HotelsCombined</a> has an easy-to-access and useful service to download the data feed files with hotels. 590 hotels in St. Petersburg - good enough! Here is how you get it:</p>
<ol>
<li>Go to <a href="http://www.hotelscombined.com/Affiliates.aspx">http://www.hotelscombined.com/Affiliates.aspx</a></li>
<li>Register there (no company or bank data is needed)</li>
<li>Open &ldquo;Data feeds&rdquo; page</li>
<li>Choose &ldquo;Standard data feed&rdquo; -&gt; &ldquo;Single file&rdquo; -&gt; &ldquo;CSV format&rdquo; (you may get XML as well)</li>
</ol>
<p>Parsing the CSV file is a piece of cake, here is a sample Python code to filter out hotels from St. Petersburg:</p>
<pre><code>def filter_hotels(from_file):
    with open(from_file, 'r') as fr:
        while True:
            line = fr.readline()
            if len(line) == 0:
                break # EOF
            hotel = line.split(',')
            city_code = hotel[5]
            country_code = hotel[10]
            if city_code == 'St_Petersburg' and country_code == 'RU':
                hotel_name = hotel[2]
                print hotel_name
</code></pre>
<p>Here is the complete list of fields in CSV/XML:</p>
<pre><code>hotelId, hotelFileName, hotelName, rating, cityId, cityFileName, cityName, stateId, stateFileName, stateName, countryCode, countryFileName, countryName, imageId, address, minRate, currencyCode, Latitude, Longitude, NumberOfReviews, ConsumerRating, PropertyType, ChainID, Facilities
</code></pre>
<p><strong>Update:</strong> Unfortunately, HotelsCombined.com has introduced the new regulations: they&rsquo;ve restricted the access to data feeds by default. To get the access, a partner must submit some information on why one needs the data. The HC team will review it and then (maybe) will grant access. Sad but true. I&rsquo;m in the middle of getting through this guarg, I&rsquo;ll let you know about the result.</p>
<p><strong>Update 2:</strong> Yes, we got the access to data feeds again. After reviewing the application form, HotelsCombined asked us to let them know our IP, white-listed it and now we can download the files. Still, I don&rsquo;t know why they need all this procedure at all.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/api" term="api" label="API" />
                             
                                <category scheme="https://mikhail.io/tags/python" term="python" label="Python" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Books]]></title>
            <link href="https://mikhail.io/tags/books/"/>
            <id>https://mikhail.io/tags/books/</id>
            
            <published>2012-05-16T00:00:00+00:00</published>
            <updated>2012-05-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Books that I’ve recently read, part 2]]></title>
            <link href="https://mikhail.io/2012/05/16/books-that-ive-recently-read-part-2/"/>
            <id>https://mikhail.io/2012/05/16/books-that-ive-recently-read-part-2/</id>
            
            <published>2012-05-16T00:00:00+00:00</published>
            <updated>2012-05-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Reviews of &ldquo;Rework&rdquo; by Jason Fried and David Heinemeier Hansson, &ldquo;Don&rsquo;t Make Me Think&rdquo; by Steve Krug, &ldquo;The Non-Designer&rsquo;s Design Book&rdquo; by Robin Williams and &ldquo;The Toilet Paper Entrepreneur&rdquo; by Mike Michaliwicz.</blockquote><p><img src="rework3.jpg" alt="Rework" title="rework"><strong>&ldquo;Rework&rdquo;</strong> by Jason Fried and David Heinemeier Hansson (<a href="http://www.amazon.com/Rework-Jason-Fried/dp/0307463745" title="Rework book">link</a>)</p>
<p>Short but practical and inspiring  entrepreneurial handbook for the twenty-first century. The guys from <a href="http://37signals.com/" title="37 signals">37signals</a> give no boring theory but dozens of useful hints for a business &ldquo;starter&rdquo;.</p>
<p>The book is not deap, just a &lsquo;one-evening-reading&rsquo;. And the advice they give is quite the opposite to what people teach in business schools.</p>
<p>Great reading for those who want to start their own business, have their own business, or are just interested in the subject. But don&rsquo;t believe them blind. Score is 4 of 5.</p>
<p><img src="krug.png" alt="Krug" title="krug"></p>
<p><strong>&ldquo;Don&rsquo;t Make Me Think&rdquo;</strong> by Steve Krug (<a href="http://www.amazon.com/exec/obidos/ASIN/0789723107" title="Don">link</a>)</p>
<p>A very well laid introduction into web site usability. Steve gives several generic rules (&ldquo;the Krug&rsquo;s laws&rdquo;) on how to get into user&rsquo;s head, how to think usability, how to make pages clear, understandable and consise. Also be ready to good humor and excellent illustrations.</p>
<p>This book may seem old (first published in 2000) but the rules are still up-to-date, and the book became an industry standard. Must-read for software developers, product managers and everyone who&rsquo;s job is related to web site creation.</p>
<p>Score is 5.</p>
<p><img src="nondesigner.jpg" alt="Non-designer" title="Non-designer"> <strong>&ldquo;The Non-Designer&rsquo;s Design Book&rdquo;</strong> by Robin Williams (<a href="http://www.amazon.com/exec/obidos/ASIN/0321193857" title="The Non-Designer">link</a>)</p>
<p>I&rsquo;m far from being a good designer but I&rsquo;d really love to get at least the very basics of the skill to make beautiful things (UI mostly).</p>
<p>The author of this book explains some really generic principles of design, i.e. contrast, repetition, alignment, proximity and proper font usage. She gives the clear overview of each principle and provides examples and exercises to illustrate them.</p>
<p>We&rsquo;ll see whether the book will help me build good-looking web pages&hellip; In any case, it&rsquo;s absolutely worth reading for persons who have interest in the subject.</p>
<p><img src="toiletpaperentrepreneur.jpg" alt="" title="toiletpaperentrepreneur"> <strong>&ldquo;The Toilet Paper Entrepreneur&rdquo;</strong> by Mike Michaliwicz (<a href="http://www.toiletpaperentrepreneur.com/book/overview/" title="The Toilet paper entrepreneur">link</a>)</p>
<p>The book is about running a startup without any significant budget available.</p>
<p>To me, this book is mostly about motivation and call to action. Stop crying about not having this or that. Don&rsquo;t wait for a million to come before you start your business. Just start today and work hard!</p>
<p>Otherwise, I don&rsquo;t think I&rsquo;ll take much of Mike&rsquo;s advice. Score is 3 of 5.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/books" term="books" label="Books" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Google Transit API research]]></title>
            <link href="https://mikhail.io/2012/04/10/google-transit-api-research/"/>
            <id>https://mikhail.io/2012/04/10/google-transit-api-research/</id>
            
            <published>2012-04-10T00:00:00+00:00</published>
            <updated>2012-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Recently, Google has launch a Transit service for the biggest cities of Russia: a service to caculate routes with public transport options. As a part of RnD for upcoming project, I need to understand whether there&rsquo;s any feasible way to calculate public transport route programmatically. I.e. I need Google Transit API.</blockquote><p>Recently, Google has launch a Transit service for the biggest cities of Russia: a service to caculate routes with public transport options. As a part of RnD for upcoming project, I need to understand whether there&rsquo;s any feasible way to calculate public transport route programmatically. I.e. I need Google Transit API.</p>
<p>Here&rsquo;s what I&rsquo;ve got so far:</p>
<ol>
<li>
<p>There&rsquo;s no official Google Transit API at the momemnt. Transit feeds are provided by third party agencies, and most of those feeds are not public. So, Google is not allowed to open them as API.</p>
</li>
<li>
<p>You may try to consume the &ldquo;unofficial&rdquo; API using the following sample link:
<a href="http://maps.google.com/?saddr=St.%20Petersburg%20Shavrova%2013&amp;daddr=St.%20Petersburg%20Sadovaya%2032&amp;dirflg=r&amp;output=json" title="Sample Google Transit JSON result">http://maps.google.com/?saddr=St.%20Petersburg%20Shavrova%2015&amp;daddr=St.%20Petersburg%20Sadovaya%2030&amp;dirflg=r&amp;output=json</a></p>
</li>
<li>
<p>However, the result won&rsquo;t be a valid JSON. Instead, that&rsquo;s something, that can be easily converted to a JavaScript object. (The differences are: there is no quotes around property names, the strings are not properly encoded etc.)</p>
</li>
<li>
<p>Imagine you got this JavaScript object. However, it won&rsquo;t allow you to easily get the structured route details. Object&rsquo;s properties contain the route points coordinates, but no descriptions. The only place where the descriptions may be found is &lsquo;panel&rsquo; property, which contains a chunk of HTML text (sample <a href="https://skydrive.live.com/redir.aspx?cid=c010011792a4b538&amp;resid=C010011792A4B538!129&amp;parid=C010011792A4B538!124&amp;authkey=!AEzvBnLErMyVDbo" title="Sample HTML of ">here</a>).</p>
</li>
<li>
<p>So, you&rsquo;ll have to convert this HTML into XML (X-HTML) and then build the parser of this XML to get the essence data of a trip.</p>
</li>
</ol>
<p>Seems like a bit of overkill to me. Having in mind, that &ldquo;unofficial&rdquo; API may change in the future, including slight changes in &lsquo;panel&rsquo; HTML structure that will kill your parser.</p>
<p>Also, I&rsquo;m still not sure whether I can change the language of route directions.</p>
<p><strong>Update</strong>: language is specified with &ldquo;hl&rdquo; request parameter.</p>
<p><strong>Update 2</strong>: <a href="https://mikhail.io/2012/08/14/google-adds-public-transit-into-api/">Google Transit API is now available</a>!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/api" term="api" label="API" />
                             
                                <category scheme="https://mikhail.io/tags/google-maps" term="google-maps" label="Google Maps" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Multi-level telerik's TileView]]></title>
            <link href="https://mikhail.io/2012/03/13/multi-level-teleriks-tileview/"/>
            <id>https://mikhail.io/2012/03/13/multi-level-teleriks-tileview/</id>
            
            <published>2012-03-13T00:00:00+00:00</published>
            <updated>2012-03-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>We were investigating the options for visual representation of hierarchical dashboard in our Silverlight application. This means, we want to display the number of gauges with tree-like structure. Here is a sample&hellip;</blockquote><p>We were investigating the options for visual representation of hierarchical dashboard in our Silverlight application. This means, we want to display the number of gauges with tree-like structure. Here is a sample:</p>
<pre><code>Dashboard
--- Fuel economy index
------ Preventative driving
--------- Accelaration style
--------- Braking frequency
--------- Stop approach
------ Gear shifting index
--------- ...
--- Degree of difficulty
------ ...
--- Events
------ ......
</code></pre>
<p>One of the options was to use <a href="http://www.telerik.com/products/silverlight/controls/tileview.aspx" title="Telerik">Telerik&rsquo;s TileView control</a>. It doesn&rsquo;t support hierachies out of the box, but we&rsquo;ve made a prototype control, which proves that the task is doable. Here is how nested TileView&rsquo;s look like:
<img src="tileview.png" alt="Tile view" title="Tile view"></p>
<p>Everything works with templates and data binding, no dirty tricks needed. You can download the sample project with multi-level TileView&rsquo;s from <a href="https://skydrive.live.com/redir.aspx?cid=c010011792a4b538&amp;resid=C010011792A4B538!128&amp;parid=root" title="Multi-level TileView sample project">here</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                             
                                <category scheme="https://mikhail.io/tags/silverlight" term="silverlight" label="Silverlight" />
                             
                                <category scheme="https://mikhail.io/tags/telerik" term="telerik" label="Telerik" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Silverlight]]></title>
            <link href="https://mikhail.io/tags/silverlight/"/>
            <id>https://mikhail.io/tags/silverlight/</id>
            
            <published>2012-03-13T00:00:00+00:00</published>
            <updated>2012-03-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Telerik]]></title>
            <link href="https://mikhail.io/tags/telerik/"/>
            <id>https://mikhail.io/tags/telerik/</id>
            
            <published>2012-03-13T00:00:00+00:00</published>
            <updated>2012-03-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Books that I've recently read]]></title>
            <link href="https://mikhail.io/2012/03/01/books-that-ive-recently-read/"/>
            <id>https://mikhail.io/2012/03/01/books-that-ive-recently-read/</id>
            
            <published>2012-03-01T00:00:00+00:00</published>
            <updated>2012-03-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>My reviews of &ldquo;Steve Jobs&rdquo; by Walter Isaacson, &ldquo;The Art of the Start&rdquo; by Guy Kawasaki, &ldquo;How to Become a Businessman&rdquo; and &ldquo;I&rsquo;m Just Like Anyone Else&rdquo; by Oleg Tinkov.</blockquote><p><img src="stevejobsbook2.jpg" alt="Steve Jobs book" title="Steve Jobs book">
**&ldquo;Steve Jobs&rdquo; **by Walter Isaacson (<a href="http://www.amazon.com/Steve-Jobs-Walter-Isaacson/dp/1451648537" title="Steve Jobs on Amazon">link</a>)</p>
<p>I guess it&rsquo;s one of the most popular books in the world at the moment. The story of the most influential person in IT world ever, that reveals the intense personality and extraordinary talent.</p>
<p>I never used a single product of Apple company, but still I&rsquo;m very impressed with what Steve did and with the way how he did that. Now I understand that many things are possible, even if everyone <em>knows</em> they are not.</p>
<p>The book is pretty long and detailed, but it&rsquo;s worth the time. I rate it 4 out of 5.</p>
<p><img src="tinkov1.jpg" alt="Tinkov" title="Tinkov"> <strong>&ldquo;How to Become a Businessman&rdquo;</strong> by Oleg Tinkov (<a href="http://tinkov.com/payment/5/" title="Tinkov books">link</a>)</p>
<p>A book by the most famous Russian entrepreneur. Short, simple and sharp, it gives a portion of good advice for young businessmen.</p>
<p>To me it appeared to be really motivating. The main idea is &ldquo;keep your head clean and your work hard - and you&rsquo;ll earn your million&rdquo;.</p>
<p>The book is quite short (you&rsquo;ll probably finish it in one or two nights) and well structured (how to find the idea, make the brand, start, develop and sell the business) with real life example.</p>
<p>5 star reading for a rookie businessman in Russia.</p>
<p><img src="tinkov2.jpg" alt="Tinkov 2" title="Tinkov">
<strong>&ldquo;I&rsquo;m Just Like Anyone Else&rdquo;</strong> by Oleg Tinkov (<a href="http://itunes.apple.com/us/book/im-just-like-anyone-else/id407756421" title="Oleg Tinkov">link</a>)</p>
<p>Another book of same author. He didn&rsquo;t mean to teach the reader, but instead he tells the story of his life and all of his businesses to the moment.</p>
<p>The biography of strong entrepreneur and charismatic leader is definitely worth reading. And while doing that, you&rsquo;ll probably learn some hints and ideas on business in Russia.</p>
<p>Rating is 4*.</p>
<p>**
<img src="the_art_of_the_start.jpg" alt="" title="the_art_of_the_start">&ldquo;The Art of  the Start&rdquo;** by Guy Kawasaki (<a href="http://www.guykawasaki.com/the-art-of-the-start/" title="Guy Kawasaki">link</a>)</p>
<p>Guy is a former evangelist for Apple&rsquo;s Macintosh computer and now - a Silicon Valley venture capitalist.</p>
<p>While he gives some generic valuable hints on start-up business, the book did not seem the right one for me at the moment. The author goes into great detail when it comes to raising capital, preparing awesome presentations and self-marketing.</p>
<p>You know, I don&rsquo;t expect myself to form the board of directors any soon&hellip; So I&rsquo;ll give this book 3*, as it&rsquo;s good but not relevant enough for my needs.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/books" term="books" label="Books" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Gauge control with two values (Silverlight)]]></title>
            <link href="https://mikhail.io/2012/02/28/gauge-control-with-two-values-silverlight/"/>
            <id>https://mikhail.io/2012/02/28/gauge-control-with-two-values-silverlight/</id>
            
            <published>2012-02-28T00:00:00+00:00</published>
            <updated>2012-02-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>While developing the dashboard to display several performance indexes, I bumped into the task of visualizing the pair of numbers on a single gauge. One number would show a person&rsquo;s performance metric, while the other one would display the person group&rsquo;s performance indicator to compare with. I couldn&rsquo;t find a ready control to use, so I decided to design my own.</blockquote><p>While developing the dashboard to display several performance indexes, I bumped into the task of visualizing the pair of numbers on a single gauge. One number would show a person&rsquo;s performance metric, while the other one would display the person group&rsquo;s performance indicator to compare with. I couldn&rsquo;t find a ready control to use, so I decided to design my own.</p>
<p>I took the look idea from <a href="http://www.fusioncharts.com/demos/gallery/gauges/chart.asp?id=linear_1" title="Fusion Chart gauge sample">Fusion Charts</a> flash controls (highly recommended), which I was using a couple of years ago:
<img src="fusiongauge.png" alt="" title="FusionGauge"></p>
<p>I needed the same thing, but with two values displayed at the same time. Requirements are pretty simple:</p>
<ul>
<li>Three colored zones (low-red / average-yellow / high-green) with configurable thresholds</li>
<li>Configurable min and max values</li>
<li>One value marker on top of the gauge and another one on the bottom</li>
<li>Bubble with exact value next to each marker</li>
<li>All control properties are ready to be data-bound</li>
<li>Flexible size</li>
<li>No ticks needed
Here is what I ended up with:
<img src="mygauge.png" alt="" title="MyGauge"></li>
</ul>
<p>Quite minimalistic and sexy ;-)  The control usage is self-explanatory:</p>
<pre><code>&lt;local:Gauge x:Name=&quot;Gauge&quot; Value=&quot;71&quot; SecondValue=&quot;84&quot; RedThreshold=&quot;60&quot; YellowThreshold=&quot;80&quot;
                 MinValue=&quot;0&quot; MaxValue=&quot;100&quot; Width=&quot;400&quot; /&gt;
</code></pre>
<p>Source code has no rocket-science inside, so I won&rsquo;t post it here. You can download the Gauge control test application <a href="https://skydrive.live.com/redir.aspx?cid=c010011792a4b538&amp;resid=C010011792A4B538!127&amp;parid=root">here</a>.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/silverlight" term="silverlight" label="Silverlight" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Hunting memory leaks in Silverlight]]></title>
            <link href="https://mikhail.io/2012/02/05/hunting-memory-leaks-in-silverlight/"/>
            <id>https://mikhail.io/2012/02/05/hunting-memory-leaks-in-silverlight/</id>
            
            <published>2012-02-05T00:00:00+00:00</published>
            <updated>2012-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Recently, I&rsquo;ve spent a couple of days seeking and fixing memory leaks in our Silverlight application. It might be tough sometimes, but it&rsquo;s a good &lsquo;brain-teasing&rsquo; practice and it&rsquo;s a good way to learn how inner things work.</blockquote><p>Recently, I&rsquo;ve spent a couple of days seeking and fixing memory leaks in our Silverlight application. It might be tough sometimes, but it&rsquo;s a good &lsquo;brain-teasing&rsquo; practice and it&rsquo;s a good way to learn how inner things work.</p>
<p>Memory leaks in Silverlight applications may be found with WinDbg tool. Have a look here for a quick introduction:
<a href="http://davybrion.com/blog/2009/08/finding-memory-leaks-in-silverlight-with-windbg/">http://davybrion.com/blog/2009/08/finding-memory-leaks-in-silverlight-with-windbg/</a></p>
<p>Basically, I only use 4 commands in WinDbg. First, I enable debugging Silverlight 5 applications:</p>
<pre><code>!load C:\Program Files (x86)\Microsoft Silverlight\5.0.61118.0\sos.dll
</code></pre>
<p>Then, I get the amount of instances for class names matching the search pattern:</p>
<pre><code>!dumpheap -stat -type &lt;type name&gt;
</code></pre>
<p>Next, I use the result to locate class instances' addresses:</p>
<pre><code>!dumpheap -MT &lt;MT from previous&gt;
</code></pre>
<p>Finally, I get the reference chain for each of address found:</p>
<pre><code>!gcroot &lt;address&gt;
</code></pre>
<p>Having done this, you&rsquo;ll have to turn your brain on and try to figure out whether you found a memory leak and why if yes, why it takes place. Here are several examples of memory leaks that I found during last session:</p>
<ol>
<li>
<p>HtmlPage.Document.AttachEvent() call, which was never detached. We were using some browser-level API to subscribe on HTML document events, and never cared about unsubscribing. I got it refactored to native Silverlight event handlers, and this solved the issue.</p>
</li>
<li>
<p>Deriving from MultiScaleTileSource. We used to have several classes for map layer rendering, inherited from MultiScaleTileSource. If an instance of such class is never used for map rendering (no assignment to MultiScaleImage.Source), then the GC never collects it (together with everything it references, of course). Another portion of refactoring helped: do not derive from MultiScaleTileSource whenever it&rsquo;s not needed for MultiScaleImage population. Sounds trivial, right? But it helps a lot.</p>
</li>
<li>
<p>Improper usage of CompositionTarget.Rendering event. For our custom animated Progress control, we used to subscribe to CompositionTarget.Rendering in control constructor, and then animated the progress in event handler. As the Progress control used to be a part of control tree (in Expanded/Collapsed state), the event was never unsubsribed, which means&hellip; ta-dam&hellip; a memory leak. Now I use CompositionTarget.Rendering more carefully: subscribe to it at the point when Progress control is shown, and unsubscribe immediately after it gets collapsed.</p>
</li>
<li>
<p>One of our major controls used to fail to unsubscribe itself from global error handler. There were a couple other similar event-related leaks. All fixed.</p>
</li>
<li>
<p>A weird Telerik-related leak. We actively use RadGridView control throughout our application, and we dynamically set HeaderCellStyle for some of the columns depending on grid configuration. Unexpectedly enough, this led to a memory leak with the following reference stack:</p>
<p>HandleTable:
02f311f4 (pinned handle)
-&gt; 11ee6210 System.Object[]
-&gt; 10f21690 OurNamespace.Utilities.AppletHelper
-&gt; 10f21670 System.EventHandler<code>1[[OurNamespace.Utilities.EventArgs</code>1[[System.Exception, mscorlib]], OurNamespace.Utilities]]
-&gt; 10f2103c QFVClient.App
-&gt; 1111a9bc QFVClient.PageContainer
-&gt; 11122138 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11122184 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 1188ba6c System.Windows.Controls.Grid
-&gt; 1188bad4 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 118912d8 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11122dc8 System.Windows.Controls.Border
-&gt; 11122e30 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11122e7c System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11128ea4 System.Windows.Controls.Grid
-&gt; 11128f0c System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11128f58 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11128fa4 System.Windows.Controls.Border
-&gt; 1112900c System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11129058 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11132e00 System.Windows.Controls.Grid
-&gt; 11132e68 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11135bc4 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11133758 QFVClient.Controls.SearchBox
-&gt; 11133ca8 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11133cf4 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11133c40 System.Windows.ResourceDictionary
-&gt; 11133d30 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11133d7c System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11133e18 System.Windows.Style
-&gt; 11133e68 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 11133eb4 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 11133ef0 System.Collections.Generic.Dictionary<code>2[[System.UInt32, mscorlib],[System.Windows.DependencyObject, System.Windows]] -&gt; 11133f3c System.Collections.Generic.Dictionary</code>2+Entry[[System.UInt32, mscorlib],[System.Windows.DependencyObject, System.Windows]][]
-&gt; 110dced0 System.Windows.Style
-&gt; 110dcf20 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 110dcf6c System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 110dce6c System.Windows.SetterBaseCollection
-&gt; 110dcfa8 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 110dcff4 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 110dcd0c System.Windows.Setter
-&gt; 110dcd5c System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 110dcda8 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 110dcde4 System.Collections.Generic.Dictionary<code>2[[System.UInt32, mscorlib],[System.Windows.DependencyObject, System.Windows]] -&gt; 110dce30 System.Collections.Generic.Dictionary</code>2+Entry[[System.UInt32, mscorlib],[System.Windows.DependencyObject, System.Windows]][]
-&gt; 110dcad0 System.Windows.Controls.ControlTemplate
-&gt; 110dcb34 System.Windows.ResourceDictionary
-&gt; 110dd0d8 MS.Internal.ResourceDictionaryCollection
-&gt; 110dd21c System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 1111609c System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 110df418 System.Windows.ResourceDictionary
-&gt; 110df470 System.Collections.Generic.Dictionary<code>2[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]] -&gt; 110eb8b4 System.Collections.Generic.Dictionary</code>2+Entry[[MS.Internal.IManagedPeerBase, System.Windows],[System.Object, mscorlib]][]
-&gt; 110ecde0 System.Windows.Style
-&gt; 116d1cd4 OurNamespace.Controls.TextColumn</p>
</li>
</ol>
<p>PageContainer is the application container which always exists, while TextColumn is a custom column class for a Telerik grid, which is already removed from application tree at the time point. Our TextColumn gets bound with root controls through ResourceDictionary/Style chain; and I can&rsquo;t explain the reason of such a leak. Though, clearing HeaderCellStyle when grid is not needed anymore does fix the problem. Weird enough.</p>
<p>As you can see, there are numerous reasons for memory leaks in a Silverlight application. Some are obvious (e.g. global event handlers never unsubscribed), other ones are hard to understand intuitively and sometimes depend on runtime or third party internals. In any case, a way to fix or workaround always exists. Good luck with bug hunting!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/csharp" term="csharp" label="CSharp" />
                             
                                <category scheme="https://mikhail.io/tags/memory-leaks" term="memory-leaks" label="Memory leaks" />
                             
                                <category scheme="https://mikhail.io/tags/silverlight" term="silverlight" label="Silverlight" />
                             
                                <category scheme="https://mikhail.io/tags/windbg" term="windbg" label="WinDbg" />
                            
                        
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Memory leaks]]></title>
            <link href="https://mikhail.io/tags/memory-leaks/"/>
            <id>https://mikhail.io/tags/memory-leaks/</id>
            
            <published>2012-02-05T00:00:00+00:00</published>
            <updated>2012-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[WinDbg]]></title>
            <link href="https://mikhail.io/tags/windbg/"/>
            <id>https://mikhail.io/tags/windbg/</id>
            
            <published>2012-02-05T00:00:00+00:00</published>
            <updated>2012-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Machine Learning]]></title>
            <link href="https://mikhail.io/tags/machine-learning/"/>
            <id>https://mikhail.io/tags/machine-learning/</id>
            
            <published>2012-02-01T00:00:00+00:00</published>
            <updated>2012-02-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Machine Learning class at Stanford online]]></title>
            <link href="https://mikhail.io/2012/02/01/machine-learning-class-at-stanford-online/"/>
            <id>https://mikhail.io/2012/02/01/machine-learning-class-at-stanford-online/</id>
            
            <published>2012-02-01T00:00:00+00:00</published>
            <updated>2012-02-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Several days ago, I&rsquo;ve received a Statement of Accomplishment document from Andrew Ng and Machine Learning online class team.</blockquote><p>Several days ago, I&rsquo;ve received a Statement of Accomplishment document from Andrew Ng and Machine Learning online class team.
<img src="mlstatement.png" alt="" title="MLStatement"></p>
<p>My score is below the maximum just because I started about two weeks late, and thus got a penalty for first exercises.</p>
<p>Overall, that was a great experience of online learning, thanks to Andrew and his team. If you still haven&rsquo;t heard of Stanford online courses, please visit <a href="http://http://www.ml-class.org" title="Machine Learning class">Machine Learning class page</a>. There you&rsquo;ll also find the list of other classes to be available soon, I&rsquo;m already signed to some of them.</p>
<p>Join, it&rsquo;s gonna be fun!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="https://mikhail.io/tags/machine-learning" term="machine-learning" label="Machine Learning" />
                             
                                <category scheme="https://mikhail.io/tags/learning" term="learning" label="Learning" />
                            
                        
                    
                
            
        </entry>
    
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[WPF]]></title>
            <link href="https://mikhail.io/tags/wpf/"/>
            <id>https://mikhail.io/tags/wpf/</id>
            
            <published>2012-01-26T00:00:00+00:00</published>
            <updated>2012-01-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/colleagues/"/>
            <id>https://mikhail.io/drafts/colleagues/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="what-i-loved-about-my-colleagues">What I loved about my colleagues</h2>
<p>Susan: Engaged, actively fosters communication within team, cares about others.</p>
<p>Pat: Deep technical knowledge. Holistic views on problems and solutions. Care about the precise definitions, fighting ambiguity, articulate. Open, willing to share opinions even they aren&rsquo;t pleasant to hear or simple to deliver.</p>
<p>Levi: Caring deeply about a specific area, owning everything around it.</p>
<p>Lee Z: Delivering negative feedback. Taking negative feedback.</p>
<p>Evan: Digging deeper inside the essense of questions. Questioning the status-quo and the common way of thinking about something. Loop technical decisions with business outcomes. Push items that thinks are important to the agenda and plans.</p>
<p>Joe: Fast thinker. Fast coder. Extreme breadth of interests and knowledge. Reading thousands of books. Deep understanding of systems, even not the ones took part in directly.</p>
<p>Christian: Humble, open to learning.</p>
<p>Luke: Keeping a 1,000 things in mind at the same time. Principled thinking (guided by principles). Multiple perspective on every subject. Driven by numbers. Seeking for concensus and leading to it.</p>
<p>Lee B: Deeply caring about users.</p>
<p>Paul: Jack of all trades.</p>
<p>Komal: Being humble and ready to admit not knowing things.</p>
<p>Cyrus: Extremely deep in the topic, knowing tiny details of a technology. Very deep and detailed code reviews.</p>
<p>Yurii, Bjorn: Persistence in following one&rsquo;s ideals, pushing through, making the case and educating people around you why it&rsquo;s important.</p>
<p>Yogesh: Always being there for you, truly willing to help, enabling others. Optimist.</p>
<p>Paul K: Seeking concencus. Personal approach to everyone on the team. Caring about people hapiness.</p>
<p>Tomasz: Excellent written communication.</p>
<p>Vadimir: Avid reader. Seeing positive. Pation about broad set of things.</p>
<p>Sergei: Encyclodepic lnowledge in multiple domains and being able to combine the right ones for the problem. Yet, being humble and focused on helping out.</p>
<h2 id="what-i-disliked">What I disliked</h2>
<p>Vivek: Unreliable. Not keeping the word.</p>
<p>Lee: Being unrealistic (obviously optimistic) about the future work.</p>
<p>Cyrus: Know-it-all. Looking from above.</p>
<p>Yurii: Considering your way the only way anything can be done. Not accepting someone&rsquo;s code until it perfectly matches your own idea.</p>
<p>Bjorn: Legacy curse, not invented by us syndrom.</p>
<p>Burak: Very slow, relying on others to do part of your job. Committing to something without really understanding it.</p>
<p>Yogesh: Always saying yes.</p>
<p>Antoine, Paul C.: Complaining about everything. Being constantly toxically negative. Especially, when you are smart and persuasive, and everyone thinks you are right.</p>
<p>Tomasz: Let&rsquo;s first write a framework for this.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/cloud-pact/"/>
            <id>https://mikhail.io/drafts/papers/cloud-pact/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="new-directions-in-cloud-programming">New Directions in Cloud Programming</h2>
<p><a href="http://cidrdb.org/cidr2021/papers/cidr2021_paper16.pdf">http://cidrdb.org/cidr2021/papers/cidr2021_paper16.pdf</a></p>
<p>UC Berkeley about their future work on creating a new programming paradygm for the cloud.</p>
<h3 id="abstract">ABSTRACT</h3>
<p>Nearly twenty years after the launch of AWS, it remains difficult for most developers to harness the enormous potential of the cloud. In this paper we lay out an agenda for a new generation of cloud programming research aimed at bringing research ideas to programmers in an evolutionary fashion. Key to our approach is a separation of distributed programs into a PACT of four facets: Program semantics, Availablity, Consistency and Targets of optimization. We propose to migrate developers gradually to PACT programming by lifting familiar code into our more declarative level of abstraction. We then propose a multi-stage compiler that emits human-readable code at each stage that can be hand-tuned by developers seeking more control. Our agenda raises numerous research challenges across multiple areas including language design, query optimization, transactions, distributed consistency, compilers and program synthesis.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/cloudburst/"/>
            <id>https://mikhail.io/drafts/papers/cloudburst/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="cloudburst-stateful-functions-as-a-service">Cloudburst: Stateful Functions-as-a-Service</h2>
<p><a href="https://arxiv.org/pdf/2001.04592.pdf">https://arxiv.org/pdf/2001.04592.pdf</a></p>
<p>Berkley (RISELabs) stateful serverless system</p>
<p>Good summary: <a href="https://blog.acolyer.org/2020/02/07/cloudburst/">https://blog.acolyer.org/2020/02/07/cloudburst/</a></p>
<p>Related: &ldquo;serverless&rdquo; key-value store Anna <a href="https://dsf.berkeley.edu/jmh/papers/anna_ieee18.pdf">https://dsf.berkeley.edu/jmh/papers/anna_ieee18.pdf</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/deployment-metamodel/"/>
            <id>https://mikhail.io/drafts/papers/deployment-metamodel/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="the-essential-deployment-metamodel-a-systematic-review-of-deployment-automation-technologies-2019">The essential deployment metamodel: a systematic review of deployment automation technologies (2019)</h2>
<p><a href="https://link.springer.com/content/pdf/10.1007/s00450-019-00412-x.pdf">https://link.springer.com/content/pdf/10.1007/s00450-019-00412-x.pdf</a></p>
<p>The paper identifies relevant cloud deployment technologies: Puppet, Chef, Ansible, Kubernetes, OpenStack HEAT, Terraform, AWS CloudFormation, SaltStack, Juju, CFEngine, Azure Resource Manager, Docker Compose, Cloudify.</p>
<p>It gives an example of an application deployment, consisting of components and relations of two types:</p>
<p><img src="./deployment-metamodel-1.png" alt="1"></p>
<p>It classifies the technologies as General-Purpose, Provider-Specific, and Platform-Specific.</p>
<p>It defines an Essential Deployment Metamodel (EDMM) - a model to represent cloud deployments:</p>
<p><img src="./deployment-metamodel-2.png" alt="2"></p>
<p>It then shows that the model is generic enough to map it to every technology in the list.</p>
<p>&ldquo;A understanding of essential deployment model elements helps to compare technologies regarding deployment features and mechanism and supports decision making processes when selecting an appropriate technology for an use case. The introduced classification and the presented EDMM technology mapping support the migration from one deployment technology into another one. Further, this does not only support industry to compare and select technologies, but also helps researcher to evaluate concepts in the area of deployment automation research: If new research can be realized using EDMM, our mappings prove that this research can be also applied to the technologies analyzed in this paper. This significantly eases practically validating new concepts.&rdquo;</p>
<h2 id="cloud-native-deploy-ability-an-analysis-of-required-features-of-deployment-technologies-to-deploy-arbitrary-cloud-native-applications-2019">Cloud-native Deploy-ability: An Analysis of Required Features of Deployment Technologies to Deploy Arbitrary Cloud-native Applications (2019)</h2>
<p><a href="https://www.scitepress.org/Link.aspx?doi=10.5220%2f0009571001710180">https://www.scitepress.org/Link.aspx?doi=10.5220%2f0009571001710180</a> (downloaded in the folder)</p>
<p>It build on another paper (Kratzke and Quint (2017)) to define what Cloud-Native means:</p>
<ul>
<li>Service-based Architectures</li>
<li>API-based Interactions</li>
<li>State Isolation</li>
<li>Self-contained Service Deployment</li>
<li>Disposability</li>
<li>Fault-resilience</li>
<li>Infrastructure Abstraction</li>
<li>Infrastructure as Code</li>
<li>Policy-driven Elasticity</li>
<li>CI/CD Compliance</li>
</ul>
<p>It then discusses what are some requirements for deployment of Cloud-Native apps.</p>
<p>Cloud-native Deploy-ability: describes the ability to deploy arbitrary application components, concerning all cloud service models, that can be vertically “hosted on” or horizontally “connected to” any other component or service hosted on any cloud provider, cloud platform, or hy- brid environment. This includes support- ing the processing of declarative deploy- ment models given in machine-readable formats fostering automation.</p>
<p>Support for Multiple Cloud Providers and Platforms
Support for All Cloud Service Models (XaaS)
Usage of Deployment Models Supporting Arbitrary Components</p>
<p>It then takes the EDMM model from the above paper and basically argues that this is a useful way to describe cloud-native deployments.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/distributed-systems/"/>
            <id>https://mikhail.io/drafts/papers/distributed-systems/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="keeping-calm-when-distributed-consistency-is-easy">Keeping CALM: When Distributed Consistency is Easy</h2>
<p><a href="https://arxiv.org/pdf/1901.01930.pdf">https://arxiv.org/pdf/1901.01930.pdf</a></p>
<p>The CALM Theorem presents a positive result that delineates thefrontier of the possible. CALM shows that monotonicity, a propertyof a program, implies consistency, a property of the output of anyexecution of that program. The inverse is also established: non-monotonic programs require runtime enforcement (coordination) toensure consistent execution. As a program property, CALM enablesreasoning via static program analysis, and limits or eliminates theuse of runtime checks. This is in contrast to storage consistency likelinearizability or serializablity, which required expensive runtimeenforcement.</p>
<p>CALM falls short of being aconstructiveresult—it does not actu-ally tell us how to write consistent, coordination-free distributedsystems. Even armed with the CALM theorem, a system buildermust answer two key questions. First, and most difficult, is whetherthe problem they are trying to solve has a monotonic specification.Most programmers begin with pseudo-code of some implementa-tion in mind, and the theory behind CALM would appear to provideno guidance on how to extract a monotone specification from a can-didate implementation. The second question is equally important:given a monotonic specification for a problem, how can I imple-ment it in practice? Languages such as Bloom point the way to new paradigms for programming distributed systems that favor and(conservatively) test for monotonic specification. There is remain-ing work to do making these languages attractive to developers,and efficient at runtime.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/faas-benchmarks/"/>
            <id>https://mikhail.io/drafts/papers/faas-benchmarks/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="the-state-of-research-on-function-as-a-service-performance-evaluationa-multivocal-literature-review-2020">The State of Research on Function-as-a-Service Performance Evaluation:A Multivocal Literature Review (2020)</h2>
<p><a href="https://arxiv.org/pdf/2004.03276.pdf">https://arxiv.org/pdf/2004.03276.pdf</a></p>
<p>A multivocal literature review (MLR) covering 112 studies from academic (51) and grey literature (61) that report performance measurements of FaaS offerings of different platforms. We also provide methodological recommendations aimed at future FaaS performance evaluation studies.</p>
<p>Benchmark types:</p>
<ul>
<li>Micro-Benchmarks (an artificial thing focused on one aspect)</li>
<li>Application-Benchmarks (multi-component, closer to real life)</li>
</ul>
<p>Study design: a great detailed description of how they were searching for literature sources.</p>
<h3 id="study-results">Study results</h3>
<h4 id="studied-platforms">Studied Platforms</h4>
<p>88% of all our selectedstudies perform experiments on AWS Lambda, followed byAzure (26%), Google (23%), self-hosted platforms (14%),IBM (13%), and CloudFlare (4%).  In comparison to other surveys, our overall results for percentage by provider closely (±5%) match theself-reported experience per cloud provider in a 2018 FaaS survey.</p>
<h4 id="evaluated-benchmark-types">Evaluated Benchmark Types</h4>
<p>More focused on microbenchmarks, but quite some are application studies. App studies are mostly focused on a single platform.</p>
<h4 id="evaluated-micro-benchmarks">Evaluated Micro-Benchmarks</h4>
<p>CPU is by far the most evaluated micro-benchmark characteristic, used by 40% of all studies. TheOtherscategory mainly consists of platform overhead and work-load concurrency evaluated through micro-benchmarks.</p>
<h4 id="evaluated-general-characteristics">Evaluated General Characteristics</h4>
<p>General performance characteristics focus onparticularly relevant aspects of FaaS and only few studiesaim towards reverse-engineering hosted platforms. Elas-ticity and automatic scalability have been identified asthe most significant advantage of using FaaS in a previ-ous survey [2], which justifies the widespread evaluation ofconcurrency behavior. Given the importance of this char-acteristic, we argue that concurrent workloads should bean inherent part of all FaaS performance evaluations goingforward (going beyond the 50% of studies observed in ourcorpus).  Container start-up latency has been identifiedas one of the major challenges for using FaaS services inprior work [2] and motivates a large body of work relatedto quantifying platform overheads.</p>
<h4 id="used-platform-configurations">Used Platform Configurations</h4>
<p>Language: mostly Node and Python.
Triggers: HTTP triggers are by farthe most commonly evaluated type of trigger, and are usedby 57% of all studies.
Used External Services: Almost none (beyong API Gateway)</p>
<h4 id="reproducibility-great-topic">Reproducibility (great topic!)</h4>
<p>see the  &ldquo;Methodological principles for reproducible perfor-mance evaluation in cloud computing&rdquo; paper</p>
<ul>
<li>Repeated Experiments</li>
<li>Workload and Configuration Coverage</li>
<li>Experimental Setup Description</li>
<li>Open Access Artifact</li>
<li>Probabilistic Result Description</li>
<li>Statistical Evaluation</li>
<li>Measurement Units</li>
<li>Cost</li>
</ul>
<p>Basically, most studies suck on all of them (except measurement units)</p>
<h3 id="implications-and-gaps-in-literature">Implications and Gaps in Literature</h3>
<p>They give ideas for future studies.</p>
<h2 id="benchmarking-elasticity-of-faas-platforms-as-a-foundation-forobjective-driven-design-of-serverless-applications-2020">Benchmarking Elasticity of FaaS Platforms as a Foundation forObjective-driven Design of Serverless Applications (2020)</h2>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3341105.3373948">https://dl.acm.org/doi/pdf/10.1145/3341105.3373948</a></p>
<p>They create a CPU-bound Function (checking if a number is prime based on theSieve of Eratosthenesalgorithm.)</p>
<p>The workload has 3 phases: P0 warmup (stable), P1 linear scaleup, P2 stable cooldown. Five variations (rps):
P0    P1         P2
WL0    0    0+0.5∗t    15
WL1    0    0+1.0∗t     60
WL2    0    0+2.0∗t    120
WL3   60    60+0.5∗t   75
WL4   60    60+1.0∗t   120</p>
<p>They measure success rate, latency, throughput, cost, and host number and distribution. They deploy to AWS Lambda, Azure Functions, Google Cloud Functions, IBM Cloud Functions.</p>
<p>They then try to generalize the applicability of FaaS services to three real-world scenarios: web serving, exploratory data analysis, periodical batch processing.</p>
<h2 id="a-lightweight-design-for-serverless-function-as-a-service">A lightweight design for serverless Function-as-a-Service</h2>
<p><a href="https://arxiv.org/ftp/arxiv/papers/2010/2010.07115.pdf">https://arxiv.org/ftp/arxiv/papers/2010/2010.07115.pdf</a></p>
<p>The paper &ldquo;A lightweight design for serverless Function-as-a-Service&rdquo; sounded ambitious but it wasn&rsquo;t exactly what I expected. 1/4</p>
<p>My summary:</p>
<ul>
<li>FaaS relies on Docker-like tech for isolation</li>
<li>Let&rsquo;s benchmark Docker vs. WebAssembly</li>
<li>WASM looks faster than Docker =&gt; FaaS should use WASM (when it&rsquo;s ready) 2/4</li>
</ul>
<p>Well, maybe, except none of FaaS in AWS, Azure, GCP actually uses Docker. So, I don&rsquo;t quite buy the conclusion &ldquo;We evaluated the use of WebAssembly runtimes in serverless FaaS&rdquo;. 3/4</p>
<p>Anyway, if you are curious about WASM runtimes, have a look at the benchmark results /fin
<a href="https://arxiv.org/ftp/arxiv/papers/2010/2010.07115.pdf">https://arxiv.org/ftp/arxiv/papers/2010/2010.07115.pdf</a></p>
<h2 id="benchmarking-parallelism-in-faas-platforms">Benchmarking parallelism in FaaS platforms</h2>
<p><a href="https://arxiv.org/pdf/2010.15032v2.pdf">https://arxiv.org/pdf/2010.15032v2.pdf</a></p>
<p>Great paper comparing AWS Lambda, Azure Functions, and others in terms of scalabiliy. Need to review! A good further reading: <a href="https://d1.awsstatic.com/whitepapers/Overview-AWS-Lambda-Security.pdf">https://d1.awsstatic.com/whitepapers/Overview-AWS-Lambda-Security.pdf</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/faas-coldstarts/"/>
            <id>https://mikhail.io/drafts/papers/faas-coldstarts/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="using-application-knowledge-to-reduce-cold-starts-in-faas-services">Using Application Knowledge to Reduce Cold Starts in FaaS Services</h2>
<p><a href="https://dlnext.acm.org/doi/pdf/10.1145/3341105.3373909">https://dlnext.acm.org/doi/pdf/10.1145/3341105.3373909</a></p>
<p>Idea: if we have 4 serverless functions that are chained together (called in sequence), we can do tricks to prewarm functions 2 to 4 when function 1 is called. They discuss several approaches and share some experimental results.</p>
<p>Not very compelling.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/faas-orchestration/"/>
            <id>https://mikhail.io/drafts/papers/faas-orchestration/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="faas-orchestration-of-parallel-workloads-dec-2019">FaaS Orchestration of Parallel Workloads (Dec 2019)</h2>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3366623.3368137">https://dl.acm.org/doi/pdf/10.1145/3366623.3368137</a></p>
<p>It compares serverless orchestration tools like AWS Step Functions or Azure DurableFunctions in terms of performance overhead. It concludes that OpenWhisk Composer is more efficient for the presented benchmarks.</p>
<p>However, Composer has a syncrhonous wait, which makes it non-suitable for long-ish workflow. They suggest an improvement to make Composer more reactive, non-blocking, and tolerant to long delays.</p>
<p>The paper is affiliated with IBM and seems to be biased.</p>
<h2 id="triggerflow-trigger-based-orchestration-of-serverless-workflows-jun-2020">Triggerflow: Trigger-based Orchestration of Serverless Workflows (Jun 2020)</h2>
<p><a href="https://arxiv.org/pdf/2006.08654v2.pdf">https://arxiv.org/pdf/2006.08654v2.pdf</a></p>
<p>The paper is again affiliated with IBM, it&rsquo;s using open-source FaaS frameworks, but seems to be less biased towards IBM tech in particular.</p>
<p>The major contributions of this paper are the following:</p>
<ol>
<li>
<p>We present a Rich Trigger framework following: an Event-Condition-Action (ECA) architecture that is extensible atall levels (Event Sources and Programmable Conditions andActions). Our architecture ensures that composite eventdetection and event routing mechanisms are mediated byreactive event-based middleware.</p>
</li>
<li>
<p>We demonstrate Triggerflow&rsquo;s extensibility and universal-ity creating atop it a state machine workow scheduler,a DAG engine, an imperative Workow as Code (usingevent sourcing) scheduler, and integration with an exter-nal scheduler like PyWren. We also validate performanceand overhead of our scheduling solutions compared toexisting Cloud Serverless Orchestration systems like Ama-zon Step Functions, Amazon Express Workows, AzureDurable Functions and IBM Composer.</p>
</li>
<li>
<p>We finally propose a generic implementation of our modelover standard CNCF Cloud technologies like Kubernetes,Knative Eventing and CloudEvents. We validate that oursystem can support high-volume event processing work-loads, auto-scale on demand and transparently optimize sci-entic workows. e project is available as open-sourcein [1]</p>
</li>
</ol>
<p>They show a theoretic definition of what a workflow orchestration should contain, and then describe the implementations on a) Knative b) KEDA.</p>
<p><img src="triggerflow-keda.png" alt=""></p>
<p>It&rsquo;s a bit hard to grasp the exact implementation details of the paper, but there is source code available at <a href="https://github.com/triggerflow/triggerflow">https://github.com/triggerflow/triggerflow</a> (in Python).</p>
<p>They describe mappings from DAG, State Machines, and Workflow-as-Code implementations to Triggerflow.</p>
<p>They evaluate scaling and performance characteristics on KEDA. It looks okay but not a large-scale scenario.</p>
<h2 id="wukong-a-scalable-and-locality-enhancedframework-for-serverless-parallel-computing-oct-2020">Wukong: A Scalable and Locality-EnhancedFramework for Serverless Parallel Computing (Oct 2020)</h2>
<p><a href="https://arxiv.org/pdf/2010.07268v1.pdf">https://arxiv.org/pdf/2010.07268v1.pdf</a></p>
<p>It&rsquo;s a bit hard to grasp the exact value of this beyond the scientici specialized examples that they show.</p>
<p>Executing complex, burst-parallel, directed acyclic graph (DAG) jobs poses a major challenge for serverless executionframeworks, which will need to rapidly scale and scheduletasks at high throughput, while minimizing data movementacross tasks. We demonstrate that, for serverless parallel com-putations, decentralized scheduling enables scheduling to bedistributed across Lambda executors that can schedule tasksin parallel, and brings multiple benefits, including enhanceddata locality, reduced network I/Os, automatic resource elas-ticity, and improved cost effectiveness. We describe the im-plementation and deployment of our new serverless parallelframework, calledWukong, on AWS Lambda.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/faas-stateful/"/>
            <id>https://mikhail.io/drafts/papers/faas-stateful/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="a-fault-tolerance-shim-for-serverless-computing">A Fault-Tolerance Shim for Serverless Computing</h2>
<p><a href="https://www.vikrams.io/papers/aft-eurosys20.pdf">https://www.vikrams.io/papers/aft-eurosys20.pdf</a></p>
<p>They built an atomic-write storage to avoid dirty reads when a function writes multiple pieces and fails in the middle. The rest guarantees is quite loose.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/faas-workloads/"/>
            <id>https://mikhail.io/drafts/papers/faas-workloads/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="ripple-a-practical-declarative-programming-framework-for-serverless-compute-jan-2020">Ripple: A Practical Declarative Programming Framework for Serverless Compute (Jan 2020)</h2>
<p><a href="https://arxiv.org/pdf/2001.00222.pdf">https://arxiv.org/pdf/2001.00222.pdf</a></p>
<p>They create a mini &ldquo;language&rdquo; to define computation problems with split, combine, top, match, map, sort, partition, run to describe computations. Examples: visual border identification (k-means), protein sequence analysis, DNA compression. They then exprerementally find an optimal layout of this computation on multiple Lambdas to meet the deadline of the job and minimize cost.</p>
<p>The evaluation shows it performs great.</p>
<p>There is source code available but I miss the tech details in the paper to make it feel tangible and understandable.</p>
<h2 id="starling-a-scalable-query-engine-on-cloud-functions">Starling: A Scalable Query Engine on Cloud Functions</h2>
<p><a href="https://arxiv.org/pdf/1911.11727.pdf">https://arxiv.org/pdf/1911.11727.pdf</a>
<a href="https://dlnext.acm.org/doi/pdf/10.1145/3318464.3380609">https://dlnext.acm.org/doi/pdf/10.1145/3318464.3380609</a> (a shorter version)</p>
<p>Analytics query on serverless compute. You have a bunch of data in S3 (CSV or other formats) and want to run infrequent queries on them. Instead of Athena or Reshift with static cost, you could use AWS Lambda. Quite well written.</p>
<p>Potentially, two review topics:</p>
<ol>
<li>
<p>Shuffle. How to efficiently implement reduce phase of map-reduce (or sorting) with lambdas and S3, when there is not direct communication, and avoid the cost of N*N complexity. In combination with the other paper: &ldquo;Shuffling, Fast and Slow: Scalable Analytics  on Serverless Infrastructure&rdquo; <a href="https://www.usenix.org/system/files/nsdi19-pu.pdf">https://www.usenix.org/system/files/nsdi19-pu.pdf</a></p>
</li>
<li>
<p>Strangler mitigation when reading/writing to/from S3. S3 is sometimes slow, which drives up your high percentile. They have a smart detection of stranglers and they hit a second request. A nice touch: you save money by doing that because you save on AWS Lambda duration.</p>
</li>
</ol>
<h2 id="lambada-interactive-data-analytics-on-cold-data-using-serverless-cloud-infrastructure">Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud Infrastructure</h2>
<p><a href="https://arxiv.org/pdf/1912.00937.pdf">https://arxiv.org/pdf/1912.00937.pdf</a></p>
<p>Super similar to Starling and released at the same time. Serverless analytics on cold data. Shuffle in S3 with multiple layers.</p>
<h2 id="serverless-containers--rising-viable-approach-to-scientific-workflows">Serverless Containers – rising viable approach to Scientific Workflows</h2>
<p><a href="https://arxiv.org/pdf/2010.11320v1.pdf">https://arxiv.org/pdf/2010.11320v1.pdf</a></p>
<p>Running scientific workflows on top of AWS Fargate and Google Cloud Run and comparing performance results and applicability.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/farm-db/"/>
            <id>https://mikhail.io/drafts/papers/farm-db/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="fast-general-distributed-transactions-with-opacity-using-global-time">Fast General Distributed Transactions with Opacity using Global Time</h2>
<p><a href="https://arxiv.org/pdf/2006.14346v1.pdf">https://arxiv.org/pdf/2006.14346v1.pdf</a></p>
<p>Slides from the previous paper (FaRM v1): <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2014/02/aleksandar-dragojevic.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2014/02/aleksandar-dragojevic.pdf</a>
V1 paper itself: <a href="https://www.usenix.org/system/files/conference/nsdi14/nsdi14-paper-dragojevic.pdf">https://www.usenix.org/system/files/conference/nsdi14/nsdi14-paper-dragojevic.pdf</a>
Although it&rsquo;s less fascinating, e.g. because of no harcore clock sync usage.</p>
<p>A fascinating but a very hard to fully understand paper about in-memory distributed database with strong serializable transactions called FaRM. They use it for the A1 graph database in Bing. The idea is to provide a programming model so that a application developer could write code ~as if the whole data set was in local memory of a single large machine.</p>
<p>They use syncronized clocks but much more precise than Spanner and without atomic clock.</p>
<p>If I were to write a review, I should probably focus on Figure 3:  FaRMv2 commit protocol (currently page 8). Start with a single machine with a graph in-memory. How do we make transactions there? Read A, read B, lock A, check that B hasn&rsquo;t changed, write A.</p>
<p>Then start introducing problems. What if the graph doesn&rsquo;t fit into a single machine? A cluster. The same protocol could work but there&rsquo;s no time. Well, FaRM knows how to solve it.</p>
<p>Plug RDMA (remote direct memory access).</p>
<p>What if the machine crashes? Primary-backup model.</p>
<p>And then pile on other problems but don&rsquo;t really explain solutions (they are hard). Give some perf numbers at the end.</p>
<h3 id="abstract">Abstract</h3>
<p>Transactions can simplify distributed applications by hiding data distribution, concur-rency, and failures from the application developer. Ideally the developer would see the ab-straction of a single large machine that runs transactions sequentially and never fails. Thisrequires the transactional subsystem to provideopacity(strict serializability for both commit-ted and aborted transactions), as well as transparent fault tolerance with high availability.As even the best abstractions are unlikely to be used if they perform poorly, the system mustalso provide high performance.Existing distributed transactional designs either weaken this abstraction or are not de-signed for the best performance within a data center. This paper extends the design of FaRM— which provides strict serializability only for committed transactions — to provide opac-ity while maintaining FaRM’s high throughput, low latency, and high availability within amodern data center. It uses timestamp ordering based on real time with clocks synchronizedto within tens of microseconds across a cluster, and a failover protocol to ensure correctnessacross clock master failures. FaRM with opacity can commit 5.4 million neworder transactionsper second when running the TPC-C transaction mix on 90 machines with 3-way replication.</p>
<h2 id="a1-a-distributed-in-memory-graph-database">A1: A Distributed In-Memory Graph Database</h2>
<p><a href="https://arxiv.org/pdf/2004.05712.pdf">https://arxiv.org/pdf/2004.05712.pdf</a></p>
<p>Transactions in a distributed systemfrees up application developers from worrying about com-plex problems like atomicity, consistency and concurrencycontrol, and instead allows them to focus on core businessproblems</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/kafka-like/"/>
            <id>https://mikhail.io/drafts/papers/kafka-like/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="logplayer-fault-tolerant-exactly-once-delivery-using-grpcasynchronous-streaming">LogPlayer: Fault-tolerant Exactly-once Delivery using gRPCAsynchronous Streaming</h2>
<p><a href="https://arxiv.org/pdf/1911.11286.pdf">https://arxiv.org/pdf/1911.11286.pdf</a></p>
<p>Tags: Kafka, TLA+, Flink</p>
<p>ABSTRACT</p>
<p>In this paper, we present the design of our LogPlayer that is acomponent responsible for fault-tolerant delivery of transactional mutations recorded on a WAL (write-ahead log) to the backend storage shards. Log-Player relies on gRPC for asynchronous streaming. However, thedesign provided in this paper can be used with other asynchronousstreaming platforms. We model check the correctness of LogPlayerby TLA+. In particular, our TLA+ specification shows that LogPlayerguarantees in-order exactly-once delivery of WAL entries to thestorage shards, even in the presence of shards or LogPlayer failures.Our experiments show LogPlayer is capable of efficient deliverywith sub-millisecond latency, and it is significantly more efficientthan Apache Kafka for designing a WAL system with exactly-onceguarantee</p>
<p>CONCLUSION</p>
<p>In this paper, we presented the design of LogPlayer and our ex-perience in using gRPC asynchronous streaming for deliveringtransactional mutations written to a log to backend storage shards.We model checked the correctness of LogPlayer using TLA+. Specif-ically, we proved LogPlayer is masking fault-tolerant for satisfyingin-order exactly-once delivery of the log entries to the storageshards. TLA+ helped us find several bugs in our initial design thatare fixed in the algorithms provided in this paper. We explained how LogPlayer can be configured with LogStore for a distributeddatabase architecture. Our experimental results with the C++ imple-mentation of the LogPlayer shows our design with gRPC asynchro-nous streaming provides efficient delivery of transactions to thestorage shards. In all of our experiments, the median and averageLogPlayer delay remained less than 1 millisecond. We showed thatour system is significantly more efficient than existing streamingplatforms such as Apache Kafka for designing a WAL system withthe exactly-once guarantee.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/ml/"/>
            <id>https://mikhail.io/drafts/papers/ml/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="optimizing-prediction-serving-on-low-latency-serverless-dataflow">Optimizing Prediction Serving on Low-Latency Serverless Dataflow</h2>
<p>Cloudflow: ML model serving for interactive applications built on top of some stateful FaaS research platform Cloudburst (<a href="https://arxiv.org/pdf/2001.04592.pdf">https://arxiv.org/pdf/2001.04592.pdf</a>)</p>
<p><a href="https://arxiv.org/pdf/2007.05832v1.pdf">https://arxiv.org/pdf/2007.05832v1.pdf</a></p>
<h2 id="serverless-inferencing-on-kubernetes">Serverless inferencing on Kubernetes</h2>
<p>Using KFServing to serve ML inference models on Kubernetes. Built on top of KNative and optimizes things like GPU-VMs scaling.</p>
<p><a href="https://arxiv.org/pdf/2007.07366v1.pdf">https://arxiv.org/pdf/2007.07366v1.pdf</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/quality/"/>
            <id>https://mikhail.io/drafts/papers/quality/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="fastcloud--a-framework-of-assessment-and-selectionfor-trustworthy-cloud-service-based-on-qos-nov-2020">FASTCloud:  A framework of assessment and selectionfor trustworthy cloud service based on QoS (Nov 2020)</h2>
<p><a href="https://arxiv.org/pdf/2011.01871v1.pdf">https://arxiv.org/pdf/2011.01871v1.pdf</a></p>
<p>Proposed framework to measure cloud QoS (quality of service).</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/sizeless/"/>
            <id>https://mikhail.io/drafts/papers/sizeless/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="sizeless-predicting-the-optimal-size-of-serverless-functions">Sizeless: Predicting the optimal size of serverless functions</h2>
<p><a href="https://arxiv.org/pdf/2010.15162v1.pdf">https://arxiv.org/pdf/2010.15162v1.pdf</a></p>
<p>The only resource management taskthat developers are still in charge of is resource sizing, that is, se-lecting how much resources are allocated to each worker instance. However, due to the challenging nature of resource sizing, devel-opers often neglect it despite its significant cost and performancebenefits.</p>
<p>In this paper, we introduceSizeless— an approach to predict theoptimal resource size of a serverless function using monitoring datafrom a single resource size.</p>
<p>As our approach requires only produc-tion monitoring data, developers no longer need to implement andmaintain representative performance tests.</p>
<p>Furthermore, it allowscloud providers, which cannot engage in testing the performanceof user functions, to implement resource sizing on a platform leveland automate the last resource management task associated withserverless functions.</p>
<h3 id="introduction">Introduction</h3>
<p>Resource sizing is thetask of selecting how much CPU, memory, I/O bandwidth, etc. areallocated to a worker instance. sizing of serverless functions as a configurablememory size, where other resources such as CPU, network, or I/Oare scaled accordingly. A recent survey revealed that 47% of the serverlessfunctions in production have the default memory size, indicatingthat developers often neglect resource sizing.</p>
<p>memory size optimization of serverless functions are the AWSpower tuning tool [9]</p>
<p>In this paper, we introduceSizeless— an approach to predict theoptimal memory size of serverless functions based on monitoringdata for a single memory size. Towards this goal, we first build aserverless function generator capable of generating a large numberof synthetic serverless functions by randomly combining repre-sentative function segments. Next, we measure the execution timeand resource consumption metrics of 2 000 synthetic functions for six different memory sizes on a public cloud. Based on the result-ing dataset, we construct a multi-target regression model that canpredict the execution time of a serverless function for previouslyunseen memory sizes based on the execution time and resourceconsumption metrics for a single memory size.</p>
<p>To evaluate if our model — which wastrained on data from synthetic functions — can be transferred torealistic serverless functions, we apply it to the serverless airlineapplication [30,40], a representative production-grade serverlessapplication. Here,Sizelesspredicts the execution time of all func-tions for previously unseen memory sizes with a median predictionaccuracy of93.1%. UsingSizelessto optimize the memory size of theserverless airline booking application results in an average speedupof 16.7% while simultaneously decreasing average costs by 2.5%</p>
<h3 id="motivating-example">Motivating Example</h3>
<p>The relationship between the memory size of a serverless function,the cost per function execution, and the function execution time is quite counter-intuitive. A common assumption is that a higher memory size results in a faster execution at a higher price, since the allocated CPU, I/O, network, etc. capacity scales linearly with the selected memory size [13,42]. However, this is not the case due tothe pricing scheme most cloud providers employ, where the cost ofan execution is calculated based on the consumed GB/s of memory,that is, the execution time multiplied by memory size</p>
<p>Based on these results, we can conclude that: i) the impact ofmemory size configurations on execution time differs from func-tion to function, ii) predicting the execution time for a memorysize is challenging, as even two seemingly CPU-intensive and twonetwork-intensive functions behave differently, and iii) selecting anappropriate memory size is important as it can drastically improveperformance at a similar or reduced cost.</p>
<h3 id="approach">APPROACH</h3>
<p>We propose an approach to predict the optimal memory size ofserverless functions based on monitoring data collected for a singlememory size. During the offline phase, theSynthetic Function Generatorcreatesmany synthetic serverless functions, which are then instrumentedwith ourResource Consumption Monitoring. During theDatasetGeneration, we run performance tests to obtain the resource con-sumption metrics and execution times for all memory sizes of thou-sands of synthetic serverless functions. By applyingMulti-targetRegression Modelingto the resulting dataset, we generate a perfor-mance model that can predict the execution time for memory sizes of a real function based on monitoring data for a single memorysize, which can be obtained during production.</p>
<p>Our implementa-tion of the proposed approach is limited to AWS Lambda and thelanguage Node.js as they are by far the most common platformand programming language for serverless functions</p>
<h4 id="synthetic-function-generator">Synthetic function generator</h4>
<p>Therefore, we propose to generate synthetic server-less functions by randomly combining representative function seg-ments. Each function segment represents the smallest granularity ofcommon tasks in serverless functions.</p>
<p>Weimplemented the following sixteen function segments: FloatingPointOperations, MatrixMultiplications, ImageCompress, ImageResizes, ImageRotate, JSON2YAML, Compression, Decompression, DynamoDBRead, DynamoDBWrite, FileRead, FileWrite, S3Read, S3Write, S3Stream, Sleep.</p>
<p>The function generator randomly combines these func-tion segments and wraps them in a Lambda handler. Forthe dataset generation, we configured the function generator to useup to four function segments.</p>
<h4 id="resource-consumption-monitoring">Resource consumption monitoring</h4>
<p>Lambdacurrently does not support the monitoring of resource consumptionmetrics out of the box. we implement a custom resource consumption mon-itoring to cover a wide variety of resource consumption metrics.</p>
<p>Metric IDMetric NameMetric SourceM1Execution timeprocess.hrtime()M2User CPU timeprocess.cpuUsage()M3System CPU timeprocess.cpuUsage()M4Vol Context Switchesprocess.resourceUsage()M5Invol Context Switchesprocess.resourceUsage()M6File system readsprocess.resourceUsage()M7File system writesprocess.resourceUsage()M8Resident set sizeprocess.memoryUsage()M9Max resident set sizeprocess.resourceUsage()M10Total heapprocess.memoryUsage()M11Heap usedprocess.memoryUsage()M12Physical heapv8.getHeapStatistics()M13Available heapv8.getHeapStatistics()M14Heap limitv8.getHeapStatistics()M15Allocated memoryv8.getHeapStatistics()M16External memoryprocess.memoryUsage()M17Bytecode metadatav8.getHeapCodeStatistics()M18Bytes received/proc/net/dev/M19Bytes transmitted/proc/net/dev/M20Packages received/proc/net/dev/M21Packages transmitted/proc/net/dev/M22Min event loop lagperf_hooksM23Max event loop lagperf_hooksM24Mean event loop lagperf_hooksM25Std event loop lagperf_hooks</p>
<p>However, we consider the collected metricssufficient to characterize the resource consumption of serverlessfunctions.</p>
<h4 id="dataset-generation">Dataset generation</h4>
<p>we used the measurement harness to measure the exe-cution time and resource consumption metrics for 2 000 functionsacross six different memory sizes (128MB, 256MB, 512MB, 1024MB,2048MB, 3008MB), including the smallest and largest available mem-ory sizes on AWS for ten minutes each at 30 requests per second.This amounts to 12 000 performance measurements, 120 000 min-utes of experiment time, 360 000 Lambda executions, and roughly$2 000 worth of Lambda compute time.</p>
<p>The resulting dataset ispublicly available via CodeOcean(<a href="https://doi.org/10.24433/CO.9626622.v1">https://doi.org/10.24433/CO.9626622.v1</a>)</p>
<h4 id="multi-target-regression-modeling">Multi-target regression modeling</h4>
<p>ML</p>
<h3 id="evaluation">EVALUATION</h3>
<p>RQ1:How many and which metrics need to be monitored to accu-rately predict the response time of serverless functions for differentmemory sizes?</p>
<p>The sequential for-ward feature selection added the these six metrics in the followingorder: (1) user CPU time, (2) voluntary context switches, (3) heapused, (4) network packages transmitted, (5) system CPU time.</p>
<p>We can see that initially, it selectsmetrics that describe the CPU usage of the function (user CPU timeand voluntary context switches) and then adds information abouthow much memory the function consumes (heap used). Next, itadds information about the network usage of the function (networkpackages transmitted). Afterwards, adding another CPU-based met-ric, system CPU time (the time spent running code in the operatingsystem kernel on behalf of the function), further increases the pre-diction performance.</p>
<p>RQ3:Can our model, that was trained on a synthetic dataset, accu-rately predict the execution time of realistic serverless functions?</p>
<p>We evaluate the prediction accuracy of a model,that was trained on the synthetic dataset, for the serverless airlinebooking application, a representative, production-grade serverlessapplication The serverless airline booking application implements the flightbooking aspect of an airline. Customers can search for flights, bookflights, pay using a credit card, and earn loyalty points with eachbooking.</p>
<p>Overall, our approach has a meanprediction accuracy of 89.8%, distorted by the outlier of 68.9% predic-tion error for theCollectPaymentfunction at 128MB, and overalla median prediction accuracy of 93.1% To summarize, after training on a synthetic dataset, our approachwas able to accurately predict the execution time of previously un-seen, realistic serverless functions for all memory sizes based onmonitoring data for a single memory size.</p>
<p>RQ4:Are the execution time predictions provided by our approachsufficient to optimize the memory size of serverless functions?</p>
<p>After the execution time predictions are generated, the predictedexecution cost can be calculated based on the pricing model of thecloud provider. A common approach to determine asingle, optimal solution for multi-objective optimization problems isto define a tradeoff factor that combines the objectives into a singlescore  Our approach isable to select the optimal memory size for six out of eight functions.Using the memory sizes selected by our approach results in anaverage execution time decrease of 16.7% while simultaneouslydecreasing the mean execution cost by 2.5%.</p>
<p>This shows that our approach can be utilized by developers to se-lect a cost and performance optimal memory size. Cloud providerscan build upon this approach to provide memory size recommen-dations for user functions based on passive monitoring data.</p>
<h3 id="conclusion">CONCLUSION</h3>
<p>Serverless functions automate resource provisioning, deployment,instance management, and auto-scaling. The last resource manage-ment task that developers are still in charge of is resource sizing, i.e.,selecting how much resources are allocated to each worker instance.In this paper, we introducedSizeless, an approach to predict theoptimal resource size of serverless functions using monitoring dataof a single memory size. First, we introduced a synthetic functiongenerator and a resource consumption monitoring approach. Usingthese, we generated a large dataset on how functions with differentresource consumption behavior scale with increasing memory sizes.Based on this dataset, we trained a multi-target regression modelcapable of predicting the execution time of a serverless functionfor all memory sizes based on monitoring data for a single memorysize. These predictions then enable the automated optimization of aserverless function’s memory size. By automating the memory sizeoptimization for serverless functions, our approach removes thelast resource management task that developers still need to dealwith in serverless functions and thus makes serverless functionstruly serverless. In our evaluation,Sizelesswas able to predict theexecution time of the serverless functions of a realistic serverless
application with a median prediction accuracy of 93.1%. UsingSize-lessto optimize the memory size of this application resulted in anaverage speedup of 16.7% while simultaneously decreasing averagecosts by 2.5%.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/papers/socrates/"/>
            <id>https://mikhail.io/drafts/papers/socrates/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="socrates-the-new-sql-server-in-the-cloud">Socrates: The New SQL Server in the Cloud</h2>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/05/socrates.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2019/05/socrates.pdf</a></p>
<p>the expectation is that a database runs in the cloudat least as well as (if not better) than on premise. Specifically,customers expect a “database-as-a-service” to be highly avail-able (e.g., 99.999% availability), support large databases (e.g.,a 100TB OLTP database), and be highly performant</p>
<p>This issue does not arise in on-premise database deploymentsbecause these deployments typically make use of special, ex-pensive hardware for high availability (such as storage areanetworks or SANs); hardware which is not available in thecloud. Furthermore, on-premise deployments control thesoftware update cycles and carefully plan downtimes; thisplanning is typically not possible in the cloud.</p>
<p>One idea is to decom-pose the functionality of a database management system anddeploy the compute services (e.g., transaction processing)and storage services (e.g., checkpointing and recovery) in-dependently. The first commercial system that adopted thisidea is Amazon Aurora [20].</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/reading-list/"/>
            <id>https://mikhail.io/drafts/reading-list/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="green-algorithms-quantifying-the-carbon-emissions-of-computation">Green Algorithms: Quantifying the carbon emissions of computation</h2>
<p><a href="https://arxiv.org/ftp/arxiv/papers/2007/2007.07610.pdf">https://arxiv.org/ftp/arxiv/papers/2007/2007.07610.pdf</a></p>
<h2 id="cost--and-qos-efficient-serverless-cloud-computing">Cost- and QoS-Efficient Serverless Cloud Computing</h2>
<p><a href="https://arxiv.org/pdf/2011.11711v1.pdf">https://arxiv.org/pdf/2011.11711v1.pdf</a></p>
<p>200-pages (!) doctorate thesis.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[]]></title>
            <link href="https://mikhail.io/drafts/writing/elements-of-style/"/>
            <id>https://mikhail.io/drafts/writing/elements-of-style/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h2 id="elements-of-style">Elements of Style</h2>
<p><a href="https://www.bartleby.com/141/strunk.html">https://www.bartleby.com/141/strunk.html</a>
<a href="http://www.jlakes.org/ch/web/The-elements-of-style.pdf">http://www.jlakes.org/ch/web/The-elements-of-style.pdf</a> (another edition)</p>
<p>Correct: Charles&rsquo;s friend</p>
<p>Correct: Oxford comma</p>
<p>&ldquo;etc.&rdquo; always in commas</p>
<p>Do not use &ldquo;so&rdquo; to join two sentences into one. Use &ldquo;As &hellip;, &hellip;&rdquo; instead.</p>
<p>&ldquo;Blake and I stayed home.&rdquo; (not and me, and myself)</p>
<p>Start paragraphs with a topic sentence, then clarify, then a strong statement at the end.</p>
<p>&ldquo;A survey of this region was made in 1900.&rdquo; =&gt; &ldquo;This region was surveyed in 1900.&rdquo;</p>
<p>Do not use &ldquo;not&rdquo; for vagueness. Use positive instead:
&ldquo;She did not think that studying Latin was a sensible way to use one&rsquo;s time.&rdquo; =&gt; &ldquo;She thought the study of Latin a waste of time.&rdquo;</p>
<p>Save the auxiliaries would, should, could, may, might, and can for situations involving real uncertainty. (avoid otherwise)</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
        
        
    
        
        <entry>
            <title type="html"><![CDATA[Categories]]></title>
            <link href="https://mikhail.io/categories/"/>
            <id>https://mikhail.io/categories/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Drafts]]></title>
            <link href="https://mikhail.io/drafts/"/>
            <id>https://mikhail.io/drafts/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[HTTP/2]]></title>
            <link href="https://mikhail.io/tags/http/2/"/>
            <id>https://mikhail.io/tags/http/2/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        <entry>
            <title type="html"><![CDATA[Mikhail Shilkov]]></title>
            <link href="https://mikhail.io/about/"/>
            <id>https://mikhail.io/about/</id>
            
            <published>0001-01-01T00:00:00+00:00</published>
            <updated>0001-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Serverless fanboy, Microsoft Azure MVP, functional thinker, tech blogger and speaker</blockquote>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
    
        
        
    
</feed>