<!DOCTYPE html>
<html lang="en-us" prefix="og: http://ogp.me/ns#"><head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	
	<link rel="icon" type="image/png" href="/favicon.ico">
	
	
	<title>From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing | Mikhail Shilkov</title>
	
	
	
        <link rel="stylesheet" href="https://kit-free.fontawesome.com/releases/latest/css/free.min.css" crossorigin="anonymous">
	<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
	
	<link rel="stylesheet" href="/sass/main.css">

	
	<link rel="canonical" href="https://blog.binaris.com/from-0-to-1000-instances/">
	
	
	<meta property="og:title" content="From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing" />
	<meta property="og:description" content="Comparison of queue processing scalability for FaaS across AWS, Azure and GCP" />
	<meta property="og:type" content="article" />
	<meta property="og:url" content="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/" />

	
	
	
	<meta property="og:image" content="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/teaser.jpg" />
	
	


	
	
	<meta property="article:tag" content="Azure" />
	
	<meta property="article:tag" content="Azure Functions" />
	
	<meta property="article:tag" content="Serverless" />
	
	<meta property="article:tag" content="Performance" />
	
	<meta property="article:tag" content="Scalability" />
	
	<meta property="article:tag" content="AWS" />
	

	
	<meta name="twitter:card" content="summary_large_image"/>
	
	<meta name="twitter:image" content="https://mikhail.io/2018/11/from-0-to-1000-instances-how-serverless-providers-scale-queue-processing/teaser.jpg"/>
	

	<meta name="twitter:title" content="From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing"/>
	<meta name="twitter:description" content="Comparison of queue processing scalability for FaaS across AWS, Azure and GCP"/>
	<meta name="twitter:creator" content="@MikhailShilkov"></meta>
</head>
<body><script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>
<script type="text/javascript">
    google.charts.load("current", {packages:["corechart"]});
    function addChart(make) {
        google.charts.setOnLoadCallback(drawChart);
        function drawChart() {
            const data = new google.visualization.DataTable();

            const options = {
                height: 420,  
                chartArea: { width: '85%', height: '70%' },
                legend: 'none',
                hAxis: { minValue: 0 },
                vAxis: {},
                series: {      
                    0: { tooltip : false}
                }
            };
            const chart = make(data, options);
            options.hAxis.textStyle = options.hAxis.titleTextStyle 
                = options.vAxis.textStyle = options.vAxis.titleTextStyle = { fontName: 'Merriweather' };

            chart.draw(data, options);
        }
    }
</script>
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container pr-0">
        <a class="navbar-brand" href="/">
            <img class="author-thumb" src="/images/author.jpg" alt="Mikhail Shilkov">
            <span class="text-primary">Mikhail Shilkov</span>
        </a>

        
               


        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent"
            aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        
        <div class="collapse navbar-collapse" id="navbarMediumish">
            
            <ul class="navbar-nav ml-auto">
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/tags/">TOPICS</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/archives/">ARCHIVES</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/lab/">LABS</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/talks/">TALKS</a>
                </li>
                 
                <li class="nav-item ">
                    <a class="nav-link" href="/about/">ABOUT</a>
                </li>
                <li class="nav-item">
    <a class="nav-link" href="https://mikhail.io/feed/"><i class="fas fa-rss social-icon" aria-hidden="true"></i></a>
</li>

<li class="nav-item">
    <a class="nav-link" href="https://www.twitter.com/MikhailShilkov"><i class="fab fa-twitter social-icon" aria-hidden="true"></i></a>
</li>


<li class="nav-item">
    <a class="nav-link" href="https://dev.to/mikhailshilkov"><i class="fab fa-dev social-icon" aria-hidden="true"></i></a>
</li>


<li class="nav-item">
    <a class="nav-link" href="https://medium.com/@MikhailShilkov"><i class="fab fa-medium social-icon" aria-hidden="true"></i></a>
</li>


<li class="nav-item">
    <a class="nav-link" href="https://github.com/MikhailShilkov"><i class="fab fa-github social-icon" aria-hidden="true"></i></a>
</li>


<li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/MikhailShilkov"><i class="fab fa-linkedin social-icon" aria-hidden="true"></i></a>
</li>

</ul>
        </div>
        
    </div>
</nav>


        <div class="container">

<div class="main-content">
    
    <div class="container">
        <div class="row">
            
            <div class="col-lg-2 pl-0"><div class="share">
    <ul>
        <li class="ml-1 mr-1" title="Say 'Thank You' for this article">
            <a href="#" onclick="heart();return false;">
                <i class="fas fa-heart" style="color: red"></i>
            </a>
            <div class="count" style="float: right; padding-left: .5em; padding-top: .25em" id="heartcount"></div>
        </li>

        <li class="ml-1 mr-1" title="Tweet this article">
            <a target="_blank"
                href="https://twitter.com/intent/tweet?text=From%200%20to%201000%20Instances%3a%20How%20Serverless%20Providers%20Scale%20Queue%20Processing%20by%20@MikhailShilkov&url=https%3a%2f%2fmikhail.io%2f2018%2f11%2ffrom-0-to-1000-instances-how-serverless-providers-scale-queue-processing%2f"
                onclick="window.open(this.href, 'twitter-share', 'width=550,height=435');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1" title="See the responses or write a response">
            <a href="#comments">
                <i class="fas fa-comment"></i>
            </a>
        </li>
    </ul>
</div></div>
            
            <div class="col-lg-9 flex-first flex-lg-unordered">

                <div class="mainheading">
                    
                    <h1 class="posttitle">From 0 to 1000 Instances: How Serverless Providers Scale Queue Processing</h1>
                    
                    <div class="post-top-meta">
                        <div>
                            
                                
                                <span class="post-date">Originally published on the <a
                                        href='https://blog.binaris.com/from-0-to-1000-instances/'>Binaris Blog</a> on
                                    Nov 19, 2018
                                    
                                        
                                        &nbsp;&middot;&nbsp; 12 min read</span></span>
                        </div>
                    </div>
                </div>
                
                
                
            </div>
        </div>
        <div class="row">
            <div class="col-lg-11 offset-lg-1 flex-first flex-lg-unordered">
                <img class="featured-image img-fluid" src="teaser.jpg" alt="thumbnail for this post">
            </div>
        </div>
        <div class="row">
            <div class="col-lg-2 pl-0">
            </div>
            <div class="col-lg-9 flex-first flex-lg-unordered">
                
                
                

                
                <div class="article-post">
                    <p>Whenever I see a &ldquo;Getting Started with Function-as-a-Service&rdquo; tutorial, it usually shows off
a synchronous HTTP-triggered scenario. In my projects, though, I use a lot of asynchronous
functions triggered by a queue or an event stream.</p>
<p>Quite often, the number of messages passing through a queue isn&rsquo;t uniform over time. I might
drop batches of work now and then. My app may get piles of queue items arriving from upstream
systems that were down or under maintenance for an extended period. The system might see some
rush-hour peaks every day or only a few busy days per month.</p>
<p>This is where serverless tech shines: You pay per execution, and then the promise is that the
provider takes care of scaling up or down for you. Today, I want to put this scalability under
test.</p>
<p>The goal of this article is to explore queue-triggered serverless functions and hopefully distill
some practical advice regarding asynchronous functions for real projects. I will be evaluating
the problem:</p>
<ul>
<li>Across Big-3 cloud providers (Amazon, Microsoft, Google)</li>
<li>For different types of workloads</li>
<li>For different performance tiers</li>
</ul>
<p>Let&rsquo;s see how I did that and what the outcome was.</p>
<p><em>DISCLAIMER. Performance testing is hard. I might be missing some crucial factors and parameters
that influence the outcome. My interpretation might be wrong. The results might change over time.
If you happen to know a way to improve my tests, please let me know, and I will re-run them and
re-publish the results.</em></p>
<h2 id="methodology">Methodology</h2>
<p>In this article I analyze the execution results of the following cloud services:</p>
<ul>
<li>AWS Lambda triggered via SQS queues</li>
<li>Azure Function triggered via Storage queues</li>
<li>Google Cloud Function triggered via Cloud Pub/Sub</li>
</ul>
<p>All functions are implemented in Javascript and are running on GA runtime.</p>
<p>At the beginning of each test, I threw 100,000 messages into a queue that was previously idle.
Enqueuing never took longer than one minute (I sent the messages from multiple clients in
parallel).</p>
<p>I disabled any batch processing, so each message was consumed by a separate function invocation.</p>
<p>I then analyzed the logs (AWS CloudWatch, Azure Application Insights, and GCP Stackdriver
Logging) to generate charts of execution distribution over time.</p>
<h2 id="how-scaling-actually-works">How Scaling Actually Works</h2>
<p>To understand the experiment better, let&rsquo;s look at a very simplistic but still useful model of
how cloud providers scale serverless applications.</p>
<p>All providers handle the increased load by
<a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">scaling out</a>, i.e.,
by creating multiple instances of the same application that execute the chunks of work in
parallel.</p>
<p>In theory, a cloud provider could spin up an instance for each message in the queue as soon as
the messages arrive. The backlog processing time would then stay very close to zero.</p>
<p>In practice, allocating instances is not cheap. The Cloud provider has to boot up the function
runtime, hit a <a href="https://mikhail.io/2018/08/serverless-cold-start-war/">cold start</a>, and waste
expensive resources on a job that potentially will take just a few milliseconds.</p>
<p>So the providers are trying to find a sweet spot between handling the work as soon as possible
and using resources efficiently. The outcomes differ, which is the point of my article.</p>
<h3 id="aws">AWS</h3>
<p>AWS Lambda defines scale out with a notion of Concurrent Executions. Each instance of your AWS
Lambda is handling a single execution at any given time. In our case, it&rsquo;s processing a single
SQS message.</p>
<p>It&rsquo;s helpful to think of a function instance as a container working on a single task. If execution
pauses or waits for an external I/O operation, the instance is on hold.</p>
<p>The model of concurrent executions is universal to all trigger types supported by Lambdas. An
instance doesn&rsquo;t work with event sources directly; it just receives an event to work on.</p>
<p>There is a central element in the system, let&rsquo;s call it &ldquo;Orchestrator&rdquo;. The Orchestrator is the
component talking to an SQS queue and getting the messages from it. It&rsquo;s then the job of the
Orchestrator and related infrastructure to provision the required number of instances for working
on concurrent executions:</p>
<figure >
    
        <img src="aws-lambda-queue-scaling.png"
            alt="Model of AWS Lambda Scale-Out"
             />
        
    
    <figcaption>
        <h4>Model of AWS Lambda Scale-Out</h4>
    </figcaption>
    
</figure>
<p>As to scaling behavior, here is what the official
<a href="https://docs.aws.amazon.com/en_us/lambda/latest/dg/scaling.html">AWS docs</a> say:</p>
<blockquote>
<p>AWS Lambda automatically scales up &hellip; until the number of concurrent function executions
reaches 1000 &hellip; Amazon Simple Queue Service supports an initial burst of 5 concurrent function
invocations and increases concurrency by 60 concurrent invocations per minute.</p>
</blockquote>
<h3 id="gcp">GCP</h3>
<p>The model of Google Cloud Functions is very similar to what AWS does. It runs a single
simultaneous execution per instance and routes the messages centrally.</p>
<p>I wasn&rsquo;t able to find any scaling specifics except the definition of
<a href="https://cloud.google.com/functions/quotas">Function Quotas</a>.</p>
<h3 id="azure">Azure</h3>
<p>Experiments with Azure Functions were run on
<a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan">Consumption Plan</a>
—the dynamically scaled and billed-per-execution runtime. The concurrency model of Azure Functions
is different from the counterparts of AWS/GCP.</p>
<p>Function App instance is closer to a VM than a single-task container. It runs multiple concurrent
executions in parallel. Equally importantly, it pulls messages from the queue on its own instead of
getting them pushed from a central Orchestrator.</p>
<p>There is still a central coordinator called Scale Controller, but its role is a bit more subtle. It
connects to the same data source (the queue) and needs to determine how many instances to provision
based on the metrics from that queue:</p>
<figure >
    
        <img src="azure-function-queue-scaling.png"
            alt="Model of Azure Function Scale-Out"
             />
        
    
    <figcaption>
        <h4>Model of Azure Function Scale-Out</h4>
    </figcaption>
    
</figure>
<p>This model has pros and cons. If one execution is idle, waiting for some I/O operation such as an
HTTP request to finish, the instance might become busy processing other messages, thus being more
efficient. Running multiple executions is useful in terms of shared resource utilization, e.g.,
keeping database connection pools and reusing HTTP connections.</p>
<p>On the flip side, the Scale Controller now needs to be smarter: to know not only the queue backlog
but also how instances are doing and at what pace they are processing the messages. It&rsquo;s probably
achievable based on queue telemetry though.</p>
<p>Let&rsquo;s start applying this knowledge in practical experiments.</p>
<h2 id="pause-the-world-workload">Pause-the-World Workload</h2>
<p>My first serverless function is aimed to simulate I/O-bound workloads without using external
dependencies to keep the experiment clean. Therefore, the implementation is extremely
straightforward: pause for 500 ms and return.</p>
<p>It could be loading data from a scalable third-party API. It could be running a database query.
Instead, it just runs <code>setTimeout</code>.</p>
<p>I sent 100k messages to queues of all three cloud providers and observed the result.</p>
<h3 id="aws-1">AWS</h3>
<p>AWS Lambda allows multiple instance sizes to be provisioned. Since the workload is neither CPU-
nor memory-intensive, I was using the smallest memory allocation of 128 MB.</p>
<p>Here comes the first chart of many, so let&rsquo;s learn to read it. The horizontal axis shows time in
minutes since all the messages were sent to the queue.</p>
<p>The line going from top-left to bottom-right shows the decreasing queue backlog. Accordingly, the
left vertical axis denotes the number of items still-to-be-handled.</p>
<p>The bars show the number of concurrent executions crunching the messages at a given time. Every
execution logs the instance ID so that I could derive the instance count from the logs. The right
vertical axis shows the instance number.</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-iobound-scaling.png"
            alt="AWS Lambda processing 100k SQS messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda processing 100k SQS messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>It took AWS Lambda 5.5 minutes to process the whole batch of 100k messages. For comparison, the
same batch processed sequentially would take about 14 hours.</p>
<p>Notice how linear the growth of instance count is. If I apply the official scaling formula:</p>
<pre><code>Instance Count = 5 + Minutes * 60 = 5 + 5.5 * 60  = 335
</code></pre><p>We get a very close result! Promises kept.</p>
<h3 id="gcp-1">GCP</h3>
<p>Same function, same chart, same instance size of 128 MB of RAM—but this time for Google Cloud Functions:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-iobound-scaling.png"
            alt="Google Cloud Function processing 100k Pub/Sub messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function processing 100k Pub/Sub messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Coincidentally, the total amount of instances, in the end, was very close to AWS. The scaling
pattern looks entirely different though: Within the very first minute, there was a burst of
scaling close to 300 instances, and then the growth got very modest.</p>
<p>Thanks to this initial jump, GCP managed to finish processing almost one minute earlier than AWS.</p>
<h3 id="azure-1">Azure</h3>
<p>Azure Function doesn&rsquo;t have a configuration for allocated memory or any other instance size parameters.</p>
<p>The shape of the chart for Azure Functions is very similar, but the instance number growth is
significantly different:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="azure-function-queue-iobound-scaling.png"
            alt="Azure Function processing 100k queue messages with &#34;Pause&#34; handler"
             />
        
    
    <figcaption>
        <h4>Azure Function processing 100k queue messages with &#34;Pause&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The total processing time was a bit faster than AWS and somewhat slower than GCP. Azure Function
instances process several messages in parallel, so it takes much less of them to do the same amount
of work.</p>
<p>Instance number growth seems far more linear than bursty.</p>
<h3 id="what-we-learned">What we learned</h3>
<p>Based on this simple test, it&rsquo;s hard to say if one cloud provider handles scale-out better than
the others.</p>
<p>It looks like all serverless platforms under stress are making decisions at the resolution of 5-15
seconds, so the backlog processing delays are likely to be measured in minutes. It sounds quite far
from the theoretical &ldquo;close to zero&rdquo; target but is most likely good enough for the majority of
applications.</p>
<h2 id="crunching-numbers">Crunching Numbers</h2>
<p>That was an easy job though. Let&rsquo;s give cloud providers a hard time by executing CPU-heavy workloads
and see if they survive!</p>
<p>This time, each message handler calculates a <a href="https://en.wikipedia.org/wiki/Bcrypt">Bcrypt</a>
hash with a cost of 10. One such calculation takes about 200 ms on my laptop.</p>
<h3 id="aws-2">AWS</h3>
<p>Once again, I sent 100k messages to an SQS queue and recorded the processing speed and instance count.</p>
<p>Since the workload is CPU-bound, and AWS allocates CPU shares proportionally to the allocated
memory, the instance size might have a significant influence on the result.</p>
<p>I started with the smallest memory allocation of 128 MB:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-cpubound-scaling.png"
            alt="AWS Lambda (128 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda (128 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>This time it took almost 10 minutes to complete the experiment.</p>
<p>The scaling shape is pretty much the same as last time, still correctly described by the formula
<code>60 * Minutes + 5</code>. However, because AWS allocates a small fraction of a full CPU to each 128 MB
execution, one message takes around 1,700 ms to complete. Thus, the total work increased
approximately by the factor of 3 (47 hours if done sequentially).</p>
<p>At the peak, 612 concurrent executions were running, nearly double the amount in our initial
experiment. So, the total processing time increased only by the factor of 2—up to 10 minutes.</p>
<p>Let&rsquo;s see if larger Lambda instances would improve the outcome. Here is the chart for 512 MB of
allocated memory:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="aws-lambda-sqs-cpubound-scaling-512.png"
            alt="AWS Lambda (512 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>AWS Lambda (512 MB) processing 100k SQS messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>And yes it does. The average execution duration is down to 400 ms: 4 times less, as expected.
The scaling shape still holds, so the entire batch was done in less than four minutes.</p>
<h3 id="gcp-2">GCP</h3>
<p>I executed the same experiment on Google Cloud Functions. I started with 128 MB, and it looks
impressive:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling.png"
            alt="Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (128 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>The average execution duration is very close to Amazon&rsquo;s: 1,600 ms. However, GCP scaled more
aggressively—to a staggering 1,169 parallel executions! Scaling also has a different shape:
It&rsquo;s not linear but grows in steep jumps. As a result, it took less than six minutes on the
lowest CPU profile—very close to AWS&rsquo;s time on a 4x more powerful CPU.</p>
<p>What will GCP achieve on a faster CPU? Let&rsquo;s provision 512 MB. It must absolutely crush the
test. Umm, wait, look at that:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling-512.png"
            alt="Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (512 MB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>It actually&hellip; got slower. Yes, the average execution time is 4x lower: 400 ms, but the scaling
got much less aggressive too, which canceled the speedup.</p>
<p>I confirmed it with the largest instance size of 2,048 MB:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="gcp-cloud-function-pubsub-cpubound-scaling-2048.png"
            alt="Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Google Cloud Function (2 GB) processing 100k Pub/Sub messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>CPU is fast: 160 ms average execution time, but the total time to process 100k messages went up
to eight minutes. Beyond the initial spike at the first minute, it failed to scale up any further
and stayed at about 110 concurrent executions.</p>
<p>It seems that GCP is not that keen to scale out larger instances. It&rsquo;s probably easier to find
many small instances available on the pool rather than a similar number of giant instances.</p>
<h3 id="azure-2">Azure</h3>
<p>A single invocation takes about 400 ms to complete on Azure Function. Here is the burndown chart:</p>
</div>
</div>
</div>
<div class="row no-gutter">
    <div class="col-lg-1 pl-0"></div>
    <div class="col-sm-11">
        
<figure >
    
        <img src="azure-function-queue-cpubound-scaling.png"
            alt="Azure Function processing 100k queue messages with &#34;Bcrypt&#34; handler"
             />
        
    
    <figcaption>
        <h4>Azure Function processing 100k queue messages with &#34;Bcrypt&#34; handler</h4>
    </figcaption>
    
</figure>

    </div>
</div>
<div class="row">
    <div class="col-lg-2 pl-0"></div>
    <div class="col-lg-9 flex-first flex-lg-unordered">
        <div class="article-post">
<p>Azure spent 21 minutes to process the whole backlog. The scaling was linear, similarly to AWS,
but with a much slower pace regarding instance size growth, about <code>2.5 * Minutes</code>.</p>
<p>As a reminder, each instance could process multiple queue messages in parallel, but each such
execution would be competing for the same CPU resource, which doesn&rsquo;t help for the purely
CPU-bound workload.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<p>Time for some conclusions and pieces of advice to apply in real serverless applications.</p>
<h3 id="serverless-is-great-for-async-data-processing">Serverless is great for async data processing</h3>
<p>If you are already using cloud services, such as managed queues and topics, serverless functions
are the easiest way to consume them.</p>
<p>Moreover, the scalability is there too. When was the last time you ran 1,200 copies of your
application?</p>
<h3 id="serverless-is-not-infinitely-scalable">Serverless is not infinitely scalable</h3>
<p>There are limits. Your functions won&rsquo;t scale perfectly to accommodate your spike—a
provider-specific algorithm will determine the scaling pattern.</p>
<p>If you have large spikes in queue workloads, which is quite likely for medium- to high-load
scenarios, you can and should expect delays up to several minutes before the backlog is fully
digested.</p>
<p>All cloud providers have quotas and limits that define an upper boundary of scalability.</p>
<h3 id="cloud-providers-have-different-implementations">Cloud providers have different implementations</h3>
<p><strong>AWS Lambda</strong> seems to have a very consistent and well-documented linear scale growth for
SQS-triggered Lambda functions. It will happily scale to 1,000 instances, or whatever other
limit you hit first.</p>
<p><strong>Google Cloud Functions</strong> has the most aggressive scale-out strategy for the smallest instance
sizes. It can be a cost-efficient and scalable way to run your queue-based workloads. Larger
instances seem to scale in a more limited way, so a further investigation is required if you
need those.</p>
<p><strong>Azure Functions</strong> share instances for multiple concurrent executions, which works better
for I/O-bound workloads than for CPU-bound ones. Depending on the exact scenario that you
have, it might help to play with instance-level settings.</p>
<h3 id="dont-forget-batching">Don&rsquo;t forget batching</h3>
<p>For the tests, I was handling queue messages in the 1-by-1 fashion. In practice, it helps
if you can batch several messages together and execute a single action for all of them in one go.</p>
<p>If the destination for your data supports batched operations, the throughput will usually
increase immensely.
<a href="https://blogs.msdn.microsoft.com/appserviceteam/2017/09/19/processing-100000-events-per-second-on-azure-functions/">Processing 100,000 Events Per Second on Azure Functions</a>
is an excellent case to prove the point.</p>
<h3 id="you-might-get-too-much-scale">You might get too much scale</h3>
<p>A month ago, Troy Hunt published a great post
<a href="https://www.troyhunt.com/breaking-azure-functions-with-too-many-connections/">Breaking Azure Functions with Too Many Connections</a>.
His scenario looks very familiar: He uses queue-triggered Azure Functions to notify
subscribers about data breaches. One day, he dropped 126 million items into the queue, and Azure
scaled out, which overloaded Mozilla&rsquo;s servers and caused them to go all-timeouts.</p>
<p>Another consideration is that non-serverless dependencies limit the scalability of your serverless
application. If you call a legacy HTTP endpoint, a SQL database, or a third-party web service—be
sure to test how they react when your serverless function scales out to hundreds of concurrent
executions.</p>
<p>Stay tuned for more serverless performance goodness!</p>

                </div>

                
                <div class="after-post-tags">
                    <ul class="tags">
                        
                        <li>
                            
                            
                            <a href="/tags/azure">Azure</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/azure-functions">Azure Functions</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/serverless">Serverless</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/performance">Performance</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/scalability">Scalability</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/aws">AWS</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/aws-lambda">AWS Lambda</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/gcp">GCP</a>
                            
                        </li>
                        
                        <li>
                            
                            
                            <a href="/tags/google-cloud-functions">Google Cloud Functions</a>
                            
                        </li>
                        
                    </ul>
                </div>
                

            </div>
            
        </div>
    </div>
    <div class="container">
    <div id="sharepane" class="row justify-content-center mb-5">
        <div class="col-md-8">
            <section class="article-open_author">
                <div class="article-open_author_bio">
                    <h5 itemprop="author" itemscope="" itemtype="http://schema.org/Person">Mikhail Shilkov</h5>
                    <p>
                        <div>Hi, I'm Mikhail Shilkov, I write this blog and work as a software engineer at Pulumi.</div>
                        <div>I am a Microsoft Azure MVP but I love all the clouds!</div>
                        <div>Cloud is amazing, now I'm trying to make it simple and fun too.</div>
                    </p>
                </div>
            </section>
            <a href="https://twitter.com/MikhailShilkov?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @MikhailShilkov</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>
    </div>
</div>
<div class="container">
    <div id="comments" class="row justify-content-center mb-5">
        <div class="col-md-8">
            
            <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mikhailio" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
            
        </div>
    </div>
</div></div>

        </div>
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                &copy; Copyright Mikhail Shilkov - All rights reserved
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                
            </div>
        </div>
    </div>
</footer>











<script src="/js/bundle.min.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-59218480-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    </body>
</html>
